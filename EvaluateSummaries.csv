,Unnamed: 0,Cleaned_Transcript,Summary,Evaluation_Summary
0,0,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation, or to view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: o, Professor Jerison is relaxing in sunny London, Ontario today and sent me in as his substitute again. m glad to the here and see you all again. o our agenda today: he said that hed already talked about power series and Taylors formula, guess on last week right, on Friday? o m going to go a little further with that and show you some examples, show you some applications, and then have this course evaluation survey that ll hand out in the last 10 minutes or so of the class. also have this handout that he made that says 18.01 end of term 2007. f you didnt pick this up coming in, grab it going out. People tend not to pick it up when they walk in, see. o grab this when youre going out. Theres some things missing from it. He has not decided when his office hours will be at the end of term. He will have them, just hasnt decided when. o, check the website for that information. And were looking forward to the final exam, which is uh arent we? Any questions about this technical stuff? All right, lets talk about power series for a little bit. o thought should review for you what the story with power series is. OK, could have your attention please? o, power series is a way of writing a function as a sum of integral powers of x. These a_0, a_1, and so on, are numbers. An example of a power series is a polynomial. Not to be forgotten, one type of power series is one which goes on for a finite number of terms and then ends, so that all of the other, all the higher a_is are all 0. This is a perfectly good example of a power series; its a very special kind of power series. And part of what want to tell you today is that power series behave, almost exactly like, polynomials. Theres just one thing that you have to be careful about when youre using power series that isnt a concern for polynomials, and ll show you what that is in a minute. o, you should think of them as generalized polynomials. The one thing that you have to be careful about is that there is a number o one caution. Theres a number which ll call R, where R can be between 0 and it can also be infinity. ts a number between 0 and infinity, inclusive, so that when the absolute value of x is less than R. o when x is smaller than R in size, the sum converges. This sum that sum converges to a finite value. And when x is bigger than R in absolute value, the sum diverges. This R is called the radius of convergence. o well see some examples of what the radius of convergence is in various powers series as well, and how you find it also. But, let me go on and give you a few more of the properties about power series which think that professor Jerison talked about earlier. o one of them is theres a radius of convergence. Heres another one. f youre inside of the radius convergence, then the function has all its derivatives, has all its derivatives, just like a polynomial does. You can differentiate it over and over again. And in terms of those derivatives, the number a_n in the power series can be expressed in terms of the value of the derivative at 0. And this is called Taylors formula. o m saying that inside of this radius of convergence, the function that were looking at, this f(x), can be written as the value of the function at 0, thats a_0, plus the value of the derivative. This bracket n means you take the derivative n times. o when n is 1, you take the derivative once at 0, divided by 1!, which is !, and multiply it by x. Thats the linear term in the power series. And then the quadratic term is you take the second derivative. Remember to divide by 2!, which is 2. ultiply that by x^2 and so on out. o, in terms o the coefficients in the power series just record the values of the derivatives of the function at x = 0. They can be computed that way also. Lets see. think thats the end of my summary of things that he talked about. think he did one example, and ll repeat that example of a power series. This example wasnt due to David Jerison; it was due to Leonard Euler. ts the example of where the function is the exponential function e^x. o, lets see. Lets compute what will just repeat for you the computation of the power series for e^x, just because its such an important thing to do. o, in order to do that, have to know what the derivative of e^x is, and what the second derivative of e^x is, and so on, because that comes into the Taylor formula for the coefficients. But we know what the derivative of e^x is, its just e^x again, and its that way all the way down. All the derivatives are e^x over and over again. o when evaluate this at x = 0, well, the value of e^x is 1, the value of e^x is 1 at x = 0. You get a value of 1 all the way down. o all these derivatives at 0 have the value 1. And now, when plug into this formula, find e^x is 1 plus 1*x plus 1/2! x^2 plus 1/3! x^3, plus and so on. o all of these numbers are 1, and all you wind up with is the factorials in the denominators. Thats the power series for e^x. This was a discovery of Leonhard Euler in 1740 or something. Yes, aam. ADENE: When youre writing out the power series, how far do you have to write it out? PROFEOR: How far do you have to write the power series before it becomes well defined? Before its a satisfactory solution to an exam problem, suppose, is another way to phrase the question. ntil you can see what the pattern is. can see what the pattern is. s there anyone whos in doubt about what the next term might be? ome people would tell you that you have to write the summation convention thing. Dont believe them. f you right out enough terms to make it clear, thats good enough. OK? s that an answer for you? ADENE: Yes, Thank you. PROFEOR: OK, so thats a basic example. Lets do another basic example of a power series. Oh yes, and by the way, whenever you write out a power series, you should say what the radius of convergence is. And for now, will just to tell you that the radius of convergence of this power series is infinity; that is, this sum always converges for any value of x. ll say a little more about that in a few minutes. Yeah? ADENE: o which functions can be written as power series? PROFEOR: Which functions can be written as power series? Thats an excellent question. Any function that has a reasonable expression can be written as a power series. m not giving you a very good answer because the true answer is a little bit complicated. But any of the functions that occur in calculus like sines, cosines, tangents, they all have power series expansions, OK? Well see more examples. Lets do another example. Heres another example. guess this was example one. o, this example, think, was due to Newton, not Euler. Lets find the power series expansion of this function: 1/(1+x). Well, think that somewhere along the line, you learned about the geometric series which tells you that which tells you what the answer to this is, and ll just write it out. The geometric series tells you that this function can be written as an alternating sum of powers of x. You may wonder where these minuses came from. Well, if you really think about the geometric series, as you probably remembered, there was a minus sign here, and that gets replaced by these minus signs. think maybe Jerison talked about this also. Anyway, heres another basic example. Remember what the graph of this function looks like when x = 1. Then theres a little problem here because the denominator becomes 0, so the graph has a pole there. t goes up to infinity at x = 1, and thats an indication that the radius of convergence is not infinity. Because if you try to converge to this infinite number by putting in x = 1, here, youll have a big problem. n fact, you see when you put in x = 1, you keep getting 1 in every term, and it gets bigger and bigger and does not converge. n this example, the radius of convergence is 1. OK, so, lets do a new example now. Oh, and by the way, should say you can calculate these numbers using Taylors formula. f you havent seen it, check it out. alculate the iterated derivatives of this function and plug in x = 0 and see that you get +1, 1, +1, 1, and so on. Yes sir. ADENE: For the radius of convergence see that if you do 1 itll blow out. f you put in 1 though, it seems like it would be fine. PROFEOR: The questions is can see that theres a problem at x = 1, why is there also a problem at x = 1 where the graph is perfectly smooth and innocuous and finite. Thats another excellent question. The problem is that if you go off to a radius of 1 in any direction and theres a problem, thats it. Thats what the radius of convergence is. Here, what does happen if put an x = +1? o, lets look at the partial sums. Do x = +1 in your mind here. o ll get a partial sum 1, then 0, and then 1, and then 0, and then 1. o even though it doesnt go up to infinity, it still does not converge. ADENE: And anything in between? PROFEOR: Any of these other things will also fail to converge in this example. Well, thats the only two real numbers at the edge. Right? OK, lets do a different example now. How about a trig function? The sine of x. m going to compute the power series expansion for sin(x). and m going to do it using Taylors formula. o Taylors formula says that have to start computing derivatives of sin(x). ounds like its going to be a lot of work. Lets see, the derivative of the sine is the cosine. And the derivative of the cosine, thats the second derivative of the sine, is what? Remember the minus, its sin(x). OK, now want to take the third derivative of the sine, which is the derivative of sine prime prime, so its the derivative of this. And we just decided the derivative of sine is cosine, so get cosine, but have this minus sign in front. And now want to differentiate again, so the cosine becomes a minus sine, and that sign cancels with this minus sign to give me sin(x). You follow that? ts a lot of 1s canceling out there. o, all of a sudden, m right back where started; these two are the same and the pattern will now repeat forever and ever. Higher and higher derivatives of sines are just plus or minus sines and cosines. Now Taylors formula says should now substitute x = 0 into this and see what happens, so lets do that. When x is equals to 0, the sine is 0 and the cosine is 1. The sine is 0, so minus 0 is also 0. The cosine is 1, but now theres a minus one, and now m back where started, and so the pattern will repeat. OK, so the values of the derivatives are all zeros and plus and minus ones and they go through that pattern, fourfold periodicity, over and over again. And so we can write out what sin(x) is using Taylors formula, using this formula. o put in the value at 0 which is 0, then put in the derivative which is 1, multiplied by x. Then, have the second derivative divided by 2!, but the second derivative at 0 is 0. o m going to drop that term out. Now have the third derivative which is 1. And remember the 3! in the denominator. Thats the coefficient of x^3. Whats the fourth derivative? Well, here we are, its on the board, its 0. o drop that term out go up to the fifth term, the fifth power of x. ts derivative is now 1. Weve gone through the pattern, were back at +1 as the value of the iterated derivative, so now get 1/5! x^5. Now, you tell me, have we done enough terms to see what the pattern is? guess the next term will be a 1/7! x^7, and so on. Let me write this out again just so we have it. x^3 / 3! o its x minus x^3 / 3! plus x^5 / 5!. You guessed it, and so on. Thats the power series expansion for the sine of x, OK? And so, the sign alternate, and these denominators get very big, dont they? Exponentials grow very fast. Let me make a remark. R is infinity here. The radius of convergence of this power series again is infinity, and let me just say why. The reason is that the general term is going to be like x^(2n+1) / (2n+1)!. An odd number can write as 2n + 1. And what want to say is that the size of this, what happens to the size of this as n goes to infinity? o lets just think about this. For a fixed x, lets fix the number x. Look at powers of x and think about the size of this expression when n gets to be large. o lets just do that for a second. o, x^(2n+1) / (2n+1)!, can write out like this. ts x / 1 times x / 2 sorry times x / 3, times x / (2n+1). ve multiplied x by itself 2n+1 times in the numerator, and ve multiplied the numbers 1, 2, 3, 4, and so on, by each other in the denominator, and that gives me the factorial. o ve just written this out like this. Now x is fixed, so maybe its a million, OK? ts big, but fixed. What happens to these numbers? Well at first, theyre pretty big. This is 1,000,000 / 2, this is 1,000,000 / 3. But when n gets to be aybe if n is 1,000,000, then this is about 1/2. f n is a billion, then this is about 1/2,000, right? The denominators keep getting bigger and bigger, but the numerators stay the same; theyre always x. o when take the product, if go far enough out, m going to be multiplying, by very, very small numbers and more and more of them. And so no matter what x is, these numbers will converge to 0. Theyll get smaller and smaller as x gets to be bigger. Thats the sign that x is inside of the radius of convergence. This is the sign for you that this series converges for that value of x. And because could do this for any x, this works. This convergence to 0 for any fixed x. Thats what tells you that you can take that the radius of convergence is infinity. Because in the formula, in the fact, in this property that the radius of convergence talks about, if R is equal to infinity, this is no condition on x. Every number is less than infinity in absolute value. o if this convergence to 0 of the general term works for every x, then radius of convergence is infinity. Well that was kind of fast, but think that youve heard something about that earlier as well. Anyway, so weve got the sine function, a new function with its own power series. ts a way of computing sin(x). f you take enough terms youll get a good evaluation of sin(x). for any x. This tells you a lot about the function sin(x) but not everything at all. For example, from this formula, its very hard to see that the sine of x is periodic. ts not obvious at all. omewhere hidden away in this expression is the number pi, the half of the period. But thats not clear from the power series at all. o the power series are very good for some things, but they hide other properties of functions. Well, so want to spend a few minutes telling you about what you can do with a power series, once you have one, to get new power series, so new power series from old. And this is also called operations on power series. o what are the things that we can do to a power series? Well one of the things you can do is multiply. o, for example, what if want to compute a power series for x sin(x)? Well have a power series for sin(x), just did it. How about a power series for x? Actually, did that here too. The function x is a very simple polynomial. ts a polynomial where thats 0, a_1 is 1, and all the other coefficients are 0. o x itself is a power series, a very simple one. sin(x) is a powers series. And what want to encourage you to do is treat power series just like polynomials and multiply them together. Well see other operations too. o, to compute the power series for x sin(x), of just take this one and multiply it by x. o lets see if can do that right. t distributes through: x^2 minus x^4 / 3! plus x^6 / 5!, and so on. And again, the radius of convergence is going to be the smaller of the two radii of convergence here. o its R equals infinity in this case. OK, you can multiply power series together. t can be a pain if the power series are very long, but if one of them is x, its pretty simple. OK, thats one thing can do. Notice something by the way. You know that even and odd functions? o, sine is an odd function, x is an odd function, the product of two odd functions is an even function. And thats reflected in the fact that all the powers that occur in the power series are even. For an odd function, like the sine, all the powers that occur are odd powers of x. Thats always true. OK, we can multiply. can also differentiate. o lets just do a case of that, and use the process of differentiation to find out what the power series for cos(x) is by writing the cos(x) as the derivative of the sine and differentiating term by term. o, ll take this expression for the power series of the sine and differentiate it term by term, and ll get the power series for cosine. o, lets see. The derivative of x is one. Now, the derivative of x^3 is 3x^2, and then theres a 3! in the denominator. And the derivative of x^5 5x^4, and theres a 5! in the denominator, and so on and so on. And now some cancellation happens. o this is 1 minus, well, the 3 cancels with the last factor in this 3 factorial and leaves you with 2!. And the 5 cancels with the last factor in the 5 factorial and leaves you with a 4! in the denominator. And so there you go, theres the power series expansion for the cosine. ts got all even powers of x. They alternate, and you have factorials in the denominator. And of course, you could derive that expression by using Taylors formula, by the same kind of calculation you did here, taking higher and higher derivatives of the cosine. You get the same periodic pattern of derivatives and values of derivatives at x = 0. But heres a cleaner way to do it, simpler way to do it, because we already knew the derivative of the sine. When you differentiate, you keep the same radius of convergence. OK, so we can multiply, can add too and multiply by a constant, things like that. How about integrating? Thats what half of this course was about isnt it? o, lets integrate something. o, the integration m going to do is this one: the integral from 0 to x of dt / (1+x). What is that integral as a function? o, when find the antiderivative of this, get ln(1+t), and then when evaluate that at t = x, get ln(1+x). And when evaluate the natural log at 0, get the ln 1, which is 0, so this is what you get, OK? This is really valid, by the way, for x bigger than 1. But you dont want to think about this quite like this when x is smaller than that. Now, m going to try to apply power series methods here and find use this integral to find a power series for the natural log, and ll do it by plugging into this expression what the power series for 1/(1+t) was. And know what that is because wrote it down on the board up here. hange the variable from x to t there, and so 1/(1+t) is 1 minus t plus t^2 minus t^3, and so on. o thats the thing in the inside of the integral, and now its legal to integrate that term by term, so lets do that. m going to get something which will then evaluate at x and at 0. o, when integrate 1 get x, and when integrate t, get t. m sorry. When integrate t, get t^2 / 2, and t^2 gives me t^3 / 3, and so on and so on. And then, when put in t = x, well, just replace all the ts by xs, and when put in t = 0, get 0. o this equals x. o, ve discovered that ln(1+x) is x minus x^2 / 2 plus x^3 / 3 minus x^4 / 4, and so on and so on. Theres the power series expansion for ln(1+x). And because began with a power series whose radius of convergence was just 1, began with this power series, the radius of convergence of this is also going to be 1. Also, because this function, as just pointed out, this function goes bad when x becomes less than 1, so some problem happens, and thats reflected in the radius of convergence. ool. o, you can integrate. That is the correct power series expansion for the ln(1+x), and another victory of Eulers was to use this kind of power series expansion to calculate natural logarithms in a much more efficient way than people had done before. OK, one more property, think. What are we at here, 3? 4. ubstitute. Very appropriate for me as a substitute teacher to tell you about substitution. o m going to try to find the power series expansion of e^(t^2). OK? And the way ll do that is by taking the power series expansion for e^x, which we have up there, and make the substitution x = t^2 in the expansion for e^x. Did you have a question? ADENE: Well, its just concerning the radius of convergence. You cant define x so that is always positive, and if so, it wouldnt have a radius of convergence, right? PROFEOR: Like say, again the worry is this ln(1+x) function is perfectly well behaved for large x. Why does the power series fail to converge for large x? Well suppose that x is bigger than 1, then here you get bigger and bigger powers of x, which will grow to infinity, and they grow large faster than the numbers 2, 3, 4, 5, 6. They grow exponentially, and these just grow linearly. o, again, the general term, when x is bigger than one, the general term will go off to infinity, even though the function that youre talking about, log of net of 1 plus x is perfectly good. o the power series is not good outside of the radius of convergence. ts just a fact of life. Yes? ADENE: PROFEOR: d rather talk to me after class. The question is why is it the smaller of the two radii of convergence? The basic answer is, well, you cant expect it to be bigger than that smaller one, because the power series only gives you information inside of that range about the function, so. ADENE: PROFEOR: Well, in this case, both of the radii of convergence are infinity. x has radius of convergence infinity for sure, and sin(x) does too. o you get infinity in that case, OK? OK, lets just do this, and then m going to integrate this and thatll be the end of what have time for today. o whats the power series expansion for this? The power series expansion of this is going to be a function of t, right, because the variable here is t. get it by taking my expansion for e^x and putting in what x is in terms of t. Whoops! And so on and so on. just put in t^2 in place of x there in the series expansion for e^x. can work this out a little bit better. t^2 is what it is. This is going to give me a t^4 and the minus squared is going to give me a plus, so get t^4 / 2!. Then get (t)^3, so therell be a minus sign and a t^6 and the denominator 3!. o the signs are going to alternate, the powers are all even, and the denominators are these factorials. everal times as this course has gone on, the error function has made an appearance. The error function was, guess it gets normalized by putting a 2 over the square root of pi in front, and its the integral of e^(t^2) dt from 0 to x. And this normalization is here because as x gets to be large the value becomes 1. o this error function is very important in the theory of probability. And think you calculated this fact at some point in the course. o the standard definition of the error function, you put a 2 over the square root of pi in front. Lets calculate its power series expansion. o theres a 2 over the square root of pi that hurts nobody here in the front. And now want to integrate e^(t^2), and m going to use this power series expansion for that to see what you get. o m just going to write this out think. did it out carefully in another example over there, so ll do it a little quicker now. ntegrate this term by term, youre just integrating powers of t so its pretty simple, so get and then m evaluating at x and then at 0. o get x minus x^3 / 3, plus x^5 / (5*2!), 5 from integrating the t^4, and the 2! from this denominator that we already had. And then theres a x^7 / (7*3!), and plus, and so on, and you can imagine how they go on from there. guess to get this exactly in the form that we began talking about, should multiply through. o the coefficient of x is 2 over the square root of pi, and the coefficient of x^3 is 2 over 3 times the square root of pi, and so on. But this is a perfectly good way to write this power series expansion as well. And, this is a very good way to compute the value of the error function. ts a new function in our experience. Your calculator probably calculates it, and your calculator probably does it by this method. OK, so thats my sermon on examples of things you can do with power series. o, were going to do the EG thing in just a minute. Professor Jerison wanted me to make an ad for 18.02. Just in case you were thinking of not taking it next term, you really should take it. t will put a lot of things in this course into context, for one thing. ts about vector calculus and so on. o youll learn about vectors and things like that. But it comes back and explains some things in this course that might have been a little bit strange, like these strange formulas for the product rule and the quotient rule and the sort of random formulas. Well, one of the things you learn in 18.02 is that theyre all special cases of the chain rule. And just to drive that point home, he wanted me to show you this poem of his that really drives the points home forcefully, think.","And for now, will just to tell you that the radius of convergence of this power series is infinity; that is, this sum always converges for any value of x. ll say a little more about that in a few minutes. o, in order to do that, have to know what the derivative of e^x is, and what the second derivative of e^x is, and so on, because that comes into the Taylor formula for the coefficients. The power series expansion of this is going to be a function of t, right, because the variable here is t. get it by taking my expansion for e^x and putting in what x is in terms of t. Whoops! o, to compute the power series for x sin(x), of just take this one and multiply it by x. o lets see if can do that right. o m saying that inside of this radius of convergence, the function that were looking at, this f(x), can be written as the value of the function at 0, thats a_0, plus the value of the derivative. And the way ll do that is by taking the power series expansion for e^x, which we have up there, and make the substitution x = t^2 in the expansion for e^x. o lets just do a case of that, and use the process of differentiation to find out what the power series for cos(x) is by writing the cos(x) as the derivative of the sine and differentiating term by term. This convergence to 0 for any fixed x. Thats what tells you that you can take that the radius of convergence is infinity. And now want to integrate e^(t^2), and m going to use this power series expansion for that to see what you get. And the derivative of the cosine, thats the second derivative of the sine, is what? Oh yes, and by the way, whenever you write out a power series, you should say what the radius of convergence is. Thats the power series expansion for the sine of x, OK? o, ll take this expression for the power series of the sine and differentiate it term by term, and ll get the power series for cosine. o when n is 1, you take the derivative once at 0, divided by 1!, which is !, and multiply it by x. Thats the linear term in the power series. o thats the thing in the inside of the integral, and now its legal to integrate that term by term, so lets do that. And again, the radius of convergence is going to be the smaller of the two radii of convergence here. The error function was, guess it gets normalized by putting a 2 over the square root of pi in front, and its the integral of e^(t^2) dt from 0 to x. And this normalization is here because as x gets to be large the value becomes 1. o this error function is very important in the theory of probability. o, in terms o the coefficients in the power series just record the values of the derivatives of the function at x = 0. alculate the iterated derivatives of this function and plug in x = 0 and see that you get +1, 1, +1, 1, and so on. Lets see, the derivative of the sine is the cosine. n this example, the radius of convergence is 1. This is the sign for you that this series converges for that value of x. And because could do this for any x, this works. And in terms of those derivatives, the number a_n in the power series can be expressed in terms of the value of the derivative at 0. o well see some examples of what the radius of convergence is in various powers series as well, and how you find it also. And so there you go, theres the power series expansion for the cosine. OK, lets just do this, and then m going to integrate this and thatll be the end of what have time for today. The basic answer is, well, you cant expect it to be bigger than that smaller one, because the power series only gives you information inside of that range about the function, so. The radius of convergence of this power series again is infinity, and let me just say why. And then, when put in t = x, well, just replace all the ts by xs, and when put in t = 0, get 0. o this equals x. o, ve discovered that ln(1+x) is x minus x^2 / 2 plus x^3 / 3 minus x^4 / 4, and so on and so on. Now, m going to try to apply power series methods here and find use this integral to find a power series for the natural log, and ll do it by plugging into this expression what the power series for 1/(1+t) was. Thats the sign that x is inside of the radius of convergence. And when evaluate the natural log at 0, get the ln 1, which is 0, so this is what you get, OK? Because in the formula, in the fact, in this property that the radius of convergence talks about, if R is equal to infinity, this is no condition on x. Every number is less than infinity in absolute value. The sine of x. m going to compute the power series expansion for sin(x). Well, think that somewhere along the line, you learned about the geometric series which tells you that which tells you what the answer to this is, and ll just write it out. And what want to say is that the size of this, what happens to the size of this as n goes to infinity? And because began with a power series whose radius of convergence was just 1, began with this power series, the radius of convergence of this is also going to be 1. And, this is a very good way to compute the value of the error function. Not to be forgotten, one type of power series is one which goes on for a finite number of terms and then ends, so that all of the other, all the higher a_is are all 0. And thats reflected in the fact that all the powers that occur in the power series are even. Thats what the radius of convergence is. o what are the things that we can do to a power series? o the power series is not good outside of the radius of convergence. Theres just one thing that you have to be careful about when youre using power series that isnt a concern for polynomials, and ll show you what that is in a minute. When x is equals to 0, the sine is 0 and the cosine is 1. Lets find the power series expansion of this function: 1/(1+x). Well one of the things you can do is multiply. o when evaluate this at x = 0, well, the value of e^x is 1, the value of e^x is 1 at x = 0. o put in the value at 0 which is 0, then put in the derivative which is 1, multiplied by x. Then, have the second derivative divided by 2!, but the second derivative at 0 is 0. o m going to drop that term out. OK, now want to take the third derivative of the sine, which is the derivative of sine prime prime, so its the derivative of this. Lets compute what will just repeat for you the computation of the power series for e^x, just because its such an important thing to do. But we know what the derivative of e^x is, its just e^x again, and its that way all the way down. The derivative of x is one. Thats the power series for e^x. For example, from this formula, its very hard to see that the sine of x is periodic. For a fixed x, lets fix the number x. Look at powers of x and think about the size of this expression when n gets to be large. The question is why is it the smaller of the two radii of convergence? And we just decided the derivative of sine is cosine, so get cosine, but have this minus sign in front. That is the correct power series expansion for the ln(1+x), and another victory of Eulers was to use this kind of power series expansion to calculate natural logarithms in a much more efficient way than people had done before. o, the integration m going to do is this one: the integral from 0 to x of dt / (1+x). in the denominator, and so on and so on.",0.1991507430997877
1,1,"n this sequence of segments, we review some mathematical background that will be useful at various places in this course. ost of what is covered, with the exception of the last segment, is material that you may have seen before. But this could still be an opportunity to refresh some of these concepts. should say that this is intended to be just a refresher. Our coverage is not going to be complete in any sense. What we will talk about is sets, various definitions related to sets, and some basic properties, including De organs laws. We will talk about what a sequence is and what it means for a sequence to converge to something. We will talk about infinite series. And as an example, we will look at the geometric series. Then we will talk about some subtleties that arise when you have sums of terms that are indexed with multiple indices. And finally, probably the most sophisticated part, will be a discussion of countable versus uncountable sets. ountable sets are like the integers. ncountable sets are like the real line. And theyre fundamentally different. And this fundamental difference reflects itself into fundamentally different probabilistic models models that involve discrete experiments and outcomes versus models that involve continuous outcomes.","What we will talk about is sets, various definitions related to sets, and some basic properties, including De organs laws. We will talk about what a sequence is and what it means for a sequence to converge to something. n this sequence of segments, we review some mathematical background that will be useful at various places in this course. Then we will talk about some subtleties that arise when you have sums of terms that are indexed with multiple indices.",0.3404255319148936
2,2,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: All right everyone. Lets get started. o todays lecture and Wednesdays lecture, were going to talk about this thing called object oriented programming. And if you havent programmed before, think this is a fairly tough concept to grasp. But hopefully with many, many examples and just by looking at the code available from lectures, youll hopefully get the hang of it quickly. o lets talk a little bit about objects. And weve seen objects in Python so far. Objects are basically data in Python. o every object that weve seen has a certain type. OK, that we know. Behind the scenes, though, every object has these two additional things. One is some data representation. o how Python represents the object just behind the scenes and what are different ways that you can interact with the object. o for example, every one of these is a different object. For example, this is the number 1,234. ts a specific object that is of type integer. The number 5 is a different object thats of type integer and so on. Weve seen floats. Weve seen strings. Weve seen lists. Lists and dictionaries are more complicated objects. Object types. orry. But every object has a type, some sort of way that its represented in Python and some ways that we can interact with them. OK. o the idea behind object oriented programming is, first of all, everything in Python is an object. Weve said that before and in this lecture think well really get at what that means. o weve seen strings, integers, dictionaries, lists. Those are all objects. When we did functions, we saw that we could pass as a parameter another function. o functions were also objects in Python. o literally everything in Python is an object. o what are the kinds of things we can do with objects? Well, once you have a type, you can create a new object that is of some type. And you can create as many objects as youd like of that particular type, right? An integer 5 and integer 7. Those all work in a program. Once youve created these new objects, you can manipulate them. o for a list, for example, you can append an item to the end of the list, you can delete an item, remove it, concatenate two lists together. o thats ways that you can interact with objects. And the last thing you can do is you can destroy them. o and with lists, we saw explicitly that you can delete elements from a list, or you can just forget about them by reassigning a variable to another value, and then at some point, Python will collect all of these dead objects and reclaim the memory. o lets continue exploring what objects are. o lets say have these two separate objects. One is a blue car. One is a pink car. o objects are really data abstractions. o these two cars can be created by the same blueprint. OK? This is a blueprint for a car and if an object is a data abstraction, theres two things that this abstraction is going to capture. The first is some sort of representation. What is going to represent the car, what data represents a car object? And the second is what are ways that we can interact with the object? o if we think about a car blueprint, some general representation for a car could be the number of wheels it has, the number of doors it has, maybe its length, maybe its height, so this is all part of what data represents the car. OK? The interface for the car is what are ways that you can interact with it. o for example, you could paint a car, right? o you could change its color. You could have the car make a noise and different cars might make different noises. Or you can drive the car, right? o these are all ways that you can interact with the car. Whereas the representation are what makes up the car. What data abstractions make up the car. Lets bring it a little closer to home by looking at a list. o we have this data type of list, right? Weve worked with lists before. The list with elements 1, 2, 3, and 4 is a very specific object that is of type list. Again, we think about it in terms of two things. One is what is the data representation of the list? o behind the scenes how does Python see lists? And the second is, how do you interact with lists? o what are ways that you can manipulate a list object once its created? o behind the scenes you have a list, L, which is going to be made up of essentially two things. One is going to be the value at specific index. OK? o at index 0, it has the value 1, right, because its the first element in the list. And the second thing that represents a list is going to be this second part, which is a pointer. And internally this pointer is going to tell Python where is the memory location in the computer where you can access the element index 1. o its just essentially going to be a chain, going from one index to the other. And at the next memory location you have the value at index 1, and then you have another pointer that takes you to the location in memory where the index 2 is located. And in index 2 you have the value and then the next pointer, and so on and so on. o this is how Python internally represents a list. OK? How you manipulate lists, weve done this a lot, right? You can index into a list, you can add two lists together, you can get the length, you can append to the end of a list, you can sort a list, reverse a list, and so many other things, right? o these are all ways that you can interact with the list object as soon as youve created it. o notice both of these, the internal representation and how you manipulate lists, you dont actually know internally how these are represented, right? How did whoever wrote the list class decide to implement a sort. We dont know. You also werent aware of how these lists were represented internally. And you didnt need to know that. Thats the beauty of object oriented programming and having these data abstractions. The representations are private of these objects and they are only known by what you can find out how its done, but they only should be known by whoever implemented them. You, as someone who uses this class, doesnt really need to know how a list is represented internally in order to be able to use it and to write cool programs with them. OK? o just find a motivation here before we start writing our own types of objects is the advantages of object oriented programming is really that youre able to bundle this data, bundle some internal representation, and some ways to interact with a program into these packages. And with these packages, you can create objects and all of these objects are going to behave the exact same way. Theyre going to have the same internal representation and the same way that you can interact with them. And ultimately, this is going to contribute to the decomposition and abstraction ideas that we talked about when we talked about functions. And that means that youre going to be able to write code thats a lot more reusable and a lot easier to read in the future. OK. o just like when we talked about functions, were going to sort of separate the code that we talk about today into code where you implement a data type and code where you use an object that you create. OK? o remember when we talked about functions, you were thinking about it in terms of writing a function, so you had to worry about the details of how you implement a function. And then you had to worry about just how to use a function, right? o its sort of the same idea today. o when youre thinking about implementing your own data type, you do that with this thing called a class. And when you create a class, youre basically going to figure out what name you want to give your class and youre going to find some attributes. And attributes are going to be the data representation and ways that you can interact with your object. o you, as the programmer of this class, are going to decide how you want people to interact with the object and what data this object is going to have. o for example, someone wrote code that implements a list class, right, and we dont actually know how that was done. But we can find out. o creating the class is implementing the class and figuring out data representation and ways to interact with the class. Once thats done, you can then use your class. And you use the class by creating new instances of the class. o when you create a new instance, you essentially create a new object that has the type, the name of your class. And you can create as many objects as youd like. You can do all the operations that youve defined on the class. o for example, someone wrote the code to implement list class and then you can just use the list class like this. You can create a new list, you can get the length pf the list, you can append to the end of the list, and so on and so on. o lets start defining our own types, OK? o now youre going to define classes, youre going to write classes which are going to define your own types of objects. o for todays lecture were going to look at code thats going to be in the context of a coordinate object. And a coordinate object is essentially going to be an object thats going to define a point in an xy plane. o x, y is going to be a coordinate in a 2D plane. o were going to write code thats going to allow us to define that kind of object. o the way we do that is we have to define a class. o we have to tell Python, hey, m defining my own object type. o you do that with this class key word. o you say class, then you say the name of your type. n this case, were creating a type called coordinate. Just like we had type list, type string, and so on. This is going to be a type called coordinate. And then in parentheses here, you put what the parents of the class are. For todays lecture, the parent of the classes are going to be this thing called object, and object is the very basic type in Python. t is the most basic type in Python. And it implements things like being able to assign variables. o really, really basic operations that you can do with objects. o your coordinate is therefore going to be an object in Python. All right. o weve told Python we wanted to define an object. o inside the class definition were going to put attributes. o what are attributes? Attributes are going to be data and procedures that belong to the class, OK? Data are going to be the data representations and procedures are going to be ways that we can interact with the object. The fact that they belong to the class means that the data and the procedures that we write are only going to work with an object of this type. OK. f you try to use any of the data or the procedures with an object of a different type, youre going to get an error because these data and these attributes will belong to this particular class. o the data attributes is, what is the object, right? What is the data that makes up the object? o for our coordinate example, its going to be the x and y values for coordinate. We can decide that can be ints, we can decide that we can let them be floats, but its going to have one value for the xcoordinate and one value for the ycoordinate. o those are data attributes. And procedure attributes are better known as methods. And you can think of a method as a function. Except that its a function that only works with this particular type of object. o with a coordinate object, in this case. o the methods are going to define how you can interact with the object. o in a list, for example, weve said that you can append an item to the end of the list, we can sort a list, things like that. o when youre defining methods, youre defining ways that people can interact with your object. o for example, for a coordinate object, we can say that we can take the distance between two coordinate points. OK? And thats going to be a way that you can interact with two coordinate points. And just to be clear, these are going to belong to this class, which means that if you try to use this distance method on two lists, for example, youre going to get an error. Because this distance method was only defined to work with two coordinate type objects. All right, so lets carry on and continue implementing our class. o weve written this first line so far, class coordinate object. o now lets define attributes. First thing were going to define are data attributes. Generally you define data attributes inside this init, and this is underscore, underscore, init, underscore, underscore, and its a special method or function in a class. And the special method tells Python, when you implement the special method, it tells Python when you first create an object of this type, call this method or call this function. o how do we do that? o lets implement it. o we say df because its just a function. The name is the special name, init. And we give it some parameters, right, just like any other function. These last two parameters are x and y, which are going to represent how you create a coordinate object. o you give it a value for the xcoordinate and you give it a value for the ycoordinate. The self, however, is a little bit trickier. o the self is going to be a parameter when you define this class that represents a particular instance of the class. o were defining this coordinate object in sort of a general way, right? We dont have a specific instance yet because we havent created an object yet. But this self is going to be sort of a placeholder for any sort of instance when you create the object. o in the definition of the class, whenever you want to refer to attributes that belong to an instance, you have to use self dot. o this dot notation. And the dot is going to say look for a data attribute x that belongs to this class. o for methods that belong to the class, the first parameter is always going to be self. t can be named anything you want, but really by convention its always named self. o try to stick to that. And then any other parameters beyond it are going to be just parameters as you would put in a normal function. OK. n this particular case, were going to choose to initialize a coordinate object by two values, one for the x and one for the y. And inside this init method, were going to have two assignments. The first one says, the x data attribute of a coordinate object. m going to assign it to whatever was passed in. And the y data attribute for a particular object is going to be assigned whatever y was passed in. Questions so far about how to write this init? Yeah, question. ADENE: PROFEOR: How do you make sure that x and y are inits or floats? o this is something that you could write in the specifications, so the docstring with the triple quotes. o whoever uses the class would then know that if they do something outside the specification, the code might not work as expected. Or you could put in a cert statement inside the definition of the init just to sort of force that. Force that to be true. Great question. Yeah, question. ADENE: PROFEOR: Does the x, does this self x and this x have to be the same name. The answer is no. And were going to see in class exercise that you can have it be different. OK. Great. o this defines the way that we create an object. o now we have sort of a nice class. ts very simple, but we can start actually creating coordinate objects. o when you create coordinate objects, youre creating instances of the class. o this line here, is equal to coordinate 3,4, is going to call the init method. ts going to call the init method with x is equal to 3 and y is equal to 4. m just going to go over here and wrote this previously, because notice when were creating an object here, were only giving it two parameters. But in the init method, we have actually three parameters, right? We have these three parameters here, but when were creating an object, we only give it two parameters. And thats OK because implicitly, Python is going to say self is going to be this object , so just by default, OK? o when youre creating a coordinate object, youre passing it all the variables except for self. o this line here is going to call the init and its going to do every line inside the init. o its going to create an x data attribute for , a y data attribute for , and its going to assign 3 and 4 to those respectively. This next line here is origin equals coordinate 0, 0 creates another object. OK? ts another coordinate object whose value for x is 0 and whose value for y is 0. o now we have two coordinate objects. We can access the data attributes using this dot notation and weve seen that before, right? When weve worked with lists wed say something like, L dot append, right, when we create a list. o the same dot notation can be used with your own objects in order to access data attributes. o here, this is going to print 3 because the x value for object is 3, and the next line, print origin x is going to print 0 because the x value for the object origin is 0. OK. o weve created a coordinate object. We have to find the init method so we have a way to create objects when we use the class. And then we can access the data attributes. But thats kind of lame, right, because there isnt anything cool we can do with it. There isnt ways to interact with this object. o lets add some methods. Remember methods are going to be procedural attributes that allow us to interact with our object. ethods are like functions except that theres a couple of differences which youll see in a moment. And when youre calling methods, youre using the dot operator, like L dot append, for example, for lists. o lets go back to defining our coordinate class and lets define a method for it. o so far weve defined that part there, class coordinate and an init. o we have that. o in this slide were going to add this method here. o this method here is going to say m going to define a method called distance and m going to pass in two parameters. Remember self, the first parameter, is always going to be the instance of an object that youre going to perform the operation on. o pretty much by convention its always named self. And then for this particular method, m going to give it another parameter, and can name this whatever want. m naming it other. And this is going to represent the other coordinate object for which want to find the distance from my self. o here m going to just implement the Euclidean distance formula, which is x1 minus x2 squared, plus Y1 minus Y2 squared, and square root of all that. o thats what m doing inside here. elf and other are coordinate objects. nside this method, have to refer to the x data attributes of each object if want to find the difference between the 2x values from them. o thats why m doing self dot x here, right. f just did x, would be accessing just some variable named x in a program which actually isnt even defined. o you always have to refer when as were thinking about classes, you always have to refer to whose data attribute do you want to access? n this case, want to access the x data attribute of my self, and want to subtract the x data attribute of this other coordinate, square that, same for y, square that, and then add those and take the square root of that. o notice this method is pretty much like a function, right? You have DF, some name, it takes in parameters. t does some stuff and then it returns a value. The only difference is the fact that you have a self here as the first thing and the fact that you always have to be conscious about whose data attributes youre accessing. o you have to use the dot notation in order to decide whose data attributes you want access. o weve defined the method here, distance. o this is in the class definition. Now how do we use it? o lets assume that the definition of distance is up here. didnt include the code. But really all you need to know is what it takes. t takes a self and an other. o when you want to use this method to figure out a distance between two coordinate objects, this is how you do it. o the first line, create one coordinate object. econd line, create another coordinate object. First one is named , the second one is named 0. These are two separate objects. And m going to find the distance. And want to first call it on one object, so m going to say dot, so m using the dot notation to call the method distance on object . o Python says this object is of type coordinate. ts going to look up at the class coordinate that you defined. ts going to find this method called distance and then its going to say what parameters does it take? o it takes another parameter, right, for the other and then, in the parentheses, just have to give it this other perimeter. An easier way to see what happens is by looking at what this line here is equivalent to. o the third line here prints dot distance 0 is equivalent to this one on the right. And this one on the right essentially says, whats the name of the class, dot, dot notation, whats the method you want to call, and then in parentheses you give it all of the variables including self. OK. o in this case youre explicitly telling Python that self is and other is 0. o this is a little bit easier to understand, like that. But its a little cumbersome because you always have to write coordinate dot, coordinate dot, coordinate dot, for every data attribute you might want to access, for every procedural attribute you might want to access. o by convention, its a lot easier to do the one on the left. And as mentioned, Python implicitly says, if youre doing the one on the left, you can call this method on a particular object and its going to look up the type of the object and its going to essentially convert this on the left to the one on the right. And this is what youve been using so far. o when you create a list, you say L is equal to 1, 2, and then you say L.append, you know, 3 or whatever. o weve been using this notation on the left pretty much from the beginning of class. o we have a coordinate class, we can create a coordinate object, we can get the distance between two objects. As youre using the class, if you wanted to use this coordinate class, and you were maybe debugging at some point, a lot of you probably use print as a debug statement, right? And maybe you want to print the value of a coordinate object. o if you create a coordinate object, is equal to coordinate 3, 4, right? Thats what weve done so far. f you print , you get this funny message. Very uninformative, right? t basically says, well, is an object of type coordinate at this memory location in the computer. Which is not what you wanted at all, right? aybe you wanted to know what the values for x and y were. That would be a lot more informative. o by default, when you create your own type, when you print the object of that type, Python tells you this sort of information which is not what you want. o what you need to do is you need to define your own method that tells Python what to do when you call print on an object of this type. o this is going to be a special method, just like init is, because it starts and ends with double underscores. And the name of the method is underscore, underscore, str, underscore, underscore. And if you define this method in your class, that tells Python, hey, when you see a print statement thats on an object of type coordinate, call this method, look what it does, and do everything thats inside it. And you can choose to make it do whatever you want inside your definition of str. n this case, lets say when we print a coordinate object, were going to print its x and y values surrounded by angle brackets. That seems reasonable, right? o then from now on when you print coordinate objects, youre going to see things like this, which is a lot more informative. o how do we define this? o so far weve defined all that and the last part is going to be new. o we define the init and the distance, and lets define this str. o underscore, underscore, str, underscore, underscore, is a method. ts only going to take self because youre just calling print on the object itself. Theres no other parameters to it. tr has to return a string, and in this particular case, were going to return the string thats the angle brackets concatenated with the x value of the object, self.x, concatenated with a comma, concatenated with the y value of this particular instance of an object, self.y, and then concatenated with the angle brackets. o now any time you have print on an object of type coordinate, youre going to call this special method str, if its implemented in your code. Any questions? OK. o lets try to wrap our head around types and classes because weve seen a lot today. Lets create a coordinate object, assign it 3, 4, as we have been, and assign it to variable . Weve implemented the str method, so when we print , its going to print out this nice three comma for our angle brackets. f we print the type of , this is actually going to give us class main coordinate, which tells us that is going to be an object that is of type class coordinate. f we look at coordinate as a class, if we print what coordinate is, coordinate is a class, right? o this is what Python tells us, if we print coordinate, its a class named coordinate. And if we print the type of a coordinate, well thats just going to be a type. o class is going to be a type. o youre defining the type of an object. f youd like to figure out whether a particular object is an instance of a particular class, you use this special function called is instance. o if you print is instance comma coordinate, this is going to print true because is an object that is of type coordinate. ouple more words on these special operators. o these special operators allow you to customize your classes which can add some cool functionality to them. o these special operators are going to be things like addition, subtraction, using the equal equal sign, greater than, less than, length and so on and so on. o just like str, if you implement any of these in your classes, this is going to tell Python. o for example, if weve implemented this underscore, underscore, add, underscore, underscore in our class, this is going to tell Python when you use this plus operator between two objects of type coordinate to call this method. f you have not implemented this method and you try to add two objects of type coordinate, youre going to get an error because Python doesnt actually know right off the bat how to add two coordinate objects, right? You have to tell it how to do that. And you tell it how to do that by implementing this special method. ame with subtract. ame with equals. o if you want to figure out whether two objects are equal. And when you implement these methods in your own class, you can decide exactly what you want to do. o what happens when you add two coordinate objects? Do you just add the x values, do you just add the y values, do you get them both together, do you do whatever youd like to do. And then you document what youve decided. o lets create a fraction object. o weve looked at coordinate, we saw sort of a higher level car object. Lets look at a fraction object. Fraction object is going to be, is going represent a number thats going to be a numerator slash denominator. OK. o thats going to be a fraction object. o the way ve decided to internally represent a fraction object is with two numbers. And ve decided that will not let them be floats. They have to be integers, hence the assert over here. o inside the init, ve decided m going to represent my fracture with two numbers, one for the numerator and one for the denominator. o when create a fraction object, m going to pass in a numerator and a denominator. And a particular instance is going to have self dot numerator and self dot denominator as its data attributes and m assigning those to be whatevers passed into my init. ince plan on debugging this code maybe possibly sometime in the future, m also including an str method and the str method is going to print a nice looking string thats going to represent the numerator, and then a slash, and then the denominator. And then ve also implemented some other special methods. How do add two fractions? How do subtract two fractions? And how do convert a fraction to a float? The add and subtract are almost the same, so lets look at the add for the moment. How do we add two fractions? Were going to take self, which is the instance of an object that want to do the add operation on, and were going to take other, which is the other instance of an object that want to do the operation on, so the addition, and m going to figure out the new top. o the new top of the resulting fraction. o its my numerator multiplied by the other denominator plus my denominator multiplied by the other numerator and then divided by the multiplication of the two denominators. o the top is going to be that, the bottom is going to be that. Notice that were using self dot, right? Once again, were trying to access the data attributes of each different instance, right, of myself and the other object that m working with. o thats why have to use self dot here. Once figure out the top and the bottom of the addition, m going to return, and here notice m returning a fraction object. ts not a number, its not a float, its not an integer. ts a new object that is of the exact same type as the class that m implementing. o as its the same type of object, then on the return value can do all of the exact same operations that can do on a regular fraction object. ub is going to be the same. m returning a fraction object. Float is just going to do the division for me, so its going to take the numerator and then divide it by the denominator, just divide the numbers. And then m defining here my own method called inverse. And this is just going to take the inverse of the instance m calling this method on. And so its going to also return a new fraction object that just has the denominator as the top part and the numerator as the bottom part. o then we have some code here. o thats how implement my fraction object. o now lets use it and see what it gives us. A is equal to a fraction 1, 4. This is going to be 1 over 4 for a. And b is going to be 3 over four. When do , notice m using the plus operator between two fraction objects, right? A and b are fraction objects so Pythons going to say, OK, is there an underscore, underscore, add, underscore, underscore, method implemented? t is and its just going to do whatevers inside here. o its going to say self dot numerator plus other dot denominator. ts going to calculate the top and the bottom. ts going to turn a new fraction object. o this is going to be 4 plus 12 divided by 16, and 16 over 16. o as a fraction object is going to be 16 for the numerator and 16 for the denominator because its a fraction object. f print , it should print 16 over 16, so we can even run it, so print 16 over 16. f print floats , so this special method float here is going to say, is there a method that converts a fraction to a float and there is. ts this one implemented right here. o its just going to divide the two numbers, top and bottom, which gives me 1. o its this one here and here. Notice m doing the exact same method call, except m doing it the other way where you type in the name of the class, name of the method, and then what youre calling it on, and this gives the exact same value here, 1.0. And then here m calling the method inverse on object B which is going to invert 3 over 4 to be 4 over 3. And then m converting it to a float and then m printing the value. o it gives me 1.33. o take a look at this code in more detail and see if you can trace through all of those different things and see if you can also write your own new fraction objects. OK. o last slide. Power of object oriented programming is that you can bundle together objects that are of the exact same type. And all of these objects are going to have the same data representation and the same methods that you can do on them. And ultimately, youre going to be building these layers of abstraction. o youre going to be building on a basic object type in Python, youre going to have integer objects, float objects. On top of those, you can create lists, dictionaries. And on top of those, you can even create your own object types as we saw in this lecture today.","o you, as the programmer of this class, are going to decide how you want people to interact with the object and what data this object is going to have. And attributes are going to be the data representation and ways that you can interact with your object. Were going to take self, which is the instance of an object that want to do the add operation on, and were going to take other, which is the other instance of an object that want to do the operation on, so the addition, and m going to figure out the new top. And as mentioned, Python implicitly says, if youre doing the one on the left, you can call this method on a particular object and its going to look up the type of the object and its going to essentially convert this on the left to the one on the right. And all of these objects are going to have the same data representation and the same methods that you can do on them. The fact that they belong to the class means that the data and the procedures that we write are only going to work with an object of this type. Data are going to be the data representations and procedures are going to be ways that we can interact with the object. f we print the type of , this is actually going to give us class main coordinate, which tells us that is going to be an object that is of type class coordinate. Attributes are going to be data and procedures that belong to the class, OK? And thats OK because implicitly, Python is going to say self is going to be this object , so just by default, OK? o the methods are going to define how you can interact with the object. And the second is what are ways that we can interact with the object? o the top is going to be that, the bottom is going to be that. And just to be clear, these are going to belong to this class, which means that if you try to use this distance method on two lists, for example, youre going to get an error. o the self is going to be a parameter when you define this class that represents a particular instance of the class. f you try to use any of the data or the procedures with an object of a different type, youre going to get an error because these data and these attributes will belong to this particular class. And this is going to represent the other coordinate object for which want to find the distance from my self. o what you need to do is you need to define your own method that tells Python what to do when you call print on an object of this type. o how Python represents the object just behind the scenes and what are different ways that you can interact with the object. o the data attributes is, what is the object, right? o in the definition of the class, whenever you want to refer to attributes that belong to an instance, you have to use self dot. And if you define this method in your class, that tells Python, hey, when you see a print statement thats on an object of type coordinate, call this method, look what it does, and do everything thats inside it. For todays lecture, the parent of the classes are going to be this thing called object, and object is the very basic type in Python. f you have not implemented this method and you try to add two objects of type coordinate, youre going to get an error because Python doesnt actually know right off the bat how to add two coordinate objects, right? o when you want to use this method to figure out a distance between two coordinate objects, this is how you do it. n this particular case, were going to choose to initialize a coordinate object by two values, one for the x and one for the y. And inside this init method, were going to have two assignments. And this one on the right essentially says, whats the name of the class, dot, dot notation, whats the method you want to call, and then in parentheses you give it all of the variables including self. Remember self, the first parameter, is always going to be the instance of an object that youre going to perform the operation on. ts going to call the init method with x is equal to 3 and y is equal to 4. m just going to go over here and wrote this previously, because notice when were creating an object here, were only giving it two parameters. And the dot is going to say look for a data attribute x that belongs to this class. The interface for the car is what are ways that you can interact with it. o the way we do that is we have to define a class. o if you print is instance comma coordinate, this is going to print true because is an object that is of type coordinate. o this line here, is equal to coordinate 3,4, is going to call the init method. o here, this is going to print 3 because the x value for object is 3, and the next line, print origin x is going to print 0 because the x value for the object origin is 0. And internally this pointer is going to tell Python where is the memory location in the computer where you can access the element index 1. o its just essentially going to be a chain, going from one index to the other. o for methods that belong to the class, the first parameter is always going to be self. And if we print the type of a coordinate, well thats just going to be a type. And this is just going to take the inverse of the instance m calling this method on. Notice m doing the exact same method call, except m doing it the other way where you type in the name of the class, name of the method, and then what youre calling it on, and this gives the exact same value here, 1.0. We have to find the init method so we have a way to create objects when we use the class. o by default, when you create your own type, when you print the object of that type, Python tells you this sort of information which is not what you want. o this method here is going to say m going to define a method called distance and m going to pass in two parameters. o class is going to be a type. And when you create a class, youre basically going to figure out what name you want to give your class and youre going to find some attributes. You can create a new list, you can get the length pf the list, you can append to the end of the list, and so on and so on. And thats going to be a way that you can interact with two coordinate points. But this self is going to be sort of a placeholder for any sort of instance when you create the object. And want to first call it on one object, so m going to say dot, so m using the dot notation to call the method distance on object . One is what is the data representation of the list? This is going to be a type called coordinate. And were going to see in class exercise that you can have it be different. o as its the same type of object, then on the return value can do all of the exact same operations that can do on a regular fraction object. And maybe you want to print the value of a coordinate object. The only difference is the fact that you have a self here as the first thing and the fact that you always have to be conscious about whose data attributes youre accessing. And with these packages, you can create objects and all of these objects are going to behave the exact same way. o now any time you have print on an object of type coordinate, youre going to call this special method str, if its implemented in your code. n this case, want to access the x data attribute of my self, and want to subtract the x data attribute of this other coordinate, square that, same for y, square that, and then add those and take the square root of that. o we have a coordinate class, we can create a coordinate object, we can get the distance between two objects. o for todays lecture were going to look at code thats going to be in the context of a coordinate object. o for example, someone wrote the code to implement list class and then you can just use the list class like this. o this line here is going to call the init and its going to do every line inside the init. These last two parameters are x and y, which are going to represent how you create a coordinate object.",0.1742033383915023
3,3,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation, or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: The things we can talk about today, we can talk about this code. We can talk a little bit more about the hash functions. And we can talk a little bit more about amortization. What to do guys want to hear? ADENE: Amoritizaiton. PROFEOR: OK, so one vote for amortization. o who wants to look at the PET code? Who wants to talk about hashes? Who wants to talk about amortization? Two, three, four, five, OK. o then lets try this. Lets look at the PET code then talk about amortization a bit at the end. do have to talk a little bit about hashes though, because owe someone a question from last time. And the question was, we have rolling hashes, so the hashes look like this. K where K is a big number, modulo p. And we argue that its really nice if p is a prime. And then the question was, what if instead p is 2 to the w, and is not prime, as long as the base that were using is coprime with p? Does this work? And the answer is didnt want to say yes without making sure that dont say something stupid but the answer is yes, this works just fine, because the way a compute multiplicative inverse is is you use so something called the extended Euclids method. And if we have b and p, then if we compute their GD, the thats the greatest common divisor so GD is greatest f you use extended Euclid you get something like xb plus yp equals GD of b and p. o if this is 1, then you have xb plus yp equals 1. And if youre working modulo p, whatever that is, then you have that xb is 1 mod p. o theres your multiplicative inverse. Well so now thats nice math, right? But that doesnt tell me why are we not using this. o with the multiplicative inverse would work, but theres something else thats wrong with using 2 to the w. Will this give me a good hash function? OK, the fact that its p might be confusing. o lets say h equals K mod 2 to the w. And remember that the K is some digits in base b, right? ts a big number made out of digits in base b. o K is d1, d2, d3, all the way up until d length in base b. And ll make things easier and say that b is 2 to the 8, because were working with A characters, or colors, or something that fits nicely in a bit. o what could go wrong with using this? ADENE: Well if your series of if your K is bigger than 2 if its K is bigger than 2 to the w PROFEOR: t will be, for sure. ADENE: Yes, thats the problem, because then youll loop. Youll get the same hashes for PROFEOR: Yeah, yeah. o you will get o hashing takes a lot of possible inputs and maps them to a relatively small set of outputs. nputs hash output. And we argued last time that were going to have collisions no matter what, because we have a ton of inputs and not that many outputs. For example, if were hashing strings that are a million characters then this is going to be 2 to the 8 to the 1 million possible strings. And then the number of possible values is, if were using the word size, 2 to the 32. There is no way we can design a function that will take this many inputs, map them to this many outputs, and not do collisions. But instead, what do we want? What makes a good hash function? ay my hash function is 0 for all the Ks. s that a good hash function? ADENE: ts an excellent hash function. PROFEOR: Whats wrong with it? ADENE: You would put everything in one so that its searching, or it would take a long time? PROFEOR: Yeah, searching takes a long time. And weve dont do sorting with this yet. earching takes a long time, string submatching will take a long time, its horrible. ADENE: o what would that distribute like over all PROFEOR: All right, so we want something that looks sort of random. The ideals hash function takes an input then gives it a random output, and then stays consistent. o when it sees an input, returns the same output. o think distribute is the keyword here. Whats wrong with this hash function? f it takes random data, its going to distribute it randomly. Thats true, so thats all good. But what data that we might see in real life will make it behave badly? ADENE: The K is a series of characters, right? PROFEOR: aybe. ADENE: t just could be anything. But we know for sure that L will be larger than w. PROFEOR: ay L is a million. ADENE: OK, well that sucks. PROFEOR: Oh, no. That in itself, that doesnt suck. Thats what lets us do substring matching really fast, even if we have large strings. ADENE: say for 2 to the w, though, because then it will be much larger, like the number of PROFEOR: Yeah, but thats OK. o m OK with doing this as long as all the values here are distributed sort of uniformly here. o thats fine. ADENE: OK. PROFEOR: But theres m arguing that there are some values which will make this hash function behave badly. And that those values are so simple that we might see them in real life. OK, what if all these numbers are what if all the digits are even? o d is 0 mod 2. What happens to K? ADENE: Well, youre saying that instead of 2 to the w, were just using 2. PROFEOR: o no, the modulo is 2 to the w. ay its 2 to the 32. o d are the digits that make up my K. o what if the base is 2 to the 8? o have digits from 0 to 255, 256 of them. And all the digits are 0 modulo 2. For my substring matching example, what if all the characters in the substring are even? ADENE: PROFEOR: Not the same thing. But theres a problem. They will hash to so if all the digits are 0 modulo 2 then what about the K? ADENE: 0 modulo 2 PROFEOR: Yep. o its just like when you have numbers in base 10. 10 happens to be divisible by 2. o if your last digit is even, then the entire number is even. That makes sense, right? Thats math. Please nod, tell me that m making sense. OK, so here, the base is 256. And its also divisible by 2. o if your last digit is divisible by 2, then the whole number is divisible by 2. o then if take this K modulo 2 to the 32 then the hash is also going to be divisible by 2. ADENE: Why does it matter if the hash is divisible by 2? PROFEOR: o it matters because this is supposed to be my universe, right? These are supposed to be all the outputs. And m saying that if my inputs look like this, then the hash function will not distribute them uniformly. nstead, if this is my possible set of outputs, the hash function will always put outputs in this half. o the outputs will always be here. And these are the numbers that are divisible by 2. o these are even, and these are odd. And this area gets no love. Absolutely no number will hash here. o ADENE: Wait, what about something with all odds? ADENE: omething with all odd digits? ADENE: Because youre asking ADENE: You have all As rather than all Bs in your substring or in your string. ADENE: Or because your last digit was odd. PROFEOR: f all of our digits are odd then the last digit is odd. And then youd also get something odd, right? ADENE: Yeah. ADENE: o theres a pattern. But theres an even distribution. PROFEOR: Well if your hash function is always odd, then its not an even distribution. ts ADENE: Wait, our hash function? thought we were talking about ADENE: snt it even if your K is even? And if its odd ? PROFEOR: Yeah, so thats bad. Because if all your Ks happens to be even say if youre doing the nucleotides, and the nucleotides are A, , G, T. f they happen to be encoded as, say, 0, 2, 4, 6, then these are all even. o the hash function will always be even and m wasting the last bit. o if m building a hash table, half the entries will be wasted. Theyll never get anything in there. m just wasting memory. ADENE: o if you could guarantee that your inputs would be evenly distributed PROFEOR: o if our inputs are random then the hash function most hash functions will do a good job of producing a random output. The problem is real life inputs are not random. For example, if you get asides from this if you get data from a camera, so if you get your color pixels from a camera, then because of noise those might have the last few bits, always be the same thing. Also it seems like in real life , in his book, argues about this. t seems like in real life there are a lot of sequences that look like that, that would make your hash function behave poorly. o again, the keyword is distribute. f some nonrandom property in the input is reflected in the output, then thats a bad hash function. ADENE: Would you gain a lot of time from your mod operation? Because in mod 2 to the n you just truncate any bits to the left of the n. PROFEOR: Yeah, so thats why we would do this, right? Thats why were even considering this case. ADENE: Because thatd be really nice to be able to not PROFEOR: o modulo is faster, but in return my hash function is crap here. o usually we prefer it turns out that in practice nicer hash functions give better speed improvements overall. o if you think of how a hash is laid out in memory, youll see that because of caching. And everything gets better to take more time on the mod function and use up all your memory for the hash table. o this is why we dont use the and we use this. Not because of this argument. o a good question required a lot of talking and remembering whats a good hash function, whats a bad hash function. Thank you. OK, lets look at the code a little bit. Everyone looked at it, right? o this time we have modules. We dont have everything in one big file. an someone tell me what are the modules we care about, and why? ADENE: The problem with the ones we have to code ourselves. PROFEOR: OK, lets start with that. ADENE: ubsequence hashes interval subsequence hashes. PROFEOR: OK, so these are all in DNA seq, right? o the module is so yeah, the PET hopefully says that you need to upload this file because its the only file youll need to modify. o everything that we need to write is here. Now pretty much everything thats in that file needs to be modified. o m not going to list them out. What else do we want to read in that PET? ADENE: Rolling PROFEOR: OK, where is rolling hash? ADENE: n the PROFEOR: o whats different between the AP in rolling hash and the AP that we talked about last time? Yes? ADENE: Them having the pop, or it would skip. And thats something else just has a slide, it puts everything in one operation. PROFEOR: All right, so we have append and skip. And we built some beautiful code with that. And we looked at some fancy math because of it. But it turns out that for this PET we can get away with slide. And we started from slide and built these two methods last time. o m not going to explain slide again. ts exactly what we had in the code before we started breaking them up. OK so this is the rolling hash. t is good. Do we care about anything else? ADENE: guess you can look at the rest of the code, if you feel like it. PROFEOR: You can look at the rest of the code if you feel like it, yep. o highlighted one file that might be useful, and thats Kfasta.py. That file has a FATA sequence class, and thats reads from a file and returns something. And the important thing is it doesnt return a list. f you remember the doc dists, doc dist 1 thorugh doc dist 8 dot P, fun times. What we had there was we took the input file, and we read it all a list. This time were not doing that. Were writing, what, 20 lines of code instead of what could be five lines of code to read the input. Why is that? ADENE: Less memory? PROFEOR: Less memory, OK. o if were doing it this way, chances are that if we tried to shove the whole input into memory, it wouldnt fit. And it would crash and you would get 0 on the test because of that. o thats not good. o what do we use instead? Does anyone know what this thing is called? What this class is called? ADENE: PROFEOR: terator, very good. ADENE: Why do they call it FATA? Because it goes faster? PROFEOR: think the letters are a bio acronym. ADENE: Oh, OK. PROFEOR: Does anyone, does anyone do bio here? ve seen that before. o its a bio thing. Lets not worry about it. ADENE: OK. Or, your can use that for any type of file. Like, you dont have to use it just for bio files. PROFEOR: Well, presumably its reads, it takes advantage of the format that theyre stored in, and gives you a list instead of something else. o how does an iterator work? uppose youre building your own iterator. What do you have to implement? ADENE: terator PROFEOR: OK, lets start with next, thats the fun one. What does next do? ADENE: ts like pop. PROFEOR: OK, so its like pop in what way? ADENE: t gives you the next character. PROFEOR: OK. And what happens when youre at the end of the list? ADENE: t stops. PROFEOR: How do you stop? ADENE: t raises an exception? PROFEOR: o next will either return an element, thats the next element in the sequence that youre iterating over. Or it will raise a stop iteration exception error to stop iteration, cool. o whats the other method? omeone said it before, say it again. ADENE: ter. PROFEOR: ter. What does this do in an iterator? ADENE: t returns itself. PROFEOR: All right, very good. n an iterator this is how you will implement it all the time. Does anyone know whats the point of iter? ADENE: o you can return an iterator? Because thats what it told us to do in the PET. PROFEOR: OK, so iter returns and iterator. But it doesnt you dont have to start from an iterator. You can start from any object. And if it has a method iter, then it should give you an iterator that iterates over that object. o if you have something like a list 1, 2, 3, 4 then if you call iter on this, youll get an iterator for it, hopefully, right? And this is what Python uses when you say for i in. o behind the scenes, whatever object you give it here, gets an iter call. And then that produces an iterator. And then Python calls next until stop iteration happens. o you can write an iterator that almost behaves like a list. You can use it in these instructions, and it works as if it was a list, except it uses a lot less memory, because it computes the elements. Hopefully every time next is called, youre computing the next element that youre returning. f youre storing everything in a list then returning the elements that way, thats not the very smart iterator. OK lets look at the last page. o the last page has an iterator on top. And the iterator computes given a list, it computes the reverse of that list. And you can see that it doesnt reverse the list and then keep the reversed list in memory. nstead, every time you call next, it does some magic with the indexes think the magic is called math and then it return something for as long as it can. o this is how you implement reverse without producing a new list. f the original list was order, say had n elements, then if youd produce a new list, youd consume order and memory. This think consumes order 1 memory, and the running time is the same, asymptotically. OK, any question on iterators? ADENE: o its going from the very end, oh, to the very beginning, and then its stepping back. PROFEOR: o reverse, if give it the list 1, 2, 3, 4, want reverse to give it back 4, 3, 2, 1. Except its not going to return a list, its going to return something that can use here. ADENE: m hm, ah, OK. PROFEOR: OK, yes. ADENE: s it ever possible to, sort of, rewind the iterator to like, sort of, reset it? PROFEOR: OK, is it? ADENE: No. PROFEOR: Nope. o Python iterators are simple. All you can do is go forward. ADENE: OK. PROFEOR: The reason that is good is because you can use them for streams. o if you get data from a file, or if you can get data from the network, you can wrap it in an iterator. f you wanted to support resume on data that you get from the network, youd have to buffer all the data. ADENE: o you would have to call the iter about that again and PROFEOR: Yeah. Yeah, if you want to rewind, get another iterator. OK, thats a good question, thank you. o these are iterators. Now were going to go over some Python magic, which is called generators. o look at the iterator code, and then look at the equivalent code right below it. o 12 lines of Python turned into three lines of Python that do exactly the same thing. o the reverse method will return an object that is an iterator, and that you can use just like the iterator in the reverse class. Do people understand what that code does? f you do m so out of here, were done. ADENE: What does yield do? PROFEOR: What does yield do? All right, thats the hard question, what does yield do? will probably spend the rest of the session on the answer to that question. Youre asking all the had questions today, man. o yield, does anyone know conceptually what yield does? Not in detail, just whats it supposed to do so that the rest of the code works? Yes. ADENE: f youre driving someplace and theres a yield sign, you pause. PROFEOR: OK, Python yield. o like the word pause in there. The word pause is useful. o say, instead of implementing this, say were implementing subsequence hashes. ADENE: t kind of spit something out, but keeps going. PROFEOR: Yep. ADENE: Returns PROFEOR: OK, so suppose youre implementing subsequence hashes. Whats the worst, worst possible way you could implement this? ADENE: Return a list. PROFEOR: OK, so the worst, worst way is to go all the way, brute force, dont use the rolling hashes, dont use anything. The next best way is to make a list, right? o youre going to start with an empty list. Then youre going to use the rolling hash in some way. And in some loop youre going to say list.append e. And then youre going to return the list. Does this makes sense? OK, whats the problem with this code? ADENE: Youre going to have a huge list. PROFEOR: Going to have a huge list. o the way we fix it with iterators is we remove this, we replace this with yield e, and we remove this. And now its a generator. And now this consumes a constant amount of memory, instead of building a list. And as long as you only want an iterator out of this method, youll get the right thing. Your code will still work in exactly the same way. OK, so the big question is what does this guy do, right? This is where the magic is. o already said, as a first hint, that this guy will return an iterator. o can someone try to imagine their Python, and see this? o suppose its your Python, you see this. What do you do? ADENE: You wait for some sort of command of some sort, right? PROFEOR: No, lets try something else. ADENE: OK. PROFEOR: o the execution of this pauses. What happens? o were looping somewhere, we got a yield. We stop, whats the first thing we do? ADENE: pit out e. PROFEOR: o youre saying you return e from this guy? ADENE: out e PROFEOR: o want to return something want to return something else from this. o want to use this as if it was a list, yes? ADENE: We store e somewhere. PROFEOR: OK, store e somewhere. ADENE: Do you return the pointer of e? PROFEOR: Almost, so theres a word for the object that m returning. o want to use it as if it was a list. o want to pretend that had returned list in this method, right? o whats the closest thing to a list that can return. ADENE: An iterator. PROFEOR: An iterator, thank you, all right. o we will grab some information from here. Well put it in a nice box. And that box will behave like an iterator. OK, so the first thing, someone said put e away, so thats when we call next were going to spit that out. What else do need to put away? ADENE: PROFEOR: Yep, so this is a lot of magic. This tiny box actually has a lot of magic in it. Because when call next, want to get e. But want to come back here and keep going, right? o have my code thats using the iterator. And theres this code here, thats sort of in a frozen state. Did you guys see any movies where people are frozen up and then, in the future, theyre unfrozen and they start moving again? ADENE: movies. PROFEOR: All right, cool. o this is like that, this takes up the whole function, freezes it up and puts it in a box here. And it returns an iterator that can use the box in the future. o when you call next, it gives e, which is the guy that you put in here. And then it take the function out of the box, unfreezes it, and lets it run again until it hits yield again. Then what happens the next time it hits yield? o, youre looping, and youre yielding again. And say this time youre yielding. ADENE: Just do the same thing? ADENE: Do you put it in that iterator? Or do you make another iterator? PROFEOR: ame iterator. o while this is looping, the code outside should get the values that its yielding. o this has to behave as one iterator. o the code is unfrozen, its allowed to execute until it says yield again. And then it says yield with a new element. put this guy in the box. Then return the old guy as the return value for next. ADENE: Oh. PROFEOR: And then its frozen again. o this guys still in a frozen state. n the movies, think youre only unfrozen once. And then you keep going, right? And theres a happy ending. Where here, every time you call yield youre frozen again, until someone calls next. Does this make sense? ADENE: ts kind of like Groundhog Day. PROFEOR: Yes, except youre allowed to go forward. o this keeps going forward. ADENE: up, thought. o its looping. ts the same day, really. ts doing different things, though. PROFEOR: Yeah. But all your state is saved. o there, some of the state is rolled back. Here all the state is saved. ADENE: OK. PROFEOR: OK, but if that analogy helps, keep it. ADENE: When you call next, are you computing e or e prime to be returned? PROFEOR: o when youre calling next, youre computing e prime and returning e. ADENE: o the value you get from next is precomputed? PROFEOR: o the value you get form next is what you yielded before. ADENE: Wait, so you would just take some sequence hashes instance of that, and then just by putting in yield, now its magically become an iterator and you can call that next on it? PROFEOR: Yep. And inside, you dont have to know that its an iterator. o you dont have a method next here, right? dont implement next or iter here. write this as if its printing stuff to the output. You can think of yield is a print. f you wanted an iterator, then pretend youre printing what you want to iterate over. And instead of saying print you say yield. And then you use that. OK, now what happens when were done? What happens when this loop is done and you return from this method? We said theres no return value. ADENE: t raises a stop? PROFEOR: o when we return, its going to keep in have to remember that its done, right? And the first time, it has some element here that it has to return. o every time you call yield we put a new element in the box, and return the old one. o now we would return the old one. Weve returned e prime, take it out, and put done in the box. o in the future, if next is called again, raise stop iteration. No more freezing, unfreezing, because were done. Were returned. ADENE: o if you called next it would just give you nothing? PROFEOR: t has to raise this exception. ADENE: o you mean, like oh, so it oh, see. t would give you red text then? PROFEOR: f you called it directly, yes, it would give you red text. Yes? ADENE: o this takes a sequence or a list, not another iterator, ever? PROFEOR: This? Whats this? This other code here? ADENE: Yeah. PROFEOR: Not necessarily. ADENE: Or you could give it a procedure. PROFEOR: can give it an iterator if m iterating over it using forin. ADENE: Like, for something in one iterator, yield that something, and then ADENE: Oh, OK. PROFEOR: Yeah, thats a good point. ll get to that later, when we talk about how were going to solve the PET. No, were not solving the PET for you. But well talk about it a little bit. But yeah, thats a good point. o theres no reason why you cant have an argument here that, either a list or an iterator, and then youre iterating over it. And then you have nested generators. o you have generators returned in other generators, and you have a whole chain of things happening when you say next. ADENE: Wait, so this is a generator then, because it produces well it is an iterator though? PROFEOR: o a generator returns an iterator from this method. o a generator acts like an iterator, except when you call next, it unfreezes this code here, and it lets it run. ADENE: But mean, its basically an iterator then? PROFEOR: Yeah. ADENE: But were just calling it a generator because PROFEOR: Because theres a lot more magic. ADENE: OK. PROFEOR: o an iterator just says next and iter. This is all that an iterator is, nothing more. Any object that has these two methods is an iterator. ADENE: Oh, OK. PROFEOR: Now a generator is a piece of Python magic that lets you write shorter iterators. o three lines, as opposed to 13 lines. And we came up with a way to turn in a code that would build a list, and easily turn it into a code that uses a generator, and that uses constant memory instead of building that list. ADENE: OK, now know how an iterator functions. PROFEOR: Exactly. OK, do generators make sense now? Yes. ADENE: f you wanted to loop through all of the values in a generator, do you just wait until the exceptions raised? Or should you, like, keep track of how many things are going to be in that generator? PROFEOR: o, when you have a generator, youd have no idea how many things there are. Thats a good point. o youre wondering if have an iterator, say any iterator, not necessarily a generator, how do know how many things its going to return, right? Do have ln? do not have ln. o an iterator does not have ln. o you have to iterate through it. And most importantly, some iterators can never return. o you can have an iterator that streams data for you across the network. Or you can have an iterator that iterates over the Fibonacci numbers. Thats an infinite sequence, right? ts never going to end. o ln would not even be defined then. Good question, like it. ADENE: s there an isnext method for either iterators or generators? PROFEOR: Nope. This is what you get, if there is no in. ADENE: f that is mature then PROFEOR: Yeah. o in Java you have this belief that you shouldnt get exceptions. You should be able to check for them, right? o maybe thats why youre asking. o if people coming from Java know that any time a method raises an exception, there should be another method that tells you whether this first method is going to raise an exception or not. n Python the exception is just raised. o exceptions are not a lot more expensive than regular instructions, because were using an interpreted language, and its already reasonably slow. o it can do exceptions for free, yay. o this is how it works. This is how forin works. Every time you do a forin, an exception is raised. ADENE: We dont have to catch that, then? PROFEOR: Nope, the forin catches it for you. ADENE: Thats tricky stuff. PROFEOR: But its nice because then you can build any iterator that acts like a list. And then you can do even more fancy stuff, and build a generator. And youre using constant memory instead of order and memory for producing an order and size list. Yes? ADENE: o if we get passed in an iterator and then just yielded what we passed in, yielded the iterator, would that just, essentially, delay everything by one? PROFEOR: o youre yielding the iterator as next, right? ADENE: What? Yeah. PROFEOR: You want to yield the iterator as next. Because if you yield the iterator object, youre going to return that object every time. o youre thinking of something that ADENE: o you need to increase PROFEOR: Youll yield up next, right? ADENE: Right. PROFEOR: You can have a method that says this is the method. And then you take in an iterator. And then you yield it up next. But then youll, basically, get the same thing. ADENE: The same thing. But is it delayed by one or no? PROFEOR: Nope. No, so you have to work through this to convince yourself that its not delayed. o if it would be delayed by one, whats the first thing that youre yielding. ADENE: dont know. PROFEOR: Yeah, so no delay. ADENE: OK. PROFEOR: OK, cool. o lets see, what do we have to implement in DNA seq, subsequence hashes. Do people have an idea of how to implement that now? Yes? Does it make sense for everyone? o you build it as if you were building a list, and then you use yield to make it fast. And by fast mean less memory. OK, how about interval subsequence hashes? The one below. ADENE: s that just like rolling hash, except you, like, have a step in your range? PROFEOR: OK, so its like having a step in your range. o how can you do that? Whats one way of doing it? ADENE: hashes? PROFEOR: Did anyone solve the PET yet? Yes, OK how did you guys do it? Wait, no. Thats a bad question because you guys can answer too much. o interval subsequence hashes versus subsequence hashes. Did you copy paste the code? ADENE: Absolutely. PROFEOR: OK, so one way of doing it is copy and pasting the code. The problem if you copy paste the code is then youre not DRY. Theres this engineering thing DRY means do not repeat yourself. o if youre not DRY, if you copy paste, then suppose you find the bug later. uppose you run the big test and it crashes somewhere. And you fix a bug in subsequence hashes. ADENE: Oh, were supposed to, like, call subsequence hashes from interval subsequence hashes, right? PROFEOR: Thats another way of doing it that is DRY. o this way youre not copy pasting the code. ADENE: Were inlining the code. PROFEOR: Youre inlining it manually, right? All right. o the problem, if you do this on a large scale, like when you go work somewhere, is that you end up with 20 copies of the same code. And then five of them have bug fixes and the other 15 dont, because people forgot where they are. o ideally, try to keep your code DRY. ADENE: o, basically, a list of tuples, right? PROFEOR: OK, so a list of tuples. What does a tuple have? ADENE: The index at which the subsequence operates? PROFEOR: o two indexes, right? The index in the first subsequence, say ADENE: PROFEOR: OK, say i1 and then the index in a second sequence, for the same subsequence, r right? And then i1, i2 prime, i1, i2 second, so on and so forth. o you have the same subsequence in the first sequence matches more things in the second one. This is how youre supposed to return them. ADENE: Does the order matter? PROFEOR: hope not. OK, any questions on this? We went through generators fast. You guys are smart. Yes? ADENE: an you explain how the imaging works? Like, how they create the PROFEOR: orry, do not know. ADENE: Wait, which part? ADENE: o we yield the tuples. But dont really get how they come up with the image from it. ADENE: From the tuples? Oh, mean, guess theyre probably values. ADENE: Yeah, because thought if you compared two strings of DNA that had the exact same, thought it would be like a diagonal line down, not just a small black box. PROFEOR: OK. ADENE: o dont think m understanding how they, like, image it. PROFEOR: o youre supposed to get your image has some things here, and a match is going to give you a big diagonal line thats stronger than everything else, right? ADENE: ts really fanned out. PROFEOR: Well dont have thin chalk. ADENE: No, no, theres like one really dark black box, thats like really black. o thought that meant that all the tuples are there, and everything else is just kind of gray. PROFEOR: Good question. will have to think about that ADENE: supposed to be there. s it like a notation thing, or PROFEOR: think that black box is supposed to be there. Did anyone try comparing two things that shouldnt match, like the dog and the monkey? ADENE: Yeah. And the entire thing was like dark. PROFEOR: Yeah. ADENE: against, like, two same DNAs everything was very light. And there was like a very, very light gray line. But thought that would be like black. PROFEOR: o think how black it is means relative to all the subsequences, how long it is how long the subsequence youre recording is. Either that or how many. There is a function somewhere in there that computes the intensity of a pixel, thats square root of order 4 of something. OK, and can look at that now and tell you. ADENE: ts OK. ts not super important. PROFEOR: Or we can talk about amortized analysis for a bit. Yay! Lets talk about amortized analysis. o this is what youre supposed to get, thats what matters. ADENE: PROFEOR: OK, so amortized analysis, whats the example that we talked about in class? ADENE: ts like list expansion? PROFEOR: OK, so you have you have a list. And we know that the list is stored as an array, right? o this means that you can do indexing in constant time. o if you want to get the first element, order 1. f you want to get the millionth element, order 1. This is not true if you had a link list instead. The millionth element would be order a million. o this is an array. What do we implement? Whats the operation that we implement on this list? ADENE: nsert PROFEOR: nsert, append, push. Lets go for append, because thats what Python calls it. OK, so append puts an element at the end of the list, right? o how does append work? ADENE: The array is not full. PROFEOR: OK. o say have some count variable here. o if the length of the array is bigger than count then what do do? ADENE: Then we can directly insert. And because were looking up in an array and were doing constant time. PROFEOR: OK. ADENE: And so an order amount of information in x ? PROFEOR: orry? ADENE: Order amount of information of x ? Or do we just PROFEOR: Lets say this is our reference, so its constant time. ADENE: Otherwise we dont have enough room in our array. o we need to make it bigger. PROFEOR: OK. o we have array 2 becomes new array of size 2 times count, right? opy everything from ADENE: length of the array. guess theyre the same. PROFEOR: hope theyre the same. ADENE: t is. PROFEOR: Yeah, d say that. o copy from array to lets do this to array 2. And then array 2 becomes array. And then this code here goes here, right? o theres a better way to write this if statement so the code isnt duplicated. OK, so if the length is bigger than how many elements have, if still have room in the array, whats the cost? Whats the running time? onstant. Oh, lets put it on the left. OK, if have to resize the array, whats the cost? ADENE: PROFEOR: o, if did an operations, what then, right? N is the size of the array. f the only operation have is append, then can say n operations will cause the array of grow to size n. o n where n is the number of operations. ADENE: You mean, like, readding to the PROFEOR: o an operation is a data structure operation, like a query or an update. This is my update and this is my query. ADENE: Wait, but like, its order n though, because PROFEOR: Yeah. ADENE: know, its order n. But because we have like an array, and then you have to make a new one, and you have to move all those old items over, right? PROFEOR: Yep. ADENE: OK. But, mean, sometimes like, if your actual array, if you expand it before like, lets say you notice youre getting full and you decide to like make it bigger at that point, is it still order n, as in the number of elements that are PROFEOR: t depends on how you decide. Theres a problem on the PET that asks you about that. o, depends on when you make the decision and how you make the decision, the answer is either yes, youre still constant time, or no. o if you understand the amortized analysis then you can argue of whether it still holds or not. f this breaks down at any point, not going to be constant time. Yes? ADENE: o the only cost is really copying everything from the old array to the new array? PROFEOR: Yes. ADENE: Actually allocating that space is PROFEOR: We assume that allocating the space is constant time. Good question, because you cant take that for granted, right? o we assume that this is order 1, copying is order n. And then the insertion is order 1, just like before. o allocating may not be constant. n real life, allocating is actually logarithmic either of the size that youre asking for or logarithmic of how many buffers youve allocated. And you can make a constant time allocator. But thats lower than a logarithmic allocator, because the constant factor behind it is so big. But even if this allocation would be order n, which would be terrible, it would still get absorbed here. o the overall model works no matter what the allocation is. ts reasonable, from a theoretical standpoint, to say that allocation is order 1, from a theoretical standpoint. o this is the real cost copying the elements. And this makes an append order n worst case. o if you look at this data structure then suppose we want to compute the cost of an append. o say we have code like this, 4, 1, 2, n. First we have L be an empty list. Then we want to compute the cost of this. o if we do it without amortized analysis, line by line analysis, just like we learned in the first lecture, whats the cost of this, making a new list constant? Whats the cost of one append? ADENE: onstant. PROFEOR: One append. o an append can either branch here or branch here. o whats the cost of one append? ADENE: t would be showing with an empty list? ADENE: Depends. PROFEOR: t depends. o worst case. We have to look at a worst case. o this is line by line analysis. Were going to get one number for this. ADENE: N. ADENE: An n. PROFEOR: Yep. o in the worst case, the list will be full. And youll have to make a new one. And then youre going on this branch of the if, so the cost is order n. o order n, worst case. o the cost of one call is order n, worst case. How many calls do we make? o what is the total cost of this thing? ADENE: ts not actually n squared. PROFEOR: Yes, its not actually n squared. But if we do line by line analysis, before we learn amortized analysis, all we can say its order of n squared. And this is correct, its not bigger than n squared, right? o O is correct. But its not the tight bound. o if we had a multiple choice, and you selected this, you wouldnt get the score because we usually ask you what the tightest bound that you can get. OK, so line by line analysis. We worked through that a lot in doc dist. Doesnt work all the time. When it doesnt work, we tell you to use amortized analysis instead. o whats the goal of amortized analysis? What do we want? You guys are yelling at me that this is not n squared, why? mean not why, what? What is it instead? What do we want from amortized analysis? ADENE: ADENE: ts a thats an n. PROFEOR: o we want amortized analysis to say that this is order 1 amortized, and this is PROFEOR: Am out of time? Yeah. OK, so theres a difference between the worst case and amortized, right? We can argue that this is order 1 amortized. And if this is order 1 amortized, then this is order n amortized. o does the difference between worst case and amortized make sense now? o this is what want, the rest is fancy math. f you forget the fancy math after youre done with this class, thats OK. f you remember that this is order 1 amortized, and thats order n amortized, thats good. Thats all you need to know to write code if you dont design algorithms. o this is an important piece of knowledge on its own. OK, so questions about the difference between worst case and amortized? OK, what does amortized mean? ADENE: Average. PROFEOR: Yep, averaged out over multiple operations. o instead of doing line by line analysis, we have to look at what happens over multiple operations, right? o there are two methods that think are useful in LR. There are three in total, but the last one is horribly complicated. o theres something called aggregate analysis. And theres something called the cost based accounting. o last time when we looked at the costs for append, we argued that, hey, its order 1 for a lot of times. And then its only order n for an operation thats a power of 2. o if were looking at the Kith append, then this is order K for K equals 2 to the i. And its order 1 otherwise. Right? o if we sum up all these costs, we get plus sum over log n of O of 2 to the i. And this is clearly order n. And if you do the math here, this is also order n. o this is aggregate analysis. This is what we taught you in lecture. Does this make sense? o the key here is that whenever we are increasing the array, were increasing it to 2 times. And we start with a size of 1, count is 1. We start with an array with 1 element. o the size of the array will first be 1, then 2, then 4, then 8, then 16, 32, 64, 128, so on so forth. t increases exponentially. o on the first append ll have to do a resize. On the second one, resize. Fourth one, resize. Eighth, resize, so on and so forth. o if m adding up the cost for n operations, each operation is order 1 because m inserting everywhere. And then all these operations are all order n. But theres few of them. Theyre few and far out. o if you write the sum this way, and you do the math, you get that its order n. o aggregate analysis says, look at n operations and add the costs up together. And last time we had that good example of walking over a tree, and in order traversal where we drew arrows across edges. o thats aggregate analysis. And then you should look at the cost method in LR because thats also useful sometimes. Does this help? Any questions? No, everyone wants to go home. ADENE: Wait PROFEOR: Almost. ADENE: For log n, so youre starting from log n going to PROFEOR: o m starting from 1 going to log n. ADENE: Oh, oh, so after youre buffering. PROFEOR: o this is fancy math for saying only add up powers of two. o thats what m trying to say, add these guys up. ADENE: Well thats your step . PROFEOR: Yeah. ADENE: Oh, OK. Oh, like that. OK. PROFEOR: OK.","ADENE: ADENE: ts a thats an n. PROFEOR: o we want amortized analysis to say that this is order 1 amortized, and this is PROFEOR: Am out of time? Because in mod 2 to the n you just truncate any bits to the left of the n. PROFEOR: Yeah, so thats why we would do this, right? PROFEOR: o no, the modulo is 2 to the w. ay its 2 to the 32. o d are the digits that make up my K. o what if the base is 2 to the 8? ADENE: say for 2 to the w, though, because then it will be much larger, like the number of PROFEOR: Yeah, but thats OK. ADENE: o you would have to call the iter about that again and PROFEOR: Yeah. PROFEOR: OK, is it? n an iterator this is how you will implement it all the time. ADENE: know, its order n. But because we have like an array, and then you have to make a new one, and you have to move all those old items over, right? o if we sum up all these costs, we get plus sum over log n of O of 2 to the i. And this is clearly order n. And if you do the math here, this is also order n. o this is aggregate analysis. ADENE: Do you put it in that iterator? But, mean, sometimes like, if your actual array, if you expand it before like, lets say you notice youre getting full and you decide to like make it bigger at that point, is it still order n, as in the number of elements that are PROFEOR: t depends on how you decide. ADENE: Like, for something in one iterator, yield that something, and then ADENE: Oh, OK. And then the question was, what if instead p is 2 to the w, and is not prime, as long as the base that were using is coprime with p? o the reverse method will return an object that is an iterator, and that you can use just like the iterator in the reverse class. ADENE: Wait, so you would just take some sequence hashes instance of that, and then just by putting in yield, now its magically become an iterator and you can call that next on it? PROFEOR: You can look at the rest of the code if you feel like it, yep. PROFEOR: OK, so you have you have a list. PROFEOR: OK, so one way of doing it is copy and pasting the code. This is what you get, if there is no in. o if you have something like a list 1, 2, 3, 4 then if you call iter on this, youll get an iterator for it, hopefully, right? ADENE: guess you can look at the rest of the code, if you feel like it. ADENE: t is. OK, so the big question is what does this guy do, right? And then youre going on this branch of the if, so the cost is order n. o order n, worst case. PROFEOR: You want to yield the iterator as next. o if you write the sum this way, and you do the math, you get that its order n. o aggregate analysis says, look at n operations and add the costs up together. ADENE: PROFEOR: OK, so amortized analysis, whats the example that we talked about in class? Not in detail, just whats it supposed to do so that the rest of the code works? ADENE: Wait, so this is a generator then, because it produces well it is an iterator though? And it returns an iterator that can use the box in the future. And the answer is didnt want to say yes without making sure that dont say something stupid but the answer is yes, this works just fine, because the way a compute multiplicative inverse is is you use so something called the extended Euclids method. ADENE: OK. ADENE: OK. ADENE: OK. ADENE: OK. ADENE: OK. ADENE: OK. ADENE: OK. ADENE: OK. And then its only order n for an operation thats a power of 2. o if were looking at the Kith append, then this is order K for K equals 2 to the i. And its order 1 otherwise. ADENE: PROFEOR: Not the same thing. And you can see that it doesnt reverse the list and then keep the reversed list in memory. What does this do in an iterator? OK so this is the rolling hash. PROFEOR: You can have a method that says this is the method. o the problem, if you do this on a large scale, like when you go work somewhere, is that you end up with 20 copies of the same code. And we know that the list is stored as an array, right? ADENE: N. ADENE: An n. PROFEOR: Yep. ADENE: f that is mature then PROFEOR: Yeah. PROFEOR: This? ADENE: o what would that distribute like over all PROFEOR: All right, so we want something that looks sort of random. o you build it as if you were building a list, and then you use yield to make it fast. They will hash to so if all the digits are 0 modulo 2 then what about the K? The index in the first subsequence, say ADENE: PROFEOR: OK, say i1 and then the index in a second sequence, for the same subsequence, r right? ADENE: PROFEOR: Yep, so this is a lot of magic. OK, if have to resize the array, whats the cost? PROFEOR: The reason that is good is because you can use them for streams. PROFEOR: OK. PROFEOR: OK. PROFEOR: OK. PROFEOR: OK. PROFEOR: OK. PROFEOR: OK. ADENE: o you can return an iterator? ADENE: PROFEOR: o, if did an operations, what then, right? nstead, every time you call next, it does some magic with the indexes think the magic is called math and then it return something for as long as it can. PROFEOR: o when we return, its going to keep in have to remember that its done, right? And in some loop youre going to say list.append e. And then youre going to return the list. And we argued last time that were going to have collisions no matter what, because we have a ton of inputs and not that many outputs. OK, so if the length is bigger than how many elements have, if still have room in the array, whats the cost? o this is why we dont use the and we use this. And then the number of possible values is, if were using the word size, 2 to the 32. o if you look at this data structure then suppose we want to compute the cost of an append. o this is like that, this takes up the whole function, freezes it up and puts it in a box here. ADENE: f you wanted to loop through all of the values in a generator, do you just wait until the exceptions raised? PROFEOR: OK, yes. Then we want to compute the cost of this. And what happens when youre at the end of the list? And inside, you dont have to know that its an iterator. ADENE: Do you return the pointer of e? o when you call next, it gives e, which is the guy that you put in here. o youre thinking of something that ADENE: o you need to increase PROFEOR: Youll yield up next, right? o if the length of the array is bigger than count then what do do? And this is what Python uses when you say for i in. OK, so append puts an element at the end of the list, right? And as long as you only want an iterator out of this method, youll get the right thing. o if we had a multiple choice, and you selected this, you wouldnt get the score because we usually ask you what the tightest bound that you can get. And then you take in an iterator. o lets say h equals K mod 2 to the w. And remember that the K is some digits in base b, right? And the question was, we have rolling hashes, so the hashes look like this. ADENE: n the PROFEOR: o whats different between the AP in rolling hash and the AP that we talked about last time? ADENE: Well, youre saying that instead of 2 to the w, were just using 2. o theres no reason why you cant have an argument here that, either a list or an iterator, and then youre iterating over it. o if we do it without amortized analysis, line by line analysis, just like we learned in the first lecture, whats the cost of this, making a new list constant? The problem if you copy paste the code is then youre not DRY. And then you use that. ADENE: Because thatd be really nice to be able to not PROFEOR: o modulo is faster, but in return my hash function is crap here.",0.2082385070550751
4,4,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: All right. o brought a few problems. Theyre obviously not the quiz problems, though some of them are supposed to be similar. What have here might not be what you have on the quiz because we might drop quiz problems or because some of them are just meant to make you think and not to give away the solutions to the quiz. Now, before we get started on this, do you guys have any burning questions or any concepts that you want covered? Based on that, ll select which problems we do. Yes? ADENE: This actually relates not too much to the Pset. f youre looking at the time complexity to maybe transfer something from one table to another, it takes a lot more time, would assume, to move the actual item to the new table than it does just to look at your point and be like, oh, theres nothing there. o if you were just going to look through an empty table of size m, the time to look through that empty table, m assuming, is much less than the time to actually move an item. PROFEOR: o youre saying we have m things here. ADENE: Yes. PROFEOR: ome might be nil, and some might have stuff in them, and youre going to resize that to presumably 2 times m, and the way you do that is youre going to move the elements, presumably by rehashing them, right? ADENE: Yes. PROFEOR: o these elements, at least when we use Python, we dont really store big elements anywhere. f you have a big object, we always work with references to that object. o you remember the address where that object lies in memory, and since the memory is finite and small, addresses are all from 0 to a small number, so theyre constant. o what you have here is not a big object. ts the address of the big object, so moving is always constant time. ADENE: What m saying is lets say that the table is completely full versus completely empty table. t would take more time to move everything out of the full table than it does just to look the empty table, right? PROFEOR: Lets see. o writing something here is order 1 time, right? o moving is order 1 time. oving one element is order 1 time. Whats accessing an element in a table in a list? You have a Python list. Whats the cost of doing an index access? ADENE: ts also order 1, right? PROFEOR: OK. o order 1, index. Order 1, move. uppose you have an empty table. How many indices do you do? How many times the index? ADENE: You look at each one, so its order 1 times the length of the table. PROFEOR: m. o if the table is empty, you have order m indices and 0 moves. Total running time, order m. f you have a full table, how many times do you index in the table? ADENE: till order m. PROFEOR: OK. How many times do you move stuff? ADENE: Order m. PROFEOR: Total running time? ADENE: ts order 2m, which is order m. PROFEOR: o it doesnt matter whether the tables full or empty. ADENE: OK. Just wanted to confirm that. PROFEOR: And this is how you do that. ool. Thanks. Any other questions? Then we will go over problems in the order in which like them, which is easiest to hardest so that dont have to explain the hard ones. Warm up problem one. o you have this recursion and you have to solve it, and you get a hint that n to the power of 1 over log n is 2, which is theta 1. o based on the hint, you can see that its going to involve some math. ts going to get a bit ugly. o how do we solve recursions? Two methods. What are they? ADENE: Expand. PROFEOR: OK. ubstitution formally, but basically, we expand this guy over and over again. And? ADENE: Trees. PROFEOR: Recursion trees. Which one do guys want to do first? f you only have one t here, anything works because you can keep expanding it and that works, so we can do either method. Which one do you guys want to go over? Trees. OK. o we start with the first node. The size of the problem is n. Whats the cost inside here? ADENE: 1. PROFEOR: OK. o this creates one subproblem. Whats the size? ADENE: n to the 1/2. PROFEOR: OK. quare root of n equals n to the 1/2. You solved it already. Whats the cost? ADENE: 1. PROFEOR: Do people remember this? s anyone confused about whats going on here? OK. o two terms, something involving t and something not involving t. The thing involving t is what we want to get rid of. When we do our recursion tree, whatever is in here goes inside here, and this tells me how this number relates to this number. o when go from one level to the next level, this is the transformation. n and becomes square root of n, so the transformation here is the same as the transformation here. Whats next? ADENE: n to the 1/4. PROFEOR: OK. ost? ADENE: 1. PROFEOR: OK. Do we need to do one more or do people see the pattern? ilence means one more. f you guys dont speak, were going to go slow. Whats here? ADENE: n to the 1/8. PROFEOR: Whats here? ADENE: 1. PROFEOR: Lets hope everyone saw the pattern, and suppose weve done this for l levels, so were at the bottom. What should the cost be at the bottom? ADENE: . PROFEOR: orry. We dont start with the cost. What should the size of the problem be at the bottom? ADENE: n to the 1 over 2 to the i. PROFEOR: Lets say that this is level h, where h is the height of the tree. ADENE: Dont you want to do something like n to the 1 over n? PROFEOR: Yeah. OK, so we want something that looks like that? ADENE: f the recursion tree is height i, it is n to the 1 over 2 to the i, but 2 to the i should equal n, or approximately n. PROFEOR: Why? like that, but why? ADENE: Because you need to go down until youre only looking at one element, and that would be one nth of the problem. PROFEOR: OK. o we want this guy to look like what? n fact, it doesnt exactly have to look like 1, but whats the advantage if we manage to get this guy to look like 1? We have a recursion. We dont have a base case here, right? A reasonable base case is T of 1 is theta 1. Whatever function that is, if you evaluate it at 1, youre going to get a constant, so you can say that. Now, at the same time can say that for any constant, c, T of c is theta 1. o if take this constant here, which happens to be 2, but thats not to worry about that. f take this guy here, can put it in here. And know that this guy here equals this guy here. o if can make this guy here look like this guy here, then m done. ake sense? f it makes sense, everyone should nod so that know and can go forward, or smile or something. o this should look like 1. This is order 1 if not 1. Lets make it order 1, because its 2 in this case. Whats the cost here? ADENE: 1. PROFEOR: 1. Everything inside the bubbles is order of already, so dont need to write an order of. What do we do next? ADENE: olve the . PROFEOR: Youre skipping one step. Thats exactly what you do when you have the substitution method. Youre going to get to something and you need to solve the equation. But for the tree, there are two steps. o we need to add up all the costs here and thats the total cost here. n order to do that, first, we sum up over each level. And in this case, its really simple because theres only one node per level, but if you have multiple nodes per level, you have to sum up for each level, and then you have to do a big sum. Whats the sum for this level? 1. ome on, guys. Youre scaring me. ADENE: 1. ADENE: 1. ADENE: ts all 1. PROFEOR: Excellent. o the only thing that m missing is to know how many levels have, because the sum is going to be order h, whatever h is. How do we do that? n to the 1 over 2 to the power of h has to equal this guy, right? ADENE: Why would it equal that guy? We know its less than that guy, but we dont know its equal to that guy. PROFEOR: We have to make it equal because we can only stop when we get to the base case. o we have to expand the recursion tree until we get to a base case, and then we stop, and this is our base case because this is what the problem says should be our base case. ADENE: Right, but n to the 1 over 2 to the h is not equal to n to the 1 over log n. PROFEOR: Well, we can set h to be whatever we want. h is the height of the tree, so we dont know what it is. We have to find out what it is. ADENE: o lets say 2 to the h is equal to log n if you want to make it look like that. PROFEOR: Let me write down the equation to make sure youre right. Youre probably right because youre thinking faster than me, but let me not embarrass myself and do this the right way. o you said 2 to the h is log n, right? Looks about right. o whats h? ADENE: Log base 2. PROFEOR: All right. Log log n. o T of n is order h. We got this from here. T of n is order h is order of log log n. ath people drowning, right? Any questions about this? Yes? ADENE: The first line on the right PROFEOR: This? ADENE: Yeah. s that your base case? What is that? PROFEOR: We got a hint with the problem that said, n to the power of 1 over log n is 2, which is order 1. o for the base case, we always want them to look like this. f we dont get a base case, we write our own base case, which is if you plug in a constant, youre going to get a constant. And since were told that this guy is a constant, thats a pretty good hint that we want to get to it. Lets see how were doing on time. Good. Ready to move on to the next problem? Lets do a fun one. ome people might remember it from elementary school, but this time, were going to look at it with our 6.006 eyes. o suppose you have m coins, gold coins. One of them is face. The fake one is super light because its not real gold. ts something that looks like gold. And we have a scale, and the scale is super accurate. t can weigh any coins on either side and tell us which side is heavier. Perfect accuracy, no need to worry about errors. want to find out which coin is the bad coin. What is the minimum number of experiments have to do? o there is a strategy, and we can worry about that later, but using 6.006, what is the minimum number of experiments have to do? ADENE: Log N times. PROFEOR: Not quite. o this is what you think is, and you can do log n with binary search, right? The problem with binary search is if put half of my coins on the left, half of my coins on the right, one side is going to be heavier, right? o the answers are going to be this or this, but never get this. only get one bit of information instead of getting one trit. A trit is a base three digit. How many bits of information in a trit? ADENE: One and a half. PROFEOR: Roughly. ADENE: Log 3. PROFEOR: Log 3. And we know that its base 2 because thats what we use in . o were discarding a fractional bit of information if were not allowing for this to happen. Anyone want to try something else? We have to prove this, by the way. We have to prove the minimum that we come up with. ADENE: You could just do it coin by coin, but that would take forever. PROFEOR: Thats N. Thats worse. ADENE: How about log base 4 N or something like that? ADENE: an you explain to me why we cant just do binary search? PROFEOR: We can. ts definitely going to give us the correct answer, but its not the minimum number of weighings because were discarding a possible answer. o if you do binary search, you will never get that the two sides are equal. ADENE: Log base 3. PROFEOR: Log base 3 would be better because we have three choices all the time. Lets prove that. o the right answer happens to be log base 3 of N. Lets see how we would get it aside from guessing. ADENE: o you divide it into thirds and compare one third and one third, and if theyre equal, then the light one is in the other third. And if theyre not, light one. Then you just keep dividing by 3. PROFEOR: OK. o thats the strategy. What if dont know the strategy? How do do this without knowing the strategy? ADENE: What if the number of coins isnt divisible by 3? PROFEOR: ath people. ADENE: Yeah, but then how do you OK, never mind. ADENE: Just take the two extra coins and toss them out. PROFEOR: f its not divisible by 3, you add fake coins that are good. mean, you use good coins. But were not worried about the strategy. want us to think of a lower bound. This is a lower bound for an algorithm, right? You cannot do better than log 3 N experiments. Does the word ""lower bound"" ring any bells? s there any lecture where we talked about lower bounds? o if you sort and youre using a comparison model, whats the best you can do? ADENE: N log N. PROFEOR: N log N. Good. o sorting using P, the comparison model, is N log N. How did we prove that? One word. Well, two words. Decision trees. Does anyone remember what decision trees are? One person. ADENE: ts just a comparison thing, right? Youre like, is it greater, is it less than, or is there some sort of question youre asking about each key. PROFEOR: ool. Lets go over that a little bit. No matter what your algorithm is, its going to weigh some coins and its going to get an answer from the scale. And then based on that, its going to weigh some other coins and get some answer from the scale. And it will do some experiments and then it will give you an answer. o if you draw a decision tree, it would look like this. First, we start with 0 information. We weigh some coins. Based on that, we have three possible answers smaller, equal, greater. Now, if were here, were going to do another experiment. Three possible answers. f were here, another experiment, three possible answers. f were here, another experiment, three possible answers. ay we do a third experiment. One, two, three, one, two, three, one, two, three, one, two, three, one, two, three, one, two, three, one, two, three, one, two, three, one, two, three. And then suppose we stop. f we stop, we have to give an answer. o this is an answer, this is an answer, this is an answer, answer, answer, answer, answer, answer. o how many answers do have at the bottom if have three levels? Here have three experiments, so three levels in the decision tree. How many answers? ADENE: . PROFEOR: 3 to the third because start three at the first level, nine at the second, 27 at the third. Each time, multiply by 3. o if do three weighings, can give at most 27 answers. f have more than 27 coins, cant possibly decide which one is bad because say if have 30 coins, then need to be able to give out 30 answers. y algorithm has to have a place where it says the bad coin is coin one, coin two, coin three, all the way to coin 30. Here only have 27 possible answers, so this isnt going to cut it for 30 coins. need to do one more comparison so that have a deeper tree. o suppose have h comparisons instead. How many leaves? How many possible answers? ADENE: h to the third. PROFEOR: Almost. ADENE: 3 to the h. PROFEOR: 3 to the h. o 3 multiplied by 3 multiplied by 3 multiplied by 3 h times, so 3 to the h. ts no longer equal to 27. 3 to the h is the number of possible answers. This has to be bigger or equal to N. Otherwise, the algorithm is incorrect. o what can we say about h? ADENE: . PROFEOR: We did all this without even thinking of what an algorithm would look like. This works for any algorithm. No matter how smart you are, no matter how much math you know, your algorithm is going to be bound by this. o the fact that the answer looks like this gives you some intuition for how to solve the problem. f you want to solve the problem now and figure out the strategy, you know that you have a 3 here. o if you divide into 2 every time, youre not going to get to the right limit. o first you do this, you get a lower bound, and then you use your intuition to figure out what the lower bound means. n this case, it would mean the strategy that we heard earlier. You have to divide into 3 every time and then figure out what you do based on the comparison. o your answer works perfectly once we have this. And also, once we have this, you know that your answer is correct because its optimal. You cant do better than that. Any questions on decision trees? o lower bounds are a boring topic in general. They tell you what you cant do. They dont tell you anything useful about what you can do. n some cases, being able to reason about a lower bound gives you a hint of the solution. New problem. uppose we have a 2D map. Theres a hill, and you take a satellite picture of it at night, and you get a picture with bright pixels and not bright pixels. There are numbers showing how bright your pixels are. 1, 2, 1, 2, 3, 0, 0. m going to draw out an example so we can use our intuition. 0, 0, 1. o suppose this is our map. ts W times H W of what these are, think theyre columns, and H of the other ones. And you want to find a certain picture inside it. You want to see how many times does a certain pattern show up. ay the pattern is small w times small h, and it looks like this. But this will be the input to your problem, so the pattern might be different. You cant hard code this in. And this is useful. This problem is called a bunker hill problem. This is a hill, and this is a bunker. You take a picture of the hill. You want to know where the bunkers are so you can bomb them at night so then you can attack the place. ADENE: Thats awful. PROFEOR: Thank you. ll take that as a compliment o a nice way of solving this? ADENE: You could just go through each row, and then look for a match for the first row, and then PROFEOR: Yep. s this a match? Thats what youre saying. ADENE: Yeah. We can see thats a match. PROFEOR: s this a match? s this a match? By the way, this is a match. This is not a match, this is not a match, this is not a match, this is not a match. Now we go down here. This is not a match, this is not a match, this is not a match, so on and so forth. ADENE: That wasnt what was suggesting, but thats a good idea. PROFEOR: aybe youre suggesting something smarter, and dont want to let you do something smarter so that we look at the brute force approach first. ADENE: mean, was just saying take the first row of your bunker, and then compare it to other rows, and once you hit that, then check and see if the rest of PROFEOR: Yeah, thats a bit smarter, so thats harder to reason about. Lets take this one and figure out the running time of it. ADENE: Does that mean even if you know its not a match, you keep checking all nine of them? PROFEOR: Yeah. ay at the worst case, you only find out its not a match all the way at the end. ADENE: Are you trying to look for all matches or just one? PROFEOR: All matches. ADENE: Youre limited to n squared time almost no matter what, right? f you have a small bunker in a large field, you have to hit the small bunker every time. PROFEOR: Are you going to solve the problem for me? ADENE: Are we trying to find the ? PROFEOR: No. Were trying to find out the running time for the dumb algorithm first. Humor me and lets solve this first, and then lets get to the efficient algorithm, OK? ADENE: Big W minus small w plus 1 times big H minus small h plus 1. ADENE: Whered you get plus 1 from? ADENE: o its WH? ADENE: Yeah. PROFEOR: Well, theres something missing here. This is how many positions have that have to look at. How much time does it take to compare the small images? ADENE: . PROFEOR: This is smaller than wh, which is the input size, so its scary if you have an algorithm that runs faster than the input size because it means youre not looking at all the input. o this is definitely bigger than the input size once we add the w times h here. Dont forget this guy. This is the naive algorithm, and if we discard the small order factors, we get that this is order of WHwh. How can you do better? You have the answer, right? Lets let everyone else think for a minute, and then you can give me the answer if you want, or someone else can give me the answer. guess you should because you thought of it first. Any ideas? o youre thinking about the input size, right? omeone was thinking about the input size. o the input size is W times H, right? o if have an algorithm thats W times H, thats optimal because it has to look at all the input. Well, were going to have an algorithm thats W times H, so with that out of the way, does that inspire anyone as to what the solution is? ADENE: Do that thing that was saying, just take the first row, but then you still have a W term. PROFEOR: Yeah. o lets make it better. t is the correct intuition. Now try to use a trick we learned in lecture to make that faster. ADENE: Just use the top left corner instead of the whole row. PROFEOR: OK. o we could use the top left corner, and if the top left corner doesnt match, then we dont have to check for matches. o this works for reasonably random data. As long as we dont have a lot of false positives, were going to run fast. Now, the top corner of this one, if the map has a lot of 1s and then some 2s sprinkled all over it, most of the time, well have to go through the whole image so were going to have a lot of false positives. How do we make our false positive rate go down? ADENE: Looks kind of like a rolling hash problem. PROFEOR: Looks like a rolling hash problem, exactly. Lets see if we can use rolling hashes. ADENE: But then you still have that lowercase w term, though. PROFEOR: How do we get rid of it? ADENE: . PROFEOR: orry? ADENE: Wouldnt it be w? mean the running time if we were just going through one row would be big W minus little w, times PROFEOR: o wheres your rolling hash? ADENE: guess you can use the entire thing as a hash, too. That would kind of work. PROFEOR: o we want a hash for the whole thing. nstead of using this as the hash, we want a smarter hash. ADENE: ts the entire thing, and then as you move to the right, you can add those and subtract, and compare that with the hash. PROFEOR: OK. o wed have a rolling hash that has everything in here, and then as move to the right, add these guys and remove these guys. This is big W times big H, roughly, times small h because every time move to the right, have to do order h work. o m down from this thing to order of WHh, o its better. ts one step forward. Now, lets make this even faster. What if could do this in order 1 instead of order h? How would do this in order 1? ADENE: Youd have to compress all the rows, and then take the hash of each column. PROFEOR: How would we compress them? ADENE: Take the hash of the column. PROFEOR: You want to compress the rows? ADENE: Yes. You divide it PROFEOR: Lets not compress the rows. ADENE: You could take just your bunker, and then figure out the hashes of the three columns, and just run through like that. Youd still have to access each of those items. dont really see how its faster. guess its less, though. ts less. aybe its only 1. ADENE: o do you want to hash each little column ? PROFEOR: o were going to hash all these guys, and then were going to have hashes for them, and were going to do the regular RabinKarp for the hashes. Now, what happens when go down? PROFEOR: You have to recompute everything. PROFEOR: Lets do better than recompute everything. ADENE: Do you want to downward on each column? PROFEOR: Yep. Rolling hash. want to make this faster, so have big W hashes. Theyre all little h inside. Here, have to compute them brute force. cant do anything better. But when go from here to here, theres only one element going out and one element going in. ame for all these guys. Lets not make the picture uglier than it needs to be. o have big W rolling hashes. Theyre vertical rolling hashes. And then the rolling hashes hash columns, so my the sliding window that have is little w rolling hashes. Each rolling hash is little h in size, so its a hash of hashes. ts nested hashes. And then when go down, only have to roll down each of the rolling hashes by 1, so thats constant time. o to go from here to here, to the slide the window one down, have to roll this hash down, roll this one, this one, this one, this one, this one, and all of them roll down in constant time. o when m adding a column to the hash, say m here and want to go here, roll down this hash and have the answer. ts order 1. m adding it in order 1. Does this make sense? ADENE: ts tricky. PROFEOR: But its not too bad, right? ADENE: You just need to do the vertical roll first, right? PROFEOR: Yep. To have the simplest possible code, you start with big W rolling hashes, do 1D RabinKarp, you roll everything down, 1D RabinKarp, and keep doing that. OK Whats the running time for this? ADENE: WH. PROFEOR: WH. ADENE: Does this one have a space complexity about W, then, because ? PROFEOR: Yeah. o my memory requirement went up to 4W. s everyone happy with this? ts one of the few cases where an approach for solving a 1D problem generalizes to 2D. n most problems, you have to rethink the whole situation. Lets do a hard problem. Enough with the easy ones. Two lists, roughly size N, and theyre both sorted. Let me fill them out with random numbers. 5, 13, 22, 43, 56, 62, 81, 86, 87, 2, 3, 7, 9, 15, 19, 24, 28, 32. o have these lists. Lets be generous and say that all the numbers are different. Theyre sorted. want to find the nth number, so the number with rank n, out of both lists as fast as possible. ADENE: Wouldnt it just be index? PROFEOR: o the thing is if, for example, n is 1, then its this. This is the second number. This is the third. o if you take the lists and you combine them, then want the result out of that. ADENE: Lets do merge sort and the combined index, right? PROFEOR: Full merge sort, N log N? No. ADENE: ts already sorted, though. PROFEOR: OK. o what do we do? ADENE: We just do merge. Thats just order N. PROFEOR: o merge. ADENE: erge . PROFEOR: OK. o merge and then index is the first approach, which is order N. Then you said run the merge algorithm, but stop when you get to the little nth element, so thats a little bit better and we dont have to produce an array so the space complexity is down to order 1, right? Now lets do ADENE: Logarighmic times n. PROFEOR: Yeah, exactly. This is linear. We have to get to logarithms. How do we get to logarithms? ADENE: Do a modified binary search. ADENE: What if you first looked at actually, dont know what m going to say. PROFEOR: Anyone else? o modified binary search. Do you know the full answer, or do you want to start looking at the solution? ADENE: have an idea . PROFEOR: Lets see how it would work. ADENE: o if we take the n over second element on each row, the one that is lower, thats at least the n over second element, and the one thats higher PROFEOR: o lets say this is our N1 and N2, and theyre both order N initially. o this is N2 over 2 if this is smaller. ADENE: The lower one is at least the N over second element since everything before it is less than N. Does that make sense? PROFEOR: Yeah. o this is N over 2 are greater, right? This guy. ADENE: We also know that the element above it is at most the nth element because its greater than PROFEOR: o its at most N1 plus N2 because thats how many you have in total, and the one on the top, you know that these ones are bigger than h, right? But you dont know anything about these ones, so its minus N1 over 2. o its at most N1 over 2 plus N2, the top element. ADENE: o then we can take the element three quarters of the way through N2 and one quarter of the way through N1 to do more ? ADENE: Yes. PROFEOR: Lets see what happens in each case. o if little n is here, then you divide. o if n is smaller than this, then you chop them up here and youve divided the problem into half. Youre good. f its bigger than this other number here, youve chopped the problem up and youre here. Youre good. Now, the hard case seems to be when its in between. o what do we do then? ADENE: Arent those two numbers the same? ADENE: f its between, take the upper half of the bottom one and the lower half of the upper one, right? f its between 15 and 43, then you take everything in the upper half of N2 and you take the lower half of N1. ADENE: Yeah, you should always be taking the upper half of one and the lower half of the other in this case. PROFEOR: Really? ADENE: N2, 15 is at least the nth over 2 element. think were using three ns at the same time. ADENE: Are you using the little n or the big N? ADENE: orry. is element and the lists are called N and this is confusing. an we rename the nth element to something like m or some other useful number? The kth element, OK. o the 15 at least the kth over 2 element, so it cant be anything on the left half of the PROFEOR: k over 2? Why k over 2? This list is size N. ADENE: orry. didnt pick the elements at N over 2. picked the elements at k over 2. PROFEOR: Why would do that? f N1 is greater than k, then chop off the end of the list, right? f N2 is bigger than k, then chop off the end of the list completely. f this list is sorted and want the third element, know that these are not the answer. No matter whats down here, these are not the answer. o know for sure that k is going to be bigger than N1, N2. o instead of going there, lets go at k over 2. And here, lets go for k over 2. Now this one looks a bit nastier. N1 plus N2 stays what it was before. ADENE: On the list where you got the element that was lower, you know that everything to the left of it is less than k over 2. The element number is lower than k over 2, so were not using anything to the left of the 15. You can kill that section for us. PROFEOR: OK, so we can kill it, but then whats the rank? When recurse, how am going to know the rank that m looking for? ADENE: k minus what you killed. ADENE: You can save the branches. PROFEOR: o you want to kill this guy, right? o you want to kill these numbers. But here have k over 2 numbers, and here have k over 2 numbers. How do know that its not somewhere here? ADENE: How do you know that whats not somewhere there? ADENE: You compare 15 and 43, right? And then you see that 43 is bigger, and you see 15 is smaller, so then you would go to, guess, k over 4 index in N1, and 3k over 4 in N2. ADENE: When you recurse, you said that youve killed k over 2 elements. ADENE: But you can also kill everything to the right of 43. ADENE: Yes. You can kill everything to the right of 43 since it cant be any of those elements, and you can kill everything to the left of 15. And then you repeat the algorithm again with the lists you didnt kill, except you also put in a term of weve already covered k over 2 elements. PROFEOR: o we want the element with the rank k over 4 over these lists. o know for sure that what have is either k over 2 or less than k over 2, right? o this is less than k over 2, and then m looking for a rank of k over 4. That seems to work. How does the running time look? ADENE: t should be O of log k. ADENE: think its log . PROFEOR: o log k, log N1 plus N2. Are these different? ADENE: Yes. ADENE: f k is 1, the algorithm should only recurse once, even if N is 20 million. PROFEOR: OK. ADENE: But if k is 20 million and the list lengths are two million long, itll take approximately those lengths to run. PROFEOR: OK. o what gets reduced, aside from the list size, k gets reduced. k seems to define the input size for the next iteration because ll have at least k over 2 elements in one of these buckets. o it sounds like it should be log k. k is bigger than N1, N2, but it should hopefully be smaller than the sum because otherwise, why am doing the problem? o this is definitely order of N1 plus N2. o this is a bound. This is a slightly tighter bound. We have a different solution to the problem. All the possible solutions are hard to argue. They all come down to something like this. The one that we have requires you to use you have two indices, and you know that the sum of the indices is k, and you do binary search on the top and adjust the index on the bottom to keep the constraint that the sum of the two indices in N. And you can look at that in the notes that were going to post. Are you guys tired? Do you want to look at one more thing, or are we done? ADENE: . PROFEOR: Lets look at something reasonably easy. You guys can read this on your own. m not going to bore you with that. o suppose we have some functions, and we want to order them according to their asymptotic growth rate. Do people remember how to do this from Pset one? o the idea is that you take each function, you simplify it, and then you sort them. o lets have a couple of simple ones and then some hard ones, and were going to stop in five minutes. Whats this? ADENE: n to the fourth. PROFEOR: OK. Lets see. n choose 3. What is this? ADENE: . PROFEOR: OK, very good. Why? ADENE: omething about choose is n times n minus 1 times n minus 2. ADENE: ts n factorial over n minus 2 factorial. PROFEOR: This comes out to be roughly n cubed. ool. How about n plus log to the fourth of n? ADENE: N. PROFEOR: Yep. o even if have a polynomial in a logarithm, its still dominated by pure n. Now, suppose we want to order these guys together with which one doesnt look boring at all? n to the log n and 2 to the n. Lets sort them. Which ones the smallest? Which ones the biggest? ADENE: 2 to the n is bigger. PROFEOR: Lets start with the smallest ones, because think that will be easy. o which ones the absolute smallest out of all these guys? ADENE: n. PROFEOR: OK. Then? ADENE: n to the third. PROFEOR: ubed and fourth. o we have to compare these guys. How do we compare them? n to the power of log n and 2 to the n something. ADENE: . PROFEOR: Take the logs. o when we have something confusing with exponentials, take the logs and see what we get. Logs are monotonic, so if you take the logs, youll have the same relationship afterwards. o log of this is log of n to the power of log n, so its log n times log n, so its log 2 n. Log of 2 to the n is n. Which ones bigger? ADENE: n. PROFEOR: All right. And were not going to solve this, but how would you go about solving this guy? What do you do to it? ADENE: terling. PROFEOR: terling, yep. You do terling, you go through the numbers, and you figure out the answer. And then if you have to do logarithms, you use logarithms to figure out where it belongs among these guys. ADENE: Whats terling? ADENE: terlings formula. ts that gross thing that was on the board before we came in here. PROFEOR: o terling says that n factorial is ugly. 2 pi n here times n over e to the power of n. o whats this binomial? Whats the formula for it? OK, formula for n choose k. Anyone? ADENE: ts n times 1 over 2 factorial times n minus k factorial. PROFEOR: n this case, its n factorial over n over 2 factorial raised to the power of 2, right? And then we chug through the math and get to some answer. All right?","ADENE: n to the 1/8. ADENE: n to the 1/4. ADENE: n to the 1/2. PROFEOR: And this is how you do that. ADENE: n to the 1 over 2 to the i. PROFEOR: Lets say that this is level h, where h is the height of the tree. ADENE: Right, but n to the 1 over 2 to the h is not equal to n to the 1 over log n. PROFEOR: Well, we can set h to be whatever we want. The one that we have requires you to use you have two indices, and you know that the sum of the indices is k, and you do binary search on the top and adjust the index on the bottom to keep the constraint that the sum of the two indices in N. And you can look at that in the notes that were going to post. PROFEOR: We got a hint with the problem that said, n to the power of 1 over log n is 2, which is order 1. o for the base case, we always want them to look like this. ADENE: o lets say 2 to the h is equal to log n if you want to make it look like that. o you have this recursion and you have to solve it, and you get a hint that n to the power of 1 over log n is 2, which is theta 1. o based on the hint, you can see that its going to involve some math. ADENE: f the recursion tree is height i, it is n to the 1 over 2 to the i, but 2 to the i should equal n, or approximately n. PROFEOR: Why? ADENE: Dont you want to do something like n to the 1 over n? ADENE: We also know that the element above it is at most the nth element because its greater than PROFEOR: o its at most N1 plus N2 because thats how many you have in total, and the one on the top, you know that these ones are bigger than h, right? ADENE: o if we take the n over second element on each row, the one that is lower, thats at least the n over second element, and the one thats higher PROFEOR: o lets say this is our N1 and N2, and theyre both order N initially. ADENE: On the list where you got the element that was lower, you know that everything to the left of it is less than k over 2. ADENE: n. PROFEOR: OK. o merge and then index is the first approach, which is order N. Then you said run the merge algorithm, but stop when you get to the little nth element, so thats a little bit better and we dont have to produce an array so the space complexity is down to order 1, right? ADENE: 2 to the n is bigger. f you want to solve the problem now and figure out the strategy, you know that you have a 3 here. ADENE: ts the entire thing, and then as you move to the right, you can add those and subtract, and compare that with the hash. ADENE: The first line on the right PROFEOR: This? o this is what you think is, and you can do log n with binary search, right? n to the power of log n and 2 to the n something. ADENE: mean, was just saying take the first row of your bunker, and then compare it to other rows, and once you hit that, then check and see if the rest of PROFEOR: Yeah, thats a bit smarter, so thats harder to reason about. n to the 1 over 2 to the power of h has to equal this guy, right? PROFEOR: We have to make it equal because we can only stop when we get to the base case. o the only thing that m missing is to know how many levels have, because the sum is going to be order h, whatever h is. n to the log n and 2 to the n. Lets sort them. ADENE: You look at each one, so its order 1 times the length of the table. Well, were going to have an algorithm thats W times H, so with that out of the way, does that inspire anyone as to what the solution is? ADENE: Because you need to go down until youre only looking at one element, and that would be one nth of the problem. This is the naive algorithm, and if we discard the small order factors, we get that this is order of WHwh. o to go from here to here, to the slide the window one down, have to roll this hash down, roll this one, this one, this one, this one, this one, and all of them roll down in constant time. ADENE: n. PROFEOR: All right. o we have to expand the recursion tree until we get to a base case, and then we stop, and this is our base case because this is what the problem says should be our base case. Now, at the same time can say that for any constant, c, T of c is theta 1. o if take this constant here, which happens to be 2, but thats not to worry about that. PROFEOR: o were going to hash all these guys, and then were going to have hashes for them, and were going to do the regular RabinKarp for the hashes. PROFEOR: ome might be nil, and some might have stuff in them, and youre going to resize that to presumably 2 times m, and the way you do that is youre going to move the elements, presumably by rehashing them, right? ADENE: n to the third. ADENE: h to the third. h is the height of the tree, so we dont know what it is. PROFEOR: o the thing is if, for example, n is 1, then its this. PROFEOR: o you want to kill this guy, right? Now, the top corner of this one, if the map has a lot of 1s and then some 2s sprinkled all over it, most of the time, well have to go through the whole image so were going to have a lot of false positives. What do you do to it? f this list is sorted and want the third element, know that these are not the answer. o you said 2 to the h is log n, right? ADENE: 1. ADENE: 1. ADENE: 1. ADENE: 1. ADENE: 1. ADENE: 1. ADENE: 1. ADENE: . ADENE: . ADENE: . ADENE: . ADENE: . ADENE: . ADENE: . ADENE: . ADENE: o then we can take the element three quarters of the way through N2 and one quarter of the way through N1 to do more ? PROFEOR: This is smaller than wh, which is the input size, so its scary if you have an algorithm that runs faster than the input size because it means youre not looking at all the input. ADENE: Yeah, you should always be taking the upper half of one and the lower half of the other in this case. PROFEOR: OK, so we can kill it, but then whats the rank? o the fact that the answer looks like this gives you some intuition for how to solve the problem. PROFEOR: Are you going to solve the problem for me? o when m adding a column to the hash, say m here and want to go here, roll down this hash and have the answer. ADENE: Youd have to compress all the rows, and then take the hash of each column. o there is a strategy, and we can worry about that later, but using 6.006, what is the minimum number of experiments have to do? 3 to the h is the number of possible answers. Then we will go over problems in the order in which like them, which is easiest to hardest so that dont have to explain the hard ones. The size of the problem is n. Whats the cost inside here? And then you see that 43 is bigger, and you see 15 is smaller, so then you would go to, guess, k over 4 index in N1, and 3k over 4 in N2. What have here might not be what you have on the quiz because we might drop quiz problems or because some of them are just meant to make you think and not to give away the solutions to the quiz. Do you know the full answer, or do you want to start looking at the solution? ADENE: The lower one is at least the N over second element since everything before it is less than N. Does that make sense? The element number is lower than k over 2, so were not using anything to the left of the 15. What is the minimum number of experiments have to do? o if you take the lists and you combine them, then want the result out of that. o the 15 at least the kth over 2 element, so it cant be anything on the left half of the PROFEOR: k over 2? ADENE: But you can also kill everything to the right of 43. And were not going to solve this, but how would you go about solving this guy? ADENE: You could take just your bunker, and then figure out the hashes of the three columns, and just run through like that. ADENE: Take the hash of the column.",0.1897079276773296
5,5,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation, or to view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: Today were moving on from theoretical things, from the mean value theorem, to the introduction to whats going to occupy us for the whole rest of the course, which is integration. o, in order to introduce that subject, need to introduce for you a new notation, which is called differentials. m going to tell you what a differential is, and well get used to using it over time. f you have a function which is y = f(x), then the differential of y is going to be denoted dy, and its by definition f(x) dx. o heres the notation. And because y is really equal to f, sometimes we also call it the differential of f. ts also called the differential of f. Thats the notation, and its the same thing as what happens if you formally just take this dx, act like its a number and divide it into dy. o it means the same thing as this statement here. And this is more or less the Leibniz interpretation of derivatives. Of a derivative as a ratio of these so called differentials. ts a ratio of what are known as infinitesimals. Now, this is kind of a vague notion, this little bit here being an infinitesimal. ts sort of like an infinitely small quantity. And Leibniz perfected the idea of dealing with these intuitively. And subsequently, mathematicians use them all the time. Theyre way more effective than the notation that Newton used. You might think that notations are a small matter, but they allow you to think much faster, sometimes. When you have the right names and the right symbols for everything. And in this case it made it very big difference. Leibnizs notation was adopted on the continent and Newton dominated in Britain and, as a result, the British fell behind by one or two hundred years in the development of calculus. t was really a serious matter. o its really well worth your while to get used to this idea of ratios. And it comes up all over the place, both in this class and also in multivariable calculus. ts used in many contexts. o first of all, just to go a little bit easy. Well illustrate it by its use in linear approximations, which weve already done. The picture here, which weve drawn a number of times, is that you have some function. And heres a value of the function. And its coming up like that. o heres our function. And we go forward a little increment to a place which is dx further along. The idea of this notation is that dx is going to replace the symbol delta x, which is the change in x. And we wont think too hard about well, this is a small quantity, this is a small quantity, were not going to think too hard about what that means. Now, similarly, if you see how much weve gone up well, this is kind of low, so its a small bit here. o this distance here is, previously we called it delta y. But now were just going to call it dy. o dy replaces delta y. o this is the change in level of the function. And well represent it symbolically this way. Very frequently, this just saves a little bit of notation. For the purposes of this, well be doing the same things we did with delta x and delta y, but this is the way that Leibniz thought of it. And he would just have drawn it with this. o this distance here is dx and this distance here is dy. o for an example of linear approximation, well say whats 64.1, say, to the 1/3 power, approximately equal to? Now, m going to carry this out in this new notation here. The function involved is x^1/3. And then its a differential, dy. Now, want to use this rule to get used to it. Because this is what were going to be doing all of today is, were differentiating, or taking the differential of y. o that is going to be just the derivative. Thats 1/3 x^(2/3) dx. And now m just going to fill in exactly what this is. At x = 64, which is the natural place close by where its easy to do the evaluations, we have y = 64^(1/3), which is just 4. And how about dy? Well, so this is a little bit more complicated. Put it over here. o dy = 1/3 64^(2/3) dx. And that is 1/3 * 1/16 dx, which is 1/48 dx. And now m going to work out what 64 to the, whatever it is here, this strange fraction. just want to be very careful to explain to you one more thing. Which is that were using x = 64, and so were thinking of x + dx is going to be 64.1. o that means that dx is going to be 1/10. o thats the increment that were interested in. And now can carry out the approximation. The approximation says that 64.1^(1/3) is, well, its approximately what m going to call y + dy. Because really, the dy that m determining here is determined by this linear relation. dy = 1/48 dx. And so this is only approximately true. Because whats really true is that this is equal to y + delta y. n our previous notation. o this is in disguise. What this is equal to. And thats the only approximately equal to what the linear approximation would give you. o, really, even though wrote dy is this increment here, what it really is if dx is exactly that, is its the amount it would go up if you went straight up the tangent line. o m not going to do that because thats not what people write. And thats not even what they think. Theyre really thinking of both dx and dy as being infinitesimally small. And here were going to the finite level and doing it. o this is just something you have to live with, is a little ambiguity in this notation. This is the approximation. And now can just calculate these numbers here. y at this value is 4. And dy, as said, is 1/48 dx. And that turns out to be 4 + 1/480, because dx is 1/10. o thats approximately 4.002. And thats our approximation. Now, lets just compare it to our previous notation. This will serve as a review of, if you like, of linear approximation. But what want to emphasize is that these things are supposed to be the same. Just that its really the same thing. ts just a different notation for the same thing. remind you the basic formula for linear approximation is that f(x) is approximately f(a) + f(a) (xa). And were applying it in the situation that a = 64 and f(x) = x^(1/3). And so f(a), which is f(64), is of course 4. And f(a), which is 1/3 a^(2/3), is in our case 1/16. No, 1/48. OK, thats the same calculation as before. And then our relationship becomes x^(1/3) is approximately equal to 4 plus 1/48 times x minus a, which is 64. o look, every single number that ve written over here has a corresponding number for this other method. And now if plug in the value we happen to want, which is the 64.1, this would be 4 + 1/48 1/10, which is just the same thing we had before. o again, same answer. ame method, new notation. Well, now get to use this notation in a novel way. o again, heres the notation. This notation of differential. The way m going to use it is in discussing something called antiderivative Again, this is a new notation now. But its also a new idea. ts one that we havent discussed yet. Namely, the notation that want to describe here is whats called the integral of g(x) dx. And ll denote that by a function capital G of x. o its, you start with a function g(x) and you produce a function capital G(x), which is called the antiderivative of g. Notice theres a differential sitting in here. This symbol, this guy here, is called an integral sign. Or an integral, or this whole thing is called an integral. And another name for the antiderivative of g is the indefinite integral of g. And ll explain to you why its indefinite in just very shortly here. Well, so lets carry out some examples. Basically what d like to do is as many examples along the lines of all the derivatives that we derived at the beginning of the course. n other words, in principle you want to be able to integrate as many things as possible. Were going to start out with the integral of sin x dx. Thats a function whose derivative is sin x. o what function would that be? osine x, minus, right. ts cos x. o cos x differentiated gives you sin x. o that is an antiderivative of sine. And it satisfies this property. o this function, G(x) = cos x, has the property that its derivative is sin x. On the other hand, if you differentiate a constant, you get 0. o this answer is whats called indefinite. Because you can also add any constant here. And the same thing will be true. o, c is constant. And as said, the integral is called indefinite. o thats an explanation for this modifier in front of the ""integral"". ts indefinite because we actually didnt specify a single function. We dont get a single answer. Whenever you take the antiderivative of something its ambiguous up to a constant. Next, lets do some other standard functions from our repertoire. We have the integral of x^a dx. ome power, the integral of a power. And if you think about it, what you should be differentiating is one power larger than that. But then you have to divide by 1/(a+1), in order that the differentiation be correct. o this just is the fact that d/dx of x^(a+1), or maybe should even say it this way. aybe ll do it in differential notation. d(x^(a+1)) = (a+1) x^a dx. o if divide that through by a+1, then get the relation above. And because this is ambiguous up to a constant, it could be any additional constant added to that function. Now, the identity that wrote down below is correct. But this one is not always correct. Whats the exception? Yeah. a equals TDENT: 0. PROFEOR: Negative 1. o this one is OK for all a. But this one fails because weve divided by 0 when a = 1. o this is only true when a is not equal to 1. And in fact, of course, whats happening when a = 0, youre getting 0 when you differentiate the constant. o theres a third case that we have to carry out. Which is the exceptional case, namely the integral of dx/x. And this time, if we just think back to what our o what were doing is thinking backwards here, which a very important thing to do in math at all stages. We got all of our formulas, now were reading them backwards. And so this one, you may remember, is ln x. The reason why want to do this carefully and slowly now, is right now also want to write the more standard form which is presented. o first of all, first we have to add a constant. And please dont put the parentheses here. The parentheses go there. But theres another formula hiding in the woodwork here behind this one. Which is that you can also get the correct formula when x is negative. And that turns out to be this one here. o m treating the case, x positive, as being something that you know. But lets check the case, x negative. n order to check the case x negative, have to differentiate the logarithm of the absolute value of x in that case. And thats the same thing, again, for x negative as the derivative of the logarithm of negative x. Thats the formula, when x is negative. And if you carry that out, what you get, maybe ll put this over here, is, well, its the chain rule. ts 1/(x) times the derivative with respect to x of x. o see that there are two minus signs. Theres a x in the denominator and then theres the derivative of x in the numerator. Thats just 1. This part is 1. o this 1 over x, which is 1/x. o the negative signs cancel. f you just keep track of this in terms of ln(x) and its graph, thats a function that looks like this. For x negative. And its derivative is 1/x, claim. And if you just look at it a little bit carefully, you see that the slope is always negative. Right? o here the slope is negative. o its going to be below the axis. And, in fact, its getting steeper and steeper negative as we go down. And its getting less and less negative as we go horizontally. o its going like this, which is indeed the graph of this function, for x negative. Again, x negative. o thats one other standard formula. And very quickly, very often, we wont put the absolute value signs. Well only consider the case x positive here. But just want you to have the tools to do it in case we want to use, we want to handle, both positive and negative x. Now, lets do two more examples. The integral of sec^2 x dx. These are supposed to get you to remember all of your differentiation formulas, the standard ones. o this one, integral of sec^2 dx is what? tan x. And here we have + c, all right? And then the last one of, a couple of, this type would be, lets see. should do at least this one here, square root of 1 x^2. This is another notation, by the way, which is perfectly acceptable. Notice ve put the dx in the numerator and the function in the denominator here. o this one turns out to be sin^(1) x. And, finally, lets see. About the integral of dx / (1 + x^2). That one is tan^(1) x. For a little while, because youre reading these things backwards and forwards, youll find this happens to you on exams. t gets slightly worse for a little while. You will antidifferentiate when you meant to differentiate. And youll differentiate when youre meant to antidifferentiate. Dont get too frustrated by that. But eventually, youll get them squared away. And it actually helps to do a lot of practice with antidifferentiations, or integrations, as theyre sometimes called. Because that will solidify your remembering all of the differentiation formulas. o, last bit of information that want to emphasize before we go on some more complicated examples is this. ts obvious because the derivative of a constant is 0. That the antiderivative is ambiguous up to a constant. But its very important to realize that this is the only ambiguity that there is. o the last thing that want to tell you about is uniqueness of antiderivatives up to a constant. The theorem is the following. The theorem is if F = G, then F equals G so F(x) equals G(x) plus a constant. But that means, not only that these are antiderivatives, all these things with these plus cs are antiderivatives. But these are the only ones. Which is very reassuring. And thats a kind of uniqueness, although its uniqueness up to a constant, its acceptable to us. Now, the proof of this is very quick. But this is a fundamental fact. The proof is the following. f F = G, then if you take the difference between the two functions, its derivative, which of course is F G, is equal to 0. Hence, F(x) G(x) is a constant. Now, this is a key fact. Very important fact. We deduced it last time from the mean value theorem. ts not a small matter. ts a very, very important thing. ts the basis for calculus. ts the reason why calculus make sense. f we didnt have the fact that the derivative is 0 implied that the function was constant, we would be done. We would have alculus would be just useless for us. The point is, the rate of change is supposed to determine the function up to this starting value. o this conclusion is very important. And we already checked it last time, this conclusion. And now just by algebra, can rearrange this to say that F(x) is equal to G(x) plus a constant. Now, maybe should leave differentials up here. Because want to illustrate o lets go on to some trickier, slightly trickier, integrals. Heres an example. The integral of, say, x^3 (x^4 + 2)^5 dx. This is a function which you actually do know how to integrate, because we already have a formula for all powers. Namely, the integral of x^a is equal to this. And even if it were a negative power, we could do it. o its OK. On the other hand, to expand the 5th power here is quite a mess. And this is just a very, very bad idea. Theres another trick for doing this that evaluates this much more efficiently. And its the only device that were going to learn now for integrating. ntegration actually is much harder than differentiation, symbolically. ts quite difficult. And occasionally impossible. And so we have to go about it gently. But for the purposes of this unit, were only going to use one method. Which is very good. That means whenever you see an integral, either youll be able to divine immediately what the answer is, or youll use this method. o this is it. The trick is called the method of substitution. And it is tailormade for notion of differentials. o tailormade for differential notation. The idea is the following. m going to to define a new function. And its the messiest function that see here. ts u = x^4 + 2. And then, m going to take its differential and what discover, if look at its formula, and the rule for differentials, which is right here. ts formula is what? 4x^3 dx. Now, lo and behold with these two quantities, can substitute, can plug in to this integral. And will simplify it considerably. o how does that happen? Well, this integral is the same thing as, well, really should combine it the other way. o let me move this over. o there are two pieces here. And this one is u^5. And this one is 1/4 du. Now, that makes it the integral of u^5 du / 4. And thats relatively easy to integrate. That is just a power. o lets see. ts just 1/20 u to the whoops, not 1/20. The antiderivative of u^5 is u^6. With the 1/6, so its 1/24 u^6 + c. Now, thats not the answer to the question. ts almost the answer to the question. Why isnt it the answer? t isnt the answer because now the answers expressed in terms of u. Whereas the problem was posed in terms of this variable x. o we must change back to our variable here. And we do that just by writing it in. o its 1/24 (x^4 + 2)^6 + c. And this is the end of the problem. Yeah, question. TDENT: PROFEOR: The question is, can you see it directly? Yeah. And were going to talk about that in just one second. OK. Now, m going to do one more example and illustrate this method. Heres another example. The integral of x dx over the square root of 1 + x^2. Now, heres another example. Now, the method of substitution leads us to the idea u = 1 + x^2. du = 2x dx, etc. t takes about as long as this other problem did. To figure out whats going on. ts a very similar sort of thing. You end up integrating u^(1/2). t leads to the integral of u^(1/2) du. s everybody seeing where this...? However, there is a slightly better method. o recommended method. And call this method advanced guessing. What advanced guessing means is that youve done enough of these problems that you can see two steps ahead. And you know whats going to happen. o the advanced guessing leads you to believe that here you had a power 1/2, here you have the differential of the thing. o its going to work out somehow. And the advanced guessing allows you to guess that the answer should be something like this. (1 + x^2)^(1/2). o this is your advanced guess. And now you just differentiate it, and see whether it works. Well, here it is. ts 1/2 (1 + x^2)^(1/2) 2x, thats the chain rule here. Which, sure enough, gives you x over square root of 1 + x^2. o were done. And so the answer is square root of (1 + x^2) + c. Let me illustrate this further with another example. strongly recommend that you do this, but you have to get used to it. o heres another example. e^(6x) dx. y advanced guess is e^(6x). And if check, when differentiate it, get 6e^(6x). Thats the derivative. And so know that the answer, so now know what the answer is. ts 1/6 e^(6x) + c. Now, OK, you could, its also OK, but slow, to use a substitution, to use u = 6x. Then youre going to get du = 6dx, dot, dot, dot. ts going to work, its just a waste of time. Well, m going to give you a couple more examples. o how about this one. x e^(x^2) dx. Whats the guess? Anybody have a guess? Well, you could also correct. o dont want you to bother yeah, go ahead. TDENT: PROFEOR: Yeah, so youre already one step ahead of me. Because this is too easy. When they get more complicated, you just want to make this guess here. o various people have said 1/2, and they understand that theres 1/2 going here. But let me just show you what happens, OK? f you make this guess and you differentiate it, what you get here is e^(x^2) times the derivative of negative 2x, so thats 2x. x^2, so its 2x. o now you see that youre off by a factor of not 2, but 2. o a number of you were saying that. o the answer is 1/2 e^(x^2) + c. And can guarantee you, having watched this on various problems, that people who dont write this out make arithmetic mistakes. n other words, there is a limit to how much people can think ahead and guess correctly. Another way of doing it, by the way, is simply to write this thing in and then fix the coefficient by doing the differentiation here. Thats perfectly OK as well. All right, one more example. Were going to integrate sin x cos x dx. o whats a good guess for this one? TDENT: PROFEOR: omeone suggesting sin^2 x. o lets try that. Over 2 well, well get the coefficient in just a second. o sin^2 x, if differentiate, get 2 sin x cos x. o thats off by a factor of 2. o the answer is 1/2 sin^2 x. But now want to point out to you that theres another way of doing this problem. ts also true that if you differentiate cos^2 x, you get 2 cos x (sin x). o another answer is that the integral of sin x cos x dx is equal to 1/2 cos^2 x + c. o what is going on here? Whats the problem with this? TDENT: PROFEOR: ntegrals arent unique. Thats part of the but somehow these two answers still have to be the same. TDENT: PROFEOR: OK. What do you think? TDENT: f you add them together, you just get c. PROFEOR: f you add them together you get c. Well, actually, thats almost right. Thats not what you want to do, though. You dont want to add them. You want to subtract them. o lets see what happens when you subtract them. m going to ignore the c, for the time being. get sin^2 x, 1/2 sin^2 x (1/2 cos^2 x). o the difference between them, we hope to be 0. But actually of course its not 0. What it is, is its 1/2 sine squared plus cosine squared, which is 1/2. ts not 0, its a constant. o whats really going on here is that these two formulas are the same. But you have to understand how to interpret them. The two constants, heres a constant up here. Theres a constant, c_1 associated to this one. Theres a different constant, c_2 associated to this one. And this family of functions for all possible c_1s and all possible c_2s, is the same family of functions. Now, whats the relationship between c_1 and c_2? Well, if you do the subtraction, c_1 c_2 has to be equal to 1/2. Theyre both constants, but they differ by 1/2. o this explains, when youre dealing with families of things, they dont have to look the same. And there are lots of trig functions which look a little different. o there can be several formulas that actually are the same. And its hard to check that theyre actually the same. You need some trig identities to do it. Lets do one more example here. Heres another one. Now, you may be thinking, and a lot of people are, thinking ugh, its got a ln in it. f youre experienced, you actually can read off the answer just the way there were several people who were shouting out the answers when we were doing the rest of these problems. But, you do need to relax. Because in this case, now this is definitely not true in general when we do integrals. But, for now, when we do integrals, theyll all be manageable. And theres only one method. Which is substitution. And in the substitution method, you want to go for the trickiest part. And substitute for that. o the substitution that proposed to you is that this should be, u should be ln x. And the advantage that that has is that its differential is simpler than itself. o du = dx / x. Remember, we use that in logarithmic differentiation, too. o now we can express this using this substitution. And what we get is, the integral of, so ll divide the two parts here. ts 1 / ln x, and then its dx / x. And this part is 1 / u, and this part is du. o its the integral of du / u. And that is ln u + c. Which altogether, if put back in what u is, is ln (ln x) + c. And now we see some uglier things. n fact, technically speaking, we could take the absolute value here. And then this would be absolute values there. o this is the type of example where really would recommend that you actually use the substitution, at least for now. All right, tomorrow were going to be doing differential equations. And were going to review for the test. m going to give you a handout telling you just exactly whats going to be on the test. o, see you tomorrow.","And because y is really equal to f, sometimes we also call it the differential of f. ts also called the differential of f. Thats the notation, and its the same thing as what happens if you formally just take this dx, act like its a number and divide it into dy. Because this is what were going to be doing all of today is, were differentiating, or taking the differential of y. o that is going to be just the derivative. f you have a function which is y = f(x), then the differential of y is going to be denoted dy, and its by definition f(x) dx. The idea of this notation is that dx is going to replace the symbol delta x, which is the change in x. And we wont think too hard about well, this is a small quantity, this is a small quantity, were not going to think too hard about what that means. And now if plug in the value we happen to want, which is the 64.1, this would be 4 + 1/48 1/10, which is just the same thing we had before. o another answer is that the integral of sin x cos x dx is equal to 1/2 cos^2 x + c. o what is going on here? And this one is u^5. Which is that were using x = 64, and so were thinking of x + dx is going to be 64.1. o that means that dx is going to be 1/10. And so this one, you may remember, is ln x. The reason why want to do this carefully and slowly now, is right now also want to write the more standard form which is presented. Namely, the notation that want to describe here is whats called the integral of g(x) dx. o this function, G(x) = cos x, has the property that its derivative is sin x. On the other hand, if you differentiate a constant, you get 0. o this answer is whats called indefinite. For the purposes of this, well be doing the same things we did with delta x and delta y, but this is the way that Leibniz thought of it. f you make this guess and you differentiate it, what you get here is e^(x^2) times the derivative of negative 2x, so thats 2x. And thats the same thing, again, for x negative as the derivative of the logarithm of negative x. Thats the formula, when x is negative. And another name for the antiderivative of g is the indefinite integral of g. And ll explain to you why its indefinite in just very shortly here. o this is it. Namely, the integral of x^a is equal to this. And then, m going to take its differential and what discover, if look at its formula, and the rule for differentials, which is right here. o sin^2 x, if differentiate, get 2 sin x cos x. o thats off by a factor of 2. o the answer is 1/2 sin^2 x. But now want to point out to you that theres another way of doing this problem. o the substitution that proposed to you is that this should be, u should be ln x. And the advantage that that has is that its differential is simpler than itself. But just want you to have the tools to do it in case we want to use, we want to handle, both positive and negative x. Now, lets do two more examples. And if you carry that out, what you get, maybe ll put this over here, is, well, its the chain rule. But its very important to realize that this is the only ambiguity that there is. PROFEOR: Negative 1. o this one is OK for all a. But this one fails because weve divided by 0 when a = 1. o this is only true when a is not equal to 1. o the advanced guessing leads you to believe that here you had a power 1/2, here you have the differential of the thing. And now m going to work out what 64 to the, whatever it is here, this strange fraction. o its 1/24 (x^4 + 2)^6 + c. And this is the end of the problem. o its going like this, which is indeed the graph of this function, for x negative. f F = G, then if you take the difference between the two functions, its derivative, which of course is F G, is equal to 0. And what we get is, the integral of, so ll divide the two parts here. The way m going to use it is in discussing something called antiderivative Again, this is a new notation now. o, really, even though wrote dy is this increment here, what it really is if dx is exactly that, is its the amount it would go up if you went straight up the tangent line. o this just is the fact that d/dx of x^(a+1), or maybe should even say it this way. ts 1 / ln x, and then its dx / x. And this part is 1 / u, and this part is du. And that turns out to be this one here. And now m just going to fill in exactly what this is. This is the approximation. With the 1/6, so its 1/24 u^6 + c. Now, thats not the answer to the question. And this time, if we just think back to what our o what were doing is thinking backwards here, which a very important thing to do in math at all stages. o this is just something you have to live with, is a little ambiguity in this notation. We have the integral of x^a dx. Another way of doing it, by the way, is simply to write this thing in and then fix the coefficient by doing the differentiation here. The point is, the rate of change is supposed to determine the function up to this starting value. Which is that you can also get the correct formula when x is negative. n order to check the case x negative, have to differentiate the logarithm of the absolute value of x in that case. o its the integral of du / u. And that is ln u + c. Which altogether, if put back in what u is, is ln (ln x) + c. And now we see some uglier things. o this one, integral of sec^2 dx is what? What this is equal to. f we didnt have the fact that the derivative is 0 implied that the function was constant, we would be done. m going to tell you what a differential is, and well get used to using it over time. o this is the type of example where really would recommend that you actually use the substitution, at least for now. And the advanced guessing allows you to guess that the answer should be something like this. o this distance here is, previously we called it delta y. But now were just going to call it dy. strongly recommend that you do this, but you have to get used to it. Basically what d like to do is as many examples along the lines of all the derivatives that we derived at the beginning of the course. And in the substitution method, you want to go for the trickiest part. And this one is 1/4 du. o the last thing that want to tell you about is uniqueness of antiderivatives up to a constant. But what want to emphasize is that these things are supposed to be the same. Now, m going to carry this out in this new notation here. And that is 1/3 * 1/16 dx, which is 1/48 dx. But for the purposes of this unit, were only going to use one method. Now, m going to do one more example and illustrate this method. Well, this integral is the same thing as, well, really should combine it the other way. Which is the exceptional case, namely the integral of dx/x. The antiderivative of u^5 is u^6. And now just by algebra, can rearrange this to say that F(x) is equal to G(x) plus a constant.",0.1529933481152993
6,6,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. AARTYA HANKHA BWA: Feel free to populate the front row. m not that scary. o today, were going to look at more greedy algorithms. o think you went over Kruskals algorithm and how you do the sorting in the lecture. o going back to make change from last recitation, so this is sort of a variant on that. o instead of discrete coins, we now have continuous coins, in the sense so the analogy here is, lets say, you have N metals, and each of the metals has some value given by i dollars per kilogram, or whatever units you prefer. And you want to achieve some value T. You want to give someone T dollars worth of metal. And you want to do this while minimizing oh, so should mention this. ki is the weight of every metal that you will give to the person. o youre taking ki of metal i, and you are going to and you have to ensure, so basically, you have to ensure that some summation of ki i over all i is equal to T. And in doing so you want to minimize the summation over all ki. o does that make sense? o you have a bunch of metals. ome of them are more expensive than others. And you want to measure them out and give someone a certain fixed value. o anyone have any ideas how to do this? hould be should be the first thing that comes to mind. o you have much of metals some of them with certain costs. And youre trying to create a value T. o which metal would you want to pick? o it should seem intuitive that if you want to minimize the weight of the metal, you would want to pick the have the most expensive one for weight. o lets start by sort by i. And we want to sort it in decreasing order. Does make sense? o if you have the most expensive metal, you want to use as much of that as you can, so that your weight is minimized. o once you sort by i, so lets say, you have your costs right now are lets call this one 1, 2, up to n. And these are in sorted order. o its increasing this way. o you now take your value T, and you look at T by 1. And that is the amount of weight you would need to generate . o you look at how much you have here. o the amount of metal so a constraint forgot to mention, you are given a limited amount of every metal. OK, thats its not that trivial. o you have lets mention that. o you have is that used? No, its not amount. Does that make more sense? o you look at T over i. And if T over i is greater than W of i, then you just use the amount you need to construct Wi, and youre done. Otherwise, you use all of i. o if its less than Wi, in that case, you sorry, other way around. f its greater than Wi, you use all of it. And then you move on to the next one, the next one, and so on. o that seems pretty intuitive. Lets actually do a formal proof of that. o how you go about proving this is that so lets say so its what we call the current base method. o basically what you have is, lets say youre not using the most expensive metal you have at this point. o lets say your most expensive metal has cost i, but instead, you decide to use j. o lets say you decide to use some kj amount of j. o the value youre getting from this is j kj. And instead, if you use i, how much metal would you need to get the same value? You would need j kj over i. Does that make sense? o this is the value you would obtain by using kj kilograms of this metal. o if you instead used this one, youd get this value. And so this is the most expensive one. And i is greater than j, this value, so this value is less than kj. o by using this metal instead of that one, you are decreasing the amount the weight you would need, so your minimization goes down. ake sense? o thats like a very simple greedy algorithm. And its the algorithm is exactly what youd expect, and the proof isnt very hard. o lets move on to a slightly interesting one. o this is process scheduling. o lets say you have a computer, and youre running end processes. And each of the process has a time t1 through tn, again processes. And you want to order them in some way. o first, you will do process p1. Then youll process p2, and so on, and so forth. Then youll define a completion time. o completion time is simply when does process i end. o when does process i end? You just p1 plus its like the time for p1 plus time for p2 up to pn. o basically, you have all your processes. o lets says this is p1, this is p2, and so on. And the completion time for a certain process in the middle is just the sum of all times before it. Thats completion time. And now what you want to do is you want to minimize the average completion time, which is summation over all the completion times over n. o any ideas what an algorithm for this would look like? Essentially, you want to minimize the sum of all these times. o all these times, you want to minimize the average of these. o what do you want to do? Do you want to shift the slower the processes which take more time, do you want to keep them at the end, or do you want to keep them at the beginning? o if you have a bunch of small processes, what do you do with them at the end? What do you do at the beginning? ompletion time is when does o lets say this is process pi. And completion time for process pi is like this distance. ts like, when does pi get completed? o its summation of all the times so the time taken for p1, p2, up to the end, see? o you want to basically minimize the average of these values. o where do you put the smaller processes would you put the shorter processes at the end or the beginning? Which one would decrease your average? The beginning? akes sense, right? o yeah, that makes sense. o you basically want to like scrunch these lines towards the beginning, so your average is smaller. Note that this total length is always the constant. ts like summation over all ti. o lets go about so OK, this is strategy. Again, sort by ti, and this is increasing order, and thats it basically. o this is your algorithm, sorted by tn, but use the process in that order. o lets try to prove this. o the way you prove this is a pretty generic method. t is often used to prove greedy algorithms. o lets say that this is not the optimal. Lets say someone comes up to you and tells you, OK, have a better sequence. have a sequence, lets say, called lets say have a sequence of p1 to pn. And that sequence does better than a sorted order. o youre like, OK, so if this is not sorted, then you have some elements in the middle. Lets say you call them pi is greater than pj, with i is less than j. o theres some pi here, and theres some pj here, such that this is greater than that. o its not in sorted order. o you can always find a pair like that. o now m going to claim that if you swap these two values so you swap pi and pj thatll actually decrease whatever current average completion time you have. o initially, you had something like this. o no, lets not draw a line there. o lets say you had something like you had this process, so pi actually, this is the bigger process, so this is pi, and this is pj. And now m saying that and you have some stuff in the middle. And my claim is that, no, this is not optimal. Youd do much better if you moved the pj over here. o you want to go from this to this, and big process. o lets see what changes when you go from there to there. o first of all, observe that the completion times of everything behind this is the same. They all have the same completion time; nothing is affected. And youre only changing these two things. Everything after this is also the same has the same completion time. o the only things that are changing are this one, this one, and all the ones up to this one. Even this one has the same completion time. ake sense? o how much is this changing by? o lets define this. Delta is equal to t of pi minus t of pj so the difference between these two processes. o the original completion time of pi was this. And now the corresponding process down here, the completion time is decreased by delta. o completion time for us goes down like minus delta. This is a summation of completion time. This divided by n is a constant. o you just want to minimize this. o first it goes so this one goes down minus delta. o lets look at the next process. The next process is something like this. o again, these do not change. Youre only swapping these two. o this completion time also down a minus delta, and so on, and so forth. o you just get a bunch of minus deltas, which is equal to however many processes you have. But thats not even important. What is important is that just by swapping, youre going to get at least one minus delta. And delta is positive, because assumption oops, sorry, t because assumption was that t pi minus t pj is positive. o just by swapping, youre going to always decrease it. o the claim that that sequence was an optimal solution is wrong. o you can always do better by swapping two inversions. o that out of sorted order is called an inversion. o if you solve an inversion, you always get a better result. Does that proof make sense? o thats a slightly more interesting recent algorithm. o lets move on to the third one we have here. The third one is event overlap. o this is how it works. o you wake up in the morning, and you look at your calendar. And being an T student, your calendar looks pretty full. o lets say this is what it looks like. o these are your events. Lets use some colors, make it a little clearer possibly. And lets say you have another event over here. You have something here. You have something here. You have something here. And you have something here. o OK, lets move this down. o the problem is that you have this bunch of events planned out. Now clearly, theyre overlapping, so you cant attend all of them. o the idea is you make a bunch of clones of yourself. And so in this case, look at the matching colors. o if you create clone number 1 goes here, and clone number 2 goes to red, and clone number 3 goes to blue. o then clone number 1 does this. lone number 3 does the blue one. lone number 2 does red. guess, we should move the red back a little or forward a little bit just to make it clear. Yeah, there we go. And now, you could easily see that this is optimal. o you can do this with three clones and no less. o you make three clones, and then you can go off to spring break. And your schedule is fine. o now, how would you approach this problem? o what is a greedy strategy to, given a number of intervals, how do you find the minimum number of clones you need to cover your day? Any ideas? What is a naive thing you could do? ADENE: When you say to cover your day, then its like the number AARTYA HANKHA BWA: o you want to do every event. But like so this clone cant so clone number 1 does this event. Then he cant do this event or this event. ADENE: ort of maximizing your events? AARTYA HANKHA BWA: You want to do all the events? You want to minimize the number of clones. ADENE: AARTYA HANKHA BWA: o its like interval scheduling. But you want to do all the intervals, but youre allowed to use multiple people to do all the intervals. Yes? Yeah? ADENE: You could sort by end time. AARTYA HANKHA BWA: By end time, OK. What do you do after you sort by end time? ADENE: And then iterate over all the intervals once theyre sorted and just count how many intervals there are between the . That would get complicated. AARTYA HANKHA BWA: o youre close. o you do begin by sorting. But you can actually do it by sorting by end time. ts easier to visualize if you sort by start time. o leading from that, anyone want to top in? ADENE: ts when your sorting starts, and every time you get a class, you get a new clone. AARTYA HANKHA BWA: o yeah, essentially, every time you cant add it to one of your current clones, you just create a new one. You could also do it by end time, because its symmetrical, right? o if you sort by end time, then you start with the smallest last end time and go backwards, exactly the same thing. o lets write it down. o sort by start time, and so actually, lets work out this example. o in this case, you would go OK, actually, if once you sort so first you have 1, then you have 2, 3, 4, 5, 6. o thats sorted by start time. And then you have so first you go for this one. Then you go for 2, and 2 intersects with 1. o you put 2 into it. o this is clone number 1. And then you have to create a new clone for 2, so you create the new clone. And there we go. o then you go to 3. 3 clashes with both 1 and 2, so you have to create a new clone again. o in that case, you go forth and create 3. Then you go to 4. Now, 4, you see, its starts with 2 and 3, but it is good with 1. o you just put 4 over here. And if you continue like this, you essentially get this and this. ake sense? o thats how you schedule it. o does that algorithm make sense? Lets try to prove its correctness. o lets look at the instance where youre inserting the mth clone so mth clone. o when the mth is created, you already have some values in here. o you have 1, 2, all the way up to m minus 1. o now, you bring in your interval, and you see that it collides with all of these values. o lets just draw the final interval for all these guys. o lets say the final interval for this guy was out here. Lets say the final interval for this guy was out here, and so on, and blah, blah, blah, blah, blah. And so when you create the mth clone, you look at the start time. o what happens is that the start time is somewhere, lets say, here. And now, you know that because of this so youre only adding a new clone when you dont have an available slot. o that means that there is some interval here, which intersects with this guy. o how do you show that this is one interval? Well, its like consider any level. But say there is no interval that intersects with it. o that means that there is either o if there were a gap here so lets say, at this location, this interval wasnt here. Lets say if you extrapolate this line outward so this is your current starting value. And lets say you look at this line. And in this segment, you cant have something which starts after this, because this is the current highest sorting starting time. o theres no interval that starts after this. o the only interval thats going to exist have already ended here. And if theyre already ended here, that means you could evaluate here. Does that make sense? o basically, then you can show that, OK, so at every existing if youre adding a new clone, that means at every existing level, you have something which intersects. o what that means is that you have a single point of time where there are m minus 1 plus 1 intervals. That means that you absolutely need m intervals regardless of what your strategy is. o adding the mth clone is necessary. o if you go on, continue the argument lets say your total number of clones was m so you can just do this argument for m. There you will show that, oh, if followed all these rules correctly, can show that the start time for m intersects with m minus 1 other intervals. o theres no way can create a scheduling with less than m clones. Did that argument make sense, or should go over it again? o thats somewhat hand wavy, but that shouldnt be OK. n any case, well, thats the three problems. o guess we can go back to this one and sort of give the motivation for this. o this could, for example, be used in scheduling processes for servers, for instance. o lets say your server gets a request to run n processes, and they have times like that. o this is like shortest time first. o you take all the short the smallest jobs, and you execute them in the beginning. And you wait for other jobs. And this can also be done online. o you can have an online version of this. o if you take this algorithm and you do it online so lets say your server is running jobs, and you get a new request. o you get a new request, so you already have some set of t1 to tn. And lets say, at the current moment, ti is your smallest job. And youre running it, and youre currently at this point. And then in the middle of running it, you can get new requests for jobs. o how would you modify this algorithm to handle that? o you still want to maintain this lowest average completion time thing. o how would you handle this situation. o lets say youre in the middle of a job and you get a bunch of new requests. o current set is all these existing jobs plus some other things you get in here. o would you consider switching to a different job here, or would you keep doing this? Lets say one of the new jobs you get is really small. o what you would do in that case is that instead of continuing with this, you would switch to current smallest job. o you would look at the remaining time, so thats important. o you could forget about the amount of time you already spent on this. You know what the remaining time is, and that is all that is relevant. o you can just consider this problem in a different framework. ts the exact same question. You just look at remaining time, instead of total time. o if youre in the middle of a job, and a new one comes in which is smaller, you just switch to that, complete that, and then look at the remaining times for everything. o at some point of time, you might have a lot of half completed jobs just lying around. And for all of them, youll update their ti values to remaining time rather than start time. And that gives you a nice way to decide which processes to do online. And that gives you o this is assuming that all of your tasks have equal weights. o all of them have equal reward. o obviously, thats not always the case. You might be pushing back a very long job forever, because smaller things keep coming in and that might get important. But everything is equally weighted, then this is the optimal thing you can do. And its a very simple strategy that works. o those are the three problems wanted to discuss. Do you guys have any other questions or comments or anything? Good? OK. We finished pretty early, so guess, have a great spring break.","And now what you want to do is you want to minimize the average completion time, which is summation over all the completion times over n. o any ideas what an algorithm for this would look like? o in this case, you would go OK, actually, if once you sort so first you have 1, then you have 2, 3, 4, 5, 6. o thats sorted by start time. And then you have so first you go for this one. o if youre in the middle of a job, and a new one comes in which is smaller, you just switch to that, complete that, and then look at the remaining times for everything. o if you take this algorithm and you do it online so lets say your server is running jobs, and you get a new request. o youre taking ki of metal i, and you are going to and you have to ensure, so basically, you have to ensure that some summation of ki i over all i is equal to T. And in doing so you want to minimize the summation over all ki. o you have 1, 2, all the way up to m minus 1. o now, you bring in your interval, and you see that it collides with all of these values. o lets say you had something like you had this process, so pi actually, this is the bigger process, so this is pi, and this is pj. o it should seem intuitive that if you want to minimize the weight of the metal, you would want to pick the have the most expensive one for weight. o basically what you have is, lets say youre not using the most expensive metal you have at this point. o if you go on, continue the argument lets say your total number of clones was m so you can just do this argument for m. There you will show that, oh, if followed all these rules correctly, can show that the start time for m intersects with m minus 1 other intervals. You know what the remaining time is, and that is all that is relevant. o lets say that this is not the optimal. o once you sort by i, so lets say, you have your costs right now are lets call this one 1, 2, up to n. And these are in sorted order. And so when you create the mth clone, you look at the start time. o youre like, OK, so if this is not sorted, then you have some elements in the middle. o you look at T over i. And if T over i is greater than W of i, then you just use the amount you need to construct Wi, and youre done. And if you continue like this, you essentially get this and this. Do you want to shift the slower the processes which take more time, do you want to keep them at the end, or do you want to keep them at the beginning? o what you would do in that case is that instead of continuing with this, you would switch to current smallest job. o if you have the most expensive metal, you want to use as much of that as you can, so that your weight is minimized. o by using this metal instead of that one, you are decreasing the amount the weight you would need, so your minimization goes down. o you want to go from this to this, and big process. And that is the amount of weight you would need to generate . o if you have a bunch of small processes, what do you do with them at the end? And the completion time for a certain process in the middle is just the sum of all times before it. o how do you show that this is one interval? ADENE: When you say to cover your day, then its like the number AARTYA HANKHA BWA: o you want to do every event. o how you go about proving this is that so lets say so its what we call the current base method. And lets say you look at this line. And instead, if you use i, how much metal would you need to get the same value? And so this is the most expensive one. o you have is that used? And in this segment, you cant have something which starts after this, because this is the current highest sorting starting time. And that gives you o this is assuming that all of your tasks have equal weights. o the problem is that you have this bunch of events planned out. o lets say youre in the middle of a job and you get a bunch of new requests. o the only things that are changing are this one, this one, and all the ones up to this one. o instead of discrete coins, we now have continuous coins, in the sense so the analogy here is, lets say, you have N metals, and each of the metals has some value given by i dollars per kilogram, or whatever units you prefer. o this is your algorithm, sorted by tn, but use the process in that order. And then you have to create a new clone for 2, so you create the new clone. o guess we can go back to this one and sort of give the motivation for this. o if you sort by end time, then you start with the smallest last end time and go backwards, exactly the same thing. ompletion time is when does o lets say this is process pi. o what is a greedy strategy to, given a number of intervals, how do you find the minimum number of clones you need to cover your day? You want to minimize the number of clones.",0.2064755838641189
7,7,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. Yesterday we saw how to define double integrals and how to start computing them in terms of x and y coordinates. We have defined the double integral over a region R and plane of a function f of x, y dA. You cannot hear me? s the sound working? an you hear me in the back now? an we make the sound louder? Does this work? People are not hearing me in the back. s it better? People are still saying make it louder. s it better? OK. Great. Thanks. Thats not a reason to start chatting with your friends. Thanks. When we have a region in the x, y plane and we have a function of x and y, we are defining the double integral of f over this region by taking basically the sum of the values of a function everywhere in here times the area element. And the definition, actually, is we split the region into lots of tiny little pieces, we multiply the value of a function at the point times the area of a little piece and we sum that everywhere. And we have seen, actually, how to compute these things as iterated integrals. First, integrating over dy and then over dx, or the other way around. One example that we did, in particular, was to compute the double integral of a quarter of a unit disk. That was the region where x squared plus y squared is less than one and x and y are positive, of one minus x squared minus y squared dA. Well, hopefully, kind of convinced you that we can do it using enough trig and substitutions and so on, but it is not very pleasant. And the reason for that is that using x and y coordinates here does not seem very appropriate. n fact, we can use polar coordinates instead to compute this double integral. Remember that polar coordinates are about replacing x and y as coordinates for a point on a plane by instead r, which is the distance from the origin to a point, and theta, which is the angle measured counterclockwise from the positive xaxis. n terms of r and theta, you have x equals r cosine theta, y equals r sine theta. The claim is we are able, actually, to do double integrals in polar coordinates. We just have to learn how to. Just to draw a quick picture When we were integrating in x, y coordinates, in rectangular coordinates, we were slicing our region by gridlines that were either horizontal or vertical. And we used that to set up the iterated integral. And we said dA became dx dy or dy dx. Now we are going to actually integrate, in terms of the polar coordinates, r and theta. Lets say we will integrate in the order with r first and then theta. That is the order that makes the most sense usually when you do polar coordinates. What does that mean? t means that we will first focus on a slice where we fix the value of theta and we will let r vary. That means we fix a direction, we fix a ray out from the origin in a certain direction. And we will travel along this ray and see which part of it, which values of r are in our region. Here it will be actually pretty easy because r will just start at zero, and you will have to stop when you exit this quarter disk. Well, what is the equation of this circle in polar coordinates? t is just r equals one. o, we will stop when r reaches one. But what about theta? Well, the first ray that we might want to consider is the one that goes along the xaxis. That is when theta equals zero. And we will stop when theta reaches pi over two because we dont care about the rest of the disk. We only care about the first quadrant. We will stop at pi over two. Now, there is a catch, though, which is that dA is not dr d theta. Let me explain to you why. Lets say that we are slicing. What it means is we are cutting our region into little pieces that are the elementary, you know, what corresponds to a small rectangle in the x, y coordinate system, here would be actually a little piece of circle between a given radius r and r plus delta r. And given between an angle theta and theta plus delta theta. need to draw, actually, a bigger picture of that because it makes it really hard to read. Lets say that fix an angle theta and a slightly different one where have added delta theta to it. And lets say that have a radius r and add delta r to it. Then will have a little piece of x, y plane that is in here. And have to figure out what is its area? What is delta A for this guy? Well, lets see. This guy actually, you know, if my delta r and delta theta are small enough, it will almost look like a rectangle. t is rotated, but it is basically a rectangle. mean these sides, of course, are curvy, but they are short enough and it is almost straight. The area here should be this length times that length. Well, what is this length? That one is easy. t is delta r. What about that length? Well, it is not delta theta. t is something slightly different. t is a piece of a circle of radius r corresponding to angle delta theta, so it is r delta theta. o, times r delta theta. That means now, even if we shrink things and take smaller and smaller regions, dA is going to be r dr d theta. That is an important thing to remember. When you integrate in polar coordinates, you just set up your bounds in terms of r and theta, but you replace dA by r dr d theta, not just dr d theta. And then, of course, we have some function that we are integrating. Lets say that call that thing f then it is the same f that put up here. oncretely, how do do it here? Well, my function f was given as one minus x squared minus y squared. And would like to switch that to polar coordinates. want to put r and theta in there. Well, have formulas for x and y in polar coordinates so could just replace x squared by r squared cosine squared theta, y squared by r squared sine squared theta. And that works just fine. But maybe you can observe that this is x squared plus y squared. t is just the square of a distance from the origin, so that is just r squared. That is a useful thing. You dont strictly need it, but it is much faster if you see this right away. t saves you writing down a sine and a cosine. Now we just end up with the integral from zero to pi over two, integral from zero to one of one minus r squared r dr d theta. Now, if want to compute this integral, so lets first do the inner integral. f integrate r minus r cubed, will get r squared over two minus r squared over four between zero and one. And then will integrate d theta. What is this equal to? Well, for r equals one you get onehalf minus onequarter, which is going to be just onequarter. And when you plug in zero you get zero. o, it is the integral from zero to pi over two of onequarter d theta. And that just integrates to onequarter times pi over two, which is pi over eight. That is a lot easier than the way we did it yesterday. Well, here we were lucky. mean usually you will switch to polar coordinates either because the region is easier to set up. Here it is indeed easier to set up because the bounds became very simple. We dont have that square root of one minus x squared anymore. Or because the integrant becomes much simpler. Here our function, well, it is not very complicated in x, y coordinates, but it is even simpler in r theta coordinates. Here we were very lucky. n general, there is maybe a trade off. aybe it will be easier to set up bounds but maybe the function will become harder because it will have all these sines and cosines in it. f our function had been just x, x is very easy in x, y coordinates. Here it becomes r cosine theta. That means you will have a little bit of trig to do in the integral. Not a very big one, not a very complicated integral, but imagine it could get potentially much harder. Anyway, that is double integrals in polar coordinates. And the way you set up the bounds in general, well, in 99% of the cases you will integrate over r first. What you will do is you will look for a given theta what are the bounds of r to be in the region. What is the portion of my ray that is in the given region? And then you will put bounds for theta. But conceptually it is the same as before. nstead of slicing horizontally or vertically, we slice radially. We will do more examples in a bit. Any questions about this or the general method? Yes? That is a very good question. Why do measure the length inside instead of outside? Which one do want? This one. Here said this side is r delta theta. could have said, actually, r delta theta is the length here. Here it is slightly more, r plus delta r times delta theta. But, if delta r is very small compared to r, then that is almost the same thing. And this is an approximation anyway. took this one because it gives me the simpler formula. f you take the limit as delta r turns to zero then the two things become the same anyway. The length, whether you put r or r plus delta r in here, doesnt matter anymore. f you imagine that this guy is infinitely small then, really, the lengths become the same. We will also see another proof of this formula, using changes of variables, next week. But, mean, hopefully this is at least slightly convincing. ore questions? No. OK. Lets see. We have seen how to compute double integrals. have to tell you what they are good for as well. The definition we saw yesterday and the motivation was in terms of finding volumes, but that is not going to be our main preoccupation. Because finding volumes is fun but that is not all there is to life. mean, you are doing single integrals. When you do single integrals it is usually not to find the area of some region of a plane. t is for something else usually. The way we actually think of the double integral is really as summing the values of a function all around this region. We can use that to get information about maybe the region or about the average value of a function in that region and so on. Lets think about various uses of double integrals. The first one that will mention is actually something you thought maybe you could do with a single integral, but it is useful very often to do it as a double integral. t is to find the area of a given region r. give you some region in the plane and you want to know just its area. n various cases, you could set this up as a single integral, but often it could be useful to set it up as a double integral. How do you express the area as a double integral? Well, the area of this region is the sum of the areas of all the little pieces. t means you want to sum one dA of the entire region. The area R is the double integral over R of a function one. One way to think about it, if you are really still attached to the idea of double integral as a volume, what this measures is the volume below the graph of a function one. The graph of a function one is just a horizontal plane at height one. What you would be measuring is the volume of a prism with base r and height one. And the volume of that would be, of course, base times height. t would just be the area of r again. But we dont actually need to think about it that way. Really, what we are doing is summing dA over the entire region. A related thing we can do, imagine that, actually, this is some physical object. mean, it has to be a flat object because we are just dealing with things in the plane so far. But you have a flat metal plate or something and you would like to know its mass. Well, its mass is the sum of the masses of every single little piece. You would get that by integrating the density. The density for a flat object would be the mass per unit area. o, you can get the mass of a flat object with density. Lets use delta for density, which is the mass per unit area. Each little piece of your object will have a mass, which will be just the density, times its area for each small piece. And you will get the total mass by summing these things. The mass will be the double integral of the density times the area element. Now, if it has constant density, if it is always the same material then, of course, you could just take the density out and you will get density times the total area if you know that it is always the same material. But if, actually, it has varying density maybe because it is some metallic thing with various metals or with varying thickness or something then you can still get the mass by integrating the density. Of course, looking at flat objects might be a little bit strange. That is because we are only doing double integrals so far. n a few weeks, we will be triple integrals. And then we will be able to do solids in space, but one thing at a time. Another useful application is to find the average value of some quantity in a region. What does it mean to take the average value of some function f in this region r? Well, you know what the average of a finite set of data is. For example, if asked you to compute your average score on 18.02 problem sets, you would just take the scores, add them and divide by the number of problem sets. What if there are infinitely many things? ay ask you to find the average temperature in this room. Well, you would have to measure the temperature everywhere. And then add all of these together and divide by the number of data points. But, depending on how careful you are, actually, there are potentially infinitely many points to look at. The mathematical way to define the average of a continuous set of data is that you actually integrate the function over the entire set of data, and then you divide by the size of the sample, which is just the area of the region. n fact, the average of f, the notation we will use usually for that is f with a bar on top to tell us it is the average f. We say we will take the integral of f and we will divide by the area of the region. You can really think of it as the sum of the values of f everywhere divided by the number of points everywhere. And so that is an average where everything is, actually, equally likely. That is a uniform average where all the points on the region, all the little points of the region are equally likely. But maybe if want to do, say, an average of some solid with variable density or if you want to somehow give more importance to certain parts than to others then you can actually do a weighted average. What is a weighted average? Well, in the case of taking the average your problem sets, if tell you problem set one is worth twice as much as the others, then you would count twice that score in the sum and then you would count it as two, of course, when you divide. The weighted average is the sum of the values, but each weighted by a certain coefficient. And then you will divide by the sum of the weight. t is a bit the same idea as when we replace area by some mass that tells you how important a given piece. We will actually have a density. Lets call it delta again. We will see what we divide by, but what we will take is the integral of a function times the density times the area element. Because this would correspond to the mass element telling us how to weight the various points of our region. And then we would divide by the total weight, which is the mass of a region, as defined up there. f a density is uniform then, of course, the density gets out and you can simplify and reduce to that if all the points are equally likely. Why is that important? Well, that is important for various applications. But one that you might have seen in physics, we care about maybe where is the center of mass of a given object? The center of mass is basically a point that you would say is right in the middle of the object. But, of course, if the object has a very strange shape or if somehow part of it is heavier than the rest then that takes a very different meaning. trictly speaking, the center of mass of a solid is the point where you would have to concentrate all the mass if you wanted it to behave equivalently from a point of view of mechanics, if you are trying to do translations of that object. f you are going to push that object that would be really where the equivalent point mass would lie. The other way to think about it, if had a flat object then the center of mass would basically be the point where would need to hold it so it is perfectly balanced. And, of course, cannot do this. Well, you get the idea. And the center of mass of this eraser is somewhere in the middle. And so, in principle, that is where would have to put my finger for it to stay. Well, it doesnt work. But that is where the center of mass should be. think it should be in the middle. aybe shouldnt call this three. should call this 2a, because it is really a special case of the average value. How do we find the center of mass of a flat object with density delta. f you have your object in the x, y plane then its center of mass will be at positions that are actually just the coordinates of a center of mass, will just be weighted averages of x and y on the solid. o, the center of mass will be a position that will call x bar, y bar. And these are really just the averages, the average values of x and of y in the solid. Just to give you the formulas again, x bar would be one over the mass times the double integral of x times density dA. And the same thing with y. y bar is the weighted average of a y coordinate in your region. You see, if you take a region that is symmetric and has uniform density that will just give you the center of the region. But if the region has a strange shape or if a density is not homogeneous, if parts of it are heavier then you will get whatever the weighted average will be. And that will be the point where this thing would be balanced if you were trying to balance it on a pole or on your finger. Any questions so far? Yes. No. Here didnt set this up as a iterated integral yet. The function that am integrating is x times delta where density will be given to me maybe as a function of x and y. And then will integrate this dA. And dA could mean dx over dy, it could mean dy over dx, it could be mean r dr d theta. will choose how to set it up depending maybe on the shape of the region. f my solid is actually just going to be round then might want to use polar coordinates. f it is a square, might want to use x, y coordinates. f it is more complicated, well, will choose depending on how feel about it. Yes? Delta is the density. n general, it is a function of x and y. f you imagine that your solid is not homogenous then its density will depend on which piece of it you are looking at. Of course, to compute this, you need to know the density. f you have a problem asking you to find the center of mass of something and you have no information about the density, assume it is uniform. Take the density to be a constant. Even take it to be a one. That is even easier. mean it is a general fact of math. We dont care about units. f density is constant, we might as well take it to be one. That just means our mass unit becomes the area unit. Yes? That is a good question. No, dont think we could actually find the center of mass in polar coordinates by finding the average of R or the average of theta. For example, take a disk center at the origin, well, the center of mass should be at the origin. But the average of R is certainly not zero because R is positive everywhere. o, that doesnt work. You cannot get the polar coordinates of a center of mass just by taking the average of R and the average of theta. By the way, what is the average of theta? f you take theta to from zero to 2pi, the average theta will be pi. f you take it to go from minus pi to pi, the average theta will be zero. o, there is a problem there. That actually just doesnt work, so we really have to compute x bar and y bar. But still we could set this up and then switch to polar coordinates to evaluate this integral. But we still would be computing the average values of x and y. We are basically reexploring mechanics and motion of solids here. The next thing is moment of inertia. Just to remind you or in case you somehow havent seen it in physics yet, the moment of inertia is basically to rotation of a solid where the mass is to translation. n the following sense, the mass of a solid is what makes it hard to push it. How hard it is to throw something is related to its mass. How hard it is to spin something, on the other hand, is given by its moment of inertia. aybe should write this down. ass is how hard it is to impart a translation motion to a solid. am using fancy words today. And the moment of inertia The difference with a mass is that the moment of inertia is defined about some axis. You choose an axis. Then you would try to measure how hard it is to spin your object around that axis. For example, you can try to measure how hard it is to spin this sheet of paper about an axis that is in the center of it. We would try to spin it light that and see how much effort would have to make. Well, for a sheet of paper not very much. That would measure the same thing but it would be rotation motion about that axis. aybe some of you know the definition but am going to try to derive it again. am sorry but it wont be as quite as detailed as the way you have probably seen it in physics, but am not trying to replace your physics teachers. am sure they are doing a great job. What is the idea for the definition to find a formula for moment of inertia? The idea is to think about kinetic energy. Kinetic energy is really when you push something or when you try to make it move and you have to put some inertia to it. Then it has kinetic energy. And then, if you have the right device, you can convert back that kinetic energy into something else. f you try to look at the kinetic energy of a point mass, so you have something with mass m going at the velocity v, well, that will be onehalf of a mass times the square of the speed. hope you have all seen that formula some time before. Now, lets say instead of just trying to push this mass, am going to make it spin around something. nstead of just somewhere, maybe will have the origin, and am trying to make it go around the origin in a circle at a certain angular velocity. For a mass m at distance r, lets call r this distance. And angular velocity, lets call the angular velocity omega. think that is what physicists call it. Remember angular velocity is just the rate of the change of the angle over time. t is d theta dt, if you want. Well, what is the kinetic energy now? Well, first we have to find out what the speed is. What is the speed? Well, if we are going on a circle of radius r at angular velocity omega that means that in unit time we rotate by omega and we go by a distance of r times omega. The actual speed is the radius times angular velocity. And so the kinetic energy is onehalf mv squared, which is onehalf m r squared omega squared. And so, by similarity with that formula, the coefficient of v squared is the mass, and here we will say the coefficient of omega squared, so this thing is the moment of inertia. That is how we define moment of inertia. Now, that is only for a point mass. And it is kind of fun to spin just a small bowl, but maybe you would like to spin actually a larger solid and try to define this moment of inertia. Well, the moment inertia of a solid will be just the sum of the moments of inertia of all the little pieces. What we will do is just cut our solid into little chunks and will sum this thing for each little piece. For a solid with density delta, each little piece has mass which is the density times the amount of area. This is equal actually. And the moment of inertia of that small portion of a solid will be delta m, the small mass, times r squared, the square of a distance to the center of the axis along which am spinning. That means if sum these things together, well, it has moment of inertia delta m times r squared, which is r squared times the density times delta A. And so will be summing these things together. And so, the moment of inertia about the origin will be the double integral of r squared times density times dA. The final formula for the moment of inertia about the origin is the double integral of a region of r squared density dA. f you are going to do it in x, y coordinates, of course, r squared becomes x squared plus y squared, it is the square of the distance from the origin. When you integrate this, that tells you how hard it is to spin that solid about the origin. The motion that we try to do We keep this fixed and then we just rotate around the origin. orry. That is a pretty bad picture, but hopefully you know what mean. And the name we use for that is 0. And then the rotational kinetic energy is onehalf times this moment of inertia times the square of the angular velocity. o that shows as that this replaces the mass for rotation motions. OK. What about other kinds of rotations? n particular, we have been rotating things about just a point in the plane. What you could imagine also is instead you have your solid. What have done so far is have skewered it this way, and am rotating around the axis. nstead, could skewer it through, say, the horizontal axis. And then could try to spin about the horizontal axis so then it would rotate in space in that direction like that. Lets say we do rotation about the xaxis. Well, the idea would still be the same. The moment of inertia for any small piece of a solid would be its mass element times the square of a distance to the x axes because that will be the radius of a trajectory. f you take this point here, it is going to go in a circle like that centered on the xaxis. o the radius will just be this distance here. Well, what is this distance? t is just y, or maybe absolute value of y. Distance to xaxis is absolute value of y. What we actually care about is the square of a distance, so it will just be y squared. The moment of inertia about the xaxis is going to be obtained by integrating y squared times the mass element. t is slightly strange but have y in inertia about the xaxis. But, if you think about it, y tells me how far am from the xaxis, so how hard it will be to spin around the xaxis. And could do the same about any axis that want. Just would have to sum the square of a distance to the axis of rotation. aybe should do an example. Yes? ame thing as above, distance to the xaxis, because that is what we care about. For the moment of inertia, we want the square of a distance to the axis of rotation. Lets do an example. Lets try to figure out if we have just a uniform disk how hard it is to spin it around its center. That shouldnt be very hard to figure out. ay that we have a disk of radius a and we want to rotate it about its center. And lets say that it is of uniform density. And lets take just the density to be a one so that we dont really care about the density. What is the moment of inertia of that? Well, we have to integrate of our disk r squared times the density, which is one, times dA. What is r squared? You have here to resist the urge to say the radius is just a. We know the radius is a. No, it is not a because we are looking at rotation of any point inside this disk. And, when you are inside the disk, the distance to the origin is not a. t is less than a. t is actually anything between zero and a. Just to point out a pitfall, r here is really a function on this disk. And we are going to integrate this function. Dont plug r equals a just yet. What coordinates do we use to compute this integral? They are probably polar coordinates, unless you want a repeat of what happened already with x and y. That will tell us we want to integrate r squared time r dr d theta. And the bounds for r, well, r will go from zero to a. No matter which direction go from the origin, if fixed it, r goes from zero to r equals a. The part of this ray that lives inside the disk is always from zero to a. And theta goes from, well, zero to 2 pi for example. And now you can compute this integral. Well, will let you figure it out. But the inner integral becomes a to the four over four and the outer multiplies things by 2pi, so you get pi a to the four over two. OK. That is how hard it is to spin this disk. Now, what about instead of spinning it about the center we decided to spin it about a point on a second point. For example, think of a Frisbee. A Frisbee has this rim so you can actually try to make it rotate around the point on the circumference by holding it near the rim and spinning it there. How much harder is that than around the center? Well, we will try to compute now the moment of inertia about this point. We have two options. One is we keep the system of coordinates centers here. But then the formula for distance to this point becomes harder. The other option, which is the one will choose, is to change the coordinate so that this point become the origin. Lets do that. About a point on the circumference, what would have to do maybe is set up my region like that. have moved the origin so that it is on the circumference of a disk, and will again try to find the moment of inertia of this disk about the origin. t is still, for the the double integral of r squared dA. But now want to find out how to set up the integral. could try to use x, y coordinates and it would work. Or can use polar coordinates, and it works a little bit better that way. But both are doable. Lets say do it this way. have to figure out how to set up my bounds. What are the bounds for r? Well, if fix a value for theta, which means chose an angle here, now am shooting a ray from the origin in that direction. enter my region at r equals zero. That hasnt changed. The question is where do exit the region? What is that distance? aybe you have seen it in recitation, maybe not. Lets see. Actually, should have written down the radius of a circle is a. o this distance here is 2a. f you draw this segment in here, you know that here you have a right angle. You have a right triangle. The hypotenuse here has length 2a. This angle is theta. Well, this length is 2a cosine theta. The polar coordinates equation of this circle passing through the origin is r equals 2a cosine theta. o, r will go from zero to 2a cosine theta. That is the distance here. Now, what are the bounds for theta? t is not quite zero to 2pi because, actually, you see in this direction, if shoot a ray in this direction will never meet my region. We have to actually think a bit more. Well, the directions in which will actually hit my circle are all the directions in the right half of a plane. mean, of course, if shoot very close to the axis, you might think, oh, wont be in there. But, actually, that is not true because here the circle is tangent to the axis. No matter which direction take, will still have a little tiny piece. The angle actually goes from minus pi over two to pi over two. f you compute that you will get, well, the inner integral will be r to the four over four between zero and 2a cosine theta, which will turn out to be 4a to the four cosine to the four theta. And now you will integrate that for minus pi over two to pi over two. And that is, again, the evil integral that we had yesterday. Either we remember the method from yesterday or we remember from yesterday that actually there are formulas in the notes to help you. On homework, you can use these formulas. n the notes at the beginning of section 3b there are formulas for these particular kinds of integrals. And that will end up being threehalves of pi a to the four. n case you wanted to know, it is three times harder to spin a Frisbee about a point on a circumference than around the center. We got three times the moment of inertia about the center. OK. That is it. Have a nice weekend.","n fact, the average of f, the notation we will use usually for that is f with a bar on top to tell us it is the average f. We say we will take the integral of f and we will divide by the area of the region. What you will do is you will look for a given theta what are the bounds of r to be in the region. When we have a region in the x, y plane and we have a function of x and y, we are defining the double integral of f over this region by taking basically the sum of the values of a function everywhere in here times the area element. The mathematical way to define the average of a continuous set of data is that you actually integrate the function over the entire set of data, and then you divide by the size of the sample, which is just the area of the region. f you have your object in the x, y plane then its center of mass will be at positions that are actually just the coordinates of a center of mass, will just be weighted averages of x and y on the solid. For example, you can try to measure how hard it is to spin this sheet of paper about an axis that is in the center of it. have moved the origin so that it is on the circumference of a disk, and will again try to find the moment of inertia of this disk about the origin. You have here to resist the urge to say the radius is just a. We know the radius is a. No, it is not a because we are looking at rotation of any point inside this disk. And the moment of inertia of that small portion of a solid will be delta m, the small mass, times r squared, the square of a distance to the center of the axis along which am spinning. And so, by similarity with that formula, the coefficient of v squared is the mass, and here we will say the coefficient of omega squared, so this thing is the moment of inertia. The center of mass is basically a point that you would say is right in the middle of the object. We will see what we divide by, but what we will take is the integral of a function times the density times the area element. What is the moment of inertia of that? f you try to look at the kinetic energy of a point mass, so you have something with mass m going at the velocity v, well, that will be onehalf of a mass times the square of the speed. The final formula for the moment of inertia about the origin is the double integral of a region of r squared density dA. f you are going to do it in x, y coordinates, of course, r squared becomes x squared plus y squared, it is the square of the distance from the origin. And so, the moment of inertia about the origin will be the double integral of r squared times density times dA. trictly speaking, the center of mass of a solid is the point where you would have to concentrate all the mass if you wanted it to behave equivalently from a point of view of mechanics, if you are trying to do translations of that object. The moment of inertia for any small piece of a solid would be its mass element times the square of a distance to the x axes because that will be the radius of a trajectory. For the moment of inertia, we want the square of a distance to the axis of rotation. And, when you are inside the disk, the distance to the origin is not a. t is less than a. t is actually anything between zero and a. Just to point out a pitfall, r here is really a function on this disk. You cannot get the polar coordinates of a center of mass just by taking the average of R and the average of theta. f you have a problem asking you to find the center of mass of something and you have no information about the density, assume it is uniform. You see, if you take a region that is symmetric and has uniform density that will just give you the center of the region. The mass will be the double integral of the density times the area element. That is it. By the way, what is the average of theta? The function that am integrating is x times delta where density will be given to me maybe as a function of x and y. And then will integrate this dA. No, dont think we could actually find the center of mass in polar coordinates by finding the average of R or the average of theta. And the definition, actually, is we split the region into lots of tiny little pieces, we multiply the value of a function at the point times the area of a little piece and we sum that everywhere. Well, the area of this region is the sum of the areas of all the little pieces. And lets take just the density to be a one so that we dont really care about the density. When you integrate this, that tells you how hard it is to spin that solid about the origin. We can use that to get information about maybe the region or about the average value of a function in that region and so on. t is just the square of a distance from the origin, so that is just r squared. But one that you might have seen in physics, we care about maybe where is the center of mass of a given object? The way we actually think of the double integral is really as summing the values of a function all around this region. One way to think about it, if you are really still attached to the idea of double integral as a volume, what this measures is the volume below the graph of a function one. Now, if it has constant density, if it is always the same material then, of course, you could just take the density out and you will get density times the total area if you know that it is always the same material. Just to remind you or in case you somehow havent seen it in physics yet, the moment of inertia is basically to rotation of a solid where the mass is to translation. t is just y, or maybe absolute value of y. Distance to xaxis is absolute value of y. What we actually care about is the square of a distance, so it will just be y squared. t is to find the area of a given region r. give you some region in the plane and you want to know just its area. Just would have to sum the square of a distance to the axis of rotation. What it means is we are cutting our region into little pieces that are the elementary, you know, what corresponds to a small rectangle in the x, y coordinate system, here would be actually a little piece of circle between a given radius r and r plus delta r. And given between an angle theta and theta plus delta theta. Well, the moment inertia of a solid will be just the sum of the moments of inertia of all the little pieces. Well, we will try to compute now the moment of inertia about this point. And the moment of inertia The difference with a mass is that the moment of inertia is defined about some axis. n general, it is a function of x and y. f you imagine that your solid is not homogenous then its density will depend on which piece of it you are looking at. The first one that will mention is actually something you thought maybe you could do with a single integral, but it is useful very often to do it as a double integral. Just to give you the formulas again, x bar would be one over the mass times the double integral of x times density dA. The other way to think about it, if had a flat object then the center of mass would basically be the point where would need to hold it so it is perfectly balanced. Remember that polar coordinates are about replacing x and y as coordinates for a point on a plane by instead r, which is the distance from the origin to a point, and theta, which is the angle measured counterclockwise from the positive xaxis. What is the portion of my ray that is in the given region? Now we are going to actually integrate, in terms of the polar coordinates, r and theta. Well, we have to integrate of our disk r squared times the density, which is one, times dA. And these are really just the averages, the average values of x and of y in the solid. The area R is the double integral over R of a function one. The other option, which is the one will choose, is to change the coordinate so that this point become the origin. The moment of inertia about the xaxis is going to be obtained by integrating y squared times the mass element. That means you will have a little bit of trig to do in the integral. When you do single integrals it is usually not to find the area of some region of a plane. But that is where the center of mass should be. That is how hard it is to spin this disk. And the way you set up the bounds in general, well, in 99% of the cases you will integrate over r first. And the bounds for r, well, r will go from zero to a. No matter which direction go from the origin, if fixed it, r goes from zero to r equals a. The part of this ray that lives inside the disk is always from zero to a. And theta goes from, well, zero to 2 pi for example. t is still, for the the double integral of r squared dA. And lets say that it is of uniform density. That is the distance here. And the center of mass of this eraser is somewhere in the middle. The motion that we try to do We keep this fixed and then we just rotate around the origin. o, it is the integral from zero to pi over two of onequarter d theta. What does it mean to take the average value of some function f in this region r? And it is kind of fun to spin just a small bowl, but maybe you would like to spin actually a larger solid and try to define this moment of inertia. Well, in the case of taking the average your problem sets, if tell you problem set one is worth twice as much as the others, then you would count twice that score in the sum and then you would count it as two, of course, when you divide. About a point on the circumference, what would have to do maybe is set up my region like that.",0.1383627639412974
8,8,"Hi. n this video, were going to do standard probability calculations for normal random variables. Were given that x is standard normal with mean 0 and variance 1. And y is normal with mean one and variance 4. And were asked for a couple of probabilities. For the normal DF, we dont have a closed form expression. And so people generally tabulate values and for the standard normal case. o if we want little x equal to 3.49, we just look for 3.4 along the rows and 0.09 along the columns, and then pick the value appropriately. o for part A, were asked whats the probability that x is less than equal to 1.5? Thats exactly phi of 1.5 and we can look that up. 1.5 directly and thats 0.9332. Then were asked, whats the probability that x is less than equal to negative 1? Notice that negative values are not on this table. And the reason that is is because the standard normal is symmetric around zero. And we dont really need that. We just recognize that the area in this region is exactly the area in this region. And so thats equal to the probability that x is greater than equal to 1. This is equal to 1 minus the probability that x is less than 1. And we can put the equal sign in here because x is continuous, it doesnt matter. And so were going to get, this is equal to 1 minus phi of one. And we can look up phi of 1, which is 1.00, and thats 0.8413. OK. For part B, were asked for this distribution of y minus 1 over 2. o any linear function of a normal random variable is also normal. And you can see that by using the derived distribution for linear functions of random variables. o in this case, we only need to figure out whats the mean and the variance of this normal random variable. o the mean in this case, m going to write that as y over 2 minus 1/2. The expectation operator is linear and so thats going to be and the expectation in this case is 1, so thats going to be 0. Now the variance. For the shift, it doesnt affect the spread. And so the variance is exactly going to be the same without the minus 1/2. And for the constant, you can just pull that out and square it. And the variance of y we know is 4. And so thats 1/4 times 4, thats 1. OK. o now we know that y minus 1 over 2 is actually standard normal. Actually for any normal random variable, you can follow the same procedure. You just subtract its mean, which is 1 in this case. And divide by its standard deviation and you will get a standard normal distribution. All right, so for part we want the probability that y is between negative 1 and 1. o lets try to massage it so that we can use the standard normal table. And we already know that this is standard normal, so lets subtract both sides by negative 1. And thats equal to m going to call this standard normal z, so thats easier to write. And thats equal to negative 1 less than equal to z, less than equal to zero. o were looking for this region, 0, 1, negative 1. o thats just the probability that its less than zero minus probability that its less than negative 1. Well for a standard normal, half the mass is below zero and a half the mass is above. And so thats just going to be 0.5 directly. And for this, weve already computed this for a standard normal, which was x in our case. And that was 1 minus 0.8413. Done. o we basically calculated a few standard probabilities for normal distributions. And we did that by looking them up from the standard normal table.","This is equal to 1 minus the probability that x is less than 1. And so thats equal to the probability that x is greater than equal to 1. All right, so for part we want the probability that y is between negative 1 and 1. o lets try to massage it so that we can use the standard normal table. o for part A, were asked whats the probability that x is less than equal to 1.5? And so were going to get, this is equal to 1 minus phi of one. The expectation operator is linear and so thats going to be and the expectation in this case is 1, so thats going to be 0. o in this case, we only need to figure out whats the mean and the variance of this normal random variable. And so the variance is exactly going to be the same without the minus 1/2. And the variance of y we know is 4. And thats equal to m going to call this standard normal z, so thats easier to write. Then were asked, whats the probability that x is less than equal to negative 1?",0.3284023668639053
9,9,"have this matrix A here that want to put into reduced row echelon form. And weve done this multiple times. You just perform a bunch of row operations. But what want to show you in this video is that those row operations are equivalent to linear transformations on the column vectors of A. o let me show you by example. o if we just want to put A into reduced row echelon form, the first step that we might want to do if we wanted to zero out these entries right here, is let me do it right here is well keep our first entry the same. o for each of these column vectors, were going to keep the first entry the same. o theyre going to be 1, minus 1, minus 1. And actually, let me simultaneously construct my transformation. o m saying that my row operation m going to perform is equivalent to a linear transformation on the column vector. o its going to be a transformation thats going to take some column vector, a1, a2, and a3. ts going to take each of these and then do something to them, do something to them in a linear way. Theyll be linear transformations. o were keeping the first entry of our column vector the same. o this is just going to be a1. This is a line right here. Thats going to be a1. Now, what can we do if we want to get to reduced row echelon form? Wed want to make this equal to 0. o we would want to replace our second row with the second row plus the first row, because then these guys would turn out to be 0. o let me write that on my transformation. m going to replace the second row with the second row plus the first row. Let me write it out here. inus 1 plus 1 is 0. 2 plus minus 1 is 1. 3 plus minus 1 is 2. Now, we also want to get a 0 here. o let me replace my third row with my third row minus my first row. o m going to replace my third row with my third row minus my first row. o 1 minus 1 is 0. 1 minus minus 1 is 2. 4 minus minus 1 is 5, just like that. o you see this was just a linear transformation. And any linear transformation you could actually represent as a matrix vector product. o for example, this transformation, could represent it. To figure out its transformation matrix, so if we say that T of x is equal to, dont know, lets call it some matrix times x. We already used the matrix A. o have to pick another letter. o how do we find ? Well, we just apply the transformation to all of the column vectors, or the standard basis vectors of the identity matrix. o lets do that. o the identity matrix ll draw it really small like this the identity matrix looks like this, 1, 0, 0, 0, 1, 0, 0, 0, 1. Thats what that identity matrix looks like. To find the transformation matrix, we just apply this guy to each of the column vectors of this. o what do we get? ll do it a little bit bigger. We apply it to each of these column vectors. But we see the first row always stays the same. o the first row is always going to be the same thing. o 1, 0, 0. m essentially applying it simultaneously to each of these column vectors, saying, look, when you transform each of these column vectors, their first entry stays the same. The second entry becomes the second entry plus the first entry. o 0 plus 1 is 1. 1 plus 0 is 1. 0 plus 0 is 0. Then the third entry gets replaced with the third entry minus the first entry. o 0 minus 1 is minus 1. 0 minus 0 is 0. 1 minus 0 is 1. Now notice, when apply this transformation to the column vectors of our identity matrix, essentially just performed those same row operations that did up there. performed those exact same row operations on this identity matrix. But we know that this is actually the transformation matrix, that if we multiply it by each of these column vectors, or by each of these column vectors, were going to get these column vectors. o you can view it this way. This right here, this is equal to . This is our transformation matrix. o we could say that if we create a new matrix whose columns are times this column vector, times 1, minus 1, 1. And then the next column is times wanted to do it in that other color times this guy, minus 1, 2, 1. And then the third column is going to be times this third column vector, minus 1, 3, 4. We now know were applying this transformation, this is , times each of these column vectors. That is the matrix representation of this transformation. This guy right here will be transformed to this right here. Let me do it down here. wanted to show that stuff that had above here as well. Well, ll just draw an arrow. Thats probably the simplest thing. This matrix right here will become that matrix right there. o another way you could write it, this is equivalent to what? What is this equivalent to? When you take a matrix and you multiply it times each of the column vectors, when you transform each of the column vectors by this matrix, this is the definition of a matrixmatrix product. This is equal to our matrix ll do it in pink this is equal to our matrix , which is 1, 0, 0, 1, 1, 0, minus 1, 0, 1, times our matrix A, times 1, minus 1, 1, minus 1, 2, 1, minus 1, 3, 4. o let me make this very clear. This is our transformation matrix . This is our matrix A. And when you perform this product youre going to get this guy right over here. ll just copy and paste it. Edit, copy, and let me paste it. Youre going to get that guy just like that. Now the whole reason why m doing that is just to remind you that when we perform each of these row operations, were just multiplying. Were performing a linear transformation on each of these columns. And it is completely equivalent to just multiplying this guy by some matrix . n this case, we took the trouble of figuring out what that matrix is. But any of these row operations that weve been doing, you can always represent them by a matrix multiplication. o this leads to a very interesting idea. When you put something in reduced row echelon form, let me do it up here. Actually, lets just finish what we started with this guy. Lets put this guy in reduced row echelon form. Let me call this first . Lets call that 1. o this guy right here is equal to that first 1 times A. We already showed that thats true. Now lets perform another transformation. Lets just do another set of row operations to get us to reduced row echelon form. o lets keep our middle row the same, 0, 1, 2. And lets replace the first row with the first row plus the second row, because want to make this a 0. o 1 plus 0 is 1. Let me do it in another color. inus 1 plus 1 is 0. inus 1 plus 2 is 1. Now, want to replace the third row with, lets say the third row minus 2 times the first row. o thats 0 minus 2, times 0, is 0. 2 minus 2, times 1, is 0. 5 minus 2, times 2, is 1. 5 minus 4 is 1. Were almost there. We just have to zero out these guys right there. Lets see if we can get this into reduced row echelon form. o what is this? just performed another linear transformation. Actually, let me write this. Lets say if this was our first linear transformation, what just did is performed another linear transformation, T2. ll write it in a different notation, where you give me some vector, some column vector, x1, x2, x3. What did just do? What was the transformation that just performed? y new vector, made the top row equal to the top row plus the second row. o its x1 plus x2. kept the second row the same. And then the third row, replaced it with the third row minus 2 times the second row. That was a linear transformation we just did. And we could represent this linear transformation as being, we could say T2 applied to some vector x is equal to some transformation vector 2, times our vector x. Because if we applied this transformation matrix to each of these columns, its equivalent to multiplying this guy by this transformation matrix. o you could say that this guy right here we havent figured out what this is, but think you get the idea this matrix right here is going to be equal to this guy. ts going to be equal to 2 times this guy. What is this guy right here? Well, this guy is equal to 1 times A. ts going to be 2 times 1, times A. Fair enough. And you could have gotten straight here if you just multiplied 2 times 1. This could be some other matrix. f you just multiplied it by A, youd go straight from there to there. Fair enough. Now, we still havent gotten this guy into reduced row echelon form. o lets try to get there. ve run out of space below him, so m going to have to go up. o lets go upwards. What want to do is, m going to keep the third row the same, 0, 0, 1. Let me replace the second row with the second row minus 2 times the third row. o well get a 0, well get a 1 minus 2, times 0, and well get a 2 minus 2, times 1. o thats a 0. Lets replaced the first row with the first row minus the third row. o 1 minus 0 is 1. 0 minus 0 is 0. 1 minus 1 is 0, just like that. Lets just actually write what our transformation was. Lets call it T3. ll do it in purple. T3 is the transformation of some vector x let me write it like this of some vector x1, x2, x3. What did we do? We replaced the first row with the first row minus the third row, x1 minus x3. We replaced the second row with the second row minus 2 times the third row. o its x2 minus 2 times x3. Then the third row just stayed the same. o obviously, this could also be represented. T3 of x could be equal to some other transformation matrix, 3 times x. o this transformation, when you multiply it to each of these columns, is equivalent to multiplying this guy times this transformation matrix, which we havent found yet. We can write it. o this is going to be equal to 3 times this matrix right here, which is 2, 1, A. And what do we have here? We got the identity matrix. We put it in reduced row echelon form. We got the identity matrix. We already know from previous videos the reduced row echelon form of something is the identity matrix. Then we are dealing with an invertible transformation, or an invertible matrix. Because this obviously could be the transformation for some transformation. Lets just call this transformation, dont know, did already use T? Lets just call it Tnaught for our transformation applied to some vector x, that might be equal to Ax. o we know that this is invertible. We put it in reduced row echelon form. We put its transformation matrix in reduced row echelon form. And we got the identity matrix. o that tells us that this is invertible. But something even more interesting happened. We got here by performing some row operations. And we said those row operations were completely equivalent to multiplying this guy right here by multiplying our original transformation matrix by a series of transformation matrices that represent our row operations. And when we multiplied all this, this was equal to the identity matrix. Now, in the last video we said that the inverse matrix, so if this is Tnaught, Tnaught inverse could be represented its also a linear transformation t can be represented by some inverse matrix that we just called A inverse times x. And we saw that the inverse transformation matrix times our transformation matrix is equal to the identity matrix. We saw this last time. We proved this to you. Now, something very interesting here. We have a series of matrix products times this guy, times this guy, that also got me the identity matrix. o this guy right here, this series of matrix products, this must be the same thing as my inverse matrix, as my inverse transformation matrix. And so we could actually calculate it if we wanted to. Just like we did, we actually figured out what 1 was. We did it down here. We could do a similar operation to figure out what 2 was, 3 was, and then multiply them all out. We would have actually constructed A inverse. guess, something more interesting we could do instead of doing that, what if we applied these same matrix products to the identity matrix. o the whole time we did here, when we did our first row operation. o we have here, we have the matrix A. Lets say we have an identity matrix on the right. Lets call that , right there. Now, our first linear transformation we did we saw that right here that was equivalent to multiplying 1 times A. The first set of row operations was this. t got us here. Now, if we perform that same set of row operations on the identity matrix, what are we going to get? Were going to get the matrix 1. 1 times the identity matrix is just 1. All of the columns of anything times the identity times the standard basis columns, itll just be equal to itself. Youll just be left with that 1. This is 1 times . Thats just 1. Fair enough. Now, you performed your next row operation and you ended up with 2 times 1, times A. Now if you performed that same row operation on this guy right there, what would you have? You would have 2 times 1, times the identity matrix. Now, our last row operation we represented with the matrix product 3. Were multiplying it by the transformation matrix 3. o if you did that, you have 3, 2, 1 A. But if you perform the same exact row operations on this guy right here, you have 3, 2, 1, times the identity matrix. Now when you did this, when you performed these row operations here, this got you to the identity matrix. Well, what are these going to get you to? When you just performed the same exact row operations you performed on A to get to the identity matrix, if you performed those same exact row operations on the identity matrix, what do you get? You get this guy right here. Anything times that identity matrix is going to be equal to itself. o what is that right there? That is A inverse. o we have a generalized way of figuring out the inverse for transformation matrix. What can do is, lets say have some transformation matrix A. can set up an augmented matrix where put the identity matrix right there, just like that, and perform a bunch of row operations. And you could represent them as matrix products. But you perform a bunch of row operations on all of them. You perform the same operations you perform on A as you would do on the identity matrix. By the time you have A as an identity matrix, you have A in reduced row echelon form. By the time A is like that, your identity matrix, having performed the same exact operations on it, it is going to be transformed into As inverse. This is a very useful tool for solving actual inverses. Now, ve explained the theoretical reason why this works. n the next video well actually solve this. aybe well do it for the example that started off with in this video.","o this is going to be equal to 3 times this matrix right here, which is 2, 1, A. And what do we have here? o you could say that this guy right here we havent figured out what this is, but think you get the idea this matrix right here is going to be equal to this guy. o if we just want to put A into reduced row echelon form, the first step that we might want to do if we wanted to zero out these entries right here, is let me do it right here is well keep our first entry the same. This is equal to our matrix ll do it in pink this is equal to our matrix , which is 1, 0, 0, 1, 1, 0, minus 1, 0, 1, times our matrix A, times 1, minus 1, 1, minus 1, 2, 1, minus 1, 3, 4. o let me make this very clear. Were multiplying it by the transformation matrix 3. o if you did that, you have 3, 2, 1 A. But if you perform the same exact row operations on this guy right here, you have 3, 2, 1, times the identity matrix. T3 of x could be equal to some other transformation matrix, 3 times x. o this transformation, when you multiply it to each of these columns, is equivalent to multiplying this guy times this transformation matrix, which we havent found yet. To find the transformation matrix, we just apply this guy to each of the column vectors of this. And we could represent this linear transformation as being, we could say T2 applied to some vector x is equal to some transformation vector 2, times our vector x. Because if we applied this transformation matrix to each of these columns, its equivalent to multiplying this guy by this transformation matrix. 1 times the identity matrix is just 1. Now, our first linear transformation we did we saw that right here that was equivalent to multiplying 1 times A. The first set of row operations was this. Now, in the last video we said that the inverse matrix, so if this is Tnaught, Tnaught inverse could be represented its also a linear transformation t can be represented by some inverse matrix that we just called A inverse times x. And we saw that the inverse transformation matrix times our transformation matrix is equal to the identity matrix. Now, if we perform that same set of row operations on the identity matrix, what are we going to get? What want to do is, m going to keep the third row the same, 0, 0, 1. And lets replace the first row with the first row plus the second row, because want to make this a 0. o 1 plus 0 is 1. To figure out its transformation matrix, so if we say that T of x is equal to, dont know, lets call it some matrix times x. We already used the matrix A. o have to pick another letter. And then the next column is times wanted to do it in that other color times this guy, minus 1, 2, 1. What can do is, lets say have some transformation matrix A. can set up an augmented matrix where put the identity matrix right there, just like that, and perform a bunch of row operations. Wed want to make this equal to 0. o we would want to replace our second row with the second row plus the first row, because then these guys would turn out to be 0. o let me write that on my transformation. Now, want to replace the third row with, lets say the third row minus 2 times the first row. We have a series of matrix products times this guy, times this guy, that also got me the identity matrix. This right here, this is equal to . Now when you did this, when you performed these row operations here, this got you to the identity matrix. This is our matrix A. And when you perform this product youre going to get this guy right over here. That is the matrix representation of this transformation. When you just performed the same exact row operations you performed on A to get to the identity matrix, if you performed those same exact row operations on the identity matrix, what do you get? Now notice, when apply this transformation to the column vectors of our identity matrix, essentially just performed those same row operations that did up there. And then the third column is going to be times this third column vector, minus 1, 3, 4. This is our transformation matrix. This is our transformation matrix . This is 1 times . And then the third row, replaced it with the third row minus 2 times the second row. We replaced the first row with the first row minus the third row, x1 minus x3. Lets call that 1. o this guy right here is equal to that first 1 times A. We already showed that thats true. Well, this guy is equal to 1 times A. ts going to be 2 times 1, times A. Fair enough. By the time A is like that, your identity matrix, having performed the same exact operations on it, it is going to be transformed into As inverse. But we know that this is actually the transformation matrix, that if we multiply it by each of these column vectors, or by each of these column vectors, were going to get these column vectors. We replaced the second row with the second row minus 2 times the third row. When you take a matrix and you multiply it times each of the column vectors, when you transform each of the column vectors by this matrix, this is the definition of a matrixmatrix product. But what want to show you in this video is that those row operations are equivalent to linear transformations on the column vectors of A. o let me show you by example. 5 minus 2, times 2, is 1. 2 minus 2, times 1, is 0.",0.2873880082701585
10,10,"ANNONER: The following program is brought to you by altech. YAER ABOTAFA: Welcome back. Last time, we introduced some important concepts in our theoretical development. And the first concept was dichotomies. And the idea is that there is an input space behind this opaque sheet, and there is a hypothesis thats separating red regions from blue regions. But we dont get to see that. What we get to see are just the data points, holes in that sheet if you will. And there could be very exciting stuff happening behind that sheet, and all you get to see is when the boundary crosses one of these points, and a blue point turns red or viceversa. o if you think of the purpose for the dichotomies, we had a problem with counting the number of hypotheses, because we end up with a very large number. But if you restrict your attention to dichotomies, which are the hypotheses restricted to a finite set of points, the blue and red points here, then you dont have to count everything that is happening outside. You only count it as different when something different happens only on those points. o a dichotomy is a minihypothesis, if you will. And it counts the hypotheses only on the finite set of points. This resulted in a definition that parallels the number of hypotheses, which is the number of dichotomies in this case. o we define the growth function. The growth function is you pick the points x_1 up to x_N. You pick them wisely, with a view to maximizing the dichotomies, such that the number you get will be more than any number another person gets with N points. Thats the purpose. o you take your hypothesis set, which applies to the entire input space, and then apply it only to x_1 up to x_N. This will result in a pattern of +1 or 1s, N of them. And as you vary the hypothesis within this set, you will get another pattern, another pattern, another pattern. o you will get a set of different patterns that are all the dichotomies that can be generated by this hypothesis set, on this set of points. And the number of those guys is what we are interested in. t will play the role of the number of hypotheses. And that is the growth function. Now in principle, the growth function can be 2 to the N. You may be in an input space and a hypothesis set, such that you can generate any pattern you want. However, in most of the cases, the restriction of using hypotheses coming from H will result in missing out on some of the patterns. ome patterns will simply be impossible. And that led us to the idea of a break point. For the case of a perceptron in two dimensions, which is the case we studied, we realize that for four points, there will always be a pattern that cannot be realized by a perceptron. There is no way to have a line come here, and separate those red points from the blue points. And any choice of four points will also result in missing patterns. Therefore, the number k equals 4, in this case, is defined as a break point for the perceptrons. And our theoretical goal is to take that single number, which is the break point, and be able to characterize the entire growth function for every N. And therefore, be able to characterize the generalization, as we will see. We then talked about the maximum number of dichotomies, under the constraint that there is a break point. And we had an illustrative example to tell you that, when you tell me that you cannot get all patterns on any in this case, two points that is a very strong restriction on the number of dichotomies you can get on a larger number of points. o this is the simplest case. f you take any two columns, you cannot get all four patterns. Thats by decree. m telling you that the hypothesis has a break point of 2. And then m asking you, under those constraints, how many lines you can get, how many different patterns you can get. And you go and you add them up, and you end up in this case with only four. o you lost half of them. And you can see that if we have 10 points, and you apply the same restriction, there will be so many lost, because now the restriction applies to any pair of points. Now, if you look at this schedule, this does not appeal to any particulars of the hypothesis set or the input space, other than the fact that the break point is 2. could be in a situation, where the hypothesis set cannot generate some of these guys for other reasons. But here, m abstracting only a hypothesis set and an input space. dont want to bother to know more about them. Just tell me that they have a break point, and m trying to find under that single constraint, how many can possibly have? And already have, by that combinatorial constraint, a restriction which is strong enough to get me a goodenough result. Thats good, because now dont have to worry about every hypothesis set, and every input space, you give me. just ask you: what is the break point? And m able to make a statement about the growth function not being bigger than something. That is the key. We move on to todays lecture, and the title is, properly, the Theory of Generalization. ts very theoretical. And todays lecture is the most theoretical of the entire course. o fasten your seat belts, and lets start! We have two items of business. The first one is to show that the growth function, with a break point, is indeed polynomial. The second one is to show that we can actually take that notion, the growth function, and put it in place of , the number of hypotheses, in Hoeffdings inequality. o basically, we are saying in the first part: its worthwhile to study the growth function. Because being polynomial will be very advantageous. And then, the second one is: we can actually do something good with it. We can do the replacement. These are the only two items. Lets start. We are going to bound the growth function by a polynomial. And just wanted to point some of the aspects of that. f say m_H of N is polynomial, its not that am going to actually solve for the growth function, and show that it is this particular polynomial, and the coefficients. All am saying is that it is really just bounded above by a polynomial. dont have to get the particulars of m_H of N, the growth function. am going to just tell you that this is less than something, less than something, less than a polynomial. Thats all need, because eventually am going to put this in the Hoeffding inequality. And as long as its bounded by a polynomial, am in business. Because the negative exponential will kill it, as we discussed, and we are OK. o we can be a bit loose, which is very good in theory. Because now you leave a lot of artifacts that you dont need to study. And just talk about the upper bound in the general case, and still get what you want to get. The key quantity we are going to use, which is a purely combinatorial quantity, we are going to call it B of N and k. This is exactly the quantity we were seeking in the puzzle. give you N points. tell you that k is a break point, and ask you: how many different patterns can you get under those conditions? n that case, we had three points and the break point was 2. And we answered this question by construction. We played around with the patterns until we got it, and then we said its 4. Now, as develop the theory, the puzzle will come up in one of the results. would like you to keep an eye and say which slide, and which particular part of the slide, addresses the very specific puzzle we talked about. The definition here is the maximum number of dichotomies on N points, such that they have a break point k. o this is N and this is k. And the good thing here is that didnt appeal to any hypothesis set, or any input space. This is a purely combinatorial quantity. And because its a combinatorial quantity, am going to be able to pin it down exactly, as it turns out. And now, when pin it down exactly, you go and you find the fanciest input space, and the fanciest hypothesis set. You pick the break point for that, and you use that here, ridding the problem of all the other aspects, and you still are able to make an upper bound statement. You can say that the growth function, for the particular case you talked about, is less than or equal to and just go to this combinatorial quantity. The plan is clear. o lets look at the bound for B of N, k. And we are going to do it recursively. ts a very cute argument, and am going to build it very carefully. o want your attention. onsider the following table. Very much like the puzzle, we are going to list x_1, x_2, up to x_N. N points, which used to be three points. And am going to try to put as many patterns as can, under a constraint that there is a break point. o will be putting the first pattern this way, and the second pattern, and so on, trying to fill this table. Now, am going to do a structural analysis of this, and this will happen through this division. Lets look at it. till the same problem, x_1 and x_N is my vector. And am trying to fill this with as many rows as possible, under a constraint of a break point. But now am going to isolate the last point. Why am isolating the last point? Because want a recursion. want to be able to relate this fellow, to the same fellow applied to smaller quantities. And you have seen enough of that to realize that, if manage to do that, might be able to actually solve for B of N and k. Thats why m isolating the last point. After do the isolation, am going to group the rows of the big matrix, into some groups. This is just my way of looking at things. havent changed anything. What am going to do, am going to shuffle the rows around, after you have constructed them. o we have a full matrix now, and am shuffling them, and putting some guys in the first group. And the first group am going to call _1. Here is the definition of the group _1. These are the rows that appear only once, as far as x_1 up to x_N1 are concerned. Well, every row in its entirety appears only once, because these are different rows. Thats how m constructing the matrix. But if you take out the last guy, it is conceivable that the first N minus 1 coordinates happen twice, once with extension 1, once with extension +1. o am taking the guys that go with only one extension, whatever it might be. ould be 1 or could be +1, but not both, and putting them in this group. Fairly well defined. o you fill it up, and these are all the rows that have a single extension. Now, you go under this, and you define the number of rows in this group to be alpha. t is a number. am just going to call it alpha. And you can see where this is going, because now m going to claim that the B of N and k, which is the total number of rows in the entire matrix, is alpha plus something. That is obvious. have already taken care of alpha, and am going to add up the other stuff later on. o what is the other stuff? That is the stuff am going to call _2. And you probably have a good guess what these are. These are the guys that happen with both patterns. That is, they happen with extension +1 and with 1. That is disjoint from the first group. A typical member will look like this. This is the same guy from x_1 up to x_N1, as it appears here. t just appears here with +1, and appears here with 1. And keep doing it. o what m doing, just reorganize the rows of the matrix to fall into these nice categories. The other guy? Exactly the same thing. o the second one corresponds to the second one, and so on. Now, that covers all the rows. look at x_1 up to x_N1. either have both extensions, or one extension. Thats it. One extension belongs to the first group. Two extensions belong to the second group in both ways, with +1 and 1. n terms of the counting, this has beta rows, whatever beta might be. This also has beta rows, because theyre identical. And therefore, the number B of N and K, which m interested in, is alpha plus 2 beta. That is complete. Just calling things names. o now, am going to try to find a handle on alphas and betas, so that can find a recursion for the big function B of N and k. B of N and k are the maximum number of rows patterns can get on N points, such that no k columns have all possible patterns. Thats the definition. am going to relate that to the same quantity on smaller numbers, smaller N and smaller k. o the first is to estimate alpha and beta. d like to ask you to focus on the x_1 up to x_N1 columns. And am going to help you visually do that, by graying out the rest. Now for a moment, look at these. Are these rows different? They used to be different when you have the extension. Well, let me see. The first group, know they are different, because they have one extension. f there is one which is repeated, then it must be repeated with both extensions, in order to get different rows all over, and that violates the condition for being here. They are here because they have only one extension. These guys are the same. This one appears with 1, and here appears with +1. But if you cut the last guy, this guy is identical to this guy, right? This second guy is identical to the second guy. o cannot count these as different rows. can do that when gray out one of the groups. Now, these are patently different. Nothing here is repeated, because we said they have only one extension, and they are all tucked in here. These two guys, there are no two guys here that are equal, because they all have the same extension. And supposedly, the whole row makes it different rows. Therefore, these guys are different from each other. And these guys are different from here because again, if they are equal, then will have an extension. And then the guys here will belong to a row that had both extensions. Very easy. Just a verbose argument, but we end up with these guys being different. Now, like the fact that these guys are being different, because when they are different, can relate them to B of N and k. B of N and k was the maximum number of patterns different rows, thats how am counting them such that a condition occurs. o what is the condition that is occurring here? can say that alpha plus beta, which is the total number of rows or patterns in this minimatrix, can say something about a break point for this small matrix? Yeah. The original matrix, could not find all possible patterns on any k columns, right? o cannot possibly find all possible patterns on any k columns on this smaller set. Because if find all possible patterns on k columns here, they will serve as all possible patterns in the big matrix. And know, that doesnt exist. o can now confidently say that alpha plus beta, which is the number of different guys here, is less than or equal to B of N minus 1, because have only x_1 up to x_N1, and k, because that is the break point for these guys as well. Why am saying less than or equal to, not equal? When constructed the original matrix, it was equal by construction. looked at the maximum number of rows get. And told you this is what constructed. And therefore, by definition, this is B of N and k. Here, obtained this in a special way. took out a guy from the other matrix, and did that. am not sure that this is the best way to maximize the number of rows. At least its conceivably not. But for sure its at most B of N minus 1 and k, because that is the maximum number. am safe saying that its less than or equal to. o have the first one. Now, lets try to estimate beta by itself. This is the more subtle argument. n this case, we are going to focus now on the second part only, the _2 part. The guys that appear twice in the big matrix. o lets focus on them. Now, when focus on them, these guys are very easy to analyze. They are here and here exactly the same. This block is identical to this block. The interesting thing, when look at these guys, is that am going to be able to argue that these guys have a break point of k minus 1, not k. The argument is very cute. Lets say that you have all possible patterns on k minus 1 guys, in this small matrix. First, have to kill these. These are not different guys, because these are identical to these. o let me reduce it to the guys that are patently different. m now looking at this matrix. am claiming that k minus 1 is a break point here. Why is that? Because if you had k minus 1 guys here, where you get all possible patterns, then by adding both copies, +1 and 1, and adding x_N, you will be getting k columns overall that have all possible patterns, which you know you cannot have because k is a break point for the whole thing. o now m taking advantage of the fact that these guys repeat. ts very dangerous to have k minus 1 guys, because now have the k that know doesnt exist. Lets do it illustratively. Here is a pattern here. You add the +1 extension and the 1 extension, by taking this column. f you get all possible patterns on k minus 1, and you add this guy, then you have both patterns here, and you will end up with all possible patterns on k points on the overall matrix. That enables me to actually count this in terms of B of N and k, again, with the proper values of N and k. We can say that beta is less than or equal to again, less than or equal to because obtained this matrix by lots of eliminations. didnt do it deliberately to maximize the number, so dont know whether its the maximum. But sure know that its less than or equal to the maximum, by the definition of what a maximum is. And that would be of what? have N minus 1 point and argued for a break point of k minus 1. o end up with this fellow. Both arguments are very simple. Now, we pull the rabbit out of the hat! You put it together. What do we have? This is the full matrix. The first item was just calling things names, the number of rows in the big matrix is B of N and k, by definition by construction. organized it such that there is alpha, and there is beta, and there is another beta, so this one is the first result got, which is B of N and k equals alpha plus 2 beta. What else did get? got that alpha plus beta is at most B of N minus 1 and k. That was the first slide of the analysis. We have seen that. o this basically takes this matrix, and does an analysis on it. And it has a break point k, because k will be inherited when you go to the bigger one. Thats what we did. The other one is, beta is less than B of N minus 1 and k minus 1. And this is the case where only looked at this guy, and now have to be more restrictive in terms of all possible patterns, because have an extension to add, and would be violating the big constraint. o ended up with this being less than or equal to B of N minus 1 and k minus 1. Anybody notice anything in this slide? How convenient! have alpha plus 2 beta there, and have alpha plus beta on one, and beta on one. f add them, am in business. can actually now relate B of N and k to other B of N and k, and alpha and beta are gone. B of N and k, now know, has to be at most this fellow. o you can see where the recursion is going. Now know that this property holds for the B of N and k. And now all need to do is solve it, in order to find an actual numerical value for B of N and k. And that numerical value will serve as an upper bound for any growth function of a hypothesis set that has a break point k. Lets do the numerical computation first. have this recursion, and can see that, from smaller values of N and K, can get bigger values, or can get an upper bound on bigger values. Lets do it in a table. Here is a table. Here is the value of N 1, 2. This is the number of points, the number of columns in the matrix. And this is k. This is the break point am talking about. o this will be theres a break point of 1, break point 2, break point 3, et cetera. And what d like to do here, d like to fill this table with an upper bound on B of N and k. d like to put numbers here, that know that B of N and k can be at most that number. And we can construct this matrix very, very easily having this recursion. Heres what we do. First, fill the boundary conditions. Lets look at this. Here it says that there is a break point of 1. cannot get all possible patterns on one point. Well, what are all possible patterns on one point? 1 and +1. ts one point. o cannot get both 1 and +1. Thats a pretty heavy restriction. o m asking myself, lets say you have now N columns in the matrix. How many different rows can you get in that matrix, under that constraint? Well, m in trouble. Because if have the first pattern, and then put a second pattern, the second pattern must be different from the first one in at least one column. Thats what makes it different. f its identical in every column, then its not a different pattern, right? o you go to that point, where its different. And unfortunately, for that point you get both possible patterns. o we are stuck. We can only have one pattern under this constraint. Hence, the 1s 1, 1, 1, 1, 1. Thats good. Now, in the other direction, its also easy. n this case, its 2. ts very easy to argue. Now, am taking the case where have only one column. o m asking myself, how many patterns can get for one column. Well, the most is 2. Why am getting 2s here? Because in the upper diagonal of this table, the constraint am putting is vacuous. Here, for example, am telling you how many different patterns can you get on one point, such that no four points have all possible patterns. Four points, what are you talking about? You have only one point. o thats no constraint at all. Therefore, it doesnt restrict the choices, and the maximum number is the maximum number would get unrestricted, which happens to be 2. f have one point, get two patterns. Thats why you have the 2s sitting here. Now, covered the boundary conditions, and thats really all need to complete the entire table, given the nature of the constraint have. Why is that? Because that constraint looks like this. f you know the solid blue guys, will tell you the empty blue guy. Because this would be look at N and k. This is N and k. This would be N minus 1 and k. This would be N minus 1 and k minus 1. Thats exactly what this says. o if have these two points, can get a value here, which will be an upper bound on this fellow. Let us actually go through this table, and fill it up. The first guy m going to take is this 1 and 2. According to this shape, might be able to get this fellow. What would that fellow be? 3, right? You just add the two numbers. How about the next guy? Anybody has a guess here? OK, 4. And then? A bunch of 4s. Always get 2s. am actually happy about this because you see that when k grows big, much bigger than N, as we said, the constraint is vacuous. o should be getting all possible patterns on the number of points have. And as you can see, for 1, get the 2s. For 2, will get eventually the 4s. And for 3, it will be the 8s. o that is very nice. Lets go over the next row. an solve this one? Now that got this one, can become more sophisticated and get this one. ee where this came from? How about the next one, what would that be? That should be 7, right? 8. A bunch of 8s. This is kind of fun. And you can fill up the entire table. o we have it completely solved, numerically. t would be nice to have a formula, which we will have in a minute. But numerically, we will have that. Now, let me highlight one guy. Do you see anything that changed colors? claim that you have seen this before. Thats the puzzle. You had three points. Your break point was 2. And now we know for a fact that the maximum number you can get is 4, without having to go through the entire torture we went through last time. an we try this? an we try that? Oh, am violating You dont have to do that. Here are the numbers, just by computing a very simple recursion. Now, lets go for the analytic version of that. What d like to do, d like to find a formula that computes this number outright. dont have to go through this computation numerically. o lets do that. This is the analytic solution for B of N and k. Again, this is the recursion. And now we have a theorem. Yeah, when youre doing mathematical theory, you have to have theorems. Otherwise, you lose your qualifications! What does the theorem say? t tells you that this is a formula that is an upper bound for B of N and k. What is this formula? This is N choose i, the combinatorial quantity. And you sum this up from i equals 0 to k minus 1. o both N and k appear. N appears as the number here, and k appears as the limit for the index of summation appears as k minus 1. This quantity will be an upper bound for B of N and k. You can now, if you believe that, which we will argue in a moment, you compute this number. And that will be an upper bound for the growth function of any hypothesis set that has a break point k, without asking any questions whatsoever about the hypothesis set or the input space. t shouldnt come as a surprise that this quantity is right, because if you look at this, this is really screaming for something binomial or combinatorial. learly, it will come out one way or the other. But why is it this way? Well, what we are going to show, we are going to show that this is exactly the quantity we computed numerically for the other table. And we are going to do it by induction. o the recursion we did, we are just going to do it analytically. How do you do that? You start with boundary conditions. What were the boundary conditions? We argued that this is, indeed, the value of B of N and k. And hence, an upper bound on it, from the last slide. Now we want to verify that this quantity actually returns those numbers, when you plug in the value N equals 1 or k equals 1. How do do that? You just do it. Just plug in, and it will come out. m not going even to bother doing it. ts a very simple formula. You just evaluate it, and you get that. The interesting part is the recursion. would like to argue that if this formula holds for the solid blue points, then it will also hold for this guy. And then by induction, since it holds for all of these guys, can just do this step by step and fill the schedule, with the truth of this being the correct value for the numbers that appear here. Everybody is clear about the logic of it? o lets do the induction. We have the induction step. We just want to make clear what the induction step is. You are going to assume that the formula holds for this point and this point. o indeed, if you plug in the values for N and k, which here, N minus 1 and k minus 1, and here it would be N minus 1 and k, you plug it into that particular formula. Then the numbers will be correct. Thats the assumption. And then you prove that, if this is true, then the formula will hold here. Thats the induction step. Fair enough. o lets do that. This is the formula for N and k. You just need to remember it. N appears here and k appears here. inus 1 is an integral part of the formula. This is the value for k, not for k minus 1. The value, for k, happens to be the sum from i equals 0 to that k, minus 1. o this is the formula that is supposed to be here. And we would like to argue that this is equal to what is this one? This one is for N minus 1, and still k. o this would be am moved from here to here. o this will be the value here. And what is the other guy? That will be the value for N minus 1. And now its for k minus 1, because you still take the other minus 1. t becomes k minus 2. This part belongs here. o this is the induction step. We dont have it yet. Thats what we want to establish. o let me put a question mark to make sure that we havent established it yet. What am going to do, am going to take the righthand side, and keep reducing it, until the lefthand side appears. Thats all. And then well be done with the induction step. And since we have the boundary conditions, we will have proved the theorem we asserted. The first thing am going to do, am going to look at this fellow. And notice that the index goes here from i equals 0 to k minus 1. Here, it goes from i equals 0 to k minus 2. d like to merge the two summations. o in order to merge the two summations, will make them the same number of terms, first. Very easy. will just take the zeroth term, which would be N minus 1 choose 0, which is 1, out. And now the summation goes from i equals 1 to k minus 1. Now, go to the other guy and do this. What did do? just changed the name of the dummy variable i. wanted the index to go from 1 to k minus 1, in order to be able to merge it easily. Here, it goes from 0 to k minus 2. o what do do? just make this i, and make this i minus 1. o i minus 1 goes from 0 to k minus 2, as this i used to. Just changing the names. And now, having done that, am ready to merge the two summations. And they are merged. Now, would like to be able to take this, and produce one quantity. And you can do it by brute force. This is no mysterious quantity. This is what? This is N minus 1 times N minus 2 times N minus 3, i terms, divided by i factorial. And this one applies the same thing. o you end up with something, and then you do all kinds of algebra, and it looks familiar. And then you reduce it to another quantity. o theres always an algebraic way of reducing it. But am going to reduce it with a very simple combinatorial argument. am going to claim that this is the 1 remains the same. And this actually, the whole thing here reduces to N choose i. o these two guys become this one. nstead of doing the algebra, am going to give you a combinatorial argument. That is, this quantity is identical to N choose i. Lets say that am trying to choose 10 people from this room. And lets say that the room has N people. There are N people. How many ways can you chose 10 people out of this room? That is N choose 10. Lets put this on the side. Here is another way of counting it. We can count the number of ways you can pick 10 people, excluding me, plus the number of ways you can pick 10 people, including me. Right? These are disjoint, and they cover all the cases. Lets look at excluding me. How many ways can you pick 10 people from the room, excluding me? Well, then you are picking the 10 people from N minus 1. am the minus 1. o that would be N minus 1 choose 10. Put this in the bank. How many ways can you pick 10 people, including me? Well, you already decided you are including me, so you are only deciding on the 9 remaining guys. o that would be N minus 1 choose 9. o we have N minus 1 choose 10, plus N minus 1 choose 9, that equals the original number, which was N choose 10. Look at this. What do we have? This is excluding me. This is including me. And this is the original count. o its a combinatorial identity, and we dont have to go through the torture of the algebra in order to prove that its exactly the same. Now, go back. look, this goes from i equals 1 to k minus 1. have this 1, so conveniently put it back, and get this formula. Have you seen it before? Yeah, it looks familiar. Oh, this is the one we want to prove. o it means that we are done. Thats it. We have an exact solution for the upper bound on B of N and k. ince we spent some time developing it, lets look at it and celebrate it, and be happy about it. First thing: yes, its a polynomial, because all of this torture was to get a polynomial, right? f we did all of this, and its perfect math, and the end result was not a polynomial, then we are in trouble. Because although the quantity is correct, its not going to be useful in the utility that we are aiming at. o why is it polynomial? Remember that for a particular hypothesis set, the break point is a fixed point. t doesnt grow with N. You ask in a hypothesis set, can get all possible dichotomies on four points? Thats a question for the perceptron. No. Then 4, in the perceptron, is a break point. Now, can ask myself what the perceptron does on 100 points. And the break point is still 4, just a constant. You give me a hypothesis set, give you a break point. Thats a fixed number. o according to our argument now, the growth function for a hypothesis set that has a break point k is less than or equal to the purely combinatorial quantity, B of N and k, which is defined as the maximum such number of dichotomies you can get, under the constraint that k is a break point. And that was less than or equal to the nice formula we had. o we can now make this statement. You go in a real learning situation. Lets say you have a neural network making a decision, and you tell me the break point for that neural network is 17. dont ask you what is a neural network, because we dont know yet, so you dont have to know. dont ask you what is the dimensionality of the Euclidean space you are working on. You told me 17. Your growth function of your neural network that dont know, in the space that dont know, happens to be less than or equal to that, and know that m correct. o is this quantity polynomial in N? Thats what we need. Because remember, in the Hoeffding, there was a negative exponential in N. f we get this to be polynomial in N, we are in business. Well, any one of those guys is what? N times N minus 1 times N minus 2, i times, divided by i factorial. i factorial doesnt matter, its a constant. o you basically get N multiplied by itself a number of times, i times, for the ith term. The most that N will be multiplied by itself is when you get to i equals k minus 1, the maximum. And then N will be multiplied by itself k minus 1 times. Therefore, the maximum power in this quantity is N to the k minus 1. This comes from N times N minus 1 times N minus 2 times k times, that corresponds to the case where i equals k minus 1. When you get N choose k minus 1, thats what you get. Anything else, will give you a power of N, but its a smaller power. This is the most you will have. What do we know about this fellow k? We know its just a number. ts a constant. t doesnt change with N. And therefore, this is indeed a polynomial in N. And we have achieved what we set out to do. That is pretty good. Lets take three examples, in order to make this relate to experiences we had before. This is the famous quantity by now. You know it by heart. have the N. remember k. have to put minus 1. And that is the upper bound for anything that has a break point k. Now, lets take hypothesis sets we looked at before, for which we computed the growth function explicitly, and see if they actually satisfy the bound. They had better, because this is math. We proved it. But just to see that this is, indeed, the case. Positive rays. Oh, remember positive rays from some time ago? We have one dimension, so just the real line. Then we take from a point on, it goes to +1. And we said that the whole analysis here is exactly to avoid what just did. You dont have to tell me what is the input. You dont have to tell me you just have to tell me what? What is the break point? Thats all we want. you can call it positive rays. You can call it George, dont care! t has a break point of 2. Thats what pull. We did compute the growth function for the positive rays. We did it by brute force. We looked at it, and we see what the patterns are, and did a combinatorial argument. And we ended up, that the growth function for this guy is N plus 1. Let us see if this satisfies the bound. This is supposedly less than or equal to. And you substitute here for N, which is the number here. And the break point is k. o youre summing, from i equals 0 to 1, this quantity. You have N choose 0, also known as 1. Plus N choose 1, also known as N. Thats it. o you get this to be less than or equal to wow! Look at the analysis we did to get the N plus 1. And we get exactly the same. With all the bounds and we think that there is a big slack here. But here, actually its exactly tight. We get the same answer exactly, without looking at anything of the geometry of what the hypothesis set was. Lets try another one. aybe well continue to be lucky. Positive intervals. Yeah, remember those were the more sophisticated oh, m sorry. am not supposed to ask any questions about the hypothesis. m asking about the break point only. remember now. o tell me what the break point is. That was k equals 3. And we did compute the growth function. Remember, this one was a funny one. Were picking two segments out of N plus 1, and then adding the 1. o we ended up this would be the formula. What would be the bound according to the result we just had? This would be again, this formula. And now k equals 3. o have N choose 0 plus N choose 1 plus N choose 2. get 1 plus N plus something that has squared terms. And do the reduction and what do get? Boring, boring. seem to be getting it all the time. t doesnt happen this way. t will always happen that its true. But there will be a slack in many cases. o, we verified it. We are very comfortable now with the result. Lets apply it to something where we could not get the growth function. Remember this old fellow? Well, in the twodimensional perceptron, we went through a full argument just to prove that the break point is 4. But we didnt bother go through a general number of points N, and ask ourselves, how many hypotheses can the perceptron generate on N points? an you imagine the torture? We do this can get this pattern And you have to do this for every N. o we didnt do it. The growth function is unknown to us. We just know the break point. But using just that factor, we are able to bound the growth function completely. And you substitute again with k equals 4. You get another term, which is cubic. And you do the reduction. And lo and behold, you have that statement. That statement holds for the perceptrons in two dimensions. And you can see that this will apply to anything. o now it was all worth the trouble, because now we have a very simple characterization of hypothesis sets. And we can take this, and move to the other part. Remember, this part, which has now disappeared is proving that its polynomial. Proving that we are interested in the growth function. f it wasnt polynomial, we wouldnt be interested in it. Now, this is an interesting quantity. This one tells us that oh, and by the way, its not only interesting. You actually can use it. We can put it in the Hoeffding inequality, and claim that the Hoeffding inequality is true using the growth function. Now lets see what we want, to remind you of the context of substituting for the total number of hypotheses by the growth function. We wanted, instead of having this fellow this is Hoeffding, and this is the number of hypotheses using the union bound, which we said is next to useless whenever is big, or is infinite. And instead of that, we wanted really to substitute for this by the growth function. o this is what we are trying to do. We are trying to justify that instead of this, you can actually say that. Well, it turns out that this is not exactly what we are going to say. We are going to modify the constants here for technical reasons that will become clear. But the essence is the same. There would be the growth function here. t will be polynomial in N, and it will be killed by the negative exponential, provided that there is a break point any break point. Now, how are we going to prove that? We are going to have a pictorial proof. What a relief, because think you are exhausted by now. o the formal proof is in the Appendix. ts six pages. y purpose of the presentation here is to give you the key points in the proof, so that you dont get lost in epsilons and deltas. There are basically certain things you need to establish. And once you know that thats what you are looking for, you can bite the bullet and go through it line by line. The two aspects are the following. Why did we do this growth function? We used the growth function because its smaller, so it will be helpful. But how could it possibly replace ? Because was assuming no overlaps at all in the bad regions. Remember? o now that we know that there are overlaps, this will take care of it. The question is, how does the growth function actually relate to the overlaps? You need to establish that. o this is the first one. And when we establish that, we find that its a completely clean argument at everything, except for one annoying aspect. Growth function relates to a finite sample. o we will get a perfect handle on the E_in, the insample error part of the deal. But in the Hoeffding inequality, there is this E_out. And E_out relates to the performance over the entire space. o we are no longer talking about dichotomies, we are talking about full hypotheses. We lose the benefit of the growth function. o what do we do about E out? That was a question that was asked last time. What to do about E_out, in order to get the argument to conform while we are just using a finite sample, is the second step. After that, its a technical putting it together, in order to get the final result. Thats the plan. But the proofs are pictures. o lets have a blank page. And lets say you are an artist and this is your canvas. ts a very special canvas. ts the canvas of data sets. What is that? Every point here is an entire data set, x_1, x_2, up to x_N. Fix N in your mind. o this is one vector. This is another vector. This is another vector. And this canvas covers the entire set of possible data sets. Now, why am doing this space? Well, am doing this space because the event of being good or bad, whether E_in goes to E out, depends on the sample. Depends on the data set. For some data sets, you will be close to the E_out. For some data sets, you are not going to be close. o want to draw it here, in order to look at the bad regions and the overlaps, and then argue why the growth function is useful for the overlaps. Now, we assume that theres a probability distribution. And for simplicity, lets say that the area corresponds to the probability. o the total area of the canvas is 1. Now, you look at the event, the bad event, that E_in is far away from E_out. And lets say that you paint the points that correspond to that event red. o you pick is this data set good or bad? What does it mean, good or bad? look at E_in in that data set, compare it to E_out on a particular hypothesis, and then paint it red if its bad. o have a hypothesis in mind, and am painting the points here red or leaving them alone, according to whether they violate Hoeffdings inequality or not. And get this, just illustratively. And you realize that didnt paint a lot of area. And that is because of Hoeffding inequality. Hoeffding inequality tells me that that area is small. o m entitled to put a small patch. Now we went from one hypothesis, which is this guy, to the case where we have multiple hypotheses, using the union bound. o again, this is the space of data sets, exactly the same one. And now, am saying for the first hypothesis, you get this bad region. What happens when you have a second hypothesis? Because am using the very pessimistic union bound, am hedging my bets and saying you get a bad region that is disjoint. Another hypothesis two of them. ore. ore. Oh, no. We are in trouble. The colored area is the bad area. Now the canvas is the bad area. Thats why we get the problem with the union bound. Because obviously, having them disjoint fills up the canvas very quickly. Each of them is small, but have so many of them. nfinity of them as a matter of fact. This will overflow. Well, no, it wont overflow. Just figuratively speaking. o thats what m going to have. What is the argument we are applying now? We are not applying the union bound. We are going to a new canvas. And that canvas is called the V bound, as in Vapnikhervonenkis. Well see it in a moment. o what do you do? Your first hypothesis, same thing. When you take the second hypothesis, you take the overlaps into consideration. o it falls here. You get more. You get more. You get all of them. ts not as good as the first one. never expected it to be. But definitely not as bad as the second one, because now they are overlapping. And indeed, the total area, which is the bad region something bad happening is a small fraction of the whole thing. And can learn. o we are trying to characterize this overlap. Thats the whole deal with the growth function. One way to do it is the one that alluded to before. tudy the hypothesis set. tudy the probability distribution. Get the full joint probability distribution of any two events involving two hypotheses, and then characterize this. Well, good luck! We wont do that. The reason we introduced the growth function, because its an abstract quantity that is simple, and is going to characterize the overlaps. The question is, how is the growth function going to characterize the overlaps? Here is what is going to happen. will tell you that if you look at this canvas, if any point gets painted at all, it will get painted over 100 times. Lets say that have that guarantee. dont know which hypotheses will paint it again. But any point that gets a red, it will get a blue, and a green 100 times. f tell you that statement, what do you know about the total area that is colored? Now its, at most, one hundredth of what it used to be. Because when had them nonoverlapping, they filled it over. Now for every point that is colored, have to do it 100 times. o am overusing these guys, and these guys will have to shrink. And will get one hundredth of that. That is basically the essence of the argument. What the growth function tells you is that what is the growth function? Number of dichotomies. f you take a dichotomy, this is not the full hypothesis, but the hypothesis on a finite set of points. There are many, many hypotheses that return the exact same dichotomy. Remember the gray sheet with the holes. Lots of stuff can be happening behind the sheet. And as far am am concerned, they are all the same dichotomy. o all of these guys will be behaving exactly the same way. f one of them colored the point, the others will. This tells me that the redundancy is captured by the growth function. That would be a very clean argument. And it would have been a very simple proof, except for one annoyance. That the point being colored doesnt depend only on the sample, but depends also on the entire space. Because the point gets colored because its a bad point. What is a bad point? The frequency on the sample, that is patently on the sample, deviates from E_out. Oh, E_out involves the entire hypothesis. f have the gray sheet and the holes, cannot compute E out. have to peel it off, look, and get the areas in order to get E out. o the argument is great, as long as you can tell me how do go around the presence of E_out? And thats the second part of the proof. What to do about E_out. The simplest argument possible. That is really the breakthrough that Vapnik and hervonenkis did. Back to the bin, just because its an illustration of the binary case. o here, we have one hypothesis. And we have E out, which is the overall, in the entire space the error in the entire space. We pick a sample, and then we get E_in, which is the value for the error on this one. o we have seen this before. And we said this tracks this, according to the Hoeffding inequality. And the problem is that when you have many, many bins, some of these guys will start deviating from E_out, to the level if you pick according to the sample, you are no longer sure that you picked the right one, because the deviation could be big. That was the argument. Now, want to get rid of E_out. The way am going to do it is this. nstead of picking one sample, am going to pick two samples, independently. o obviously, they are not identical samples. ome of them are green or red, et cetera. But they are coming from the same distribution. Now, lets see what is going on. E_out and E_in track each other, because E_in is generated from this distribution. Now, lets say look at these two samples and give them names. am going to call them E_in and E_in dash. Theyre both insample. t happens to be a different sample. o have two samples. am going to call this E_in, and this E_in dash. y question is, does E_in track E_in dash, if you have one bin? Well, each of them tracks E_out, right? Because it was generated by it. o consequently, they track each other. A bit more loosely, because you have now two ways of getting the sample error. On the other hand, if do two presidential polls one polling asks 3000 people. Another asks 3000 people. These are different 3000 people. You fully expect that the result will be close to each other, right? o these guys track each other. OK, fine. What is the advantage? The advantage is the following. f now have multiple bins, the problem had here is reflected exactly in the new tracking. When had multiple bins, the tie between E_out and E_in became looser and looser. Because m looking for worst case, and might be unlucky enough, that the tracking now lost the tightness that one bin with Hoeffding would dictate. f am doing multiple bins, and not looking at the bin at all, just looking at the two samples from each bin, they track each other, but they also get loosely apart as go for more. Lets say, tell you this experiment. You pick two samples. They are close in terms of the fraction of red. f you keep repeating it, can you get one sample to be mostly red, and the other sample to be mostly green? Yeah. f you are patient enough, it will happen. Exactly for the same reason, because you keep looking for it until it happens. o the mathematical ramifications of multiple hypotheses happen here, exactly the same way they happen here. The finesse now is that, if characterize it using the two samples, then am completely in the realm of dichotomies. Because now m not appealing to E_out at all. am only appealing to what happens in a sample. ts a bigger sample. have 2N marbles now instead of N. But still, can define a growth function on them. And now the characterization is full, and am ready to go. These are the only two components you need to worry about as you read the proof. Now, lets put it together. This is what we wanted. This is not true. Dont hold this against me. And to make sure, this is not quite what we have. This would be direct substitution of the plainvanilla growth function in terms of . We are not going to have that, but we are going instead to have this. Lets look and compare. These look the same, except that this 2 became 4. s this good or bad? Well, its bad. We want this probability to be small. Bad, but not fatal. This one goes to here. have twice the sample. You know why have 2 now. Because now use the bigger sample for the argument, so need 2N. Oh, but all of this was about a polynomial and now dont know whether this will be a polynomial. Yes, you do. f its polynomial in N, its polynomial in N here. Because you get 2N to the k, then you get 2 to the k. Thats a constant. And you still get N to the k. o that remains a polynomial. A bigger polynomial. dont like it, but you dont have to like it. t just has to be true, and do the job we want. And finally, you can see this is minus 2, which was a very helpful factor. This is in the exponent. 2 in the exponent goes a lot of mileage. And now we knock it down all the way to 1/8. Thats really, really bad news. The reason this is happening is that, as we go through the technicalities of the proof, the epsilon will become epsilon over 2. And then will become epsilon over 4, just to take care of different steps. And when you plug in epsilon over 4 here, you get epsilon squared over 16. And so you get a factor of 1/8. Thats the reason for it. o this is what we will end up with. And you can be finicky and try to improve this constant a lot, but the basic message is that here is a statement that holds true for any hypothesis set that has a break point. And this fellow is polynomial in N, with the order of the polynomial decided by the break point. And you will eventually learn, because if N is big enough if give you enough examples using that hypothesis, you will be able to claim that E_in tracks E_out correctly. This result, which is called the Vapnikhervonenkis inequality, is the most important theoretical result in machine learning. On that happy note, we will stop here and take questions after a short break. Lets start the QA. ODERATOR: First, a few clarifications from the beginning. n slide 5, when you choose the N points, does it mean your data set is of N points, or you just chose N points from the data set? PROFEOR: When apply this to an actual hypothesis set in an input space, then these actually correspond to a particular set of points in that space. However, in the abstraction that just defines the function B, these are just abstract labels. These are labels for which column m talking about. o although call them x_1 up to x_N1, these are not really in the abstraction here, they dont correspond to any particular input space in mind. But when they do, they will correspond to a sample. And am supposed to pick the sample in that space that maximizes the number of dichotomies, et cetera, as we defined the growth function. But its a sample that pick when apply this to a particular input space and a hypothesis set. ODERATOR: Also, there are some people asking they didnt understand exactly why alpha was different to beta. PROFEOR: alpha is different from beta? ODERATOR: Yeah. Why? PROFEOR: Well, the short answer is that never made the statement that alpha is different from beta. just didnt bother ascertain any relationship between alpha and beta. just called them names. f they happen to be equal, am happy. f they happen to be unequal, am happy. o all m doing here is just calling the guys that happen to have a single extension, the number of them, calling it alpha. alling the guys that happen to have double extension beta. dont know whether alpha is bigger than beta, or smaller than beta, in any particular case. And it doesnt matter as far as the analysis is concerned. f call them this way, then it will always be true that the total number of rows here is alpha plus beta plus beta, which is alpha plus twice beta. o there is really no assertion about the relative value of alpha and beta. ODERATOR: oving on to the case where you show the break points, and how it satisfies the bound. What happens if k equals infinity? No break points, basically. PROFEOR: This is for the positive rays and whatnot? ODERATOR: Yeah. o for example, if you had the convex sets. PROFEOR: k equals infinity means there is no break point. n that case, you dont have to bother with any of the analysis did. No break points means what? eans the growth function is 2 to the N for every N, right? We just computed it exactly. f you want a bound for it, yes, its bounded by 2 to the N. Not a polynomial. o all of these cases, were addressing the case where there is a break point, because that is the case where can guarantee a polynomial. And therefore, can guarantee learning. That is the interesting case. f there is no break point, this theoretical line of analysis will not guarantee learning. o if have a hypothesis set that happens to be able to shatter every set of points, cannot make a statement using this line of analysis that it will learn. And one example we had was convex sets. o convex sets have a growth function of 2 to the N. Well, it really is a very pessimistic estimate here, because the points have to be really, really very funny. You have to build the pathological case, in order not to be able to learn. And in many cases, you might be. But again, if want a uniform statement based only on the break point, this is the most can say using this line of analysis. ODERATOR: OK. Just a quick review. How is the break point calculated? PROFEOR: alculated. The break point is this is the only time you actually need to visit the input space and the hypothesis set. You basically you are sitting in a room with your hypothesis set. omeone gave you a problem for credit approval. You decided to use perceptrons, and you decided to use a nonlinear transformation. And you do all of that, and you start programming it. And you would like to know some prediction of the generalization performance that you are going to get. o you go into the room, and ask yourself: for this hypothesis set, over this input space, what is the break point? o now you have to actually go and study your hypothesis set. And then find out that using this hypothesis set, cannot separate, lets say, 10 points in every possible way. Very much along the argument we used for the perceptron in two dimensions. We found out that we cannot separate four points in every possible way. But the good news is that, you dont have to do it anew, because for most of the famous learning models, this has already been done. For the perceptrons, we will get an exact break point. For anydimensional perceptron. o 20dimensional perceptron, heres the break point, and heres the growth function. Or, heres the bound. imilarity from neural networks, there is a break point. Not exact estimate of the break point, but a bound on the break point. And again, in most of these cases, bounds work, because we are always trying to bound above. And we have room to play with, because a polynomial is a polynomial is a polynomial. o if you become a little bit sloppy and forget something, and the break point you say 10 instead of 7 its not going to break the back of learning versus nonlearning. ts just going to tell you more pessimistically, how much resources do you need in order to learn? Which is more benign damage than deciding, oh, cannot learn at all. ODERATOR: OK. an you come up with an example, where these bounds are not tight as here? PROFEOR: Theres one case, which could have covered but didnt, where you take positive and negative rays. o positive rays, you take the real line. And from a point on its +1. Before, its 1. Positive and negative rays, it means you are also allowed to take rays that return +1 first, and then 1 later. And the union of them is the model called positive and negative rays. ts a good exercise to do. Take that home and try to find, what is the break point? And youll find that although the break point for positive rays is 2, in this case the break point is actually 3. And the reason is that for two points, now you can get everything because you know the ray can be here. o they are both minus. The ray could be here. They are both plus. The ray could be here. ts minus plus. But now, use the negative ray to get the +1, 1. o now you can shatter two points. And you would fail only for the three points, when the middle guy is different, because you cannot get it this way. o you will get and the break point is 3. When you do the break point of 3, you will get the bound, the blue bound here. You will get that to be squared. Pretty much like here, because we have a break point that corresponds directly to squared. dont care whether the 3 is coming from positive intervals, or coming from positive and negative rays. ts 3. Therefore, the blue bound is quadratic. f you compute the number of dichotomies you can get, which is the growth function, it will actually be linear. o there will be a discrepancy between linear for the exact estimate of the growth function, to quadratic of the bound. o there are cases that you can come up with, easily. And as a matter of fact, this is the exception on this, rather than the rule. n most of the cases, there will be a slack. ODERATOR: And this question drives the point of the whole lecture. t says, we have been focusing on having E_in equal to E_out, or close to E_out, not in the actual value of E_in. o using our hypotheses, there are just as many percentage errors in the training data as the real data. Why is that? PROFEOR: This goes back to separating the question of learning into the two questions. There was one question which was addressed now. We are trying to get E_in to track E_out. Why do need that? Because dont know E_out, and will not know E_out. That is simply an unknown quantity for me. And want to work with something to minimize. cannot minimize something that dont know. o if the theoretical guarantees tell me that E_in is a proxy for E_out, and if minimize E_in, E_out will follow suit. Then can now work with a quantity that know, and do it. Thats the first part, that they are tracking each other. The second part is the practical. Now, am going to go and try to minimize E_in. This is the second part of it. ODERATOR: Also, theyre asking if can you clarify more, why is the V dimension useful? PROFEOR: The V dimension, as of now, is an unknown quantity. didnt say that word ""V dimension"" at all. said every building block that will result in the definition. However, the good news is, what is the title of the next lecture? The V dimension. You will be completely content with everything you wanted to know about the V dimension, and werent afraid to ask! OK? ODERATOR: Yeah, the crowd is saying that theyre still digesting the lecture. PROFEOR: OK. As mentioned before, if you didnt follow this in real time, dont be discouraged. ts actually very sweet material. And you can look at the lecture again. And you can read the proof. And you can do all of the homework, until it settles in your mind. This is the most abstract, or the most theoretical, lecture of the entire course. And if you get through this one, and you understand it well, you are in good shape as far as the rest of the course. There will be mathematics, but it will be more friendly mathematics. Friendly, as in less abstract. For someone who is not theoretically inclined, the more abstract the mathematics is, the less they can follow it, because they cannot relate to it. o this one has the abstraction. The other mathematics will be much easier to relate to. ODERATOR: What was wrong with the ""not quite"" expression on the last slide? PROFEOR: OK. Basically, the top statement is simply false. t was my way of relating what m trying to do, to what has already happened. There used to be in place of the growth function. o the growth function is here. There used to be . o the easiest way for me to describe what is happening with the theory, is to tell you that you are going to take out, and replace it with this. As usual, its not that easy. Even remember with the Hoeffding, when complained about the 2 here and the 2 here? Well, you have to have them, in order for the statement to be true. o for the statement to be true, we needed to do some technical stuff that really didnt change the essence of the statement here, but made it a little bit different by changing the constants. And therefore, we have a proof for it. t holds. And it captures the essence of that. just didnt want to bother telling you this because, if told you this in the first place, you would have been completely lost. Why 4? Why 2? What is this 1/8? And forget about the essence. o the easiest way to do it, we are replacing it. This is not the final form, but we are replacing it. ntil you get the idea: indeed, can replace it. But oh, in order to replace it, need to have a bigger sample that we argued for. o need 2N. Oh, and now the bigger samples are not tracking to each other as well as each of them is tracking the actual outofsample error. o need to modify these values, and so on. o it becomes much easier to swallow. The technicalities will come in, in order to make the proof go through. ODERATOR: OK. an you review the definition of B of N and k? PROFEOR: B of N and k. Assume you have N points, and assume that k is a break point. o youre assured that no k points will have all possible patterns on them. After these two assumptions, make no further assumptions. You dont know where this came from. You dont know what space you are working with. You dont know what the hypothesis set is. You just know that in your setup, when you get N points that is the N here and the break point is k that is k here. nder those conditions, can you bound the growth function? an you tell me that the growth function can never be bigger than something? That something is what am calling B of N and k. o what am doing? m taking the minimal conditions you gave me. have N points, and k is a break point. And asking myself: what is the maximum number of dichotomies you can possibly have, under no other constraints, in order to satisfy these two constraints? And m calling this B of N and k. Why did do it? First, its going to help, being an upper bound for any hypothesis set that has a break point k, because it is the maximum. The second one, its a purely combinatorial quantity. o have a much better chance of analyzing it, without going through the hairy details of input spaces, and correlation between events, and so on. And that is indeed, what ended up being the case. We had a very simple recursion on it, and we found a formula for it. And that formula now serves as an upper bound for the more hairy quantity, which is the growth function that is very particular to a learning situation, an input space, and a hypothesis set. ODERATOR: Also, a particular question on the proof of B of N and k, the recursion. lide 5. The question is, why does k not change when going back from N to N minus 1? PROFEOR: OK. Here, if you look at x_1, x_2, up to x_N, the disappearing x_N here, no k columns can have all possible patterns. These k columns could involve the last column, and could involve only the first N minus 1 columns. Just no k columns whatsoever can have all possible patterns. o when look at the reduced one, N minus 1, know for a fact that no k columns of these guys can have all possible patterns. Because that would qualify as k columns of the bigger one. o k doesnt really change. The only time had a different k is when had a nice argument that, if you have k minus 1 points which have all possible patterns on the smaller set, then adding the last column will get us in trouble with k columns. o for that, needed an argument. But in general, when take the statement on face value, k is fixed. And the k columns could be anything. ould be involving the last column, or could be restricted to the first guy. ould be the first k columns, for all care. ODERATOR: How does this formalization apply to, say, a regression problem? PROFEOR: Again, this is all binary functions. o the classification of +1 and 1. And as mentioned, the entire analysis, the V analysis, can be extended to realvalued functions. ts a very technical extension that, in my humble opinion, does not add to the insight. And therefore, instead of doing that and going very technical, in order to gain very little in terms of the insight, decided that when get to regression functions, am going to apply a completely different approach, which is the biasvariance tradeoff. t will give us another insight into the situation, and will tackle the realvalued functions directly, the regression functions. And therefore, think well have both the benefit of having another way of looking at it, and covering both types of functions. ODERATOR: Theres this person that says, feel silly asking this, but is the bottom line that we can prove learnability if the learning model cannot learn everything? PROFEOR: OK. We proved learnability under a condition about the hypothesis set. When you say learning everything, you are really talking about the target function. o the target function is unknown. What am telling you here is that, if you tell me that there is a break point, can tell you that if you have enough examples, E_in will be close to E_out for the hypothesis you pick, whichever way you pick it. t remains to be seen whether you are going to be able to minimize E_in, to a level that will make you happy. will never know that until you start minimizing. o if the target function happens to be extremely difficult, or completely random unlearnable, you are not going to see this in the generalization question. The generalization question is independent of the target function. didnt bring it up here at all. t has to do with the hypothesis set only. The target function will come in if get E_in to be small, E_out will be small. know that from the generalization argument that made. an get E_in to be small? f the target function is random, you will get a sample that that is extremely difficult to fit. And you are not going to be able to get E_in to be small. But at least, you will realize that you could not learn, in that particular case. And in another target function, you will realize that you can learn, because E_in went down. o the question of whether can learn or not, the generalization part of it is independent of the target function. The second question is very much dependent on the target function, but the good news is that it happens in sample. can observe it, and realize how well, or not so well, learned. ODERATOR: Also, going back to a previous question, does this also generalize to multiclass problems? PROFEOR: Basically, there is no restriction on the inputs or the outputs. There is a counterpart. And instead of saying break point, what is a break point? And dichotomies, they are not really dichotomies. You have real values. You have no real values, so there are technicalities to be done in order to be able to reduce them to this case. But the same principle applies, regardless of the type of function you have. ODERATOR: think thats it. PROFEOR: Very good. Thank you, and well see you next week.","You just know that in your setup, when you get N points that is the N here and the break point is k that is k here. And you can see where this is going, because now m going to claim that the B of N and k, which is the total number of rows in the entire matrix, is alpha plus something. o according to our argument now, the growth function for a hypothesis set that has a break point k is less than or equal to the purely combinatorial quantity, B of N and k, which is defined as the maximum such number of dichotomies you can get, under the constraint that k is a break point. The definition here is the maximum number of dichotomies on N points, such that they have a break point k. o this is N and this is k. And the good thing here is that didnt appeal to any hypothesis set, or any input space. o can now confidently say that alpha plus beta, which is the number of different guys here, is less than or equal to B of N minus 1, because have only x_1 up to x_N1, and k, because that is the break point for these guys as well. o you will get and the break point is 3. The value, for k, happens to be the sum from i equals 0 to that k, minus 1. o this is the formula that is supposed to be here. o the easiest way for me to describe what is happening with the theory, is to tell you that you are going to take out, and replace it with this. And the problem is that when you have many, many bins, some of these guys will start deviating from E_out, to the level if you pick according to the sample, you are no longer sure that you picked the right one, because the deviation could be big. What am telling you here is that, if you tell me that there is a break point, can tell you that if you have enough examples, E_in will be close to E_out for the hypothesis you pick, whichever way you pick it. The break point is this is the only time you actually need to visit the input space and the hypothesis set. And we would like to argue that this is equal to what is this one? You can say that the growth function, for the particular case you talked about, is less than or equal to and just go to this combinatorial quantity. And we had an illustrative example to tell you that, when you tell me that you cannot get all patterns on any in this case, two points that is a very strong restriction on the number of dichotomies you can get on a larger number of points. This is the formula for N and k. You just need to remember it. You pick the break point for that, and you use that here, ridding the problem of all the other aspects, and you still are able to make an upper bound statement. The way am going to do it is this. And this is the case where only looked at this guy, and now have to be more restrictive in terms of all possible patterns, because have an extension to add, and would be violating the big constraint. Now, if you look at this schedule, this does not appeal to any particulars of the hypothesis set or the input space, other than the fact that the break point is 2. could be in a situation, where the hypothesis set cannot generate some of these guys for other reasons. And the reason is that for two points, now you can get everything because you know the ray can be here. Now know that this property holds for the B of N and k. And now all need to do is solve it, in order to find an actual numerical value for B of N and k. And that numerical value will serve as an upper bound for any growth function of a hypothesis set that has a break point k. Lets do the numerical computation first. The second one is to show that we can actually take that notion, the growth function, and put it in place of , the number of hypotheses, in Hoeffdings inequality. f say m_H of N is polynomial, its not that am going to actually solve for the growth function, and show that it is this particular polynomial, and the coefficients. The interesting thing, when look at these guys, is that am going to be able to argue that these guys have a break point of k minus 1, not k. The argument is very cute. We do this can get this pattern And you have to do this for every N. o we didnt do it. And that is the growth function. o now, am going to try to find a handle on alphas and betas, so that can find a recursion for the big function B of N and k. B of N and k are the maximum number of rows patterns can get on N points, such that no k columns have all possible patterns. And you have seen enough of that to realize that, if manage to do that, might be able to actually solve for B of N and k. Thats why m isolating the last point. This is the most you will have. And our theoretical goal is to take that single number, which is the break point, and be able to characterize the entire growth function for every N. And therefore, be able to characterize the generalization, as we will see. You are going to assume that the formula holds for this point and this point. And what d like to do here, d like to fill this table with an upper bound on B of N and k. d like to put numbers here, that know that B of N and k can be at most that number. When you do the break point of 3, you will get the bound, the blue bound here. o you will get a set of different patterns that are all the dichotomies that can be generated by this hypothesis set, on this set of points. This quantity will be an upper bound for B of N and k. You can now, if you believe that, which we will argue in a moment, you compute this number. f you compute the number of dichotomies you can get, which is the growth function, it will actually be linear. And this is k. This is the break point am talking about. t tells you that this is a formula that is an upper bound for B of N and k. What is this formula? Now, you go under this, and you define the number of rows in this group to be alpha. The key quantity we are going to use, which is a purely combinatorial quantity, we are going to call it B of N and k. This is exactly the quantity we were seeking in the puzzle. o lets look at the bound for B of N, k. And we are going to do it recursively. The first one is to show that the growth function, with a break point, is indeed polynomial. And that is the upper bound for anything that has a break point k. Now, lets take hypothesis sets we looked at before, for which we computed the growth function explicitly, and see if they actually satisfy the bound. This is the number of points, the number of columns in the matrix. Because if you had k minus 1 guys here, where you get all possible patterns, then by adding both copies, +1 and 1, and adding x_N, you will be getting k columns overall that have all possible patterns, which you know you cannot have because k is a break point for the whole thing. And you can be finicky and try to improve this constant a lot, but the basic message is that here is a statement that holds true for any hypothesis set that has a break point. o this is what we are trying to do. We pick a sample, and then we get E_in, which is the value for the error on this one. And then by induction, since it holds for all of these guys, can just do this step by step and fill the schedule, with the truth of this being the correct value for the numbers that appear here. We are not going to have that, but we are going instead to have this. Now in principle, the growth function can be 2 to the N. You may be in an input space and a hypothesis set, such that you can generate any pattern you want. We argued that this is, indeed, the value of B of N and k. And hence, an upper bound on it, from the last slide. And if you get through this one, and you understand it well, you are in good shape as far as the rest of the course. And this fellow is polynomial in N, with the order of the polynomial decided by the break point. Now, like the fact that these guys are being different, because when they are different, can relate them to B of N and k. B of N and k was the maximum number of patterns different rows, thats how am counting them such that a condition occurs. And you can see that if we have 10 points, and you apply the same restriction, there will be so many lost, because now the restriction applies to any pair of points. o want to draw it here, in order to look at the bad regions and the overlaps, and then argue why the growth function is useful for the overlaps. The reason we introduced the growth function, because its an abstract quantity that is simple, and is going to characterize the overlaps. The first guy m going to take is this 1 and 2. What we get to see are just the data points, holes in that sheet if you will. We wanted, instead of having this fellow this is Hoeffding, and this is the number of hypotheses using the union bound, which we said is next to useless whenever is big, or is infinite. Well, what we are going to show, we are going to show that this is exactly the quantity we computed numerically for the other table. Now lets see what we want, to remind you of the context of substituting for the total number of hypotheses by the growth function. Well, it turns out that this is not exactly what we are going to say. And that formula now serves as an upper bound for the more hairy quantity, which is the growth function that is very particular to a learning situation, an input space, and a hypothesis set. But if you restrict your attention to dichotomies, which are the hypotheses restricted to a finite set of points, the blue and red points here, then you dont have to count everything that is happening outside. And the number of those guys is what we are interested in. What is the break point? What the growth function tells you is that what is the growth function? y purpose of the presentation here is to give you the key points in the proof, so that you dont get lost in epsilons and deltas. And you would fail only for the three points, when the middle guy is different, because you cannot get it this way. Because you get 2N to the k, then you get 2 to the k. Thats a constant. And we ended up, that the growth function for this guy is N plus 1. And it has a break point k, because k will be inherited when you go to the bigger one. That enables me to actually count this in terms of B of N and k, again, with the proper values of N and k. We can say that beta is less than or equal to again, less than or equal to because obtained this matrix by lots of eliminations. o this is the first one. And youll find that although the break point for positive rays is 2, in this case the break point is actually 3. The only time had a different k is when had a nice argument that, if you have k minus 1 points which have all possible patterns on the smaller set, then adding the last column will get us in trouble with k columns. The most that N will be multiplied by itself is when you get to i equals k minus 1, the maximum. And you are not going to be able to get E_in to be small. And you would like to know some prediction of the generalization performance that you are going to get. am going to claim that this is the 1 remains the same. t doesnt change with N. And therefore, this is indeed a polynomial in N. And we have achieved what we set out to do. And then, the second one is: we can actually do something good with it. And am supposed to pick the sample in that space that maximizes the number of dichotomies, et cetera, as we defined the growth function. o you go into the room, and ask yourself: for this hypothesis set, over this input space, what is the break point? o its a combinatorial identity, and we dont have to go through the torture of the algebra in order to prove that its exactly the same. What to do about E_out, in order to get the argument to conform while we are just using a finite sample, is the second step. This is the analytic solution for B of N and k. Again, this is the recursion. just ask you: what is the break point? Now, go to the other guy and do this. The question is, how is the growth function going to characterize the overlaps? This one is for N minus 1, and still k. o this would be am moved from here to here. You dont have to tell me what is the input. f you get all possible patterns on k minus 1, and you add this guy, then you have both patterns here, and you will end up with all possible patterns on k points on the overall matrix. Well, you have to have them, in order for the statement to be true. o now that we know that there are overlaps, this will take care of it. And you do all of that, and you start programming it. o indeed, if you plug in the values for N and k, which here, N minus 1 and k minus 1, and here it would be N minus 1 and k, you plug it into that particular formula. And instead of that, we wanted really to substitute for this by the growth function. This is the second part of it. Now we went from one hypothesis, which is this guy, to the case where we have multiple hypotheses, using the union bound. But for sure its at most B of N minus 1 and k, because that is the maximum number. And that will be an upper bound for the growth function of any hypothesis set that has a break point k, without asking any questions whatsoever about the hypothesis set or the input space. What is this 1/8? This is what? But just to see that this is, indeed, the case. What is that? And that would be of what? But the good news is that, you dont have to do it anew, because for most of the famous learning models, this has already been done. got that alpha plus beta is at most B of N minus 1 and k. That was the first slide of the analysis. And just talk about the upper bound in the general case, and still get what you want to get. Here it says that there is a break point of 1. cannot get all possible patterns on one point. And we are going to do it by induction. And therefore, instead of doing that and going very technical, in order to gain very little in terms of the insight, decided that when get to regression functions, am going to apply a completely different approach, which is the biasvariance tradeoff. ODERATOR: oving on to the case where you show the break points, and how it satisfies the bound. And there could be very exciting stuff happening behind that sheet, and all you get to see is when the boundary crosses one of these points, and a blue point turns red or viceversa. Therefore, it doesnt restrict the choices, and the maximum number is the maximum number would get unrestricted, which happens to be 2. f have one point, get two patterns. One way to do it is the one that alluded to before. organized it such that there is alpha, and there is beta, and there is another beta, so this one is the first result got, which is B of N and k equals alpha plus 2 beta. First, its going to help, being an upper bound for any hypothesis set that has a break point k, because it is the maximum. The second question is very much dependent on the target function, but the good news is that it happens in sample. And now we know for a fact that the maximum number you can get is 4, without having to go through the entire torture we went through last time. This resulted in a definition that parallels the number of hypotheses, which is the number of dichotomies in this case. Because this would be look at N and k. This is N and k. This would be N minus 1 and k. This would be N minus 1 and k minus 1. can say that alpha plus beta, which is the total number of rows or patterns in this minimatrix, can say something about a break point for this small matrix? o you fill it up, and these are all the rows that have a single extension. We looked at it, and we see what the patterns are, and did a combinatorial argument. am not sure that this is the best way to maximize the number of rows. We then talked about the maximum number of dichotomies, under the constraint that there is a break point. You dont know what the hypothesis set is. Because although the quantity is correct, its not going to be useful in the utility that we are aiming at. And you can see that this will apply to anything. We are trying to justify that instead of this, you can actually say that. And we can take this, and move to the other part. f we did all of this, and its perfect math, and the end result was not a polynomial, then we are in trouble. have N points, and k is a break point. tell you that k is a break point, and ask you: how many different patterns can you get under those conditions? We have an exact solution for the upper bound on B of N and k. ince we spent some time developing it, lets look at it and celebrate it, and be happy about it. o all m doing here is just calling the guys that happen to have a single extension, the number of them, calling it alpha. t will be polynomial in N, and it will be killed by the negative exponential, provided that there is a break point any break point. Oh, this is the one we want to prove. f tell you that statement, what do you know about the total area that is colored? Because remember, in the Hoeffding, there was a negative exponential in N. f we get this to be polynomial in N, we are in business. And once you know that thats what you are looking for, you can bite the bullet and go through it line by line. And am going to try to put as many patterns as can, under a constraint that there is a break point. We are going to bound the growth function by a polynomial. f you take a dichotomy, this is not the full hypothesis, but the hypothesis on a finite set of points. The growth function is you pick the points x_1 up to x_N. And you substitute here for N, which is the number here. For the case of a perceptron in two dimensions, which is the case we studied, we realize that for four points, there will always be a pattern that cannot be realized by a perceptron. Take that home and try to find, what is the break point?",0.159043965771614
11,11,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: OK, lets start. o we said that the task of statistical mechanics is to assign probabilities to different microstates given that we have knowledge of the microstate. And the most kind of simple logical place to start was in the microcanonical ensemble where the microstate that we specified was one in which there was no exchange of work or heat with the surroundings so that the energy was constant, and the parameters, such as x and N, that account for chemical and mechanical work were fixed also. Then this assignment was, we said, like counting how many faces the dice has, and saying all of them are equally likely. o we would say that the probability here of a microstate is of the form 0 or 1 depending on whether the energy of that microstate is or is not the right energy that is listed over here. And as any probability, it has to be normalized. o we had this 1 over omega. And we also had a rule for converting probabilities to entropies, which in dimensionless form that is, if we divide by kB was simply minus the expectation value of the log of the probability, this probability being uniformly 1 over omega. This simply gave us log of omega E, x, N. Once we had the entropy as a function of E, x, N, we also identified the derivatives of this quantity in equilibrium. And partial derivative with respect to energy was identified as 1 o systems that were in equilibrium with each other, this derivative d by dE had to be the same for all of them. And because of mechanical stability, we could identify the next derivative with respect to x. That was minus J/T. And with respect to N by identifying a corresponding thing, we would get this. o from here, we can proceed and calculate thermodynamic properties using these microscopic rules, as well as probabilities in the entire space of microstates that you have. Now, the next thing that we did was to go and look at the different ensemble, the canonical ensemble, in which we said that, again, thermodynamically, choose a different set of variables. For example, can replace the energy with temperature. And indeed, the characteristic of the canonical ensemble is that rather than specifying the energy, you specify the temperature. But theres still no chemical or mechanical work. o the other two parameters are kept fixed also. And then the statement that we had was that by putting a system in contact with a huge reservoir, we could ensure that it is maintained at some temperature. And since system and reservoir were jointly microcanonical, we could use the probabilities that we had microcanonically. We integrated over the degrees of freedom of the reservoir that we didnt care about. And we ended up with the probability of a microstate in the canonical ensemble, which was related to the energy of that by the form exponential of minus beta h where we introduced beta to be 1 over kT. And this probably, again, had to be normalized. o the normalization we called Z. And Z is attained by summing over all of the microstates E to the minus beta H of the microstate where this could be an integration over the entire phase space if youre dealing with continuous variables. OK, now the question was, thermodynamically, these quantities T and E are completely exchangeable ways of identifying the same equilibrium state, whereas what have done now is have told you what the temperature is. But the energy of the system is a random variable. And so can ask, what is the probability that look at my system and find a particular energy E? o how can get that? Well, that probability, first of all, has to come from a microstate that has the right energy. And so will get e to the minus beta E divided by Z, which is the probability of getting that microstate that has the right energy. But only said something about the energy. And there are a huge number of microstates, as weve seen, that have the same energy. o could have picked from any one of those microstates, their number being omega of E, the omega that we had identified before. o since omega can be written as exponential of s over kB, this you can also think of as being proportional to exponential of minus beta E minus T, the entropy that would get for that energy according to the formula above, divided by Z. Now, we said that the quantity that m looking at here in the numerator has a dependence on the size of the system that is extensive. Both the entropy and energy we expect to be growing proportionately to the size of the system. And hence, this exponent also grows proportionately to the size of the system. o expect that if plot this probability as a function of energy, this probability to get energy E, it would be one of those functions. ts certainly positive. ts a probability. t has an exponential dependence. o theres a part that maybe from the density of states grows exponentially. e to the minus beta E will exponential kill it. And maybe there is, because of this competition, one or potentially more maxima. But if there are more maxima locally, dont really care. Because one will be exponentially larger than the other. o presumably, there is some location corresponding to some E star where the probability is maximized. Well, how should characterize the energy of the system? hould pick E star, or given this probability, should look at maybe the mean value, the average of H? How can get the average of H? Well, what need to do is to sum over all the microstates H of the microstate e to the minus beta H divided by sum over microstates e to the minus beta H, which is the normalization. The denominator is of course the partition function. The numerator can be obtained by taking a derivative of this with respect to beta of the minus sign. o what this is is minus the log Z by d beta. o if have calculated H, can potentially go through this procedure and maybe calculate where the mean value is. And is that a better representation of the energy of the system than the most likely value, which was E star? Well, lets see how much energy fluctuates. Basically, we saw that if repeat this procedure many times, can see that the nth moment is minus 1 to the n 1 over Z the nth derivative of Z with respect to beta. And hence the partition function, by expanding it in powers of beta, will generate for me higher and higher moments. And we therefore concluded that the cumulant would be obtained by the same procedure, except that will replace this by log Z. o this will be the nth derivative of log Z with respect to beta. o the variance H squared c is a second derivative of log Z. The first derivative gives me the first cumulant, which was the mean. o this is going to give me d by d beta with a minus sign of the expectation value of H, which is the same thing as kBT squared, because the derivative of 1 over kT is 1 over T squared times derivative with respect to T. And it goes to the other side, to the numerator. And so then have the derivative of this object with respect to temperature, which is something like a heat capacity. But most importantly, the statement is that all of these quantities are things that are of the order of the size of the system. And just like we did in the central limit theorem, with the addition of random variables, you have a situation that is very much like that. The distribution, in some sense, is going to converge more and more towards the Gaussian dominated by the first and second cumulant. And in fact, even the second cumulant we see is of the order of square root of n. And hence the fluctuations between these two quantities, which are each one of them order of n, is only of the order of square root of n. And when the limit of n goes to infinity, we can ignore any such difference. We can essentially identify either one of them with the energy of the system thermodynamically. And then this quantity, if identify this with energy of the system, is simply the usual heat capacity of constant x. o the scale of the fluctuations over here is set by square root of kBT squared the heat capacity. By the way, which is also the reason that heat capacities must be positive. Because statistically, the variances are certainly positive quantities. o you have a constraint that we had seen before on the sign emerging in its statistical interpretation. What did over here was to identify an entropy associated with a particular form of the probability. What happens if look at an entropy with this probability that have over here? OK, so this is a probability phase space. What is its entropy? o over k is expectation value of log p. And what is log of p? Well, there is log of this minus beta H. o what will have, because of the change in sign beta expectation value of H. And then have minus log Z here. The sign changes, and will get plus log Z. o yeah, thats correct. f were to rearrange this, what do get? can take this minus lets see. Yeah, OK, so if take this to the other side, then will get that log Z equals this goes to the other side minus beta. Expectation value of H we are calling E. And then have multiplying this by beta. The kBs disappear, and will get minus T. o we see that can identify log Z with the combination E minus T, which is the Helmholtz free energy. o log Z, in the same way that the normalization here that was omega, gave us the entropy. The normalization of the probability in the canonical ensemble, which is also called the partition function if take its log, will give me the free energy. And can then go and compute various thermodynamic quantities based on that. Theres an alternative way of getting the same result, which is to note that actually the same quantity is appearing over here, right? And really, should be evaluating the probability here at the maximum. aximum is the energy of the system. o if you like, can call this variable over here epsilon. This is the probability of epsilon. And it is only when m at the maximum that can replace this epsilon with E. Then since with almost probability 1, m going to see this state and none of the other states, because of this exponential dependence. When this expression is evaluated at this energy, it should give me probability 1. And so you can see again that Z has to be so basically what m saying is that this quantity has to be 1 so that you get this relationship back from that perspective also. ADENE: Question. PROFEOR: Yes. ADENE: o agree that if in order to plug in d, mean energy, into that expression, you would get a probability of 1 at that point. But because even though the other energies are exponentially less probable, they strictly speaking arent 0 probability, are they? PROFEOR: No. ADENE: o how does this get normalized? How does this probability expression get normalized? PROFEOR: OK, so what are we doing? o Z, can also write it as the normalization of the energy. o rather than picking the one energy that maximizes things, you say, you should really do this, right? Now, will evaluate this by the saddle point method. The saddle point method says, pick the maximum of this. o have e to the minus beta F evaluated at the maximum. And then have to integrate over variations around that maximum. Variations around that maximum have to expand this to second order. And if really correctly expand this to second order, will get delta E squared divided by this 2kTx, because we already established what this variance is. can do this Gaussian integration. get e to the minus beta F of e star. And then the variance of this object is going to give me root of 2 pi kT of x Tsquare. o what do mean when say that this quantity is Z? Really, the thing that we are calculating always in order to make computation is something like a free energy, which is log Z. o when take the log, log of Z is going to be this minus beta F star . But youre right. There was a weight. All of those things are contributing. How much are they contributing? 1/2 log of 2 pi kBT squared . Now, in the limit of large N, this is order of N. This is order of log N. And . o its the same saddle point. The idea of saddle point was that there is a weight. But you can ignore it. There maybe another maximum here. You say, what about that? Well, that will be exponentially small. o everything keep emphasizing only works because of this N goes to infinity limit. And its magical, you see? You can replace this sum with just one, the maximum. And everything is fine and consistent. Yes. ADENE: Not to belabor this point, but if you have an expected return for catastrophe where this outlier causes an event that brings the system down, couldnt that chase this limit in the sense that as that goes to 0, that still goes to infinity, and thus youre you understand what m saying. f an outlier causes this simulation thats my word causes this system to crumble, then so is there a paradox there? PROFEOR: No, there is here the possibility of a catastrophe in the sense that all the oxygen in this room could go over there, and you and will suffocate after a few minutes. ts possible, thats true. You just have to wait many, many ages of the universe for that to happen. Yes. ADENE: o when youre integrating, that introduces units into the problem. o we have to divide by something to . PROFEOR: Yes, and when we are doing this for the case of the ideal gas, will be very careful to do that. But it turns out that those issues, as far as various derivatives are concerned, will not make too much difference. But when we are looking at the specific example, such as the ideal gas, will be careful about that. o we saw how this transition occurs. But when we were looking at the case of thermodynamical descriptions, we looked at a couple of other microstates. One of them was the Gibbs canonical. And what we did was we said, well, lets allow now some work to take place on the system, mechanical work. And rather than saying that, say, the displacement x is fixed, allow it to vary. But will say what the corresponding force is. But lets keep the number of particles fixed. o essentially, the picture that we have is that somehow my system is parametrized by some quantity x. And m maintaining the system at fixed value of some force J. o x is allowed potentially to find the value that is consistent with this particular J that impose on the system. And again, have put the whole system in contact with the reservoir temperature T so that if now say that really maintain the system at variable x, but fix J, have to put this spring over there. And then this spring plus the system is jointly in a canonical perspective. And what that really means is that you have to keep track of the energy of the microstate, as well as the energy that you extract from the spring, which is something like J dot x. And since the joint system is in the canonical state, it would say that the probability for the joint system, which has T and J and N specified, but dont know the microstate, and dont know the actual displacement x, this joint probability is canonical. And so it is proportional to e to the minus beta H of the microstate plus this contribution Jx that m getting from the other degree of freedom, which is the spring, which keeps the whole thing at fixed J. And have to divide by some normalization that in addition includes integration over x. And this will depend on T, J, N. o in this system, both x and the energy of the system are variables. They can potentially change. What can now do is to either characterize like did over here what the probability of x is, or go by this other route and calculate what the entropy of this probability is, which would be minus the log of the corresponding probability. And what is the log in this case? will get beta expectation value of H. And will get minus beta J expectation value of x. And then will get plus log of this Z tilde. o rearranging things, what get is that log of this Z tilde is minus beta. will call this E like did over there. inus J would call this the actual thermodynamic displacement x. And from down here, the other side, will get a factor of T. o this should remind you that we had called a Gibbs free energy the combination of E minus T minus Jx. And the natural variables for this G were indeed, once we looked at the variation DE, T. They were J, and N that we did not do anything with. And to the question of, what is the displacement, since its a random variable now, can again try to go to the same procedure as did over here. can calculate the expectation value of x by noting that this exponent here has a factor of beta Jx. o if take a derivative with respect to beta J, will bring down a factor of x. o if take the derivative of log of this Z tilde with respect to beta J, would generate what the mean value is. And actually, maybe here let me show you that dG was indeed what expected. o dG would be dE. dE has a Td. This will make that into a minus dT. dE has a Jdx. This will make it minus xdJ. And then have mu dN. o thermodynamically, you would have said that x is obtained as a derivative of G with respect to J. Now, log Z is something that is like beta G. At fixed temperature, can remove these betas. What do get? This is the same thing as dG by dJ. And seem to have lost a sign somewhere. OK, log Z was minus G. o theres a minus here. Everything is consistent. o you can play around with things in multiple ways, convince yourself, again, that in this ensemble, have fixed the force, xs variable. But just like the energy here was well defined up to something that was square root of N, this x is well defined up to something that is of the order square root of N. Because again, you can second look at the variance. t will be related to two derivatives of log G, which would be related to one derivative of x. And ultimately, that will give you something like, again, kBT squared d of x by dJ. Yes. ADENE: an you mention again, regarding the probability distribution? What was the idea? PROFEOR: How did get this probability? ADENE: Yes. PROFEOR: OK, so said that canonically, was looking at a system where x was fixed. But now have told you that J know what J is, what the force is. And so how can make sure that the system is maintained at a fixed J? go back and say, how did ensure that my microcanonical system was at a fixed temperature? put it in contact with a huge bath that had the right temperature. o here, what will do is will connect the wall of my system to a huge spring that will maintain a particle force J on the system. When we do it for the case of the gas, will imagine that have a box. have a piston on top that can move up and down. put a weight on top of it. And that weight will ensure that the pressure is at some particular value inside the gas. But then the piston can slide up and down. o a particular state of the system have to specify where the piston is, how big x is, and what is the microstate of the system. o the variables that m not sure of are mu and x. Now, once have said what mu and x is, and ve put the system in contact with a bath at temperature T, can say, OK, the whole thing is canonical, the energy of the entire system composed of the energy of this, which is H of mu, and the energy of the spring, which is Jx. And so the canonical probability for the joint system is composed of the net energy of the two. And have to normalize it. Yes. ADENE: Are you taking x squared to be a scalar, so that way the dx over dJ is like a divergence? Or are you taking it to be a covariance tensor? PROFEOR: Here, have assumed that it is a scalar. But we can certainly do if you want to do it with a vector, then can say something about positivity. o really wanted this to be a variance and positive. And so if you like, then it would be the diagonal terms of whatever compressibility you have. o you certainly can generalize this expression to be a vector. You can have x xJ, and you would have d of x with respect to JJ or something like this. But here, really wanted the scalar case. Everything did was scalar manipulation. And this is now the variance of something and is positive. OK, there was one other ensemble, which was the grand canonical. o here, went from the energy to temperature. kept N fixed. didnt make it into a chemical potential. But can do that. Rather than having fixed number, can have fixed chemical potential. But then cant allow the other variable to be J. t has to be x. Because as we have discussed many times, at least one of them has to be extensive. And then you can follow this procedure that you had here. hemical work is just an analog of mechanical work mathematically. o now would have the probability that have specified this set of variables. But now dont know what my microstate is. Actually, let me put mu s. Because introduced a chemical potential mu. o mu sub s is the microstate of the system. dont know how many particles are in that microstate. The definition of mu s would potentially cover N. Or can write it explicitly. And then, what will have is e to the beta mu N minus beta H of the microstates energy divided by the normalization, which is the grand partition function Q. o this was the Gibbs partition function. This is a function of T, x at the chemical potential. And the analog of all of these expressions here would exist. o the average number in the system, which now we can use as the thermodynamic number, as weve seen, would be d of log of Q with respect to d beta mu. And you can look at the variances, et cetera. Yes. ADENE: o do you define, then, what the microstate of the system really is if the particle number is ? PROFEOR: OK, so lets say have five particles or six particles in the system. t would be, in the first case, the momentum coordinates of five particles. n the next case, in the momentum, have coordinates of six particles. o these are spaces, as it has changed, where the dimensionality of the phase space is also being modified. And again, for the ideal gas, we will explicitly calculate that. And again, you can identify what the log of this grand partition is going to be. t is going to be minus beta E minus T. Rather than minus Jx, it will be minus mu N. And this combination is what we call the grand potential g, which is E minus T minus mu N. You can do thermodynamics. You can do probability. Essentially, in the large end limit, and only in the large end limit, theres a consistent identification of most likely states according to the statistical description and thermodynamic parameters. Yes. ADENE: an one build a more general ensemble or description where, say, J is not a fixed number, but know how it varies? know that its subject to say the spring is not exactly a Hookean spring. ts not linear. The proportion is not linear. PROFEOR: think you are then describing a system, right? Because it may be that your system itself has some nonlinear properties for its force as a function of displacement. And most generally, it will. Very soon, we will look at the pressure of a gas, which will be some complicated function of its density does not necessarily have to be linear. o the system itself could have all kinds of nonlinear dependencies. But you calculate the corresponding force through this procedure. And it will vary in some nonlinear function of density. f youre asking, can do something in which couple this to another system, which has some nonlinear properties, then would say that really what youre doing is youre putting two systems together. And you should be doing separate thermodynamical calculations for each system, and then put the constraint that the J of this system should equal the J of the other system. ADENE: dont think thats quite what m getting at. m thinking here, the J is not given by another thermodynamical system. Were just applying it like were not applying thermodynamics to whatever is asserting to J. PROFEOR: No, m trying to mimic thermodynamics. o in thermodynamics, have a way of describing equilibrium systems in terms of a certain set of variables. And given that set of variables, there are conjugate variables. o m constructing something that is analogous to thermodynamics. t may be that you want to do something else. And my perspective is that the best way to do something else is to sort of imagine different systems that are in contact with each other. Each one of them you do thermodynamics, and then you put equilibrium between them. f want to solve some complicated system mechanically, then sort of break it down into the forces that are acting on one, forces that are acting on the other, and then how this one is responding, how that one is responding. dont see any advantage to having a more complicated mechanical description of an individual system. ADENE: ts not like in reality. ts hard to obtain situations where the external force is fixed. Really, if were doing an experiment here, the pressure in the atmosphere is a fixed number. But maybe in other circumstances PROFEOR: Yes, so if you are in another circumstance, like you have one gas that is expanding in a nonlinear medium, then what would do is would calculate the pressure of gas as a function of its volume, et cetera. would calculate the response of that medium as a function of stress, or whatever else m exerting on that. And say that am constrained to move around the trajectory where this force equals this force. o now lets carry out this procedure for the ideal gas. And the microscopic description of the ideal gas was that have to sum over its Hamiltonian is composed of the kinetic energies of the particles. o have their momentum. Plus the potential that we said m going to assume describes box of volume mu. And it is ideal, because we dont have interactions among particles. We will put that shortly for the next version. Now, given this, can go through the various prescriptions. can go microcanonically and say that know the energy volume and the number of particles. And in this prescription, we said that the probability is either 0 or 1 divided by omega. And this is if qi not in box and sum over i Pi squared over 2m not equal to E and one otherwise. ADENE: s there any reason why we dont use a direct delta there? PROFEOR: dont know how to use the direct delta for reading a box. For the energy, the reason dont like to do that is because direct deltas have dimensions of 1 over whatever thing is inside. Yeah, you could do that. ts just prefer not to do it. But you certainly could do that. t is certainly something that is 0 or 1 over omega on that surface. The for this, what was it? t was obtained by integrating over the entirety of the 6N dimensional phase space. From the Q integrations, we got V to the N. From the momentum integrations, we got the surface area of this hypersphere, which was 2 pi to the 3N over 2, because it was 3N dimensional. And this was the solid angle. And then we got the radius, which is 2mE square root raised to the power of number of dimensions minus 1. Then we did something else. We said that the phase space is more appropriately divided by N factorial for identical particles. Here, introduced some factor of H to dimensionalize the integrations over PQ space so that this quantity now did not carry any dimensions. And oops, this was not . This was omega. And /k, which was log of omega we took the log of that expression. We got N log of V. From the log of N factorial, tirlings approximation gave us a factor of N over e. All of the other factors were proportional to 3N over 2. o the N is out front. will have a factor of 3 over 2 here. have 2 pi mE divided by 3N over 2. And then theres E from tirlings approximation. And that was the entropy of the ideal gas. Yes. ADENE: Does that N factorial change when describing the system with distinct particles? PROFEOR: Yes, thats right. ADENE: o that definition changes? PROFEOR: Yes, so this factor of if have a mixture of gas, and one of them are one kind, and two of them are another kind, would be dividing by N1 factorial, N2 factorial, and not by N1 plus N2 factorial. ADENE: o that is the correct definition for all the cases? PROFEOR: Thats the definition for phase space of identical particles. o all of them are identical particle, is what have written. All of them are distinct. Theres no such factor. f half of them are of one type and identical, half of them are another type of identical, then will have N over 2 factorial, N over 2 factorial. ADENE: Question. PROFEOR: Yeah. ADENE: o when youre writing the formula for number of microstates, just a question of dimensions. You write V to the N. t gives you something that mentions coordinate to the power 3N times something of dimensions of . The power is 3N minus 1 divided by H to 3N. o overall, this gives you 1 over momentum. PROFEOR: OK, lets put a delta E over here actually, not a delta E, but a delta R. OK, so now its fine. ts really what want to make sure is that the extensive part is correctly taken into account. may be missing a factor of dimension that is of the order of 1 every now and then. And then you can put some to correct it. And its, again, the same story of the orange. ts all in the skin, including so the volume is the same thing as the surface area, which is essentially what m OK? o once you have this, you can calculate various quantities. Again, the is dE over T plus PdV over T minus mu dN over T. o you can immediately identify, for example, that 1 over T derivative with respect to energy would give me a factor of N/E. f take a derivative with respect to volume, P over T, derivative of this object with respect to volume will give me N over V. And mu over T is oops, actually, in all of these cases, forgot the kB. o have to restore it. And here would get kB log of the N out front. can throw out V/N 4 pi m E over 3N. And forgot the Hs. o the Hs would appear as an H squared here. They will appear as an H squared here raised to the 3/2 power. We also said that can look at the probability of a single particle having momentum P1. And we showed that essentially have to integrate over everything else if m not interested, such as the volume of the particle number one. The normalization was omega E, V, N. And integrating over particles numbers two to N, where the energy that is left to them is E minus the kinetic energy of the one particle, gave me this expression. And essentially, we found that this was proportional to one side divided by E. 1 minus P1 squared over 2m raised to the power that was very close to 3N over 2 minus/plus something, and that this was proportional, therefore, to this P1 squared over 2m times P1 squared 2mE 3N over 2E. And 3N over 2E from here we see is the same thing as 1 over kT. o we can, with this prescription, calculate all of the properties of the ideal gas, including probability to see one particle with some momentum. Now, if do the same thing in the canonical form, in which the microstate that m looking at is temperature, volume, number of particles, then the probability of a microstate, given that ve specified now the temperature, is proportional to the energy of that microstate, which is e to the minus beta sum over i Pi squared over 2m. Of course would have to have all of Qis in box. ertainly they cannot go outside the box. And this has to be normalized. Now we can see that in this canonical ensemble, the result that we had to do a couple of lines of algebra to get, which is that the momentum of a particle is Gaussian distributed, is automatically satisfied. And in this ensemble, each one of the momenta you can see is independently distributed according to this probability distribution. o somethings clearly emerge much easier in this perspective. And if were to look at the normalization Z, the normalization Z obtained by integrating over the entirety of the phase space. o have to do the integration over d cubed Pi d cubed qi. ince this is the phase space of identical particles, we said we have to normalize it by N factorial. And had this factor of H to the 3N to make things dimensionless. have to exponentiate this energy sum over i Pi squared over 2m and just ensure that the qi are inside the box. o if integrate the qi, what do get? will get V per particle. o have V to the N divided by N factorial. o thats the volume contribution. And then what are the P integrations? Each one of the P integrations is an independent Gaussian. n fact, each component is independent. o have 3N Gaussian integrations. And can do Gaussian integrations. will get root 2 pi m inverse of beta, which is kT per Gaussian integration. And there are 3N of them. And, oh, had the factor of h to the 3N. will put it here. o chose these Hs in order to make this phase space dimensionless. o the Z that have now is dimensionless. o the dimensions of V must be made up by dimensions of all of these things that are left. o m going to make that explicitly clear by writing it as 1 over N factorial V over some characteristic volume raised to the Nth power. The characteristic volume comes entirely from these factors. o have introduced lambda of T, which is h over root 2 pi mkT, which is the thermal de Broglie wavelength. At this stage, this h is just anything to make things dimensionally work. When we do quantum mechanics, we will see that this lens scale has a very important physical meaning. As long as the separations of particles on average is larger than this, you can ignore quantum mechanics. When it becomes less than this, you have to include quantum factors. o then what do we have? We have that the free energy is minus kT log Z. o F is minus kT log of this partition function. Log of that quantity will give me N log V over lambda cubed. tirlings formula, log of N factorial, will give me N log N over e. o thats the free energy. Once have the free energy, can calculate, lets say, the volume. Lets see, dF, which is d of E minus T, is minus dT minus PdV, because work of the gas we had identified as minus PdV. have mu dN. What do we have, therefore? We have that, for example, P is minus dF by dV at constant T and N. o have to go and look at this expression where the V up here, it just appears in log V. o the answer is going to be NkT over V. can calculate the chemical potential. u will be dF by dN at constant T and V, so just take the derivative with respect to N. o what do get? will get minus kT log of V over N lambda cubed. And this E will disappear when take the derivative with respected to that log inside. o thats the formula for my mu. wont calculate entropy. But will calculate the energy, noting that in this ensemble, a nice way of calculating energy is minus d log Z by d beta. o this is minus d log Z by d beta. And my Z, you can see, has a bunch of things V, N, et cetera. But if focus on temperature, which is the inverse beta, you can see it appears with a factor of 3N over 2. o this is beta to the minus 3N over 2. o this is going to be 3N over 2 derivative of log beta with respect to beta, which is 1 over beta, which is the same thing as 3 over 2 NkT. o what if wanted to maintain the system at some fixed temperature, but rather than telling you what the volume of the box is, will tell you what its pressure is and how many particles have? How can ensure that have a particular pressure? You can imagine that this is the box that contains my gas. And put a weight on top of some kind of a piston that can move over the gas. o then you would say that the net energy that have to look at is the kinetic energy of the gas particles here plus the potential energy of this weight that is going up and down. f you like, that potential energy is going to be mass times delta H. Delta H times area gives you volume. ass times G divided by area will give you pressure. o the combination of those two is the same thing as pressure times volume. o this is going to be the same thing as minus sum over i Pi squared over 2m. For all of the gas particles for this additional weight that is going up and down, it will give me a contribution that is minus beta PV. And so this is the probability in this state. t is going to be the same as this provided that divide by some Gibbs partition function. o this is the probability of the microstate. o what is the Gibbs partition function? Well, what is the normalization? now have one additional variable, which is where this piston is located in order to ensure that it is at the right pressure. o have to integrate also over the additional volume. This additional factor only depends on PV. And then have to integrate given that have some particular V that then have to integrate over all of the microstates that are confined within this volume. Their momenta and their coordinates what is that? just calculated that. Thats the partition function as a function of T, V, and N. o for a fixed V, already did the integration over all microscopic degrees of freedom. have one more integration to do over the volume. And thats it. o if you like, this is like doing a Laplace transform. To go from Z of T, V, and N, to this Z tilde of T, P, and N is making some kind of a Laplace transformation from one variable to another variable. And now know actually what my answer was for the partition function. t was 1 over N factorial V over lambda cubed raised to the power of N. o have 1 over N factorial lambda to the power of 3N. And then have to do one of these integrals that we have seen many times, the integral of V to the N against an exponential. Thats something that actually we used in order to define N factorial, except that have to dimensionalize this V. o will get a factor of beta P to the power of N plus 1. The N factorials cancel. And the answer is beta P to the power of N plus 1 divided by lambda cubed to the power of N. o my Gibbs free energy, which is minus kT log of the Gibbs partition function, is going to be minus NkT log of this object. have ignored the difference between N and N plus 1. And what will get here is the combination beta P lambda cubed. Yes. ADENE: s your beta P to the N plus 1 in the numerator or the denominator? PROFEOR: Thank you, it should be in the denominator, which means that OK, this one is correct. guess this one was going by dimensions. Because beta PV is dimensionless. All right, yes. ADENE: One other thing, it seems like theres a dimensional mismatch in your expression for Z. Because you have an extra factor of beta P PROFEOR: Exactly right, because this object is a probability density that involves a factor of volume. And as a probability density, the dimension of this will carry an extra factor of volume. o if really wanted to make this quantity dimensionless also, would need to divide by something that has some dimension of volume. But alternatively, can recognize that indeed this is a probability density in volume. o it will have the dimensions of volume. And again, as said, what m really always careful to make sure that is dimensionless is the thing that is proportional to N. f theres a log of a single dimension out here, typically we dont have to worry about it. But if you think about its origin, the origin is indeed that this quantity is a probability density. But it will, believe me, not change anything in your life to ignore that. All right, so once we have this G, then we recognize again, hopefully didnt make any mistakes. G is E plus PV minus T. o dG should be minus dT plus VdP plus mu dN. o that, for example, in this ensemble can ask well, told you what the pressure is. Whats the volume? Volume is going to be obtained as dG by dP at constant temperature and number. o these two are constant. Log P, its derivative is going to give me NkT over P. o again, get another form of the ideal gas equation of state. can ask, whats the chemical potential? t is going to be dG by dN at constant T and P. o go and look at the N dependence. And notice that theres just an N dependence out front. o what will get is kT log of beta P lambda cubed. And if you like, you can check that, say, this expression for the chemical potential and did we derive it somewhere else? Yes, we derived it over here. This expression for the chemical potential are identical once you take advantage of the ideal gas equation of state to convert the V over N in that expression to beta P. And finally, we have the grand canonical. Lets do that also. o now we are going to look at an ensemble where tell you what the temperature is and the chemical potential. But have to tell you what the volume is. And then the statement is that the probability of a particular microstate that will now indicate mu s force system not to be confused with the chemical potential is proportional to e to the beta mu N minus beta H of the microstate energy. And the normalization is this Q, which will be a grand partition function that is function of T, V, and mu. How is this probability normalized? Well, m spanning over a space of microstates that have indefinite number. Their number runs, presumably, all the way from 0 to infinity. And have to multiply each particular segment that has N of these present with e to the beta mu N. Now, once have that segment, what need to do is to sum over all coordinates and momenta as appropriate to a system of N particles. And that, once more, is my partition function Z of T, V, and N. And so since know what that expression is, can substitute it in some e to the beta mu N. And Z is 1 over N factorial V over lambda cubed raised to the power of N. Now fortunately, thats a sum that recognize. t is 1 over N factorial something raised to the Nth power summed over all N, which is the summation for the exponential. o this is the exponential of e to the beta mu V over lambda cubed. o once have this, can construct my G, which is minus kT log of Q, which is minus kT e to the beta mu V divided by lambda cubed. Now, note that in all of the other expressions that had all of these logs of something, they were extensive. And the extensivity was ensured by having results that were ultimately proportional to N for these logs. Lets say have here, have an N here. For the s, have an N there. Previously had also an N here. Now, in this ensemble, dont have N. Extensivity is insured by this thing being proportional to G. Now also remember that G was E minus T minus mu N. But we had another result for extensivity, that for extensive systems, E was T plus mu N minus PV. o this is in fact because of extensivity, we had expected it to be proportional to the volume. And so this combination should end up being in fact the pressure. can see what the pressure is in different ways. can, for example, look at what this dG is. t is minus dT. t is minus PdV minus Nd mu. could, for example, identify the pressure by taking a derivative of this with respect to volume. But it is proportional to volume. o again get that this combination really should be pressure. You say, dont recognize that as a pressure. You say, well, its because the formula for pressure that we have been using is always in terms of N. o lets check that. o what is N? can get N from minus dG by d mu. And what happens if do that? When take a derivative with respect to mu, will bring down a factor of beta. Beta will kill the kT. will get e to the beta mu V over lambda cubed, which by the way is also these expressions that we had previously for the relationship between mu and N over V lambda cubed if just take the log of this expression. And then if substitute e to the beta mu V over lambda cubed to BN, you can see that have the thing that was calling pressure is indeed NkT over V. o everything is consistent with the ideal gas law. The chemical potential comes out consistently extensivity, everything is correctly identified. aybe one more thing that note here is that, for this ideal gas and this particular form that have for this object so lets maybe do something here. Note that the N appears in an exponential with e to the beta mu. o another way that could have gotten my N would have been d log Q with respect to beta mu. And again, my log Q is simply V over lambda cubed e to the beta mu. And you can check that if do that, will get this formula that had for N. Well, the thing is that can get various cumulants of this object by continuing to take derivatives. o take m derivatives of log of Q with respect to beta mu. o have to keep taking derivatives of the exponential. And as long as keep taking the derivative of the exponential, will get the exponential back. o all of these things are really the same thing. o all cumulants of the number fluctuations of the gas are really the same thing as a number. an you remember what that says? Whats the distribution? ADENE: PROFEOR: Poisson, very good. The distribution where all of the cumulants were the same is Poisson distribution. Essentially it says that if take a box, or if just look at that imaginary volume in this room, and count the number of particles, as long as it is almost identical, the distribution of the number of particles within the volume is Poisson. You know all of the fluctuations, et cetera. Yes. ADENE: o this expression, you have N equals e to the beta mu V divided by lambda to the third. onsidering that expression, could you then say that the exponential quantity is proportional to the phase space density ? PROFEOR: Lets rearrange this. Beta mu is log of N over V lambda cubed. This is a single particle density. o beta mu, or the chemical potential up to a factor of kT, is the log of how many particles fit within one de Broglie volume. And this expression is in fact correct only in the limit where this is small. And as we shall see later on, there will be quantum mechanical corrections when this combination is large. o this combination is very important in identifying when things become quantum mechanic. All right, so lets just give a preamble of what we will be doing next time. should have erased the . o we want to now do interacting systems. o this one example of the ideal gas did for you to in all possible ensembles. And could do that, because it was a collection of noninteracting degrees of freedom. As soon as have interactions among my huge number of degrees of freedom, the story changes. o lets, for example, look at a generalization of what had written before a one particle description which if stop here gives me an ideal system, and potentially some complicated interaction among all of these coordinates. This could be a pairwise interaction. t could have three particles. t could potentially at this stage want to write down the most general form. want to see what can learn about the properties of this modified version, or nonideal gas, and the ensemble that will choose initially. icrocanonical is typically difficult. will go and do things canonically, which is somewhat easier. And later on, maybe even grand canonical is easier. o what do have to do? would say that the partition function is obtained by integrating over the entirety of this phase space product d cubed Pi d cubed qi. will normalize things by N factorial, dimensionalize them by h to the 3N. And have exponential of minus beta sum over i Pi squared over 2m. And then have the exponential of minus . Now, this time around, the P integrals are . can do them immediately. Because typically the momenta dont interact with each other. And practicality, no matter how complicated a system of interactions is, you will be able to integrate over the momentum degrees of freedom. And what you get, you will get this factor of 1 over lambda to the power of 3N from the 3N momentum integrations. The thing that is hard there will be this factor of 1 over N factorial is the integration over all of the coordinates, d cubed qi of this factor of e to the minus beta . gave you the most general possible . Theres no way that can do this integration. What will do is will divide each one of these integrations over coordinate of particle by its volume. will therefore have V to the N here. And V to the N divided by lambda to the 3N N factorial is none other than the partition function that we had calculated before for the ideal gas. And will call Z0 for the ideal gas. And claim that this object can interpret as a kind of average of a function e to the beta defined over the phase space of all of these particles where the probability to find each particle is uniform in the space of the box, lets say. o what this says is for the 0th order case, for the ideal case that we have discussed, once have set the box, the particle can be anywhere in the box uniformly. For that uniform description probability, calculate what the average of this quantity is. o what we have is that Z is in fact Z0, that average that can expand. And what we will be doing henceforth is a perturbation theory in powers of . Because know how to do things for the case of equals 0. And then hope to calculate things in various powers of . o will do that expansion. And then say, no really, what m interested in is something like a free energy, which is log Z. And so for that, will need log of Z0 plus log of that series. But the log of these kinds of series know can write as minus beta to the l over l factorial, replacing, when go to the log, moments by corresponding cumulants. o this is called the cumulant expansion, which we will carry out next time around. Yes. ADENE: n general, . PROFEOR: For some cases, you can. For many cases, you find that that will give you wrong answers. Because the phase space around which youre expanding is so broad. t is not like a saddle point where you have one variable you are expanding in a huge number of variables.","And so it is proportional to e to the minus beta H of the microstate plus this contribution Jx that m getting from the other degree of freedom, which is the spring, which keeps the whole thing at fixed J. And have to divide by some normalization that in addition includes integration over x. And this will depend on T, J, N. o in this system, both x and the energy of the system are variables. o the variables that m not sure of are mu and x. Now, once have said what mu and x is, and ve put the system in contact with a bath at temperature T, can say, OK, the whole thing is canonical, the energy of the entire system composed of the energy of this, which is H of mu, and the energy of the spring, which is Jx. o since omega can be written as exponential of s over kB, this you can also think of as being proportional to exponential of minus beta E minus T, the entropy that would get for that energy according to the formula above, divided by Z. Now, we said that the quantity that m looking at here in the numerator has a dependence on the size of the system that is extensive. What can now do is to either characterize like did over here what the probability of x is, or go by this other route and calculate what the entropy of this probability is, which would be minus the log of the corresponding probability. And what that really means is that you have to keep track of the energy of the microstate, as well as the energy that you extract from the spring, which is something like J dot x. And since the joint system is in the canonical state, it would say that the probability for the joint system, which has T and J and N specified, but dont know the microstate, and dont know the actual displacement x, this joint probability is canonical. Now, if do the same thing in the canonical form, in which the microstate that m looking at is temperature, volume, number of particles, then the probability of a microstate, given that ve specified now the temperature, is proportional to the energy of that microstate, which is e to the minus beta sum over i Pi squared over 2m. The thing that is hard there will be this factor of 1 over N factorial is the integration over all of the coordinates, d cubed qi of this factor of e to the minus beta . And so will get e to the minus beta E divided by Z, which is the probability of getting that microstate that has the right energy. And then the statement is that the probability of a particular microstate that will now indicate mu s force system not to be confused with the chemical potential is proportional to e to the beta mu N minus beta H of the microstate energy. And then, what will have is e to the beta mu N minus beta H of the microstates energy divided by the normalization, which is the grand partition function Q. o this was the Gibbs partition function. o then you would say that the net energy that have to look at is the kinetic energy of the gas particles here plus the potential energy of this weight that is going up and down. And that, once more, is my partition function Z of T, V, and N. And so since know what that expression is, can substitute it in some e to the beta mu N. And Z is 1 over N factorial V over lambda cubed raised to the power of N. Now fortunately, thats a sum that recognize. o this is going to give me d by d beta with a minus sign of the expectation value of H, which is the same thing as kBT squared, because the derivative of 1 over kT is 1 over T squared times derivative with respect to T. And it goes to the other side, to the numerator. And we ended up with the probability of a microstate in the canonical ensemble, which was related to the energy of that by the form exponential of minus beta h where we introduced beta to be 1 over kT. We have that, for example, P is minus dF by dV at constant T and N. o have to go and look at this expression where the V up here, it just appears in log V. o the answer is going to be NkT over V. can calculate the chemical potential. But if focus on temperature, which is the inverse beta, you can see it appears with a factor of 3N over 2. o this is beta to the minus 3N over 2. o this is going to be 3N over 2 derivative of log beta with respect to beta, which is 1 over beta, which is the same thing as 3 over 2 NkT. And the answer is beta P to the power of N plus 1 divided by lambda cubed to the power of N. o my Gibbs free energy, which is minus kT log of the Gibbs partition function, is going to be minus NkT log of this object. And claim that this object can interpret as a kind of average of a function e to the beta defined over the phase space of all of these particles where the probability to find each particle is uniform in the space of the box, lets say. Well, there is log of this minus beta H. o what will have, because of the change in sign beta expectation value of H. And then have minus log Z here. PROFEOR: Yes, and when we are doing this for the case of the ideal gas, will be very careful to do that. Well, what need to do is to sum over all the microstates H of the microstate e to the minus beta H divided by sum over microstates e to the minus beta H, which is the normalization. o what this is is minus the log Z by d beta. o this is the probability of the microstate. o we would say that the probability here of a microstate is of the form 0 or 1 depending on whether the energy of that microstate is or is not the right energy that is listed over here. will get e to the beta mu V over lambda cubed, which by the way is also these expressions that we had previously for the relationship between mu and N over V lambda cubed if just take the log of this expression. o this is the exponential of e to the beta mu V over lambda cubed. Now we can see that in this canonical ensemble, the result that we had to do a couple of lines of algebra to get, which is that the momentum of a particle is Gaussian distributed, is automatically satisfied. o a particular state of the system have to specify where the piston is, how big x is, and what is the microstate of the system. And then if substitute e to the beta mu V over lambda cubed to BN, you can see that have the thing that was calling pressure is indeed NkT over V. o everything is consistent with the ideal gas law. And you can check that if do that, will get this formula that had for N. Well, the thing is that can get various cumulants of this object by continuing to take derivatives. Really, the thing that we are calculating always in order to make computation is something like a free energy, which is log Z. o when take the log, log of Z is going to be this minus beta F star . o what if wanted to maintain the system at some fixed temperature, but rather than telling you what the volume of the box is, will tell you what its pressure is and how many particles have? The normalization was omega E, V, N. And integrating over particles numbers two to N, where the energy that is left to them is E minus the kinetic energy of the one particle, gave me this expression. ADENE: o do you define, then, what the microstate of the system really is if the particle number is ? o the normalization we called Z. And Z is attained by summing over all of the microstates E to the minus beta H of the microstate where this could be an integration over the entire phase space if youre dealing with continuous variables. This expression for the chemical potential are identical once you take advantage of the ideal gas equation of state to convert the V over N in that expression to beta P. And finally, we have the grand canonical. And again, you can identify what the log of this grand partition is going to be. And we therefore concluded that the cumulant would be obtained by the same procedure, except that will replace this by log Z. o this will be the nth derivative of log Z with respect to beta. And what you get, you will get this factor of 1 over lambda to the power of 3N from the 3N momentum integrations. But maybe in other circumstances PROFEOR: Yes, so if you are in another circumstance, like you have one gas that is expanding in a nonlinear medium, then what would do is would calculate the pressure of gas as a function of its volume, et cetera. And the most kind of simple logical place to start was in the microcanonical ensemble where the microstate that we specified was one in which there was no exchange of work or heat with the surroundings so that the energy was constant, and the parameters, such as x and N, that account for chemical and mechanical work were fixed also. o essentially, the picture that we have is that somehow my system is parametrized by some quantity x. And m maintaining the system at fixed value of some force J. o x is allowed potentially to find the value that is consistent with this particular J that impose on the system. And have to multiply each particular segment that has N of these present with e to the beta mu N. Now, once have that segment, what need to do is to sum over all coordinates and momenta as appropriate to a system of N particles. But most importantly, the statement is that all of these quantities are things that are of the order of the size of the system. And the microscopic description of the ideal gas was that have to sum over its Hamiltonian is composed of the kinetic energies of the particles. And we also had a rule for converting probabilities to entropies, which in dimensionless form that is, if we divide by kB was simply minus the expectation value of the log of the probability, this probability being uniformly 1 over omega. And in fact, even the second cumulant we see is of the order of square root of n. And hence the fluctuations between these two quantities, which are each one of them order of n, is only of the order of square root of n. And when the limit of n goes to infinity, we can ignore any such difference. And it is only when m at the maximum that can replace this epsilon with E. Then since with almost probability 1, m going to see this state and none of the other states, because of this exponential dependence. And the normalization is this Q, which will be a grand partition function that is function of T, V, and mu. And what is the log in this case? The normalization of the probability in the canonical ensemble, which is also called the partition function if take its log, will give me the free energy. And again, as said, what m really always careful to make sure that is dimensionless is the thing that is proportional to N. f theres a log of a single dimension out here, typically we dont have to worry about it. And V to the N divided by lambda to the 3N N factorial is none other than the partition function that we had calculated before for the ideal gas. For all of the gas particles for this additional weight that is going up and down, it will give me a contribution that is minus beta PV. And to the question of, what is the displacement, since its a random variable now, can again try to go to the same procedure as did over here. Essentially it says that if take a box, or if just look at that imaginary volume in this room, and count the number of particles, as long as it is almost identical, the distribution of the number of particles within the volume is Poisson. Thats something that actually we used in order to define N factorial, except that have to dimensionalize this V. o will get a factor of beta P to the power of N plus 1. t is going to be minus beta E minus T. Rather than minus Jx, it will be minus mu N. And this combination is what we call the grand potential g, which is E minus T minus mu N. You can do thermodynamics. o the average number in the system, which now we can use as the thermodynamic number, as weve seen, would be d of log of Q with respect to d beta mu. o thermodynamically, you would have said that x is obtained as a derivative of G with respect to J. Now, log Z is something that is like beta G. At fixed temperature, can remove these betas. And then say, no really, what m interested in is something like a free energy, which is log Z. And so for that, will need log of Z0 plus log of that series. When we do it for the case of the gas, will imagine that have a box. And again, have put the whole system in contact with the reservoir temperature T so that if now say that really maintain the system at variable x, but fix J, have to put this spring over there. And then have to do one of these integrals that we have seen many times, the integral of V to the N against an exponential. And so the canonical probability for the joint system is composed of the net energy of the two. Now, in the limit of large N, this is order of N. This is order of log N. And . And then have the exponential of minus . The numerator can be obtained by taking a derivative of this with respect to beta of the minus sign. We have that the free energy is minus kT log Z. o F is minus kT log of this partition function. But just like the energy here was well defined up to something that was square root of N, this x is well defined up to something that is of the order square root of N. Because again, you can second look at the variance. And then have to integrate given that have some particular V that then have to integrate over all of the microstates that are confined within this volume. And then this quantity, if identify this with energy of the system, is simply the usual heat capacity of constant x. o the scale of the fluctuations over here is set by square root of kBT squared the heat capacity. And that was the entropy of the ideal gas. The kBs disappear, and will get minus T. o we see that can identify log Z with the combination E minus T, which is the Helmholtz free energy. And this is now the variance of something and is positive. o once have this, can construct my G, which is minus kT log of Q, which is minus kT e to the beta mu V divided by lambda cubed. Basically, we saw that if repeat this procedure many times, can see that the nth moment is minus 1 to the n 1 over Z the nth derivative of Z with respect to beta. And so this is the probability in this state. inus J would call this the actual thermodynamic displacement x. And from down here, the other side, will get a factor of T. o this should remind you that we had called a Gibbs free energy the combination of E minus T minus Jx. o now we are going to look at an ensemble where tell you what the temperature is and the chemical potential. And so you can see again that Z has to be so basically what m saying is that this quantity has to be 1 so that you get this relationship back from that perspective also. We got N log of V. From the log of N factorial, tirlings approximation gave us a factor of N over e. All of the other factors were proportional to 3N over 2. o the N is out front. The for this, what was it? But have to tell you what the volume is. Again, the is dE over T plus PdV over T minus mu dN over T. o you can immediately identify, for example, that 1 over T derivative with respect to energy would give me a factor of N/E. can go microcanonically and say that know the energy volume and the number of particles. And partial derivative with respect to energy was identified as 1 o systems that were in equilibrium with each other, this derivative d by dE had to be the same for all of them. o this is in fact because of extensivity, we had expected it to be proportional to the volume. o if take a derivative with respect to beta J, will bring down a factor of x. o if take the derivative of log of this Z tilde with respect to beta J, would generate what the mean value is. o over k is expectation value of log p. And what is log of p? For the energy, the reason dont like to do that is because direct deltas have dimensions of 1 over whatever thing is inside. But now have told you that J know what J is, what the force is. now have one additional variable, which is where this piston is located in order to ensure that it is at the right pressure. o what this says is for the 0th order case, for the ideal case that we have discussed, once have set the box, the particle can be anywhere in the box uniformly. But the log of these kinds of series know can write as minus beta to the l over l factorial, replacing, when go to the log, moments by corresponding cumulants. ADENE: o this expression, you have N equals e to the beta mu V divided by lambda to the third.",0.1124791318864774
12,12,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PHLPPE RGOLLET: of our limiting distribution, which happen to be Gaussian. But if the central limit theorem told us that the limiting distribution of some average was something that looked like a Poisson or an then we would just have in the same way taken the quintiles of the exponential distribution. o lets go back to what we had. o generically if you have a set of observations X1 to Xn. o remember for the kiss example they were denoted by R1 to Rn, because they were turning the head to the right, but lets just go back. We say X1 to Xn, and in this case m going to assume theyre D, and m going to make them Bernoulli with p, and p is unknown, right? o what did we do from here? Well, we said p is the expectation of Xi, and actually we didnt even think about it too much. We said, well, if need to estimate the proportion of people who turn their head to the right when they kiss, just basically m going to compute the average. o our p hat was just Xn bar, which was just 1 over n sum from i over 1 2n of the Xi. The average of the observations was their estimate. And then we wanted to build some confidence intervals around this. o what we wanted to understand is, how much that this p hat fluctuates. This is a random variable. ts an average of random variables. ts a random variable, so we want to know what the distribution is. And if we know what the distribution is, then we actually know, well, where it fluctuates. What the expectation is. Around which value it tends to fluctuate et cetera. And so what the central limit theorem told us was if take square root of n times Xn bar minus p, which is its average. And then divide it by the standard deviation. Then this thing here converges as n goes to infinity, and we will say a little bit more about what it means in distribution to some standard normal random variable. o that was the central limit theorem. o what it means is that when think of this as a random variable, when n is large enough its going to look like this. And so understand perfectly its fluctuations. know that this thing here has know the probability of being in this zone. know that this number here is 0. know a bunch of things. And then, in particular, what was interested in was that the probability, thats the absolute value of a Gaussian random variable, exceeds q alpha over 2, q alpha over 2. We said that this was equal to what? Anybody? What was that? ADENE: PHLPPE RGOLLET: Alpha, right? o thats the probability. Thats my random variable. o this is by definition q alpha over 2 is the number. o that to the right of it is alpha over 2. And this is a negative q alpha over 2 by symmetry. And so the probability that i exceeds well, its not very symmetric, but the probability that i exceeds this value, q alpha over 2, is just the sum of the two gray areas. All right? o now said that this thing was approximately equal, due to the central limit theorem, to the probability, that square root of n. Xn bar minus p divided by square root p 1 minus p. Well, absolute value was larger than q alpha over 2. Well, then this thing by default is actually approximately equal to alpha, just because of virtue of the central limit theorem. And then we just said, well, ll solve for p. Has anyone attempted to solve the degree two equation for p in the homework? Everybody has tried it? o essentially, this is going to be an equation in p. ometimes we dont want to solve it. ome of the ps we will replace by their worst possible value. For example, we said one of the tricks we had was that this value here, square root of p 1 minus p, was always less than one half. ntil we could actually get the confidence interval that was larger than all possible confidence intervals for all possible values of p, but we could solve for p. Do we all agree on the principle of what we did? o thats how you build confidence intervals. Now lets step back for a second, and see what was important in the building of this confidence interval. The really key thing is that didnt tell you why formed this thing, right? We started from x bar, and then took some weird function of x bar that depended on p and n. And the reason is, because when take this function, the central limit theorem tells me that it converges to something that know. But this very important thing about the something that know is that it does not depend on anything that dont know. For example, if forgot to divide by square root of p 1 minus p, then this thing would have had a variance, which is the p 1 minus p. f didnt remove this p here, the mean here would have been affected by p. And theres no table for normal p 1. Yes? ADENE: PHLPPE RGOLLET: Oh, so the square root of n terms come from. o really you should view this. o theres a rule and sort of a quiet rule in math that you dont write a divided by b over c, right? You write c times a divided by b, because it looks nicer. But the way you want to think about this is that this is x bar minus p divided by the square root of p 1 minus p divided by n. And the reason is, because this is actually the standard deviation of this oh sorry, x bar n. This is actually the standard deviation of this guy, and the square root of n comes from the average. o the key thing was that this thing, this limiting distribution did not depend on anything dont know. And this is actually called a pivotal distribution. ts pivotal. dont need anything. dont need to know anything, and can read it in a table. ometimes theres going to be complicated things, but now we have computers. The beauty about Gaussian is that people have studied them to death, and you can open any stats textbook, and you will see a table again that will tell you for each value of alpha youre interested in, it will tell you what q alpha over 2 is. But there might be some crazy distributions, but as long as they dont depend on anything, we might actually be able to simulate from them, and in particular compute what q alpha over 2 is for any possible value .. And so thats what were going to be trying to do. Finding pivotal distributions. How do we take this Xn bar, which is a good estimate, and turn it into something which may be exactly or asymptotically does not depend on any unknown parameter. o here is one way we can actually so thats what we did for the kiss example, right? And here mentioned, for example, in the extreme case, when n was equal to 3 we would get a different thing, but here the LT would not be valid. And what that means is that my pivotal distribution is actually not the normal distribution, but it might be something else. And said we can make take exact computations. Well, lets see what it is, right? f have three observations, so m going to have X1, X2, X3. o now take the average of those guys. OK, so thats my estimate. How many values can this guy take? ts a little bit of counting. Four values. How did you get to that number? OK, so each of these guys can take value 0, 1, right? o the number of values that it can take, mean, its a little annoying, because then have to sum them, right? o basically, have to count the number of 1s. o how many 1s can get, right? orry have to yeah, so this is the number of 1s that OK, so lets look at that. o we get 0, 0, 0. 0, 0, 1. And then get basically three of them that have just the one in there, right? o theres three of them. How many of them have exactly two 1s? 2. orry, 3, right? o its just this guy where replaced the 0s and the 1. OK, so now get so here get three that take the value 1, and one that gets the value 0. And then get three that take the value 2, and then one that takes the value 1. The value 1s, right? OK, so everybody knows what m missing here is just the ones here where replaced the 0s by 1s. o the number of values that this thing can take is 1, 2, 3, 4. o someone is counting much faster than me. And so those numbers, youve probably seen them before, right? 1, 3, 3, 1, remember? And so essentially those guys, it takes only three values, which are either 1/3, 1. orry, 1/3. Oh OK, so its 0, sorry. 1/3, 2/3, and 1. Those are the four possible values you can take. And so now which is probably much easier to count like that and so now all have to tell you if want to describe the distribution of this probability of this random variable, is just the probability that it takes each of these values. o X bar 3 takes the value 0 probability that X bar 3 takes the value 1/3, et cetera. f give you each of these possible values, then you will be able to know exactly what the distribution is, and hopefully maybe to turn it into something you can compute. Now the thing is that those values will actually depend on the unknown p. What is the unknown p here? What is the probability that X bar 3 is equal to 0 for example? m sorry? ADENE: PHLPPE RGOLLET: Yeah, OK. o lets write it without making the computation o 1/8 is probably not the right answer, right? For example, if p is equal to 0, what is this probability? 1. f p is 1, what is this probability? 0. o it will depend on p. o the probability that this thing is equal to 0, is just the probability that all three of those guys are equal to 0. The probability that X1 is equal to 0, and X2 is equal to 0, and X3 is equal to 0. Now my things are independent, so do what actually want to do, which say the probability of the intersection is the product of the probabilities, right? o its just the probability that each of them is equal to 0 to the power of 3. And the probability that each of them, or say one of them is equal to 0, is just 1 minus p. And then for this guy just get the probability well, its more complicated, because have to decide which one it is. But those things are just the probability of some binomial random variables, right? This is just a binomial, X bar 3. o if look at X bar 3, and then multiply it by 3, its just this sum of independent Bernoullis with parameter p. o this is actually a binomial with parameter 3 and p. And theres tables for binomials, and they tell you all this. Now the thing is want to invert this guy, right? omehow. This thing depends on p. dont like it, so m going to have to find ways to get this things depending on p, and could make all these nasty computations, and spend hours doing this. But theres tricks to go around this. Theres upper bounds. Just like we just said, well, maybe dont want to solve the second degree equation in p, because its just going to capture maybe smaller order terms, right? Things that maybe wont make a huge difference numerically. You can check that in your problem set one. Does it make a huge difference numerically to solve the second degree equation, or to just use the p 1 minus p or even to plug in p hat instead of p. Those are going to be the problem set one is to make sure that you see what magnitude of changes you get by changing from one method to the other. o what wanted to go to is something where we can use something, which is just a little more brute force. o the probability that so here is this Hoeffdings inequality. We saw that. Thats what weve finished on last time. o Hoeffdings inequality is actually one of the most useful inequalities. f any one of you is doing anything really to algorithms, youve seen that inequality before. ts extremely convenient that it tells you something about bounded random variables, and if you do algorithms typically with things bounded. And thats the case of Bernoullis random variables, right? Theyre bounded between 0 and 1. And so when do this thing, when do Hoeffdings inequality, what this thing is telling me is for any given epsilon here, for any given epsilon, what is the probability that Xn bar goes away from its expectation? All right, then we saw that it decreases somewhat similarly to the way a Gaussian would look like. o essentially what Hoeffdings inequality is telling me, is that have this picture, when have a Gaussian with mean u, know it looks like this, right? What Hoeffdings inequality is telling me is that if actually take the average of some bounded random variables, then their probability distribution function or maybe math function this thing might not even have the density, but lets think of it as being a density just for simplicity its going to be something thats going to look like this. ts going to be somewhat well, sometimes its going to have to escape just for the sake of having integral 1. But its essentially telling me that those guys stay below those guys. The probability that Xn bar exceeds mu is bounded by something that decays like to tail of Gaussian. o really thats the picture you should have in mind. When average bounded random variables, actually have something that might be really rugged. t might not be smooth like a Gaussian, but know that its always bounded by a Gaussian. And whats nice about it is that when actually start computing probability that exceeds some number, say alpha over 2, then know that this can actually get a number, which is just sorry, the probability that it exceeds, yeah. o this number that get here is actually going to be somewhat smaller, right? o thats going to be the q alpha over 2 for the Gaussian, and thats going to be the dont know, r alpha over 2 for this random variable. Like q prime or different q. o can actually do this without actually taking any limits, right? This is valid for any n. dont need to actually go to infinity. Now this seems a bit magical, right? mean, just said we need n to be, we discussed that we wanted n to be larger than 30 last time for the central limit theorem to kick in, and this one seems to tell me can do it for any n. Now there will be a price to pay is that pick up this 2 over b minus alpha squared. o thats the variance of the Gaussian that have, right? ort of. Thats telling me what the variance should be, and this is actually not as nice. pick factor 4 compared to the Gaussian that would get for that. o lets try to solve it for our case. o just told you try it. Did anybody try to do it? o we started from this last time, right? And the reason was that we could say that the probability that this thing exceeds q alpha over 2 is alpha. o that was using LT, so lets just keep it here, and see what we would do differently. What Hoeffding tells me is that the probability that Xn bar minus well, what is mu in this case? ts p, right? ts just notation here. u was the average, but we call it p in the case of Bernoullis, exceeds lets just call it epsilon for a second. o we said that this was bounded by what? o Hoeffding tells me that this is bounded by 2 times exponential minus 2. Now the nice thing is that pick up a factor n here, epsilon squared. And what is b minus a squared for the Bernoullis? 1. o dont have a denominator here. And m going to do exactly what did here. m going to set this guy to be equal to alpha. o that if get alpha here, then that means that just solving for epsilon, m going to have some number, which will play the role of q alpha over 2, and then m going to be able to just say that p is between X bar and minus epsilon, and X bar n plus epsilon. OK, so lets do it. o we have to solve the equation. 2 exponential minus 2n epsilon squared equals alpha, which means that so here m going to get, theres a 2 right here. o that means that get alpha over 2 here. Then take the logs on both sides, and now let me just write it. And then want to solve for epsilon. o that means that epsilon is equal to square root log q over alpha divided by 2n. Yes? ADENE: PHLPPE RGOLLET: Why is b minus a 1? Well, lets just look, right? X lives in the interval b minus a. o can take b to be 25, and a to be my negative 42. But m going to try to be as sharp as can. All right, so what is the smallest value you can think of such that a Bernoulli random variable is larger than or equal to this value? What values does a Bernoulli random variable take? 0 and 1. o it takes values between 0 and 1. t just maxes the value. Actually, this is the worst possible case for the Hoeffding inequality. o now just get this one, and so now you tell me that have this thing. o when solve this guy over there. o combining this thing and this thing implies that the probability that p lives between Xn bar minus square root log 2 over alpha divided by 2n and X bar plus the square root log 2 over alpha divided by 2n is equal to? mean, is at least. What is it at least equal to? Here this controls the probability of them outside of this interval, right? t tells me the probability that Xn bar is far from p by more than epsilon. o theres a probability that theyre actually outside of the interval that just wrote. o its 1 minus the probability of being in the interval. o this is at least 1 minus alpha. o just use the fact that a probability of the complement is 1 minus the probability of the set. And since have an upper bound on the probability of the set, have a lower bound on the probability of the complement. o now its a bit different. Before, we actually wrote something that was so let me get it back. o if we go back to the example where we took the over p, we got this guy. q alpha over square root of over 2 square root n. o we had Xn bar plus minus q alpha over 2 square root n. Actually, that was q alpha over 2n, m sorry about that. And so now we have something that replaces this q alpha, and its essentially square root of 2 log 2 over alpha. Because if replace q alpha by square root of 2 log 2 over alpha, actually get exactly this thing here. And so the question is, what would you guess? s this number, this margin, square root of log 2 over alpha divided by 2n, is it smaller or larger than this guy? q alpha all over 2/3n. Yes? Larger. Everybody agrees with this? Just qualitatively? Right, because we just made a very conservative statement. We do not use anything. This is true always. o it can only be better. The reason in statistics where you use those assumptions that n is large enough, that you have this independence that you like so much, and so you can actually have the central limit theorem kick in, all these things are for you to have enough assumptions so that you can actually make sharper and sharper decisions. ore and more confident statement. And thats why theres all this junk science out there, because people make too much assumptions for their own good. Theyre saying, well, lets assume that everything is the way love it, so that can for sure any minor change, will be able to say thats because made an important scientific discovery rather than, well, that was just OK? o now heres the fun moment. And actually let me tell you why we look at this thing. o theres actually who has seen different types of convergence in the probability statistic class? students. And so theres different types of in the real numbers theres very simple. Theres one convergence, Xn turns to X. To start thinking about functions, well, maybe you have uniform convergence, you have pointwise convergence. o if youve done some real analysis, you know theres different types of convergence you can think of. And in the convergence of random variables, theres also different types, but for different reasons. ts just because the question is, what do you do with the randomness? When you see that something converges to something, it probably means that youre willing to tolerate low probability things happening or where it doesnt happen, and on how you handle those, creates different types of convergence. o to be fair, in statistics the only convergence we care about is the convergence in distribution. Thats this one. The one that comes from the central limit theorem. Thats actually the weakest possible you could make. Which is good, because that means its going to happen more often. And so why do we need this thing? Because the only thing we really need to do is to say that when start computing probabilities on this random variable, theyre going to look like probabilities on that random variable. All right, so for example, think of the following two random variables, x and minus x. o this is the same random variable, and this one is negative. When look at those two random variables, think of them as a sequence, a constant sequence. These two constant sequences do not go to the same number, right? One is plus one is x, the other one is minus x. o unless x is the random variable always equal to 0, those two things are different. However, when compute probabilities on this guy, and when compute probabilities on that guy, theyre the same. Because x and minus x have the same distribution just by symmetry of the gaps in random variables. And so you can see this is very weak. m not saying anything about the two random variables being close to each other every time m going to flip my coin, right? aybe m going to press my computer and say, what is x? Well, its 1.2. Negative x is going to be negative 1.2. Those things are far apart, and it doesnt matter, because in average those things are going to have the same probabilities thats happening. And thats all we care about in statistics. You need to realize that this is whats important, and why you need to know. Because you have it really good. f your problem is you really care more about convergence almost surely, which is probably the strongest you can think of. o what were going to do is talk about different types of convergence not to just reflect on the fact on how our life is good. ts just that the problem is that when the convergence in distribution is so weak that it cannot do anything want with it. n particular, cannot say that if X converges, Xn converges in distribution, and Yn converges in distribution, then Xn plus Yn converge in distribution to the sum of their limits. cannot do that. ts just too weak. Think of this example and its preventing you to do quite a lot of things. o this is converge in distribution to sum n 0, 1. This is converge in distribution to sum n 0, 1. But their sum is 0, and its certainly not it doesnt look like the sum of two independent Gaussian random variables, right? And so what we need is to have stronger conditions here and there, so that we can actually put things together. And were going to have more complicated formulas. One of the formulas, for example, is if replace p by p hat in this denominator. We mentioned doing this at some point. o would need that p hat goes to p, but need stronger than n distributions so that this happens. actually need this to happen in a stronger sense. o here are the first two strongest sense in which random variables can converge. The first one is almost surely. And who has already seen this notation little omega when theyre talking about random variables? All right, so very few. o this little omega is so what is a random variable? A random variable is something that you measure on something thats random. o the example like to think of is, if you take a ball of snow, and put it in the sun for some time. You come back. ts going to have a random shape, right? ts going to be a random blurb of something. But theres still a bunch of things you can measure on it. You can measure its volume. You can measure its inner temperature. You can measure its surface area. All these things are random variables, but the ball itself is omega. Thats the thing on which you make your measurement. And so a random variable is just a function of those omegas. Now why do we make all these things fancy? Because you cannot take any function. This function has to be whats called measurable, and theres entire courses on measure theory, and not everything is measurable. And so thats why you have to be a little careful why not everything is measurable, because you need some sort of nice property. o that the measure of something, the union of two things, is less than the sum of the measures, things like that. And so almost surely is telling you that for most of the balls, for most of the omegas, thats the righthand side. The probability of omega is such that those things converge to each other is actually equal to 1. o it tells me that for almost all omegas, all the omegas, if put them together, get something that has probability of 1. t might be that there are other ones that have probability 0, but what its telling is that this thing happens for all possible realization of the underlying thing. Thats very strong. t essentially says randomness does not matter, because its happening always. Now convergence in probability allows you to squeeze a little bit of probability under the rock. t tells you want the convergence to hold, but m willing to let go of some little epsilon. o m willing to allow Tn to be less than epsilon. Tn minus T to be sorry, to be larger than epsilon. But the problem is they want this to go to 0 as well as n goes to infinity, but for each n this thing does not have to be 0, which is different from here, right? o this probability here is fine. o its a little weaker, but its a slightly different one. m not going to ask you to learn and show that one is weaker than the other one. But just know that these are two different types. This one is actually much easier to check than this one. Then theres something called convergence in Lp. o this one is the fact that it embodies the following fact. f give you a random variable with mean 0, and tell you that its variance is going to 0, right? You have a sequence of random variables, their mean is 0, their expectation is 0, but their variance is going to 0. o think of Gaussian random variables with mean 0, and a variance that shrinks to 0. And this random variable converges to a spike at 0, so it converges to 0, right? And so what mean by that is that to have this convergence, all had to tell you was that the variance was going to 0. And so in L2 this is really what its telling you. ts telling you, well, if the variance is going to 0 well, its for any random variable T, so here what describe was for a deterministic. o Tn goes to a random variable T. f you look at the square the expectation of the square distance, and it goes to 0. But you dont have to limit yourself to the square. You can take power of three. You can take power 67.6, power of 9 pi. You take whatever power you want, it can be fractional. t has to be lower than 1, and thats the convergence in Lp. But we mostly care about integer p. And then heres our star, the convergence in distribution, and thats just the one that tells you that when start computing probabilities on the Tn, theyre going to look very close to the probabilities on the T. o that was our Tn with this guy, for example, and T was this standard Gaussian distribution. Now here, this is not any probability. This is just the probability then less than or equal to x. But if you remember your probability class, if you can compute those probabilities, you can compute any probabilities just by subtracting and just building things together. Well, need this for all xs, so want this for each x, o you fix x, and then you make the limit go to infinity. You make n go to infinity, and want this for the point xs at which the cumulative distribution function of T is continuous. There might be jumps, and that dont actually care for those. All right, so here mentioned it for random variables. f youre interested, theres also random vectors. A random vector is just a table of random variables. You can talk about random matrices. And you can talk about random whatever you want. Every time you have an object thats just collecting real numbers, you can just plug random variables in there. And so theres all these definitions that o where see you see an absolute value, well see a norm. Things like this. o m sure this might look scary a little bit, but really what we are going to use is only the last one, which as you can see is just telling you that the probabilities converge to the probabilities. But m going to need the other ones every once in a while. And the reason is, well, OK, so here m actually going to the important characterizations of the convergence in distribution, which is R convergence style. o i converge in distribution if and only if for any function thats continuous and bounded, when look at the expectation of f of Tn, this converges to the expectation of f of T. OK, so this is just those two things are actually equivalent. ometimes its easier to check one, easier to check the other, but in this class you wont have to prove that something converges in distribution other than just combining our existing convergence results. And then the last one which is equivalent to the above two is, anybody knows what the name of this quantity is? This expectation here? What is it called? The characteristic function, right? And so this i is the complex i, and is the complex number. And so its essentially telling me that, well, rather than actually looking at all bounded and continuous but real functions, can actually look at one specific family of complex functions, which are the functions that maps T to E to the ixT for x and R. Thats a much smaller family of functions. All possible continuous embedded functions has many more elements than just the real element. And so now can show that if limit myself to do it, its actually sufficient. o those three things are used all over the literature just to show things. n particular, if youre interested in deep digging a little more mathematically, the central limit theorem is going to be so important. aybe you want to read about how to prove it. Were not going to prove it in this class. Theres probably at least five different ways of proving it, but the most canonical one, the one that you find in textbooks, is the one that actually uses the third element. o you just look at the characteristic function of the square root of n Xn bar minus say mu, and you just expand the thing, and this is what you get. And you will see that in the end, you will get the characteristic function of a Gaussian. Why a Gaussian? Why does it kick in? Well, because what is the characteristic function of a Gaussian? Does anybody remember the characteristic function of a standard Gaussian? ADENE: PHLPPE RGOLLET: Yeah, well, mean theres two pis and stuff that goes away, right? A Gaussian is a random variable. A characteristic function is a function, and so its not really itself. t looks like itself. Anybody knows what the actual formula is? Yeah. ADENE: PHLPPE RGOLLET: E to the minus? ADENE: E to the minus x squared over 2. PHLPPE RGOLLET: Exactly. E to the minus x squared over 2. But this x squared over 2 is actually just the second order expansion in the Taylor expansion. And thats why the Gaussian is so important. ts just the second order Taylor expansion. And so you can check it out. think Terry Tao has some stuff on his blog, and theres a bunch of different proofs. But if you want to prove convergence in distribution, you very likely are going to use one this three right here. o lets move on. This is when said that this convergence is weaker than that convergence. This is what meant. f you have convergence in one style, it implies convergence in the other stuff. o the first is that if Tn converges almost surely, this a dot s dot means almost surely, then it also converges in probability and actually the two limits, which are this random variable T, are equal almost surely. Basically what it means is that whatever you measure one is going to be the same that you measure on the other one. o thats very strong. o that means that convergence almost surely is stronger than convergence in probability. f youre converge in Lp then you also converge in Lq for sum q less than p. o if you converge in L2, youll also converge in L1. f you converge in L67, you converge in L2. f youre converge in L infinity, you converge in Lp for anything. And so, again, limits are equal. And then when you converge in distribution, when you converge in probability, you also converge in distribution. OK, so almost surely implies probability. Lp implies probability. Probability implies distribution. And here note that did not write, and the limits are equal almost surely. Why? Because the convergence in distribution is actually not telling you that your random variable is converging to another random variable. ts telling you that the distribution of your random variable is converging to a distribution. And think of this, guys. x and minus x. The central limit theorem tells me that m converging to some standard Gaussian distribution, but am converging to x or am converging to minus x? ts not well identified. ts any random variable that has this distribution. o theres no way the limits are equal. Their distributions are going to be the same, but theyre not the same limit. s that clear for everyone? o in a way, convergence in distribution is really not a convergence of a random variable towards another random variable. ts just telling you the limiting distribution of your random variable which is enough for us. And one thing thats actually really nice is this continuous mapping theorem, which essentially tells you that so this is one of the theorems that we like, because they tell us you can do what you feel like you want to do. o if have Tn that goes to T, f of Tn goes to f of T, and this is true for any of those convergence except for Lp. But they have to have f, which is continuous, otherwise weird stuff can happen. o this is going to be convenient, because here dont have X to n minus p. have a continuous function. ts between a linear function of Xn minus p, but could think of like even crazier stuff to do, and it would still be true. f took the square, it would converge to something that looks like its distribution. ts the same as the distribution of a square Gaussian. o this is a mouthful, these two slides actually this particular slide is a mouthful. What have in my head since was pretty much where youre sitting, is this diagram. o what it tells me so its actually voluntarily cropped, so you can start from any Lq you want large. And then as you decrease the index, you are actually implying, implying, implying until you imply convergence in probability. onvergence almost surely implies convergence in probability, and everything goes to the that is convergence in distribution. o everything implies convergence in distribution. o thats basically rather than remembering those formulas, this is really the diagram you want to remember. All right, so why do we bother learning about those things. Thats because of this limits and operations. Operations and limits. f have a sequence of real numbers, and know that Xn converges to X and Yn converges to Y, then can start doing all my manipulations and things are happy. can add stuff. can multiply stuff. But its not true always for convergence in distribution. But it is, whats nice, its actually true for convergence almost surely. onvergence almost surely everything is true. ts just impossible to make it fail. But convergence in probability is not always everything, but at least you can actually add stuff and multiply stuff. And it will still give you the sum of the n, and the product of the n. You can even take the ratio if V is not 0 of course. f the limit is not 0, then actually you need Vn to be not 0 as well. You can actually prove this last statement, right? Because its a combination of the first statement of the second one, and the continuous mapping theorem. Because the function that maps x to 1 over x on everything but 0, is continuous. And so 1 over Vn converges to 1 over V, and then can multiply those two things. o you actually knew that one. But really this is not what matters, because this is something that you will do whatever happens. f dont tell you you cannot do it, well, you will do it. But in general those things dont apply to convergence in distribution unless the pair itself is known to converge in distribution. Remember when said that these things apply to vectors, then you need to actually say that the vector converges in distributions to the limiting factor. Now this tells you in particular, since the cumulative distribution function is not defined for vectors, would have to actually use one of the other distributions, one of the other criteria, which is convergence of characteristic functions or convergence of a function of bounded continuous function of the random variable. 0.2 or 0.3, but 0.1 is not going get you anywhere. But this is something thats going to be too hard for us to deal with, so were actually going to rely on the fact that we have something thats even better. Theres something that is waiting for us at the end of his lecture, which is called lutskys that says that if V, in this case, converges in probability but converge in distribution, can actually still do that. actually dont need both of them to converge in probability. actually need only one of them to converge in probability to make this statement. But two sum. o lets go to another example. o just want to make sure that we keep on doing statistics. And every time were going to just do a little bit too much probability, m going to reset the pressure, and start doing statistics again. All right, so assume you observe the times the interarrival time of the T at Kendall. o this is not the arrival time. ts not like 7:56, 8:15. No, its really the interarrival time, right? o say the next T is arriving in six minutes. o lets say bound. And so you have this interarrival time. o those are numbers say, 3, 4, 5, 4, 3, et cetera. o have this sequence of numbers. o m going to observe this, and m going to try to infer what is the rate of Ts going out of the station from this. o m going to assume that these things are mutually independent. Thats probably not completely true. Again, it just means that what it would mean is that two consecutive interarrival times are independent. mean, you can make it independent if you want, but again, this independent assumption is for us to be happy and safe. nless someone comes with overwhelming proof that its not independent and far from being independent, then yes, you have a problem. But it might be the fact that its actually if you have a T thats one hour late. f an interarrival time is one hour, then the other T, either they fixed it, and its going to be just 30 seconds behind, or they havent fixed it, then its going to be another hour behind. o theyre not exactly independent, but they are when things work well and approximate. And so now need to model a random variable thats positive, maybe not upper bounded. mean, people complain enough that this thing can be really large. And so one thing that people like for interarrival times is exponential distribution. o thats a positive random variable. Looks like an exponential on the righthand slide, on the positive line. And so it decays very fast towards 0. The probability that you have very large values exponentially small, and theres a lambda that controls how exponential is defined. ts exponential minus lambda times something. And so were going to assume that they have the same distribution, the same random variable. o theyre D, because they are independent, and theyre identically distributed. They all have this exponential with parameter lambda, and m going to try to learn something about lambda. What is the estimated value of lambda, and can build a confidence interval for lambda. o we observe n arrival times. o as said, the mutual independence is plausible, but not completely justified. The fact that theyre exponential is actually something that people like in all this whats called queuing theory. o exponentials arise a lot when you talk about interarrival times. ts not about the bus, but where its very important is call centers, service, servers where tasks come, and people want to know how long its going to take to serve a task. o when call at a center, nobody knows how long m going to stay on the phone with this person. But it turns out that empirically exponential distributions have been very good at modeling this. And what it means is that theyre actually you have this memoryless property. ts kind of crazy if you think about it. What does that thing say? Lets parse it. Thats the probability. o this is condition on the fact that T1 is larger than T. o T1 is just say the first arrival time. That means that conditionally on the fact that ve been waiting for the first T, well, the first . Well, should probably the first subway for more than T conditionally so ve been there T minutes already. Then the probability that wait for s more minutes. o thats the probability that T1 is learned, and the time that weve already waited plus x. Given that ve been waiting for T minutes, really wait for s more minutes, is actually the probability that wait for s minutes total. ts completely memoryless. t doesnt remember how long have you been waiting. The probability does not change. You can have waited for two hours, the probability that it takes another 10 minutes is going to be the same as if you had been waiting for zero minutes. And thats something thats actually part of your problem set. Very easy to compute. This is just an analytical property. And you just manipulate functions, and you see that this thing just happen to be true, and thats something that people like. Because thats also something that benefit. And also what we like is that this thing is positive almost surely, which is good when you model arrival times. To be fair, were not going to be that careful. Because sometimes we are just going to assume that something follows a normal distribution. And in particular, mean, dont know if were going to go into that details, but a good thing that you can model with a Gaussian distribution are heights of students. But technically with positive probability, you can have a negative Gaussian random variable, right? And the probability being its probably 10 to the minus 25, but its positive. But its good enough for us for our modeling. o this thing is nice, but this is not going to be required. When youre modeling positive random variables, you dont always have to use positive distributions that are supported on positive numbers. You can use distributions like Gaussian. o now this exponential distribution of T1, Tn they have the same parameter, and that means that in average they have the same interarrival time. o this lambda is actually the expectation. And what m just saying is that theyre identically distributed means that mean some sort of a stationary regime, and its not always true. have to look at a shorter period of time, because at rush hour and 11:00 P clearly those average interarrival times are going to be different o it means that am really focusing maybe on rush hour. orry, said its lambda. ts actually 1 over lambda. always mix the two. All right, so you have the density of T1. o f of T is this. o its on the positive real line. The fact that have strictly positive or larger to 0 doesnt make any difference. o this is the density. o its lambda E to the minus lambda T. The lambda in front just ensures that when integrate this function between 0 and infinity, get 1. And you can see, it decays like exponential minus lambda T. o if were to draw it, it would just look like this. o at 0, what value does it take? Lambda. And then decay like exponential minus lambda T. o this is 0, and this is f of T. o very small probability of being very large. Of course, it depends on lambda. Now the expectation, you can compute the expectation of this thing, right? o you integrate T times f of T. This is part of the little sheet that gave you last time. This is one of the things you should be able to do blindfolded. And then you get the expectation of T1 is 1 over lambda. Thats what comes out. o as actually tell many of my students, 99% of statistics is replacing expectations by averages. And so what youre tempted to do is say, well, if in average m supposed to see 1 over lambda, have 15 observations. m just going to average those observations, and m going to see something that should be close to 1 over lambda. o statistics is about replacing averages, expectations with averages, and thats we do. o Tn bar here, which is the average of the Tis, is a pretty good estimator for 1 over lambda. o if want an estimate for lambda, then need to take 1 over Tn bar. o here is one estimator. did it without much principle except that just want to replace expectations by averages, and then fixed the problem that was actually estimating 1 over lambda by lambda. But you could come up with other estimators, right? But lets say this is my way of getting to that estimator. Just like didnt give you any principled way of getting p hat, which is Xn bar in the kiss example. But thats the natural way to do it. Everybody is completely shocked by this approach? All right, so lets do this. o what can say about the properties of this estimator lambda hat? Well, know that Tn bar is going to 1 over lambda by the law of large number. ts an average. t converges to the expectation both almost surely, and in probability. o the first one is the strong law of large number, the second one is the weak law of large number. can apply the strong one. have enough conditions. And hence, what do apply so that 1 over Tn bar actually goes to lambda? o said hence. What is hence? What is it based on? ADENE: PHLPPE RGOLLET Yeah, continuous mapping theorem, right? o have this function 1 over x. just apply this function. o if it was 1 over lambda squared, would have the same thing that would happen just because the function 1 over x is continuous away from 0. And now the central limit theorem is also telling me something about lambda. About Tn bar, right? ts telling me that if look at my average, remove the expectation here. o if do Tn bar minus my expectation, rescale by this guy here, then this thing is going to converge to some Gaussian random variable, but here have this lambda to the negative 1 to the negative 2 here, and thats because they did not tell you that if you compute the variance so from this, you can probably extract. o if have X that follows some exponential distribution with parameter lambda. Well, lets call it T. o we know that T in expectation, the expectation of T is 1 over lambda. What is the variance of T? You should be able to read it from the thing here. 1 over lambda squared. Thats what you actually read in the variance, because the central limit theorem is really telling you the distribution goes through this n. But this numbers and this number you can read, right? f you look at the expectation of this guy its of this guy comes out. This is 1 over lambda minus 1 over lambda. Thats why you read the 0. And if you look at the variance of the dot, you get n times the variance of this average. Variance of the average is picking up a factor 1 over n. o the n cancels. And then m left with only one of the variances, which is 1 over lambda squared. OK, so were not going to do that in details, because, again, this is just a pure calculus exercise. But this is if you compute integral of lambda e to the minus t lambda times t squared. Actually t minus 1 over lambda squared dt between 0 and infinity. You will see that this thing is 1 over lambda squared. How would do this? onfiguration by or you know it. All right. o this is what the central limit theorem tells me. o this gives me if solve this, and plug in so can multiply by lambda and solve, it would give me somewhat a confidence interval for 1 over lambda. f we just think of 1 over lambda as being the p that had before, this would give me a central limit theorem for sorry, a confidence interval for 1 over lambda. o m hiding a little bit under the rug the fact that have to still define it. Lets just actually go through this. see some of you are uncomfortable with this, so lets just do it. o what weve just proved by the central limit theorem is that the probability, thats square root of n Tn minus 1 over lambda exceeds q alpha over 2 is approximately equal to alpha, right? Thats just the statement of the central limit theorem, and by approximately equal mean as n goes to infinity. orry did not write it correctly. still have to divide by square root of 1 over lambda squared, which is the standard deviation, right? And we said that this is a bit ugly. o lets just do it the way it should be. o multiply all these things by lambda. o that means now that the absolute value, so with probability 1 minus alpha asymptotically, have that square root of n times lambda Tn minus 1 is less than or equal to q alpha over 2. o what it means is that, oh, have negative q alpha over 2 less than square root of n. Let me divide by square root of n here. lambda Tn minus 1 q alpha over 2. And so now what have is that get that lambda is between thats Tn bar is between 1 plus q alpha over 2 divided by root n. And the whole thing is divided by Tn bar, and same thing on the other side except have 1 minus q alpha over 2 divided by root n divided by Tn bar. o its kind of a weird shape, but its still of the form 1 over Tn bar plus or minus something. But this something depends on Tn bar itself. And thats actually normal, because Tn bar is not only giving me information about the mean, but its also giving me information about the variance. o it should definitely come in the size of my error bars. And thats the way it comes in this fairly natural way. Everybody agrees? o now have actually built a confidence interval. But what want to show you with this example is, can translate this in a central limit theorem for something that converges to lambda, right? know that Tn bar converges to 1 over lambda, but also know that 1 over Tn bar converges to lambda. o do have a central limit theorem for 1 over Tn bar? Technically no, right? entral limit theorems are about averages, and 1 over an average is not an average. But theres something that statisticians like a lot, and its called the Delta method. The Delta method is really something thats telling you that you can actually take a function of an average, and let it go to the function of the limit, and you still have a central limit theorem. And the factor or the price to pay for this is something which depends on the derivative of the function. And so lets just go through this, and its, again, just like the proof of the central limit theorem. And actually in many of those asymptotic statistics results, this is actually just a Taylor expansion, and here its not even the second order, its actually the first order, all right? o m just going to do linear approximation of this function. o lets do it. o have that g of Tn bar actually lets use the notation of this slide, which is Zn and theta. o what know is that Zn minus theta square root of n goes to some Gaussian, this standard Gaussian. No, not standard. OK, so thats the assumptions. And what want to show is some convergence of g of Zn to g of theta. o m not going to multiply by root n just yet. o m going to do a first order Taylor expansion. o what it is telling me is that this is equal to Zn minus theta times g prime of, lets call it theta bar where theta bar is somewhere between say Zn and theta, for sum. OK, so if theta is less than Zn you just permute those two. o thats what the Taylor first order Taylor expansion tells me. There exists a theta bar thats between the two values at which m expanding so that those two things are equal. s everybody shocked? No? o thats standard Taylor expansion. Now m going to multiply by root n. And so thats going to be what? Thats going to be root n Zn minus theta. Ahha, thats something like. Times g prime of theta bar. Now the central limit theorem tells me that this goes to what? Well, this goes to sum n 0 sigma squared, right? That was the first line over there. This guy here, well, its not clear, right? Actually it is. Lets start with this guy. What does theta bar go to? Well, know that Zn is going to theta. Just because, well, thats my law of large numbers. Zn is going to theta, which means that theta bar is sandwiched between two values that converge to theta. o that means that theta bar converges to theta itself as n goes to infinity. Thats just the law of large numbers. Everybody agrees? Just because its sandwiched, right? o have Zn. have theta, and theta bar is somewhere here. The picture might be reversed. t might be that Zn end is larger than theta. But the law of large number tells me that this guy is not moving, but this guy is moving that way. o you know when n is ,, theres very little wiggle room for theta bar, and it can only get to theta. And call it the sandwich theorem, or just find your favorite food in there. o this guy goes to theta, and now need to make an extra assumption, which is that g prime is continuous. And if g prime is continuous, then g prime of theta bar goes to g prime of theta. o this thing goes to g prime of theta. But have an issue here. s that now have something that converges in distribution and something that converges in say mean, this converges almost surely or saying probability just to be safe. And this one converges in distribution. And want to combine them. But dont have a slide that tells me m allowed to take the product of something that converges in distribution, and something that converges in probability. This does not exist. Actually, if anything it told me, do not do anything with things that converge in distribution. And so that gets us to our OK, so ll come back to this in a second. And that gets us to something called lutskys theorem. And lutskys theorem tells us that in very specific cases, you can do just that. o you have two sequences of random variables, Xn bar, thats Xn that converges to X. And Yn that converges to Y, but Y is not anything. Y is not any random variable. o X converges in this distribution. orry, forgot to mention, this is very important. Xn converges in distribution, Y converges in probability. And we know that in generality we cannot combine those two things, but lutsky tells us that if the limit of Y is a constant, meaning its not a random variable, but its a deterministic number 2, just a fixed number thats not a random variable, then you can combine them. Then you can sum them, and then you can multiply them. mean, actually you can do whatever combination you want, because it actually implies that X, the vector Xn, Yn converges to the vector Xc. OK, so here just took two combinations. They are very convenient for us, the sum and the product so could do other stuff like the ratio if c is not 0, things like that. o thats what lutsky does for us. o what youre going to have to write a lot in your homework, in your midterms, by lutsky. know some people are very generous with their by lutsky. They just do numerical applications, mu is equal to 6, and therefore by lutsky mu square is equal to 36. All right, so dont do that. Just use, write lutsky when youre actually using lutsky. But this is something thats very important for us, and it turns out that youre going to feel like you can write by lutsky all the time, because thats going to work for us all the time. Everything were going to see is actually going to be where were going to have to combine stuff. ince we only rely on convergence from distribution arising from the central limit theorem, were actually going to have to rely on something that allows us to combine them, and the only thing we know is lutsky. o we better hope that this thing works. o why lutsky works for us. an somebody tell me why lutsky works to combine those two guys? o this one is converging in distribution. This one is converging in probability, but to a deterministic number. g prime of theta is a deterministic number. dont know what theta is, but its certainly deterministic. All right, so can combine them, multiply them. o thats just the second line of that in particular. All right, everybody is with me? o now m allowed to do this. You can actually you will see something like counterexample questions in your problem set just so that you can convince yourself. ts always a good thing. dont like to give them, because think its much better for you to actually come to the counterexample yourself. Like what can go wrong if Y is not a random sorry, if Y is not a sorry, if c is not the constant, but its a random variable. You can figure that out. All right, so lets go back. o we have now this Delta method that tells us that now have a central limit theorem for functions of averages, and not just for averages. o the only price to pay is this derivative there. o, for example, if g is just a linear function, then m going to have a constant multiplication. f g is a quadratic function, then m going to have theta squared that shows up there. Things like that. o just think of what kind of applications you could have for this. Here are the functions that were interested in, is x maps to 1 over x. What is the derivative of this guy? What is the derivative of 1 over x? Negative 1 over x squared, right? Thats the thing were going to have to put in there. And so this is what we get. o now when m actually going to write this, so if want to show square root of n lambda hat minus lambda. Thats my application, right? This is actually 1 over Tn, and this is 1 over 1 over lambda. o the function g of x is 1 over x in this case. o now have this thing. o know that by the Delta method oh, and knew that Tn, remember, square root of Tn minus 1 over lambda was going to sum normal with mean 0 and variance 1 over lambda squared, right? o the sigma square over there is 1 over lambda squared. o now this thing goes to what? um normal. What is going to be the mean? 0. And what is the variance? o the variance is going m going to pick up this guy, 1 over lambda squared, and then m going to have to take g prime of what? Of 1 over lambda, right? Thats my theta. o have g of theta, which is 1 over theta. o m going to have g prime of 1 over lambda. And what is g prime of 1 over lambda? o we said that g prime is 1 over negative 1 over x squared. o its negative 1 over 1 over lambda squared sorry, squared. Which is nice, because g can be decreasing. o that would be annoying to have a negative variance. And so g prime is negative 1 over, and so what get eventually is lambda squared up here, but then square it again. o this whole thing here becomes what? an somebody tell me what the final result is? Lambda squared right? o its lambda 4 divided by lambda 2. o thats whats written there. And now can just do my good old computation for a can do a good computation for a confidence interval. All right, so lets just go from the second line. o we know that lambda hat minus lambda is less than, weve done that several times already. o its q alpha over 2 sorry, should put alpha over 2 over this thing, right? o thats really the quintile of what our alpha over 2 times lambda divided by square root of n. All right, and so that means that my confidence interval should be this, lambda hat. Lambda belongs to lambda plus or minus q alpha over 2 lambda divided by root n, right? o thats my confidence interval. But again, its not very suitable, because sorry, thats lambda hat. Because they dont know how to compute it. o now m going to request from the audience some remedies for this. What do you suggest we do? What is the laziest thing can do? Anybody? Yeah. ADENE: PHLPPE RGOLLET Replace lambda by lambda hat. What justifies for me to do this? ADENE: PHLPPE RGOLLET Yeah, and lutsky tells me can actually do it, because lutsky tells me, where does this lambda come from, right? This lambda comes from here. Thats the one thats here. o actually could rewrite this entire thing as square root of n lambda hat minus lambda divided by lambda converges to sum n 0, 1. Now if replace this by lambda hat, what have is that this is actually really the original one times lambda divided by lambda hat. And this converges to n 0, 1, right? And now what youre telling me is, well, this guy know it converges to n 0, 1, and this guy is converging to 1 by the law of large number. But this one is converging to 1, which happens to be a constant. t converges in probability, so by lutsky can actually take the product and still maintain my conversion to distribution to a standard Gaussian. o you can always do this. Every time you replace some p by p hat, as long as their ratio goes to 1, which is going to be guaranteed by the law of large number, youre actually going to be fine. And thats where were going to use lutsky a lot. When we do plug in, lutsky is going to be our friend. OK, so we can do this. And thats one way. And then other ways to just solve for lambda like we did before. o the first one we got is actually dont know if still have it somewhere. Yeah, that was the one, right? o we had 1 over Tn q, and thats exactly the same that we have here. o your solution is actually giving us exactly this guy when we actually solve for lambda. o this is what we get. Lambda hat. We replace lambda by lambda hat, and we have our asymptotic convergence theorem. And thats exactly what we did in lutskys theorem. Now were getting to it at this point is just telling us that we can actually do this. Are there any questions about what we did here? o this derivation right here is exactly what did on the board showed you. o let me just show you with a little more space just so that we all understand, right? o we know that square root of n lambda hat minus lambda divided by lambda, the true lambda defined converges to sum n 0, 1. o that was LT plus Delta method. Applying those two, we got to here. And we know that lambda hat converges to lambda in probability and almost surely, and thats what? That was law of large number plus continued mapping theorem, right? Because we only knew that one of our lambda hat converges to 1 over lambda. o we had to flip those things around. And now what said is that apply lutsky, so write square root of n lambda hat minus lambda divided by lambda hat, which is the suggestion that was made to me. They said, want this, but would want to show that it converges to sum n 0, 1 so can legitimately use q alpha over 2 in this one though. And the way we said is like, well, this thing is actually really q divided by lambda times lambda divided by lambda hat. o this thing that was proposed to me, can decompose it in the product of those two random variables. The first one here converges through the Gaussian from the central limit theorem. And the second one converges to 1 from this guy, but in probability this time. That was the ratio of two things in probability, we can actually get it. And so now apply lutsky. And lutsky tells me that can actually do that. But when take the product of this thing that converges to some standard Gaussian, and this thing that converges in probability to 1, then their product actually converges to still this standard Gaussian Well, thats exactly whats done here, and think m getting there. o in our case, OK, so just a remark for lutskys theorem. o thats the last line. o in the first example we used the problem dependent trick, which was to say, well, turns out that we knew that p is between 0 and 1. o we have this p 1 minus p that was annoying to us. We just said, lets just bound it by 1/4, because thats going to be true for any value of p. But here, lambda takes any value between 0 and infinity, so we didnt have such a trick. ts something like we could see that lambda was less than something. aybe we know it, in which case we could use that. But then in this case, we could actually also have used lutskys theorem by doing plug in, right? o here this is my p 1 minus p thats replaced by p hat 1 minus p hat. And lutsky justify, so we did that without really thinking last time. But lutsky actually justifies the fact that this is valid, and still allows me to use this q alpha over 2 here. All right, so thats the end of this lecture. Tonight will post the next set of slides, chapter two. And, well, hopefully the video. m not sure when its going to come out.","o if do Tn bar minus my expectation, rescale by this guy here, then this thing is going to converge to some Gaussian random variable, but here have this lambda to the negative 1 to the negative 2 here, and thats because they did not tell you that if you compute the variance so from this, you can probably extract. And so now which is probably much easier to count like that and so now all have to tell you if want to describe the distribution of this probability of this random variable, is just the probability that it takes each of these values. o that to the right of it is alpha over 2. But the way you want to think about this is that this is x bar minus p divided by the square root of p 1 minus p divided by n. And the reason is, because this is actually the standard deviation of this oh sorry, x bar n. This is actually the standard deviation of this guy, and the square root of n comes from the average. And the probability that each of them, or say one of them is equal to 0, is just 1 minus p. And then for this guy just get the probability well, its more complicated, because have to decide which one it is. And so what mean by that is that to have this convergence, all had to tell you was that the variance was going to 0. And one thing thats actually really nice is this continuous mapping theorem, which essentially tells you that so this is one of the theorems that we like, because they tell us you can do what you feel like you want to do. What Hoeffdings inequality is telling me is that if actually take the average of some bounded random variables, then their probability distribution function or maybe math function this thing might not even have the density, but lets think of it as being a density just for simplicity its going to be something thats going to look like this. o i converge in distribution if and only if for any function thats continuous and bounded, when look at the expectation of f of Tn, this converges to the expectation of f of T. OK, so this is just those two things are actually equivalent. The probability of omega is such that those things converge to each other is actually equal to 1. o it tells me that for almost all omegas, all the omegas, if put them together, get something that has probability of 1. t might be that there are other ones that have probability 0, but what its telling is that this thing happens for all possible realization of the underlying thing. o thats going to be the q alpha over 2 for the Gaussian, and thats going to be the dont know, r alpha over 2 for this random variable. mean, just said we need n to be, we discussed that we wanted n to be larger than 30 last time for the central limit theorem to kick in, and this one seems to tell me can do it for any n. Now there will be a price to pay is that pick up this 2 over b minus alpha squared. The Delta method is really something thats telling you that you can actually take a function of an average, and let it go to the function of the limit, and you still have a central limit theorem. All right, so for example, think of the following two random variables, x and minus x. o this is the same random variable, and this one is negative. But we mostly care about integer p. And then heres our star, the convergence in distribution, and thats just the one that tells you that when start computing probabilities on the Tn, theyre going to look very close to the probabilities on the T. o that was our Tn with this guy, for example, and T was this standard Gaussian distribution. 0. o it will depend on p. o the probability that this thing is equal to 0, is just the probability that all three of those guys are equal to 0. But what want to show you with this example is, can translate this in a central limit theorem for something that converges to lambda, right? Thats what you actually read in the variance, because the central limit theorem is really telling you the distribution goes through this n. But this numbers and this number you can read, right? o you just look at the characteristic function of the square root of n Xn bar minus say mu, and you just expand the thing, and this is what you get. o the variance is going m going to pick up this guy, 1 over lambda squared, and then m going to have to take g prime of what? And now what youre telling me is, well, this guy know it converges to n 0, 1, and this guy is converging to 1 by the law of large number. But the problem is they want this to go to 0 as well as n goes to infinity, but for each n this thing does not have to be 0, which is different from here, right? o thats really the quintile of what our alpha over 2 times lambda divided by square root of n. All right, and so that means that my confidence interval should be this, lambda hat. o what weve just proved by the central limit theorem is that the probability, thats square root of n Tn minus 1 over lambda exceeds q alpha over 2 is approximately equal to alpha, right? Now my things are independent, so do what actually want to do, which say the probability of the intersection is the product of the probabilities, right? o its just the probability that each of them is equal to 0 to the power of 3. o f of T is this. ts just that the problem is that when the convergence in distribution is so weak that it cannot do anything want with it. We started from x bar, and then took some weird function of x bar that depended on p and n. And the reason is, because when take this function, the central limit theorem tells me that it converges to something that know. The reason in statistics where you use those assumptions that n is large enough, that you have this independence that you like so much, and so you can actually have the central limit theorem kick in, all these things are for you to have enough assumptions so that you can actually make sharper and sharper decisions. o m sure this might look scary a little bit, but really what we are going to use is only the last one, which as you can see is just telling you that the probabilities converge to the probabilities. o that if get alpha here, then that means that just solving for epsilon, m going to have some number, which will play the role of q alpha over 2, and then m going to be able to just say that p is between X bar and minus epsilon, and X bar n plus epsilon. What is the probability that X bar 3 is equal to 0 for example? But this is something thats very important for us, and it turns out that youre going to feel like you can write by lutsky all the time, because thats going to work for us all the time. And we know that lambda hat converges to lambda in probability and almost surely, and thats what? Does it make a huge difference numerically to solve the second degree equation, or to just use the p 1 minus p or even to plug in p hat instead of p. Those are going to be the problem set one is to make sure that you see what magnitude of changes you get by changing from one method to the other. And it will still give you the sum of the n, and the product of the n. You can even take the ratio if V is not 0 of course. Now this tells you in particular, since the cumulative distribution function is not defined for vectors, would have to actually use one of the other distributions, one of the other criteria, which is convergence of characteristic functions or convergence of a function of bounded continuous function of the random variable. And so the probability that i exceeds well, its not very symmetric, but the probability that i exceeds this value, q alpha over 2, is just the sum of the two gray areas. And so this is what we get. ts a random variable, so we want to know what the distribution is. Theres something that is waiting for us at the end of his lecture, which is called lutskys that says that if V, in this case, converges in probability but converge in distribution, can actually still do that. Here are the functions that were interested in, is x maps to 1 over x. What is the derivative of this guy? And whats nice about it is that when actually start computing probability that exceeds some number, say alpha over 2, then know that this can actually get a number, which is just sorry, the probability that it exceeds, yeah. o thats the variance of the Gaussian that have, right? o that means now that the absolute value, so with probability 1 minus alpha asymptotically, have that square root of n times lambda Tn minus 1 is less than or equal to q alpha over 2. o what it means is that, oh, have negative q alpha over 2 less than square root of n. Let me divide by square root of n here. But when take the product of this thing that converges to some standard Gaussian, and this thing that converges in probability to 1, then their product actually converges to still this standard Gaussian Well, thats exactly whats done here, and think m getting there. o what it means is that when think of this as a random variable, when n is large enough its going to look like this. And the reason is, well, OK, so here m actually going to the important characterizations of the convergence in distribution, which is R convergence style. And the reason was that we could say that the probability that this thing exceeds q alpha over 2 is alpha. And now what said is that apply lutsky, so write square root of n lambda hat minus lambda divided by lambda hat, which is the suggestion that was made to me. But this is something thats going to be too hard for us to deal with, so were actually going to rely on the fact that we have something thats even better. orry have to yeah, so this is the number of 1s that OK, so lets look at that. Well, know that Tn bar is going to 1 over lambda by the law of large number. o now just get this one, and so now you tell me that have this thing. All right, so what is the smallest value you can think of such that a Bernoulli random variable is larger than or equal to this value? And we know that in generality we cannot combine those two things, but lutsky tells us that if the limit of Y is a constant, meaning its not a random variable, but its a deterministic number 2, just a fixed number thats not a random variable, then you can combine them. Basically what it means is that whatever you measure one is going to be the same that you measure on the other one. 1. f p is 1, what is this probability? o m going to observe this, and m going to try to infer what is the rate of Ts going out of the station from this. o know that by the Delta method oh, and knew that Tn, remember, square root of Tn minus 1 over lambda was going to sum normal with mean 0 and variance 1 over lambda squared, right? o here is one way we can actually so thats what we did for the kiss example, right? And you just manipulate functions, and you see that this thing just happen to be true, and thats something that people like. And the second one converges to 1 from this guy, but in probability this time. This is actually 1 over Tn, and this is 1 over 1 over lambda. But this is if you compute integral of lambda e to the minus t lambda times t squared. But if the central limit theorem told us that the limiting distribution of some average was something that looked like a Poisson or an then we would just have in the same way taken the quintiles of the exponential distribution. Now if replace this by lambda hat, what have is that this is actually really the original one times lambda divided by lambda hat. o the function g of x is 1 over x in this case. And what that means is that my pivotal distribution is actually not the normal distribution, but it might be something else. o now said that this thing was approximately equal, due to the central limit theorem, to the probability, that square root of n. Xn bar minus p divided by square root p 1 minus p. Well, absolute value was larger than q alpha over 2. For example, if p is equal to 0, what is this probability? f give you a random variable with mean 0, and tell you that its variance is going to 0, right? o you have two sequences of random variables, Xn bar, thats Xn that converges to X. And Yn that converges to Y, but Y is not anything. What is going to be the mean? Now were getting to it at this point is just telling us that we can actually do this. That was the ratio of two things in probability, we can actually get it. What Hoeffding tells me is that the probability that Xn bar minus well, what is mu in this case? But there might be some crazy distributions, but as long as they dont depend on anything, we might actually be able to simulate from them, and in particular compute what q alpha over 2 is for any possible value .. And so thats what were going to be trying to do. And so now what have is that get that lambda is between thats Tn bar is between 1 plus q alpha over 2 divided by root n. And the whole thing is divided by Tn bar, and same thing on the other side except have 1 minus q alpha over 2 divided by root n divided by Tn bar. But dont have a slide that tells me m allowed to take the product of something that converges in distribution, and something that converges in probability. o in the first example we used the problem dependent trick, which was to say, well, turns out that we knew that p is between 0 and 1. o we have this p 1 minus p that was annoying to us. Because the convergence in distribution is actually not telling you that your random variable is converging to another random variable. o just use the fact that a probability of the complement is 1 minus the probability of the set. And then, in particular, what was interested in was that the probability, thats the absolute value of a Gaussian random variable, exceeds q alpha over 2, q alpha over 2. And this converges to n 0, 1, right? ts telling you, well, if the variance is going to 0 well, its for any random variable T, so here what describe was for a deterministic. Because the only thing we really need to do is to say that when start computing probabilities on this random variable, theyre going to look like probabilities on that random variable. And then you get the expectation of T1 is 1 over lambda. Now m going to multiply by root n. And so thats going to be what? o have that g of Tn bar actually lets use the notation of this slide, which is Zn and theta. And what is g prime of 1 over lambda? Then this thing here converges as n goes to infinity, and we will say a little bit more about what it means in distribution to some standard normal random variable. This is just a binomial, X bar 3. o if look at X bar 3, and then multiply it by 3, its just this sum of independent Bernoullis with parameter p. o this is actually a binomial with parameter 3 and p. And theres tables for binomials, and they tell you all this. o now when m actually going to write this, so if want to show square root of n lambda hat minus lambda. And so what we need is to have stronger conditions here and there, so that we can actually put things together. And then the last one which is equivalent to the above two is, anybody knows what the name of this quantity is? But if you want to prove convergence in distribution, you very likely are going to use one this three right here. ts telling you that the distribution of your random variable is converging to a distribution. o this lambda is actually the expectation. And so when do this thing, when do Hoeffdings inequality, what this thing is telling me is for any given epsilon here, for any given epsilon, what is the probability that Xn bar goes away from its expectation? The beauty about Gaussian is that people have studied them to death, and you can open any stats textbook, and you will see a table again that will tell you for each value of alpha youre interested in, it will tell you what q alpha over 2 is. And then get basically three of them that have just the one in there, right? o if have Tn that goes to T, f of Tn goes to f of T, and this is true for any of those convergence except for Lp. Well, lets call it T. o we know that T in expectation, the expectation of T is 1 over lambda. For example, if forgot to divide by square root of p 1 minus p, then this thing would have had a variance, which is the p 1 minus p. f didnt remove this p here, the mean here would have been affected by p. And theres no table for normal p 1. We say X1 to Xn, and in this case m going to assume theyre D, and m going to make them Bernoulli with p, and p is unknown, right? And what want to show is some convergence of g of Zn to g of theta. And the factor or the price to pay for this is something which depends on the derivative of the function. You will see that this thing is 1 over lambda squared. And the way we said is like, well, this thing is actually really q divided by lambda times lambda divided by lambda hat. But this very important thing about the something that know is that it does not depend on anything that dont know. And so g prime is negative 1 over, and so what get eventually is lambda squared up here, but then square it again. We just said, lets just bound it by 1/4, because thats going to be true for any value of p. But here, lambda takes any value between 0 and infinity, so we didnt have such a trick. o the number of values that it can take, mean, its a little annoying, because then have to sum them, right?",0.1629084215370455
13,13,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer highquality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. ANA BELL: o we have a class ar here, object as usual. The init method here takes in self as usual, a w, and a d. o ar gets initialized with two parameters. And we assign the wheels data attribute to the first one and the doors data attribute to the second one. And then were going to also assign the color data attribute to be the empty string. o m giving you four choices here. And the question says, which of the above is a getter method for the number of wheels? o getter method is something thats going to get a data attribute. A method is really just a function. o of course, were going to have def. get_wheels is a good name for it. ince its a method for this class, we have to have self in the parameters. o we know its going to be between and D. And so then what are we going to return? The first ones going to return wheels, which, in this particular case, is just going to be a variable that we havent defined, but its a variable. A getter returns a data attribute of a particular instance. o we actually have to say self dot if we want to return a data attribute of an instance, as opposed to just a regular variable. o the correct answer is D. Great job.","And we assign the wheels data attribute to the first one and the doors data attribute to the second one. And the question says, which of the above is a getter method for the number of wheels? And then were going to also assign the color data attribute to be the empty string. ince its a method for this class, we have to have self in the parameters. o we actually have to say self dot if we want to return a data attribute of an instance, as opposed to just a regular variable. The first ones going to return wheels, which, in this particular case, is just going to be a variable that we havent defined, but its a variable. o getter method is something thats going to get a data attribute.",0.3904109589041096
14,14,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer highquality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. JOHN TTKL: OK lets start. o weve had the quiz. And guess theres both good and bad news in it. Yesterday, as you know, the bad news. The average was a little lower than what we would have wanted. On the other hand, the good news is that the distribution was nicely spread. And thats the main purpose of this quiz is basically for you to calibrate and see roughly where you are standing. The other piece of the good news is that, as you know, this quiz doesnt count for very much in your final grade. o its really a matter of calibration and to get your mind set appropriately to prepare for the second quiz, which counts a lot more. And its more substantial. And well make sure that the second quiz will have a higher average. All right. o lets go to our material. Were talking now these days about continuous random variables. And ll remind you what we discussed last time. ll remind you of the concept of the probability density function of a single random variable. And then were going to rush through all the concepts that we covered for the case of discrete random variables and discuss their analogs for the continuous case. And talk about notions such as conditioning independence and so on. o the big picture is here. We have all those concepts that we developed for the case of discrete random variables. And now we will just talk about their analogs in the continuous case. We already discussed this analog last week, the density of a single random variable. Then there are certain concepts that show up both in the discrete and the continuous case. o we have the cumulative distribution function, which is a description of the probability distribution of a random variable and which applies whether you have a discrete or continuous random variable. Then theres the notion of the expected value. And in the two cases, the expected value is calculated in a slightly different way, but not very different. We have sums in one case, integrals in the other. And this is the general pattern that were going to have. Formulas for the discrete case translate to corresponding formulas or expressions in the continuous case. We generically replace sums by integrals, and we replace must functions with density functions. Then the new pieces for today are going to be mostly the notion of a joint density function, which is how we describe the probability distribution of two random variables that are somehow related, in general, and then the notion of a conditional density function that tells us the distribution of one random variable X when youre told the value of another random variable Y. Theres another concept, which is the conditional PDF given that the certain event has happened. This is a concept thats in some ways simpler. Youve already seen a little bit of that in last weeks recitation and tutorial. The idea is that we have a single random variable. ts described by a density. Then youre told that the certain event has occurred. Your model changes the universe that you are dealing with. n the new universe, you are dealing with a new density function, the one that applies given the knowledge that we have that the certain event has occurred. All right. o what exactly did we say about continuous random variables? The first thing is the definition, that a random variable is said to be continuous if we are given a certain object that we call the probability density function and we can calculate interval probabilities given this density function. o the definition is that the random variable is continuous if you can calculate probabilities associated with that random variable given that formula. o this formula tells you that the probability that your random variable falls inside this interval is the area under the density curve. OK. Theres a few properties that a density function must satisfy. ince were talking about probabilities, and probabilities are nonnegative, we have that the density function is always a nonnegative function. The total probability over the entire real line must be equal to 1. o the integral when you integrate over the entire real line has to be equal to 1. Thats the second property. Another property that you get is that if you let a equal to b, this integral becomes 0. And that tells you that the probability of a single point in the continuous case is always equal to 0. o these are formal properties. When you want to think intuitively, the best way to think about what the density function is to think in terms of little intervals, the probability that my random variable falls inside the little interval. Well, inside that little interval, the density function here is roughly constant. o that integral becomes the value of the density times the length of the interval over which you are integrating, which is delta. And so the density function basically gives us probabilities of little events, of small events. And the density is to be interpreted as probability per unit length at a certain place in the diagram. o in that place in the diagram, the probability per unit length around this neighborhood would be the height of the density function at that point. What else? We have a formula for calculating expected values of functions of random variables. n the discrete case, we had the formula where here we had the sum, and instead of the density, we had the PF. The same formula is also valid in the continuous case. And its not too hard to derive, but we will not do it. But lets think of the intuition of what this formula says. Youre trying to figure out on the average how much g(X) is going to be. And then you reason, and you say, well, X may turn out to take a particular value or a small interval of values. This is the probability that X falls inside the small interval. And when that happens, g(X) takes that value. o this fraction of the time, you fall in the little neighborhood of x, and you get so much. Then you average over all the possible xs that can happen. And that gives you the average value of the function g(X). OK. o this is the easy stuff. Now lets get to the new material. We want to talk about multiple random variables simultaneously. o we want to talk now about two random variables that are continuous, and in some sense that they are jointly continuous. And lets see what this means. The definition is similar to the definition we had for a single random variable, where take this formula here as the definition of continuous random variables. Two random variables are said to be jointly continuous if we can calculate probabilities by integrating a certain function that we call the joint density function over the set of interest. o we have our twodimensional plane. This is the xy plane. Theres a certain event that were interested in. We want to calculate the probability. How do we do that? We are given this function f_(X,Y), the joint density. ts a function of the two arguments x and y. o think of that function as being some kind of surface that sits on top of the twodimensional plane. The probability of falling inside the set , we calculate it by looking at the volume under the surface, that volume that sits on top of . o the surface underneath it has a certain total volume. What should that total volume be? Well, we think of these volumes as probabilities. o the total probability should be equal to 1. The total volume under this surface, should be equal to 1. o thats one property that we want our density function to have. o when you integrate over the entire space, this is of the volume under your surface. That should be equal to 1. Of course, since were talking about probabilities, the joint density should be a nonnegative function. o think of the situation as having one pound of probability thats spread all over your space. And the height of this joint density function basically tells you how much probability tends to be accumulated in certain regions of space as opposed to other parts of the space. o wherever the density is big, that means that this is an area of the twodimensional plane thats more likely to occur. Where the density is small, that means that those xys are less likely to occur. You have already seen one example of continuous densities. That was the example we had in the very beginning of the class with a uniform distribution on the unit square. That was a special case of a density function that was constant. o all places in the unit square were roughly equally likely as any other places. But in other models, some parts of the space may be more likely than others. And we describe those relative likelihoods using this density function. o if somebody gives us the density function, this determines for us probabilities of all the subsets of the twodimensional plane. Now for an intuitive interpretation, its good to think about small events. o lets take a particular x here and then x plus delta. o this is a small interval. Take another small interval here that goes from y to y plus delta. And lets look at the event that x falls here and y falls right there. What is this event? Well, this is the event that will fall inside this little rectangle. sing this rule for calculating probabilities, what is the probability of that rectangle going to be? Well, it should be the integral of the density over this rectangle. Or its the volume under the surface that sits on top of that rectangle. Now, if the rectangle is very small, the joint density is not going to change very much in that neighborhood. o we can treat it as a constant. o the volume is going to be the height times the area of the base. The height at that point is whatever the function happens to be around that point. And the area of the base is delta squared. o this is the intuitive way to understand what a joint density function really tells you. t specifies for you probabilities of little squares, of little rectangles. And it allows you to think of the joint density function as probability per unit area. o these are the units of the density, its probability per unit area in the neighborhood of a certain point. o what do we do with this density function once we have it in our hands? Well, we can use it to calculate expected values. uppose that you have a function of two random variables described by a joint density. You can find, perhaps, the distribution of this random variable and then use the basic definition of the expectation. Or you can calculate expectations directly, using the distribution of the original random variables. This is a formula thats again identical to the formula that we had for the discrete case. n the discrete case, we had a double sum here, and we had PFs. o the intuition behind this formula is the same that one had for the discrete case. ts just that the mechanics are different. Then something that we did in the discrete case was to find a way to go from the joint density of the two random variables taken together to the density of just one of the random variables. o we had a formula for the discrete case. Lets see how things are going to work out in the continuous case. o in the continuous case, we have here our two random variables. And we have a density for them. And lets say that we want to calculate the probability that x falls inside this interval. o were looking at the probability that our random variable X falls in the interval from little x to x plus delta. Now, by the properties that we already have for interpreting the density function of a single random variable, the probability of a little interval is approximately the density of that single random variable times delta. And now we want to find a formula for this marginal density in terms of the joint density. OK. o this is the probability that x falls inside this interval. n terms of the twodimensional plane, this is the probability that (x,y) falls inside this strip. o to find that probability, we need to calculate the probability that (x,y) falls in here, which is going to be the double integral over the interval over this strip, of the joint density. And what are we integrating over? y goes from minus infinity to plus infinity. And the dummy variable x goes from little x to x plus delta. o to integrate over this strip, what we do is for any given y, we integrate in this dimension. This is the x integral. And then we integrate over the y dimension. Now what is this inner integral? Because x only varies very little, this is approximately constant in that range. o the integral with respect to x just becomes delta times f(x,y). And then weve got our dy. o this is what the inner integral will evaluate to. We are integrating over the little interval. o were keeping y fixed. ntegrating over here, we take the value of the density times how much were integrating over. And we get this formula. OK. Now, this expression must be equal to that expression. o if we cancel the deltas, we see that the marginal density must be equal to the integral of the joint density, where we have integrated out the value of y. o this formula should come as no surprise at this point. ts exactly the same as the formula that we had for discrete random variables. But now we are replacing the sum with an integral. And instead of using the joint PF, we are using the joint PDF. Then, continuing going down the list of things we did for discrete random variables, we can now introduce a definition of the notion of independence of two random variables. And by analogy with the discrete case, we define independence to be the following condition. Two random variables are independent if and only if their joint density function factors out as a product of their marginal densities. And this property needs to be true for all x and y. o this is the formal definition. Operationally and intuitively, what does it mean? Well, intuitively it means the same thing as in the discrete case. Knowing anything about X shouldnt tell you anything about Y. That is, information about X is not going to change your beliefs about Y. We are going to come back to this statement in a second. The other thing that it allows you to do m not going to derive this is it allows you to calculate probabilities by multiplying individual probabilities. o if you ask for the probability that x falls in a certain set A and y falls in a certain set B, then you can calculate that probability by multiplying individual probabilities. This takes just two lines of derivation, which m not going to do. But it comes back to the usual notion of independence of events. Basically, operationally independence means that you can multiply probabilities. o now lets look at an example. Theres a sort of pretty famous and classical one. t goes back a lot more than a 100 years. And its the famous Needle of Buffon. Buffon was a French naturalist who, for some reason, also decided to play with probability. And look at the following problem. o you have the twodimensional plane. And on the plane we draw a bunch of parallel lines. And those parallel lines are separated by a length. And the lines are apart at distance d. And we throw a needle at random, completely at random. And well have to give a meaning to what ""completely at random"" means. And when we throw a needle, theres two possibilities. Either the needle is going to fall in a way that does not intersect any of the lines, or its going to fall in a way that it intersects one of the lines. Were taking the needle to be shorter than this distance, so the needle cannot intersect two lines simultaneously. t either intersects 0, or it intersects one of the lines. The question is to find the probability that the needle is going to intersect a line. Whats the probability of this? OK. We are going to approach this problem by using our standard fourstep procedure. et up your sample space, describe a probability law on that sample space, identify the event of interest, and then calculate. These four steps basically correspond to these three bullets and then the last equation down here. o first thing is to set up a sample space. We need some variables to describe what happened in the experiment. o what happens in the experiment is that the needle lands somewhere. And where it lands, we can describe this by specifying the location of the center of the needle. And what do we mean by the location of the center? Well, we can take as our variable to be the distance from the center of the needle to the nearest line. o it tells us the vertical distance of the center of the needle from the nearest line. The other thing that matters is the orientation of the needle. o we need one more variable, which we take to be the angle that the needle is forming with the lines. We can put the angle here, or you can put in there. Yes, its still the same angle. o we have these two variables that described what happened in the experiment. And we can take our sample space to be the set of all possible xs and thetas. What are the possible xs? The lines are d apart, so the nearest line is going to be anywhere between 0 and d/2 away. o that tells us what the possible xs will be. As for theta, it really depends how you define your angle. We are going to define our theta to be the acute angle thats formed between the needle and a line, if you were to extend it. o theta is going to be something between 0 and pi/2. o guess these red pieces really correspond to the part of setting up the sample space. OK. o thats part one. econd part is we need a model. OK. Lets take our model to be that we basically know nothing about how the needle falls. t can fall in any possible way, and all possible ways are equally likely. Now, if you have those parallel lines, and you close your eyes completely and throw a needle completely at random, any x should be equally likely. o we describe that situation by saying that X should have a uniform distribution. That is, it should have a constant density over the range of interest. imilarly, if you kind of spin your needle completely at random, any angle should be as likely as any other angle. And we decide to model this situation by saying that theta also has a uniform distribution over the range of interest. And finally, where we put it should have nothing to do with how much we rotate it. And we capture this mathematically by saying that X is going to be independent of theta. Now, this is going to be our model. m not deriving the model from anything. m only saying that this sounds like a model that does not assume any knowledge or preference for certain values of x rather than other values of theta. n the absence of any other particular information you might have in your hands, thats the most reasonable model to come up with. o you model the problem that way. o whats the formula for the joint density? ts going to be the product of the densities of X and Theta. Why is it the product? This is because we assumed independence. And the density of X, since its uniform, and since it needs to integrate to 1, that density needs to be 2/d. Thats the density of X. And the density of Theta needs to be 2/pi. Thats the value for the density of Theta so that the overall probability over this interval ends up being 1. o now we do have our joint density in our hands. The next thing to do is to identify the event of interest. And this is best done in a picture. And theres two possible situations that one could have. Either the needle falls this way, or it falls this way. o how can we tell if one or the other is going to happen? t has to do with whether this interval here is smaller than that or bigger than that. o we are comparing the height of this interval to that interval. This interval here is capital X. This interval here, what is it? This is half of the length of the needle, which is l/2. To find this height, we take l/2 and multiply it with the sine of the angle that we have. o the length of this interval up here is l/2 times sine theta. f this is smaller than x, the needle does not intersect the line. f this is bigger than x, then the needle intersects the line. o the event of interest, that the needle intersects the line, is described this way in terms of x and theta. And now that we have the event of interest described mathematically, all that we need to do is to find the probability of this event, we integrate the joint density over the part of (x, theta) space in which this inequality is true. o its a double integral over the set of all xs and thetas where this is true. The way to do this integral is we fix theta, and we integrate for xs that go from 0 up to that number. And theta can be anything between 0 and pi/2. o the integral over this set is basically this double integral here. We already have a formula for the joint density. ts 4 over pi d, so we put it here. And now, fortunately, this is a pretty easy integral to evaluate. The integral with respect to x theres nothing in here. o the integral is just the length of the interval over which were integrating. ts l/2 sine theta. And then we need to integrate this with respect to theta. We know that the integral of a sine is a negative cosine. You plug in the values for the negative cosine at the two end points. m sure you can do this integral . And we finally obtain the answer, which is amazingly simple for such a pretty complicatedlooking problem. ts 2l over pi d. o some people a long, long time ago, after they looked at this answer, they said that maybe that gives us an interesting way where one could estimate the value by pi, for example, experimentally. How do you do that? Fix l and d, the dimensions of the problem. Throw a million needles on your piece of paper. ee how often your needless do intersect the line. That gives you a number for this quantity. You know l and d, so you can use that to infer pi. And theres an apocryphal story about a wounded soldier in a hospital after the American ivil War who actually had heard about this and was spending his time in the hospital throwing needles on pieces of paper. dont know if its true or not. But lets do something similar here. o lets look at this diagram. We fix the dimensions. This is supposed to be our little d. Thats supposed to be our little l. We have the formula from the previous slide that p is 2l over pi d. n this instance, we choose d to be twice l. o this number is 1/pi. o the probability that the needle hits the line is 1/pi. o need needles that are 3.1 centimeters long. couldnt find such needles. But could find paper clips that are 3.1 centimeters long. o lets start throwing paper clips at random and see how many of them will end up intersecting the lines. Good. OK. o out of eight paper clips, we have exactly four that intersected the line. o our estimate for the probability of intersecting the line is 1/2, which gives us an estimate for the value of pi, which is two. Well, mean, within an engineering approximation, were in the right ballpark, right? o this might look like a silly way of trying to estimate pi. And it probably is. On the other hand, this kind of methodology is being used especially by physicists and also by statisticians. ts used a lot. When is it used? f you have an integral to calculate, such as this integral, but youre not lucky, and your functions are not so simple where you can do your calculations by hand, and maybe the dimensions are larger instead of two random variables you have 100 random variables, so its a 100fold integral then theres no way to do that in the computer. But the way that you can actually do it is by generating random samples of your random variables, doing that simulation over and over many times. That is, by interpreting an integral as a probability, you can use simulation to estimate that probability. And that gives you a way of calculating integrals. And physicists do actually use that a lot, as well as statisticians, computer scientists, and so on. ts a socalled onte arlo method for evaluating integrals. And its a basic piece of the toolbox in science these days. Finally, the harder concept of the day is the idea of conditioning. And here things become a little subtle when you deal with continuous random variables. OK. First, remember again our basic interpretation of what a density is. A density gives us probabilities of little intervals. o how should we define conditional densities? onditional densities should again give us probabilities of little intervals, but inside a conditional world where we have been told something about the other random variable. o what we would like to be true is the following. We would like to define a concept of a conditional density of a random variable X given the value of another random variable Y. And it should behave the following way, that the conditional density gives us the probability of little intervals same as here given that we are told the value of y. And heres where the subtleties come. The main thing to notice is that here didnt write ""equal,"" wrote ""approximately equal."" Why do we need that? Well, the thing is that conditional probabilities are not defined when you condition on an event that has 0 probability. o we need the conditioning event here to have posed this probability. o instead of saying that Y is exactly equal to little y, we want to instead say were in a new universe where capital Y is very close to little y. And then this notion of ""very close"" kind of takes the limit and takes it to be infinitesimally close. o this is the way to interpret conditional probabilities. Thats what they should mean. Now, in practice, when you actually use probability, you forget about that subtlety. And you say, well, ve been told that Y is equal to 1.3. Give me the conditional distribution of X. But formally or rigorously, you should say m being told that Y is infinitesimally close to 1.3. Tell me the distribution of X. Now, if this is what we want, what should this quantity be? ts a conditional probability, so it should be the probability of two things happening X being close to little x, Y being close to little y. And thats basically given to us by the joint density divided by the probability of the conditioning event, which has something to do with the density of Y itself. And if you do things carefully, you see that the only way to satisfy this relation is to define the conditional density by this particular formula. OK. Big discussion to come down in the end to what you should have probably guessed by now. We just take any formulas and expressions from the discrete case and replace PFs by PDFs. o the conditional PDF is defined by this formula where here we have joint PDF and marginal PDF, as opposed to the discrete case where we had the joint PF and the marginal PF. o in some sense, its just a syntactic change. n another sense, its a little subtler on how you actually interpret it. peaking about interpretation, what are some ways of thinking about the joint density? Well, the best way to think about it is that somebody has fixed little y for you. o little y is being fixed here. And we look at this density as a function of X. ve told you what Y is. Tell me what you know about X. And you tell me that X has a certain distribution. What does that distribution look like? t has exactly the same shape as the joint density. Remember, we fixed Y. o this is a constant. o the only thing that varies is X. o we get the function that behaves like the joint density when you fix y, which is really you take the joint density, and you take a slice of it. You fix a y, and you see how it varies with x. o in that sense, the conditional PDF is just a slice of the joint PDF. But we need to divide by a certain number, which just scales it and changes its shape. Were coming back to a picture in a second. But before going to the picture, lets go back to the interpretation of independence. f the two random the variables are independent, according to our definition in the previous slide, the joint density is going to factor as the product of the marginal densities. The density of Y in the numerator cancels the density in the denominator. And were just left with the density of X. o in the case of independence, what we get is that the conditional is the same as the marginal. And that solidifies our intuition that in the case of independence, being told something about the value of Y does not change our beliefs about how X is distributed. o whatever we expected about X is going to remain true even after we are told something about Y. o lets look at some pictures. Here is what the joint PDF might look like. Here weve got our x and yaxis. And if you want to calculate the probability of a certain event, what you do is you look at that event and you see how much of that mass is sitting on top of that event. Now lets start slicing. Lets fix a value of x and look along that slice where we obtain this function. Now what does that slice do? That slice tells us for that particular x what the possible values of y are going to be and how likely they are. f we integrate over all ys, what do we get? ntegrating over all ys just gives us the marginal density of X. ts the calculation that we did here. By integrating over all ys, we find the marginal density of X. o the total area under that slice gives us the marginal density of X. And by looking at the different slices, we find how likely the different values of x are going to be. How about the conditional? f were interested in the conditional of Y given X, how would you think about it? This refers to a universe where we are told that capital X takes on a specific value. o we put ourselves in the universe where this line has happened. Theres still possible values of y that can happen. And this shape kind of tells us the relative likelihoods of the different ys. And this is indeed going to be the shape of the conditional distribution of Y given that X has occurred. On the other hand, the conditional distribution must add up to 1. o the total probability over all of the different ys in this universe, that total probability should be equal to 1. Here its not equal to 1. The total area is the marginal density. To make it equal to 1, we need to divide by the marginal density, which is basically to renormalize this shape so that the total area under that slice, under that shape, is equal to 1. o we start with the joint. We take the slices. And then we adjust the slices so that every slice has an area underneath equal to 1. And this gives us the conditional. o for example, down here you can not even see it in this diagram but after you renormalize it so that its total area is equal to 1, you get this sort of narrow spike that goes up. And so this is a plot of the conditional distributions that you get for the different values of x. Given a particular value of x, youre going to get this certain conditional distribution. o this picture is worth about as much as anything else in this particular chapter. ake sure you kind of understand exactly all these pieces of the picture. And finally, lets go, in the remaining time, through an example where were going to throw in the bucket all the concepts and notations that we have introduced so far. o the example is as follows. We start with a stick that has a certain length. And we break it a completely random location. And yes, this 1 should be l. OK. o it has length l. And were going to break it at the random place. And we call that random place where we break it, we call it X. X can be anywhere, uniform distribution. o this means that X has a density that goes from 0 to l. guess this capital L is supposed to be the same as the lowercase l. o thats the density of X. And since the density needs to integrate to 1, the height of that density has to be 1/l. Now, having broken the stick and given that we are left with this piece of the stick, m now going to break it again at a completely random place, meaning m going to choose a point where break it uniformly over the length of the stick. What does this mean? And lets call Y the location where break it. o Y is going to range between 0 and x. x is the stick that m left with. o m going to break it somewhere in between. o pick a y between 0 and x. And of course, x is less than l. And m going to break it there. o y is uniform between 0 and x. What does that mean, that the density of y, given that you have already told me x, ranges from 0 to little x? f told you that the first break happened at a particular x, then y can only range over this interval. And m assuming a uniform distribution over that interval. o we have this kind of shape. And that fixes for us the height of the conditional density. o whats the joint density of those two random variables? By the definition of conditional densities, the conditional was defined as the ratio of this divided by that. o we can find the joint density by taking the marginal and then multiplying by the conditional. This is the same formula as in the discrete case. This is our very familiar multiplication rule, but adjusted to the case of continuous random variables. o Ps become Fs. OK. o we do have a formula for this. What is it? ts 1/l thats the density of X times 1/x, which is the conditional density of Y. This is the formula for the joint density. But we must be careful. This is a formula thats not valid anywhere. ts only valid for the xs and ys that are possible. And the xs and ys that are possible are given by these inequalities. o x can range from 0 to l, and y can only be smaller than x. o this is the formula for the density on this part of our space. The density is 0 anywhere else. o what does it look like? ts basically a 1/x function. o its sort of constant along that dimension. But as x goes to 0, your density goes up and can even blow up. t sort of looks like a sail thats raised and somewhat curved and has a point up there going to infinity. o this is the joint density. Now once you have in your hands a joint density, then you can answer in principle any problem. ts just a matter of plugging in and doing computations. How about calculating something like a conditional expectation of Y given a value of x? OK. Thats a concept we have not defined so far. But how should we define it? eans the reasonable thing. Well define it the same way as ordinary expectations except that since were given some conditioning information, we should use the probability distribution that applies to that particular situation. o in a situation where we are told the value of x, the distribution that applies is the conditional distribution of Y. o its going to be the conditional density of Y given the value of x. Now, we know what this is. ts given by 1/x. o we need to integrate y times 1/x dy. And what should we integrate over? Well, given the value of x, y can only range from 0 to x. o this is what we get. And you do your integral, and you get that this is x/2. s it a surprise? t shouldnt be. This is just the expected value of Y in a universe where X has been realized and Y is given by this distribution. Y is uniform between 0 and x. The expected value of Y should be the midpoint of this interval, which is x/2. Now lets do fancier stuff. ince we have the joint distribution, we should be able to calculate the marginal. What is the distribution of Y? After breaking the stick twice, how big is the little piece that m left with? How do we find this? To find the marginal, we just take the joint and integrate out the variable that we dont want. A particular y can happen in many ways. t can happen together with any x. o we consider all the possible xs that can go together with this y and average over all those xs. o we plug in the formula for the joint density from the previous slide. We know that its 1/lx. And whats the range of the xs? o to find the density of Y for a particular y up here, m going to integrate over xs. The density is 0 here and there. The density is nonzero only in this part. o need to integrate over xs going from here to there. o whats the ""here""? This line goes up at the slope of 1. o this is the line x equals y. o if fix y, it means that my integral starts from a value of x that is also equal to y. o where the integral starts from is at x equals y. And it goes all the way until the end of the length of our stick, which is l. o we need to integrate from little y up to l. o thats something that almost always comes up. ts not enough to have just this formula for integrating the joint density. You need to keep track of different regions. And if the joint density is 0 in some regions, then you exclude those regions from the range of integration. o the range of integration is only over those values where the particular formula is valid, the places where the joint density is nonzero. All right. The integral of 1/x dx, that gives you a logarithm. o we evaluate this integral, and we get an expression of this kind. o the density of Y has a somewhat unexpected shape. o its a logarithmic function. And it goes this way. ts for y going all the way to l. When y is equal to l, the logarithm of 1 is equal to 0. But when y approaches 0, logarithm of something big blows up, and we get a shape of this form. OK. Finally, we can calculate the expected value of Y. And we can do this by using the definition of the expectation. o integral of y times the density of y. We already found what that density is, so we can plug it in here. And were integrating over the range of possible ys, from 0 to l. Now this involves the integral for y log y, which m sure you have encountered in your calculus classes but maybe do not remember how to do it. n any case, you look it up in some integral tables or do it by parts. And you get the final answer of l/4. And at this point, you say, thats a really simple answer. houldnt have expected it to be l/4? guess, yes. mean, when you break it once, the expected value of what you are left with is going to be 1/2 of what you started with. When you break it the next time, the expected length of what youre left with should be 1/2 of the piece that you are now breaking. o each time that you break it at random, you expected it to become smaller by a factor of 1/2. o if you break it twice, you are left something thats expected to be 1/4. This is reasoning on the average, which happens to give you the right answer in this case. But again, theres the warning that reasoning on the average doesnt always give you the right answer. o be careful about doing arguments of this type. Very good. ee you on Wednesday.","And now that we have the event of interest described mathematically, all that we need to do is to find the probability of this event, we integrate the joint density over the part of (x, theta) space in which this inequality is true. o in a situation where we are told the value of x, the distribution that applies is the conditional distribution of Y. o its going to be the conditional density of Y given the value of x. Now, we know what this is. And were just left with the density of X. o in the case of independence, what we get is that the conditional is the same as the marginal. o to find that probability, we need to calculate the probability that (x,y) falls in here, which is going to be the double integral over the interval over this strip, of the joint density. Then the new pieces for today are going to be mostly the notion of a joint density function, which is how we describe the probability distribution of two random variables that are somehow related, in general, and then the notion of a conditional density function that tells us the distribution of one random variable X when youre told the value of another random variable Y. Theres another concept, which is the conditional PDF given that the certain event has happened. o this means that X has a density that goes from 0 to l. guess this capital L is supposed to be the same as the lowercase l. o thats the density of X. And since the density needs to integrate to 1, the height of that density has to be 1/l. ts 1/l thats the density of X times 1/x, which is the conditional density of Y. This is the formula for the joint density. We would like to define a concept of a conditional density of a random variable X given the value of another random variable Y. And it should behave the following way, that the conditional density gives us the probability of little intervals same as here given that we are told the value of y. And heres where the subtleties come. The first thing is the definition, that a random variable is said to be continuous if we are given a certain object that we call the probability density function and we can calculate interval probabilities given this density function. Then something that we did in the discrete case was to find a way to go from the joint density of the two random variables taken together to the density of just one of the random variables. o if we cancel the deltas, we see that the marginal density must be equal to the integral of the joint density, where we have integrated out the value of y. o this formula should come as no surprise at this point. This line goes up at the slope of 1. o this is the line x equals y. o if fix y, it means that my integral starts from a value of x that is also equal to y. o where the integral starts from is at x equals y. And it goes all the way until the end of the length of our stick, which is l. o we need to integrate from little y up to l. o thats something that almost always comes up. o the only thing that varies is X. o we get the function that behaves like the joint density when you fix y, which is really you take the joint density, and you take a slice of it. By integrating over all ys, we find the marginal density of X. o the total area under that slice gives us the marginal density of X. And by looking at the different slices, we find how likely the different values of x are going to be. ts a conditional probability, so it should be the probability of two things happening X being close to little x, Y being close to little y. And thats basically given to us by the joint density divided by the probability of the conditioning event, which has something to do with the density of Y itself. Now, by the properties that we already have for interpreting the density function of a single random variable, the probability of a little interval is approximately the density of that single random variable times delta. The way to do this integral is we fix theta, and we integrate for xs that go from 0 up to that number. To make it equal to 1, we need to divide by the marginal density, which is basically to renormalize this shape so that the total area under that slice, under that shape, is equal to 1. o we start with the joint. o that integral becomes the value of the density times the length of the interval over which you are integrating, which is delta. Thats the value for the density of Theta so that the overall probability over this interval ends up being 1. o now we do have our joint density in our hands. And this is indeed going to be the shape of the conditional distribution of Y given that X has occurred. o integral of y times the density of y. We already found what that density is, so we can plug it in here. To find this height, we take l/2 and multiply it with the sine of the angle that we have. o this is the joint density. And now we want to find a formula for this marginal density in terms of the joint density. f the two random the variables are independent, according to our definition in the previous slide, the joint density is going to factor as the product of the marginal densities. o this formula tells you that the probability that your random variable falls inside this interval is the area under the density curve. o x can range from 0 to l, and y can only be smaller than x. o this is the formula for the density on this part of our space. And if you want to calculate the probability of a certain event, what you do is you look at that event and you see how much of that mass is sitting on top of that event. o the event of interest, that the needle intersects the line, is described this way in terms of x and theta. Thats the density of X. And the density of Theta needs to be 2/pi. Well, it should be the integral of the density over this rectangle. And so this is a plot of the conditional distributions that you get for the different values of x. Given a particular value of x, youre going to get this certain conditional distribution. Finally, we can calculate the expected value of Y. And we can do this by using the definition of the expectation. Well, we can take as our variable to be the distance from the center of the needle to the nearest line. And if you do things carefully, you see that the only way to satisfy this relation is to define the conditional density by this particular formula. And that gives you the average value of the function g(X). When you want to think intuitively, the best way to think about what the density function is to think in terms of little intervals, the probability that my random variable falls inside the little interval. On the other hand, the conditional distribution must add up to 1. o the total probability over all of the different ys in this universe, that total probability should be equal to 1. Two random variables are said to be jointly continuous if we can calculate probabilities by integrating a certain function that we call the joint density function over the set of interest. Y is uniform between 0 and x. The expected value of Y should be the midpoint of this interval, which is x/2. The question is to find the probability that the needle is going to intersect a line. And you do your integral, and you get that this is x/2. ts for y going all the way to l. When y is equal to l, the logarithm of 1 is equal to 0. And we look at this density as a function of X. ve told you what Y is. f you have an integral to calculate, such as this integral, but youre not lucky, and your functions are not so simple where you can do your calculations by hand, and maybe the dimensions are larger instead of two random variables you have 100 random variables, so its a 100fold integral then theres no way to do that in the computer. Well, given the value of x, y can only range from 0 to x. o this is what we get. o y is uniform between 0 and x. What does that mean, that the density of y, given that you have already told me x, ranges from 0 to little x? And that tells you that the probability of a single point in the continuous case is always equal to 0. o these are formal properties. ts going to be the product of the densities of X and Theta. o we need one more variable, which we take to be the angle that the needle is forming with the lines. What is the distribution of Y? This is the x integral. When you break it the next time, the expected length of what youre left with should be 1/2 of the piece that you are now breaking. To find the marginal, we just take the joint and integrate out the variable that we dont want. o the volume is going to be the height times the area of the base. And the height of this joint density function basically tells you how much probability tends to be accumulated in certain regions of space as opposed to other parts of the space. Either the needle is going to fall in a way that does not intersect any of the lines, or its going to fall in a way that it intersects one of the lines. o in that place in the diagram, the probability per unit length around this neighborhood would be the height of the density function at that point. This is supposed to be our little d. Thats supposed to be our little l. We have the formula from the previous slide that p is 2l over pi d. n this instance, we choose d to be twice l. o this number is 1/pi. And lets say that we want to calculate the probability that x falls inside this interval. This is half of the length of the needle, which is l/2. And where it lands, we can describe this by specifying the location of the center of the needle. n the discrete case, we had the formula where here we had the sum, and instead of the density, we had the PF. mean, when you break it once, the expected value of what you are left with is going to be 1/2 of what you started with. This is the same formula as in the discrete case. We are going to define our theta to be the acute angle thats formed between the needle and a line, if you were to extend it. o the integral is just the length of the interval over which were integrating. Now, having broken the stick and given that we are left with this piece of the stick, m now going to break it again at a completely random place, meaning m going to choose a point where break it uniformly over the length of the stick. And were integrating over the range of possible ys, from 0 to l. Now this involves the integral for y log y, which m sure you have encountered in your calculus classes but maybe do not remember how to do it. The total volume under this surface, should be equal to 1. o thats one property that we want our density function to have. That slice tells us for that particular x what the possible values of y are going to be and how likely they are. This is a formula thats again identical to the formula that we had for the discrete case. o the definition is that the random variable is continuous if you can calculate probabilities associated with that random variable given that formula. You can find, perhaps, the distribution of this random variable and then use the basic definition of the expectation. That is, it should have a constant density over the range of interest.",0.1673740053050398
15,15,"Anyway, so ll give you this top ten list, and if you look at the final review handout, it has a very comprehensive list of topics. But that list may be too long, so heres just ten things, not in any particular order. These are not in order of importance. Okay, ll just use the big board. All right, so heres my top ten list. First is conditioning, right. onditioning is the soul of statistics, so that did have to come first. And conditioning includes a lot of things, right. t includes conditional probability, it includes conditional expectation, it includes Bayes rule, and even includes arkov trends in a certain sense because there was a certain conditional independence thing. onditional independence versus independence, thats been everywhere. econd, symmetry. ymmetry is powerful but dangerous, like to say. Because its extremely powerful, and a lot of problems where youd have to do 100 pages of algebra if you dont see the symmetry and one line if you see the symmetry. On the other hand you dont want to start hallucinating symmetries that are not there, so you have to be careful. And third, random variables and their distributions, which is one possible five word title for the whole course. Fourth is stories. o we spent a lot of time on not only story proofs, but more importantly, the stories for each distribution, like the normal and gamma and Poisson and so on. Theyre extremely important because otherwise, right, there are infinitely many possible distributions we could have looked at. Why do we spend so much time on the Poissons and exponentials and stuff? ts because they had important stories, otherwise just write down anything that integrates to one and then work with that. Why are some distributions more useful and important than others? ts because they have stories. Okay, fifth is linearity. ixth is indicator random variables, which is one of my favorite tricks, as think you know. ts extremely useful on interview problems and all kinds of stuff. eventh is LOT, just an extremely useful tool for computing expectations. And eighth is law of large numbers. just sort of had to put it there because that, and central limit theorem. These are the two most famous theorems in probability, possibly the two most important theorems in probability, but definitely the two most famous ones. And then tenth, arkov chains, which we spent the last few lectures on. Okay, so thats just the top ten list. But it kind of partitions up kind of neatly. At the first floor all have to do with the big picture question of what is randomness, right. Thats why probability is everywhere, is because randomness and uncertainty. And what is uncertainty, thats everywhere, right. How do we think about uncertainty? Thats these four, these three are all about computing expected values. ndicator random variable you can use for other purposes as well, but we were especially using it for computing expected values. And remember also that expected values doesnt just mean finding the average of the random variable, because if we want to find the standard deviation, then we need the variance. And to get the variance, we need to do expected values. o a lot of things are more than just the average reduced back down to this, so computing averages. And then these last three, would describe these as talking about long run behavior as we have, by long run mean we have a lot of random variables. o here and here, and we take the average of a large number of D random variables and see what happens, how does it behave. And then for arkov chains, thats how they were originally designed. arkovs motivation was going one step beyond D. You run this arkov chain just wandering around for a long time, right. ts not D anymore generally, but it has this nice conditional structure that makes it very nice to work with. Okay, well anyway thats top ten list. And let me know if any other questions come to mind in the mean time. But mean while ll draw a little picture. o weve been doing arkov chains. o its kind of natural to represent our current state, is stat 110, right. o this is the whole world of statistics at Harvard as a arkov chain, and were here now, and it doesnt matter how you got here, right. All that matters is that youre here in tat 110. And then the question is saying, well where are you gonna be next semester or next year or things like that? What can you do from here? o want to briefly mention, if you look at the prerequisites for stat courses above 110, most of them require 110. And theres reasons, there are very good reasons for that. And just give a few examples, both mentioning a little bit about the courses just so that you, for some of you, it may be useful to know. But also just to show you a few examples. Okay, so where can you go from 110? Well the most obvious follow up is 111. 111 and 110 in a sense should be a full year course on probability and inference. ts going back to the very first day of class, wont draw the diagram again, but on the very first day of class drew a picture of about whats the difference between probability and statistical inference. And then theyre kind of two sides of the same coin. n probability were saying, heres our model, and then were saying, well whats the probability that this will happen? Whats the probability that would happen? On average whats gonna happen, right, given that were using this particular model like were Poisson lambda or something and assume lambda is known or lambda has some distribution itself but that distribution is known and so on. nference is about going the other way. You have data, and then you want to actually then say, estimate unknown parameters, right, theta or lambda or whatever the parameter is unknown, okay. You want to estimate it or you want to make predictions about future, like you have lots of observation so far, but then in the future youre gonna get more data and you want to predict what thats gonna look like. Okay, so those are questions of statistical inference. You cant do statistical inference unless you have probability cuz it gives you both the language you need as well as a lot of the theorems. niversality of the uniform, didnt introduce that to torture you or anything. mean, its a beautiful general result, but its also very, very useful in statistical inference. o things like that. Okay, so thats 111, thats every pring. Just from 110 and 111 alone, theyre both more on the mathematical side, rather than a course where youre actually learning how do you work with data. o another course would especially recommend s 139, which is a linear models, which has some overlap with econometrics E1123. And in fact, we, in stat, have been having discussions of the ec department for years about the issue of 139 versus 1123. Because the ec departments policy is that you cant take both for credit. And the stat departments policy is that you can take both for credit. o because the stat perspective on this is that the points of view are sufficiently different that you should be able to do both. But thats a difference in point of view about the points of view. o anyway, you could include 1123 or 1126 here, m not going to list all the ec course numbers. You should have some course in regression in linear models as useful for all kinds of things. And 139 does not require 110, but if youve had 110 it gives you a better understanding of whats going on. And similarly, if you look at like 1126, take a look at the course notes for that sometime. Everything is conditional expectation, all right? o if you understand conditional expectation, then thats going to be extremely advantageous. 139, youre going to understand whats going on much better than other people in class who havent had 110, although its not required. o 139 is just a great course. And actually just ways to work with data, that youre actually using data on the computer. And one piece of advice if you want, in addition to everything else m saying, is to learn R. Thats one of the best six letter pieces of advice can think of. Also R is a statistical. also recommend learning . And so also recommend 50 or learn somewhere else as well. But were talking about statistics right now, learn R. R is very, very different from . R was designed by statisticians for statistical purposes in mind. ompletely different from in flavor, its easier in some ways. But sometimes thats a misconception, sometimes people who have been doing for years think that they could just learn R in like a couple of hours and be an R expert, and they approach it all wrong. uz its just a completely different mindset. Very, very different language. ts extremely powerful. Actually just saw a really interesting article yesterday that was arguing that learning R is not only incredibly useful that it actually helps you become a bit better statistical thinker just by studying this language. And hadnt thought about it that way before but actually that article convinced me that that was true. o this is incredibly useful. Okay, a lot of the stat courses in general, actually use R. And whether you do more stat or not, its just an incredibly useful skill. Getting more useful all the time. And R is free. You can find all kinds of very, very well written tutorials and stuff like that for free, you can download it for free. And because it has this kind of free software culture, a lot of people are writing R packages, and put them online for free. And so it keeps growing like that, because it has this open source culture to it. Okay, thats 111, 139, these are in no particular order. 123, finance. f you have any interest in finance. Not offered this spring but it will be offered next spring. Any interest at all in finance, you should definitely take 123. dont need to talk more about it now because teven Blithe already gave the 123 preview. Although would mention really liked what he said at the end. He said the two dimensional lotus was a key to winning the Nobel Prize. o then was happy have LOT here. That includes the 2D one, right here, all dimensional LOTes. Okay, thats a finance course. 115 is computational biology. Thats every spring. ts a really nice mixture of biology, computer science and statistics. To understand whats going on there, a lot of it relies on arkov chains and Bayesian thinking, the things weve been doing. Like that email that just sent. The arkov chains are everywhere in biology these days. o if youre interested in biology and stat, that would be a good one. Well, what else do wanna mention? 171 is another natural followup. 171 is stochastic processes. o if you like arkov chains, but we only had three lectures on it. n 171, maybe they spend a month on arkov chains, and then do a lot of other stochastic processes as well. o in a sense, 111 is swapping to the other side of the coin. And 171 is continuing in kind of probability thinking. Where you already have random variables evolving over time, right? Thats a stochastic process. Okay, so thats a really good stochastic process course. And any others that wanted to mention right now? think thats good. Okay, so we have that little thing there. want to do a couple, just a few quick examples of just kind of how probability shows up in some of these areas. Okay, so for example, lets take 139. And by 139, mean 139 but also mean more generally regression, which you can also see in econometrics or elsewhere. ts extremely widely used for analyzing data. And the first time saw regression, really, really hated it. thought it was the ugliest thing ever and had no idea what was going on. To start with a really simple, relatively simple, linear regression model just for the simplest case just to do an example because its not a regression course. Were looking at models like this, y equals beta 0 + beta 1x + epsilon. Beta 0 and beta 1 are constants which are generally we would think of those as unknown parameters. o were trying to estimate. The question is, can you use x to predict y? o you have a pair x and y. aybe you have 100 people in your study or something like that, or it doesnt have to be people obviously but you have two variables you wanna know how does x use x to predict y? Thats a regression. And this would be a simple example of linear regression because this looks linear, right? And epsilon is because youre assuming that x is not going to perfectly tell you y, that theres some error term here epsilon. n practice, people often assume that these epsilons are normal with mean zero, but you dont have to assume that necessarily. o sometimes normal might not be realistic. But lets assume, a common assumption would be that at least theyre centered at zero, so the expected value is zero. o on average its not gonna be, that there should be zero on average is a natural uncommon assumption. But in particular, want to assume that the expected value of epsilon given x is 0. o theres not certain values of x that are going to the errors tend to be positive rather than negative, that kind of thing. o lets assume that, mean this is just regression. Okay, and then m not going to do much with this, m just going to show you very quickly how to get some simple facts about this. o one thing we could do when we see this equation is to take the covariance of both sides with X. o, m saying that these are the same random variable, okay? o can do whatever want to both sides, so if take the covariance of both sides with X, well, covariance of a constant with anything is 0, so that part goes away. o we have the covariance of (beta 1 X with X) + the covariance of (epsilon with X). This is just a quick review of properties of covariance. Beta 1 comes out, covariance of X with itself is the variance, so thats beta 1 times the variance (X), and then lets think about the covariance of epsilon and X. ll just do that over here and figure out what this is. Well, m assuming that the conditional expectation is 0. By Adams law, the unconditional expectation is also 0, because its E(E(epsilon|x)), thats just Adams law, = 0. And so the covariance of epsilon and X is just the expected value of epsilon X, because the other term is 0, because epsilon has 0. And lets just quickly compute this expectation. Well, the strategy, again, should be condition on x. Adams law, so were just gonna go E(E(epsilon X|X). Just a quick review of Adams law, take out whats known. Once we condition on X, the next is known, so this X pops out right there. But whats left is E(epsilon|X), which we assumed is 0, so now its X times 0. X times 0 is 0, so this is just 0. o actually, its plus 0 here. o that tells us that beta 1 = the covariance. By symmetry, thats the covariance (X,Y) divided by the variance (X). Okay? o thats a very quick derivation of this fact. And if you look in a lot of books that do regression, youre gonna have a very ugly looking formula with summation signs. This is the population version rather than the sample version, okay? And a lot of books, if they dont wanna assume 110, then theyre gonna give the sample version of just like, heres the formula, the summation of something, xi minus something, yi minus, that kind of thing. And theyll either prove that thing in some long, tedious, algebraic way, or theyll just say, we dont wanna prove this, heres the formula for you, and it just looks really ugly. Right? ts hard to memorize it and its hard to understand where it came from, but its just that. o understanding this thing, you actually understand where the things come from. imilarly, a lot of these formulas that looked really ugly at first, once you really understand what theyre doing, a lot of times, its just a conditional expectation, and in particular, a lot of times its just a projection. Right? We drew a geometric picture. What does this conditional expectation mean, geometrically? ts a projection. Projections are nice, right? o, just understanding things, you actually can understand whats going on. And in particular, a few of you actually asked me for book recommendations. And if any of you want more specific book recommendations for anything, like recommending books. m just gonna mention one right now. love this book. ostly Harmless Econometrics. Have any of you read this book? ts really cheap, too. ts like $18 on Amazon, last time checked. This book is beautifully written. And cant say that thats true about many econometrics books. This one is just beautifully written. And if you flip through this book sometime, what youll see is the conditional expectation, and Adams law and Eves law are everywhere. o its all building on things we did here, and if you havent had that background, it will be much harder to understand whats going on there. But with that, then its just Adams law, Eves law. t should all come together. Okay. o wanted to mention one other course here, which is more of an obscure one, but think its an important one. ts only offered every other year. Thats that 160. o that 160 is survey sampling. ts very relevant for any of you who are interested in government and policy and things that would involve survey data, political polls, or other types of survey sampling. o just to give you a quick example of how you can use 110 ideas, and think this is actually a good review example too, m gonna do one. And you can imagine, by the way, that stuff like hypergeometrics would come up a lot there, because you have this population and youre drawing a sample, right? sually, youre just trying to learn something and thats of some policy interest, right? sually, you cannot ask every person in the population what their opinion is, so you get a sample, and then you try to infer to the population what can you do, right? o, if you sample without replacement, we know that hypergeometrics come up very, very, very naturally, right? o heres just a quick example. ts kind of a different mindset from other parts of statistics where we assume D because were assuming that were sampling from a finite population. This is one approach to it. There are some kind of foundational controversies about how do you approach this problem. We have a finite population. Well, of course, its a finite population. ts always a finite population right? A lot of times, that gets ignored. You have a finite population, lets say, of people, and suppose that their true values of something that youre interested in, that any character, that is for each person, you have some variable. t could be their height, their income, their opinion on some question, whatever youre studying. Lets say that the true values are Y1 through YN, where capital N is the size of the population. And lets assume that each person has an D number, like social security number, so theres some well defined way to list them out. And these are treated as nonrandom, at least in the approach were talking about now. These are treated as fixed. These are just constants. nitially, they are unknown constants. Right? We just assume each person has some fixed opinion, but we dont know what those opinions are yet. o what do we do? We draw a sample, we get a sample. Okay? And theres many, many different ways to do that sampling, which we discussed in tat 160. You got this sample, and then you try to infer it to the population. And in practice, it may be extremely difficult to get a simple random sample. That is where all samples of a certain size are equally likely, and its not always necessarily even desirable to have a simple random sample. But anyway, lets just assume we have some samplings to scheme where we get a sample of size lowercase n. Okay? o suppose our goal is to estimate the average of these. Or, equivalently, assuming we know N, capital N, then its equivalent to just say, try to estimate the sum. That is, we only get to collect the sample, but we wanna estimate the total or average value of the entire population. o we wanna estimate the sum of all the yjs, j = 1 to N, okay? Thats the goal. Now we have a sample of size n. And lets assume that the probability that person j, we have some ordering D number for each person, the probability that person j is in the sample is pj, which we assume right now is known. And then, of course, in practice pj might not be known. And you might have to estimate pj. And then you can ask what happens then. How do you deal with the fact that you dont know the true pj? Okay, thats more complicated. But lets assume that the true pj are known. o the simple random sampling would be the case when all the pjs are equal. That is, everyone is equally likely to be collected into your sample. But that may not be true, some people may be much easier to sample than others. Or some are just obscure, hard to reach, dont answer the phone, in some obscure part of the country or something like that. Other people are easier to actually collect into your sample. Okay, so pjs may not all be equal. Okay, now then the claim is that, suppose that our data are we get like X1, ll just say like X1, Z1 through Xn, Zn be the data, That you collect from your sample, where X is the Y value. m using a different letter, X rather than Y, just to indicate the Xs are random variables, the Ys are fixed. The Xs are random because this is the first person you collect into your survey. But that person was randomly chosen with some probabilities. o the value has become random because of the sampling, okay? And Z is just their D number. o Xj is the Y value that youre interested in, and Zj is the D number of that person, that is who did we actually get. Thats the setup, okay? o a question that is of tremendous interest is how do we get an unbiased estimator for the total? And the standard method thats used thats based on a very, very clever trick is to divide by the probability. That is, you sum up all your observations, j = 1 to n of Xj, but you divide by the probability of getting at that person. o in other words, if the values you observed were 5, 10 and 15 and those had probabilities, A, B, . What you would do is 5 divided by a, plus 10 divided by b, plus 15 divided c. o, these are the probabilities, pzj. Thats unbiased. o this just says take each measurement and divide by the probability that you actually got that person in your survey, then that is unbiased. Okay, so lets prove that is unbiased. Well take the expectation, but this looks ugly. This is kind of an interesting thing here because this denominator seems kind of tricky. Because this is P sub Zj, where Zj is the D number of that, its a random D number. o in the denominator here we have a random probability. How do we deal with that? Well, a simple way around that is to rewrite this using indicator random variables. o lets just rewrite this as the sum, not just over these values that you actually had. Lets sum over the entire population j = 1 to N. And lets sum yj, which is just a constant, divided by pj times j, where j is the indicator of person j being included. o thats exactly the same thing, n the sample. Thats the same thing because anyone whos not included just get zeroed out, and anyone who is included thats 1 and then its just the same thing here. But now its just Pj in the denominator, easy to deal with. Pzj, thats a random probability, Pj, very simple, okay? o now the expected value. ll just take the expected value of this linearity, right? o m just gonna swap the e in the sum by linearity, respective value of ij, by definition, and fundamental bridge is the probability that person j is included in the sample, by definition thats pj. o this is just pj over pj, yj, done. o thats a very, very simple trick. Just divide by the probability. This is used all over the place now. Variations on this trick is a very important idea. nverse probability, this is either called HorvitzThompson Estimator. We just derived the HorvitzThompson Estimator, it wasnt too difficult. Or its called inverse probability waiting, and youll find theres tons and tons of people using this, inverse probability waiting. Okay, well, mean so you can do that. Thats unbiased. Now we come to a deeper question, is unbiased good? Well, mean, d rather be unbiased than biased but is that good in and of itself as a criterion? s that good enough to tell us that this is actually a good estimator? Not so much a harder question. And ll tell you a quick story showing that this can be horribly, horribly bad. m not saying its always bad, either. t can be good, it can be bad. m saying this goes much, much deeper. This is one of my favorite examples. This is called Basus Elephant, and Basu was one of my favorite statisticians. o ll just tell you Basus elephant story quickly. This is an indication that theres more, theres a lot going on here. tatistics cannot be described as, you learn some formulas and you plug it in and whatever. You need to think really hard about, doesnt make sense? This is useful, you cant just say all time biased so were happy. o the story is there was a circus owner who had 50 elephants, and he wanted to know that the average weight of his elephants, equivalently he wanted to know the total weight of the 50 elephants. And ve never tried weighing an elephant, but apparently its pretty hard to actually physically weigh the elephant. Too much work to try to weigh the 50 elephants. o the circus said, okay, ll just take one that looks average. Lets call that elephant tampie. Take tampie, weigh tampie, multiply by 50. ounds like a reasonable thing to do. Right? But a statistician overhears that and gets very, very agitated. And then says, well but that would be biased, right? And so the circus owner says okay, youre the statistical expert. What should we do? And the statistician, this is actually not a very good statistician. ts someone who just kind of learned some formulas. And says, well you should use the HorvitzThompson. You know, we do this inverseprobabilityweighting thing. And otherwise youre gonna be biased, okay? And the circus owner says well, really wanna weigh tampies right here. The other elephants are off wandering somewhere else and really like tampy. Hs not gonna kick me and stuff. o anyway, then the statistician says, thats fine. And this HorvitzThompson, these probabilities pj. shouldve written pj 0 because then were not dividing by 0. But as long as pj is greater than 0, thiss perfectly fine. ts still unbiased, so okay. o they decided to give probability, theyre only gonna weigh one elephant. They decide to give probability 0.99 to tampy. And then for the other 49 elephants take the other 1%. And divide that up equally to the other 49 elephants. ure enough, 99% chance that they get to weigh tampy. o they weigh tampy and everyone seems happy. ntil what the circus owner was proposing to do is take tampys weight, multiply it by 50, right? t seems like a natural thing to do. Well HorvitzThompson says take tampys weight. Divide by the probability, which was 99 over 100. o just take take tampy and multiply by 100 over 99, which is just slightly greater than 1. Now that doesnt sound like a very good estimate for 50 elephants, just to take one of them and multiply it by 100 over 99. And so the circus owner says, well something seems a little suspicious about that. Are you sure that thats good? And the statistician says, well if youd gotten one of the other ones, well then you multiply it by 4,900. o then its unbiased. o there are very, very, very, very tricky questions about what are kind of good criteria or natural criteria for when is an estimator good. What does that mean? That goes back into 111 a lot. 111 is gonna be a lot about Bayesian versus frequentist. o weve been talking about some Bayesian stuff, right? Like we talked about conjugate priors and the beta binomial, things like that. Theyll do a lot of conjugate priors, things like that. Frequent unbiasedness is inherently a frequentist concept. But theres other frequentist concepts as well. You can ask when are these two methods or ideas. Well, first of all you can discuss whether they even are well defined methods. But when would they agree, when they would disagree, things like that. There are very, very, very subtle questions that come up, wery, very surprising. There are a lot of paradoxes in where you write down something that seems like a perfectly natural estimator and you know it. And then you can prove that actually you can always do better than that. And things like that. Okay well, so lets see. Any other courses to mention here? All right, so this is our little arkov chain, well its not really. Well the question is whether its irreducible, guess. ts irreducible right now. Now, of course, you could go From 111 to 139. You can go from 139 to 111. You can go all over the place. There are a couple more quick edges we should draw here. Once is 110 goes to jobs. o if any of you get any good interview questions that relate to probability, please send them my way. wont get into the whole Vocational debate right now, which has been coming up more. And one more. The 110 is actually a recurrent state in this chain. And William is here who took the class last year, so we have an example. omeone else just emailed me yesterday saying she took it last year. And really wished, more than anything, she could just repeat the class over and over again. Now m hoping youre not gonna repeat it over and over again in the sense that you actually have to literally retake the course. But in the sense of revisiting the material, over and over again, is a good thing. Okay, well thats all. Thank you, it was all fun.","Now we have a sample of size n. And lets assume that the probability that person j, we have some ordering D number for each person, the probability that person j is in the sample is pj, which we assume right now is known. Beta 1 comes out, covariance of X with itself is the variance, so thats beta 1 times the variance (X), and then lets think about the covariance of epsilon and X. ll just do that over here and figure out what this is. o Xj is the Y value that youre interested in, and Zj is the D number of that person, that is who did we actually get. o one thing we could do when we see this equation is to take the covariance of both sides with X. o, m saying that these are the same random variable, okay? But in particular, want to assume that the expected value of epsilon given x is 0. o theres not certain values of x that are going to the errors tend to be positive rather than negative, that kind of thing. sually, you cannot ask every person in the population what their opinion is, so you get a sample, and then you try to infer to the population what can you do, right? o this just says take each measurement and divide by the probability that you actually got that person in your survey, then that is unbiased. Okay, and then m not going to do much with this, m just going to show you very quickly how to get some simple facts about this. o because the stat perspective on this is that the points of view are sufficiently different that you should be able to do both. You have a finite population, lets say, of people, and suppose that their true values of something that youre interested in, that any character, that is for each person, you have some variable. And you can imagine, by the way, that stuff like hypergeometrics would come up a lot there, because you have this population and youre drawing a sample, right? That is, you sum up all your observations, j = 1 to n of Xj, but you divide by the probability of getting at that person. And just give a few examples, both mentioning a little bit about the courses just so that you, for some of you, it may be useful to know. That is, we only get to collect the sample, but we wanna estimate the total or average value of the entire population. Okay, now then the claim is that, suppose that our data are we get like X1, ll just say like X1, Z1 through Xn, Zn be the data, That you collect from your sample, where X is the Y value. And so the covariance of epsilon and X is just the expected value of epsilon X, because the other term is 0, because epsilon has 0. Okay, well, mean so you can do that. You want to estimate it or you want to make predictions about future, like you have lots of observation so far, but then in the future youre gonna get more data and you want to predict what thats gonna look like. o just to give you a quick example of how you can use 110 ideas, and think this is actually a good review example too, m gonna do one. And remember also that expected values doesnt just mean finding the average of the random variable, because if we want to find the standard deviation, then we need the variance. You got this sample, and then you try to infer it to the population. o m just gonna swap the e in the sum by linearity, respective value of ij, by definition, and fundamental bridge is the probability that person j is included in the sample, by definition thats pj. At the first floor all have to do with the big picture question of what is randomness, right. o lets just rewrite this as the sum, not just over these values that you actually had. Because its extremely powerful, and a lot of problems where youd have to do 100 pages of algebra if you dont see the symmetry and one line if you see the symmetry. Now m hoping youre not gonna repeat it over and over again in the sense that you actually have to literally retake the course. o this is the whole world of statistics at Harvard as a arkov chain, and were here now, and it doesnt matter how you got here, right. Now that doesnt sound like a very good estimate for 50 elephants, just to take one of them and multiply it by 100 over 99. And the statistician says, well if youd gotten one of the other ones, well then you multiply it by 4,900. o then its unbiased. On the other hand you dont want to start hallucinating symmetries that are not there, so you have to be careful. o we wanna estimate the sum of all the yjs, j = 1 to N, okay? And a lot of books, if they dont wanna assume 110, then theyre gonna give the sample version of just like, heres the formula, the summation of something, xi minus something, yi minus, that kind of thing. Okay, a lot of the stat courses in general, actually use R. And whether you do more stat or not, its just an incredibly useful skill. o a question that is of tremendous interest is how do we get an unbiased estimator for the total? And then says, well but that would be biased, right? Well, mean, d rather be unbiased than biased but is that good in and of itself as a criterion? o we spent a lot of time on not only story proofs, but more importantly, the stories for each distribution, like the normal and gamma and Poisson and so on. And then the question is saying, well where are you gonna be next semester or next year or things like that? o you have a pair x and y. aybe you have 100 people in your study or something like that, or it doesnt have to be people obviously but you have two variables you wanna know how does x use x to predict y? And the standard method thats used thats based on a very, very clever trick is to divide by the probability. o here and here, and we take the average of a large number of D random variables and see what happens, how does it behave. And epsilon is because youre assuming that x is not going to perfectly tell you y, that theres some error term here epsilon. You cant do statistical inference unless you have probability cuz it gives you both the language you need as well as a lot of the theorems. And one piece of advice if you want, in addition to everything else m saying, is to learn R. Thats one of the best six letter pieces of advice can think of. imilarly, a lot of these formulas that looked really ugly at first, once you really understand what theyre doing, a lot of times, its just a conditional expectation, and in particular, a lot of times its just a projection. And then you can prove that actually you can always do better than that. ts going back to the very first day of class, wont draw the diagram again, but on the very first day of class drew a picture of about whats the difference between probability and statistical inference. X times 0 is 0, so this is just 0. o actually, its plus 0 here. o its all building on things we did here, and if you havent had that background, it will be much harder to understand whats going on there. o lets assume that, mean this is just regression. How do you deal with the fact that you dont know the true pj? You have data, and then you want to actually then say, estimate unknown parameters, right, theta or lambda or whatever the parameter is unknown, okay. All right, so this is our little arkov chain, well its not really. ll just take the expected value of this linearity, right? Lets sum over the entire population j = 1 to N. And lets sum yj, which is just a constant, divided by pj times j, where j is the indicator of person j being included. You can find all kinds of very, very well written tutorials and stuff like that for free, you can download it for free. There are a lot of paradoxes in where you write down something that seems like a perfectly natural estimator and you know it.",0.1356538711776187
16,16,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. Let me start by basically listing the main things we have learned over the past three weeks or so. And will add a few complements of information about that because there are a few small details that didnt quite clarify and that should probably make a bit clearer, especially what happened at the very end of yesterdays class. Here is a list of things that should be on your review sheet for the exam. The first thing we learned about, the main topic of this unit is about functions of several variables. We have learned how to think of functions of two or three variables in terms of plotting them. n particular, well, not only the graph but also the contour plot and how to read a contour plot. And we have learned how to study variations of these functions using partial derivatives. Remember, we have defined the partial of f with respect to some variable, say, x to be the rate of change with respect to x when we hold all the other variables constant. f you have a function of x and y, this symbol means you differentiate with respect to x treating y as a constant. And we have learned how to package partial derivatives into a vector,the gradient vector. For example, if we have a function of three variables, the vector whose components are the partial derivatives. And we have seen how to use the gradient vector or the partial derivatives to derive various things such as approximation formulas. The change in f, when we change x, y, z slightly, is approximately equal to, well, there are several terms. And can rewrite this in vector form as the gradient dot product the amount by which the position vector has changed. Basically, what causes f to change is that am changing x, y and z by small amounts and how sensitive f is to each variable is precisely what the partial derivatives measure. And, in particular, this approximation is called the tangent plane approximation because it tells us, in fact, it amounts to identifying the graph of the function with its tangent plane. t means that we assume that the function depends more or less linearly on x, y and z. And, if we set these things equal, what we get is actually, we are replacing the function by its linear approximation. We are replacing the graph by its tangent plane. Except, of course, we havent see the graph of a function of three variables because that would live in 4dimensional space. o, when we think of a graph, really, it is a function of two variables. That also tells us how to find tangent planes to level surfaces. Recall that the tangent plane to a surface, given by the equation f of x, y, z equals z, at a given point can be found by looking first for its normal vector. And we know that the normal vector is actually, well, one normal vector is given by the gradient of a function because we know that the gradient is actually pointing perpendicularly to the level sets towards higher values of a function. And it gives us the direction of fastest increase of a function. OK. Any questions about these topics? No. OK. Let me add, actually, a cultural note to what we have seen so far about partial derivatives and how to use them, which is maybe something should have mentioned a couple of weeks ago. Why do we like partial derivatives? Well, one obvious reason is we can do all these things. But another reason is that, really, you need partial derivatives to do physics and to understand much of the world that is around you because a lot of things actually are governed by what is called partial differentiation equations. o if you want a cultural remark about what this is good for. A partial differential equation is an equation that involves the partial derivatives of a function. o you have some function that is unknown that depends on a bunch of variables. And a partial differential equation is some relation between its partial derivatives. Let me see. These are equations involving the partial derivatives of an unknown function. Let me give you an example to see how that works. For example, the heat equation is one example of a partial differential equation. t is the equation Well, let me write for you the space version of it. t is the equation partial f over partial t equals some constant times the sum of the second partials with respect to x, y and z. o this is an equation where we are trying to solve for a function f that depends, actually, on four variables, x, y, z, t. And what should you have in mind? Well, this equation governs temperature. f you think that f of x, y, z, t will be the temperature at a point in space at position x, y, z and at time t, then this tells you how temperature changes over time. t tells you that at any given point, the rate of change of temperature over time is given by this complicated expression in the partial derivatives in terms of the space coordinates x, y, z. f you know, for example, the initial distribution of temperature in this room, and if you assume that nothing is generating heat or taking heat away, so if you dont have any air conditioning or heating going on, then it will tell you how the temperature will change over time and eventually stabilize to some final value. Yes? Why do we take the partial derivative twice? Well, that is a question, would say, for a physics person. But in a few weeks we will actually see a derivation of where this equation comes from and try to justify it. But, really, that is something you will see in a physics class. The reason for that is basically physics of how heat is transported between particles in fluid, or actually any medium. This constant k actually is called the heat conductivity. t tells you how well the heat flows through the material that you are looking at. Anyway, am giving it to you just to show you an example of a real life problem where, in fact, you have to solve one of these things. Now, how to solve partial differential equations is not a topic for this class. t is not even a topic for 18.03 which is called Differential Equations, without partial, which means there actually you will learn tools to study and solve these equations but when there is only one variable involved. And you will see it is already quite hard. And, if you want more on that one, we have many fine classes about partial differential equations. But one thing at a time. wanted to point out to you that very often functions that you see in real life satisfy many nice relations between the partial derivatives. That was in case you were wondering why on the syllabus for today it said partial differential equations. Now we have officially covered the topic. That is basically all we need to know about it. But we will come back to that a bit later. You will see. OK. f there are no further questions, let me continue and go back to my list of topics. Oh, sorry. should have written down that this equation is solved by temperature for point x, y, z at time t. OK. And there are, actually, many other interesting partial differential equations you will maybe sometimes learn about the wave equation that governs how waves propagate in space, about the diffusion equation, when you have maybe a mixture of two fluids, how they somehow mix over time and so on. Basically, to every problem you might want to consider there is a partial differential equation to solve. OK. Anyway. orry. Back to my list of topics. One important application we have seen of partial derivatives is to try to optimize things, try to solve minimum/maximum problems. Remember that we have introduced the notion of critical points of a function. A critical point is when all the partial derivatives are zero. And then there are various kinds of critical points. There is maxima and there is minimum, but there is also saddle points. And we have seen a method using second derivatives to decide which kind of critical point we have. should say that is for a function of two variables to try to decide whether a given critical point is a minimum, a maximum or a saddle point. And we have also seen that actually that is not enough to find the minimum of a maximum of a function because the minimum of a maximum could occur on the boundary. Just to give you a small reminder, when you have a function of one variables, if you are trying to find the minimum and the maximum of a function whose graph looks like this, well, you are going to tell me, quite obviously, that the maximum is this point up here. And that is a point where the first derivative is zero. That is a critical point. And we used the second derivative to see that this critical point is a local maximum. But then, when we are looking for the minimum of a function, well, it is not at a critical point. t is actually here at the boundary of the domain, you know, the range of values that we are going to consider. Here the minimum is at the boundary. And the maximum is at a critical point. imilarly, when you have a function of several variables, say of two variables, for example, then the minimum and the maximum will be achieved either at a critical point. And then we can use these methods to find where they are. Or, somewhere on the boundary of a set of values that are allowed. t could be that we actually achieve a minimum by making x and y as small as possible. aybe letting them go to zero if they had to be positive or maybe by making them go to infinity. o, we have to keep our minds open and look at various possibilities. We are going to do a problem like that. We are going to go over a practice problem from the practice test to clarify this. Another important cultural application of minimum/maximum problems in two variables that we have seen in class is the least squared method to find the best fit line, or the best fit anything, really, to find when you have a set of data points what is the best linear approximately for these data points. And here have some good news for you. While you should definitely know what this is about, it will not be on the test. That doesnt mean that you should forget everything we have seen about it, OK? Now what is next on my list of topics? We have seen differentials. Remember the differential of f, by definition, would be this kind of quantity. At first it looks just like a new way to package partial derivatives together into some new kind of object. Now, what is this good for? Well, it is a good way to remember approximation formulas. t is a good way to also study how variations in x, y, z relate to variations in f. n particular, we can divide this by variations, actually, by dx or by dy or by dz in any situation that we want, or by d of some other variable to get chain rules. The chain rule says, for example, there are many situations. But, for example, if x, y and z depend on some other variable, say of variables maybe even u and v, then that means that f becomes a function of u and v. And then we can ask ourselves, how sensitive is f to a value of u? Well, we can answer that. The chain rule is something like this. And let me explain to you again where this comes from. Basically, what this quantity means is if we change u and keep v constant, what happens to the value of f? Well, why would the value of f change in the first place when f is just a function of x, y, z and not directly of you? Well, it changes because x, y and z depend on u. First we have to figure out how quickly x, y and z change when we change u. Well, how quickly they do that is precisely partial x over partial u, partial y over partial u, partial z over partial u. These are the rates of change of x, y, z when we change u. And now, when we change x, y and z, that causes f to change. How much does f change? Well, partial f over partial x tells us how quickly f changes if just change x. get this. That is the change in f caused just by the fact that x changes when u changes. But then y also changes. y changes at this rate. And that causes f to change at that rate. And z changes as well, and that causes f to change at that rate. And the effects add up together. Does that make sense? OK. And so, in particular, we can use the chain rule to do changes of variables. f we have, say, a function in terms of polar coordinates on theta and we like to switch it to rectangular coordinates x and y then we can use chain rules to relate the partial derivatives. And finally, last but not least, we have seen how to deal with nonindependent variables. When our variables say x, y, z related by some equation. One way we can deal with this is to solve for one of the variables and go back to two independent variables, but we cannot always do that. Of course, on the exam, you can be sure that will make sure that you cannot solve for a variable you want to remove because that would be too easy. Then when we have to look at all of them, we will have to take into account this relation, we have seen two useful methods. One of them is to find the minimum of a maximum of a function when the variables are not independent, and that is the method of Lagrange multipliers. Remember, to find the minimum or the maximum of the function f, subject to the constraint g equals constant, well, we write down equations that say that the gradient of f is actually proportional to the gradient of g. There is a new variable here, lambda, the multiplier. And so, for example, well, guess here had functions of three variables, so this becomes three equations. f sub x equals lambda g sub x, f sub y equals lambda g sub y, and f sub z equals lambda g sub z. And, when we plug in the formulas for f and g, well, we are left with three equations involving the four variables, x, y, z and lambda. What is wrong? Well, we dont have actually four independent variables. We also have this relation, whatever the constraint was relating x, y and z together. Then we can try to solve this. And, depending on the situation, it is sometimes easy. And it sometimes it is very hard or even impossible. But on the test, havent decided yet, but it could well be that the problem about Lagrange multipliers just asks you to write the equations and not to solve them. Well, dont know yet. am not promising anything. But, before you start solving, check whether the problem asks you to solve them or not. f it doesnt then probably you shouldnt. Another topic that we solved just yesterday is constrained partial derivatives. And guess have to reexplain a little bit because my guess is that things were not extremely clear at the end of class yesterday. Now we are in the same situation. We have a function, lets say, f of x, y, z where variables x, y and z are not independent but are constrained by some relation of this form. ome quantity involving x, y and z is equal to maybe zero or some other constant. And then, what we want to know, is what is the rate of change of f with respect to one of the variables, say, x, y or z when keep the others constant? Well, cannot keep all the other constant because that would not be compatible with this condition. mean that would be the usual or socalled formal partial derivative of f ignoring the constraint. To take this into account means that if we vary one variable while keeping another one fixed then the third one, since it depends on them, must also change somehow. And we must take that into account. Lets say, for example, we want to find am going to do a different example from yesterday. o, if you really didnt like that one, you dont have to see it again. Lets say that we want to find the partial derivative of f with respect to z keeping y constant. What does that mean? That means y is constant, z varies and x somehow is mysteriously a function of y and z for this equation. And then, of course because it depends on y, that means x will vary. orry, depends on y and z and z varies. Now we are asking ourselves what is the rate of change of f with respect to z in this situation? And so we have two methods to do that. Let me start with the one with differentials that hopefully you kind of understood yesterday, but if not here is a second chance. sing differentials means that we will try to express df in terms of dz in this particular situation. What do we know about df in general? Well, we know that df is f sub x dx plus f sub y dy plus f sub z dz. That is the general statement. But, of course, we are in a special case. We are in a special case where first y is constant. y is constant means that we can set dy to be zero. This goes away and becomes zero. The second thing is actually we dont care about x. We would like to get rid of x because it is this dependent variable. What we really want to do is express df only in terms of dz. What we need is to relate dx with dz. Well, to do that, we need to look at how the variables are related so we need to look at the constraint g. Well, how do we do that? We look at the differential g. o dg is g sub x dx plus g sub y dy plus g sub z dz. And that is zero because we are setting g to always stay constant. o, g doesnt change. f g doesnt change then we have a relation between dx, dy and dz. Well, in fact, we say we are going to look only at the case where y is constant. y doesnt change and this becomes zero. Well, now we have a relation between dx and dz. We know how x depends on z. And when we know how x depends on z, we can plug that into here and get how f depends on z. Lets do that. Again, saying that g cannot change and keeping y constant tells us g sub x dx plus g sub z dz is zero and we would like to solve for dx in terms of dz. That tells us dx should be minus g sub z dz divided by g sub x. f you want, this is the rate of change of x with respect to z when we keep y constant. n our new terminology this is partial x over partial z with y held constant. This is the rate of change of x with respect to z. Now, when we know that, we are going to plug that into this equation. And that will tell us that df is f sub x times dx. Well, what is dx? dx is now minus g sub z over g sub x dz plus f sub z dz. o that will be minus fx g sub z over g sub x plus f sub z times dz. And so this coefficient here is the rate of change of f with respect to z in the situation we are considering. This quantity is what we call partial f over partial z with y held constant. That is what we wanted to find. Now, lets see another way to do the same calculation and then you can choose which one you prefer. The other method is using the chain rule. We use the chain rule to understand how f depends on z when y is held constant. Let me first try the chain rule brutally and then we will try to analyze what is going on. You can just use the version that have up there as a template to see what is going on, but am going to explain it more carefully again. That is the most mechanical and mindless way of writing down the chain rule. am just saying here that am varying z, keeping y constant, and want to know how f changes. Well, f might change because x might change, y might change and z might change. Now, how quickly does x change? Well, the rate of change of x in this situation is partial x, partial z with y held constant. f change x at this rate then f will change at that rate. Now, y might change, so the rate of change of y would be the rate of change of y with respect to z holding y constant. Wait a second. f y is held constant then y doesnt change. o, actually, this guy is zero and you didnt really have to write that term. But wrote it just to be systematic. f y had been somehow able to change at a certain rate then that would have caused f to change at that rate. And, of course, if y is held constant then nothing happens here. Finally, while z is changing at a certain rate, this rate is this one and that causes f to change at that rate. And then we add the effects together. ee, it is nothing but the goodold chain rule. Just have put these extra subscripts to tell us what is held constant and what isnt. Now, of course we can simplify it a little bit more. Because, here, how quickly does z change if am changing z? Well, the rate of change of z, with respect to itself, is just one. n fact, the really mysterious part of this is the one here, which is the rate of change of x with respect to z. And, to find that, we have to understand the constraint. How can we find the rate of change of x with respect to z? Well, we could use differentials, like we did here, but we can also keep using the chain rule. How can do that? Well, can just look at how g would change with respect to z when y is held constant. just do the same calculation with g instead of f. But, before do it, lets ask ourselves first what is this equal to. Well, if g is held constant then, when we vary z keeping y constant and changing x, well, g still doesnt change. t is held constant. n fact, that should be zero. But, if we just say that, we are not going to get to that. Lets see how we can compute that using the chain rule. Well, the chain rule tells us g changes because x, y and z change. How does it change because of x? Well, partial g over partial x times the rate of change of x. How does it change because of y? Well, partial g over partial y times the rate of change of y. But, of course, if you are smarter than me then you dont need to actually write this one because y is held constant. And then there is the rate of change because z changes. And how quickly z changes here, of course, is one. Out of this you get, well, am tired of writing partial g over partial x. We can just write g sub x times partial x over partial z y constant plus g sub z. And now we found how x depends on z. Partial x over partial z with y held constant is negative g sub z over g sub x. Now we plug that into that and we get our answer. t goes all the way up here. And then we get the answer. am not going to, well, guess can write it again. There was partial f over partial x times this guy, minus g sub z over g sub x, plus partial f over partial z. And you can observe that this is exactly the same formula that we had over here. n fact, lets compare this to make it side by side. claim we did exactly the same thing, just with different notations. f you take the differential of f and you divide it by dz in this situation where y is held constant and so on, you get exactly this chain rule up there. That chain rule up there is this guy, df, divided by dz with y held constant. And the term involving dy was replaced by zero on both sides because we knew, actually, that y is held constant. Now, the real difficulty in both cases comes from dx. And what we do about dx is we use the constant. Here we use it by writing dg equals zero. Here we write the chain rule for g, which is the same thing, just divided by dz with y held constant. This formula or that formula are the same, just divided by dz with y held constant. And then, in both cases, we used that to solve for dx. And then we plugged into the formula of df to express df over dz, or partial f, partial z with y held constant. o, the two methods are pretty much the same. Quick poll. Who prefers this one? Who prefers that one? OK. ajority vote seems to be for differentials, but it doesnt mean that it is better. Both are fine. You can use whichever one you want. But you should give both a try. OK. Any questions? Yes? Yes. Thank you. forgot to mention it. Where did that go? think erased that part. We need to know directional derivatives. Pretty much the only thing to remember about them is that df over ds, in the direction of some unit vector u, is just the gradient f dot product with u. That is pretty much all we know about them. Any other topics that forgot to list? No. Yes? an erase three boards at a time? No, would need three hands to do that. think what we should do now is look quickly at the practice test. mean, given the time, you will mostly have to think about it yourselves. Hopefully you have a copy of the practice exam. The first problem is a simple problem. Find the gradient. Find an approximation formula. Hopefully you know how to do that. The second problem is one about writing a contour plot. And so, before let you go for the weekend, want to make sure that you actually know how to read a contour plot. One thing should mention is this problem asks you to estimate partial derivatives by writing a contour plot. We have not done that, so that will not actually be on the test. We will be doing qualitative questions like what is the sine of a partial derivative. s it zero, less than zero or more than zero? You dont need to bring a ruler to estimate partial derivatives the way that this problem asks you to. Lets look at problem 2B. Problem 2B is asking you to find the point at which h equals 2200, partial h over partial x equals zero and partial h over partial y is less than zero. Lets try and see what is going on here. A point where f equals 2200, well, that should be probably on the level curve that says 2200. We can actually zoom in. Here is the level 2200. Now want partial h over partial x to be zero. That means if change x, keeping y constant, the value of h doesnt change. Which points on the level curve satisfy that property? t is the top and the bottom. f you are here, for example, and you move in the x direction, well, you see, as you get to there from the left, the height first increases and then decreases. t goes for a maximum at that point. o, at that point, the partial derivative is zero with respect to x. And the same here. Now, lets find partial h over partial y less than zero. That means if we go north we should go down. Well, which one is it, top or bottom? Top. Yes. Here, if you go north, then you go from 2200 down to 2100. This is where the point is. Now, the problem here was also asking you to estimate partial h over partial y. And if you were curious how you would do that, well, you would try to figure out how long it takes before you reach the next level curve. To go from here to here, to go from Q to this new point, say Q prime, the change in y, well, you would have to read the scale, which was down here, would be about something like 300. What is the change in height when you go from Q to Q prime? Well, you go down from 2200 to 2100. That is actually minus 100 exactly. OK? And so delta h over delta y is about minus onethird, well, minus 100 over 300 which is minus onethird. And that is an approximation for partial derivative. o, that is how you would do it. Now, let me go back to other things. f you look at this practice exam, basically there is a bit of everything and it is kind of fairly representative of what might happen on Tuesday. There will be a mix of easy problems and of harder problems. Expect something about computing gradients, approximations, rate of change. Expect a problem about reading a contour plot. Expect one about a min/max problem, something about Lagrange multipliers, something about the chain rule and something about constrained partial derivatives. mean pretty much all the topics are going to be there.","And then, what we want to know, is what is the rate of change of f with respect to one of the variables, say, x, y or z when keep the others constant? n fact, the really mysterious part of this is the one here, which is the rate of change of x with respect to z. And, to find that, we have to understand the constraint. This is the rate of change of x with respect to z. Now, when we know that, we are going to plug that into this equation. t is the equation partial f over partial t equals some constant times the sum of the second partials with respect to x, y and z. o this is an equation where we are trying to solve for a function f that depends, actually, on four variables, x, y, z, t. And what should you have in mind? Well, it changes because x, y and z depend on u. First we have to figure out how quickly x, y and z change when we change u. Well, how quickly they do that is precisely partial x over partial u, partial y over partial u, partial z over partial u. These are the rates of change of x, y, z when we change u. And now, when we change x, y and z, that causes f to change. Well, partial g over partial y times the rate of change of y. But, of course, if you are smarter than me then you dont need to actually write this one because y is held constant. t tells you that at any given point, the rate of change of temperature over time is given by this complicated expression in the partial derivatives in terms of the space coordinates x, y, z. f you know, for example, the initial distribution of temperature in this room, and if you assume that nothing is generating heat or taking heat away, so if you dont have any air conditioning or heating going on, then it will tell you how the temperature will change over time and eventually stabilize to some final value. Just to give you a small reminder, when you have a function of one variables, if you are trying to find the minimum and the maximum of a function whose graph looks like this, well, you are going to tell me, quite obviously, that the maximum is this point up here. Remember, to find the minimum or the maximum of the function f, subject to the constraint g equals constant, well, we write down equations that say that the gradient of f is actually proportional to the gradient of g. There is a new variable here, lambda, the multiplier. Well, the rate of change of x in this situation is partial x, partial z with y held constant. And so this coefficient here is the rate of change of f with respect to z in the situation we are considering. o, at that point, the partial derivative is zero with respect to x. And the same here. One of them is to find the minimum of a maximum of a function when the variables are not independent, and that is the method of Lagrange multipliers. That tells us dx should be minus g sub z dz divided by g sub x. f you want, this is the rate of change of x with respect to z when we keep y constant. Remember, we have defined the partial of f with respect to some variable, say, x to be the rate of change with respect to x when we hold all the other variables constant. How can we find the rate of change of x with respect to z? Now we are asking ourselves what is the rate of change of f with respect to z in this situation? But another reason is that, really, you need partial derivatives to do physics and to understand much of the world that is around you because a lot of things actually are governed by what is called partial differentiation equations. One way we can deal with this is to solve for one of the variables and go back to two independent variables, but we cannot always do that. Out of this you get, well, am tired of writing partial g over partial x. We can just write g sub x times partial x over partial z y constant plus g sub z. And now we found how x depends on z. Partial x over partial z with y held constant is negative g sub z over g sub x. Now we plug that into that and we get our answer. f you take the differential of f and you divide it by dz in this situation where y is held constant and so on, you get exactly this chain rule up there. Well, partial g over partial x times the rate of change of x. How does it change because of y? But, for example, if x, y and z depend on some other variable, say of variables maybe even u and v, then that means that f becomes a function of u and v. And then we can ask ourselves, how sensitive is f to a value of u? And we have also seen that actually that is not enough to find the minimum of a maximum of a function because the minimum of a maximum could occur on the boundary. But then, when we are looking for the minimum of a function, well, it is not at a critical point. t is actually here at the boundary of the domain, you know, the range of values that we are going to consider. And what we do about dx is we use the constant. Well, to do that, we need to look at how the variables are related so we need to look at the constraint g. Well, how do we do that? Lets say that we want to find the partial derivative of f with respect to z keeping y constant. Well, the rate of change of z, with respect to itself, is just one. And then there is the rate of change because z changes. Now, y might change, so the rate of change of y would be the rate of change of y with respect to z holding y constant. Well, why would the value of f change in the first place when f is just a function of x, y, z and not directly of you? But, if we just say that, we are not going to get to that. t means that we assume that the function depends more or less linearly on x, y and z. And, if we set these things equal, what we get is actually, we are replacing the function by its linear approximation. And we know that the normal vector is actually, well, one normal vector is given by the gradient of a function because we know that the gradient is actually pointing perpendicularly to the level sets towards higher values of a function. And z changes as well, and that causes f to change at that rate. Basically, what this quantity means is if we change u and keep v constant, what happens to the value of f? Now, the problem here was also asking you to estimate partial h over partial y. And if you were curious how you would do that, well, you would try to figure out how long it takes before you reach the next level curve. Well, can just look at how g would change with respect to z when y is held constant. Well, in fact, we say we are going to look only at the case where y is constant. And so, in particular, we can use the chain rule to do changes of variables. f we have, say, a function in terms of polar coordinates on theta and we like to switch it to rectangular coordinates x and y then we can use chain rules to relate the partial derivatives. To go from here to here, to go from Q to this new point, say Q prime, the change in y, well, you would have to read the scale, which was down here, would be about something like 300. f you are here, for example, and you move in the x direction, well, you see, as you get to there from the left, the height first increases and then decreases. Anyway, am giving it to you just to show you an example of a real life problem where, in fact, you have to solve one of these things. Basically, what causes f to change is that am changing x, y and z by small amounts and how sensitive f is to each variable is precisely what the partial derivatives measure. o, that is how you would do it. And that causes f to change at that rate. But on the test, havent decided yet, but it could well be that the problem about Lagrange multipliers just asks you to write the equations and not to solve them. Again, saying that g cannot change and keeping y constant tells us g sub x dx plus g sub z dz is zero and we would like to solve for dx in terms of dz.",0.1918111398008115
17,17,"Two videos ago we asked ourselves if we could find the basis for the columns space of A. And showed you a method of how to do it. You literally put A in reduced row echelon form, so this matrix R is just a reduced row echelon form of A. And you look at its pivot columns, so this is a pivot column. t has a 1 and all 0s, this is a pivot column, 1 and all 0s, and the 1 is the leading nonzero term in its row. And this is a pivot column, let me circle them, these guys are pivot columns, and this guys a pivot column right there. You look at those in the reduced row echelon form of the matrix, and the corresponding columns in the original matrix will be your basis. o this guy, this guy, so the first, second, and forth columns. o if we call this a1, this is a2, and lets call this a4, this would be a3, and this is a5. o we could say that a1, a2, and a4 are a basis for the column span of A. And didnt show you why two videos ago. just said this is how you do it. You have to take it as a bit of an article of faith. Now in order for these to be a basis, two things have to be true. They have to be linearly independent, and showed you in the very last video, the second in our series dealing with this vector. showed you that by the fact that this guy is r1, this guy is r2, and this guy is r4, its clear that these guys are linearly independent. They each have a 1 in a unique entry, and the rest of their entries are 0. Were looking at three pivot columns right now, but its true if we had n pivot columns. That each pivot column would have a 1 in a unique place, and all the other pivot columns would have 0 in that entry. o theres no way that the other pivot columns, any linear combination of the other ones, could never add up to each of them. o these are definitely linearly independent. And showed you in the last video that if we know that these are linearly independent, we do know that they are, given that R has the same null space as A, we know that these guys have to be linearly independant, did that in the very last video. Now the next requirement for a basis, we checked this one off, is to show that a1 a2 and an, that their span equals the column space of A. Now the column space of A is a span of all five of these vectors, so had to throw a3 in there and a5. o to show that just these three vectors by themselves span our column space, we just have to show that can represent a3 and a5 as linear combinations of a1, a2, and a4. f can do that, then can say then these guys are redundant. Then the span of a1, a2, a3, a4, and a5 doesnt need the a3 and the a5 terms, that we can just reduce it to this. Because these guys can be represented as linear combinations of the other three. These guys are redundant. And if we can get rid of them we can show that these guys can be represented as linear combinations of the other, then we can get rid of them. And then the span of these three guys would be the same as the span of these five guys, which is of course the definition of the column space of A. o lets see if we can do that. Let me fill in each of these column vectors a1 through a5, and then each of these column vectors let me label them r1, r2, r3, r4, and r5. Now lets explore the null spaces again. Or not even the null spaces, lets just explore the equations Ax is equal to let me write it this way instead of x let me write x1, x2, x3, x4, x5 is equal to 0. This is how we define the solution set of this. All the potential x1s through x5s or all the potential vectors X right here, that represents our null space. And then also lets explore all of the R times x1, x2, x3, x4, x5s is equal to 0. This is the 0 vector, in which case you would have four entries in this particular case. t would be a member of Rm. o these equations can be rewritten. can rewrite this as what were the column vectors of A? They were a1, a2 through a5. o can rewrite this as x1 times a1 plus x2 times a2 plus x3 times a3 plus x4 times a4 plus x5 times a5 is equal to 0. That was from our definition of matrix vector multiplication, this is just a bunch of column vectors a1 through a5, drew it up here. can just rewrite this equation like this. imiliarly, can rewrite this equation as the vector r1 times x1 or x1 times r1 plus x2 times r2 plus x3 times r3 plus x4 times r4 plus x5 times r5 is equal to 0. Now we know that when we put this into reduced row echelon form the x variables that are associated with the pivot columns are so what are the x variables associated with the pivot columns? Well, the pivot columns are r1, r2, and r4. The x variables associated with them, we can call them pivot variables, and the ones that are not associated with our pivot columns are free variables. o the free variables in this case, x3 and x5, are free variables. And that applies to A. All of the vectors x that satisfy this equation also satisfy this equation, and vice versa. Theyre the exact same null space, the exact same solution set. We can also call this x3 and this x5 as free variables. Now what does that mean? Weve done multiple examples of this. The free variables, you can set them to anything you want. o x3 in this case and x5 you can set it to any real number. You can set to anything you want. And then from this reduced row echelon form we express the other pivot variables as functions of these guys. aybe x1 is equal to Ax3 plus Bx5. aybe x2 is equal to x3 plus Dx5. And maybe x4 is equal to Ex3 plus Fx5. That comes directly out of literally multiplying this guy times this equals 0, youd get a system of equations that you could solve for your pivot variables in terms of your free variables. Now given this, want to show you that you can always construct one of your in your original matrix. o if we go to our original matrix, you can always construct one of the vectors that are associated with the free columns. You can always construct one of the free vectors using the linear combination of the ones that were associated with the pivot columns before. And how do do that? Well, lets say that want to find some linear combination that gets me to this free column, that gets me to a3. o how could do that? Let me rearrange this equation up here. o what do get? m sorry. Thats x3 a3. f subtract x3 a3 from both sides of the equation, get minus x3 a3 is equal to x1 a1 plus x2 a2 plus dont have the 3 there plus x4 a4 plus x5 sorry, x isnt a vector x5 a5. This, guess salmon colored statement here, is just another rewriting of this equation right here. And all did is subtracted this term right here, x3 a3, from both sides of the equation. Now x3 is a free variable. We can set it to anything we want, and so is x5. o lets set x3 is equal to minus 1. Then this term right here becomes a 1, because that was a minus x3. And lets set x5 equal to 0. o if x5 is equal 0, this term disappears, and did that because x5 is a free variable. can set them to anything want. Now ve written a3 as a linear combination of, guess you could call it my potential basis vectors right now, or the vectors a1, a2, and a4. Theyre the vectors in the original matrix that were associated with the pivot columns. Now in order to show that can always do this, we have to show that for this combination theres always some x1, x2, and x4 that satisfy this. Well, of course theres always some x1, x2 that satisfy this, we just have to substitute our free variables, x3 is equal to minus 3 and x5 is equal to 0, into these equations that we get from our system when we did it with the reduced row echelon form. n this case you have x1 is equal to minus A plus 0, x2 is equal to minus , so on and so forth. o you can always do that. You can always express the vectors that are associated with the nonpivot columns as linear combinations of the vectors that are associated with the pivot columns. What just did for a3, you could just as easily have done for a5 by subtracting this term from both sides of the equation. etting x5 to negative 1 and setting x3 to 0 so that the 3 term disappears, and you could run the same exact argument. o given that, ve hopefully shown you, or least helped you see or made you comfortable with the idea, that the vectors let me do them in a nice vibrant color these magenta color vectors here that are associated with the free columns or with the free variables, the free variables were x3 and x5, those were these columns right here, that they can always be expressed as linear combinations of the other columns. Because you just have to manipulate this equation, set the coefficient for whatever youre trying to find a linear combination for equal to minus 1, and set all the other free variables equal to 0 that youre not solving for. And then you can get a linear combination of the vectors that are associated with the pivot columns. o given that, weve shown you that these free vectors, and m using my terminology very loosely, that these ones that are associated with the non pivot columns can be expressed as linear combinations of these guys. o theyre unnecessary. The span of this is equivalent to the span of this, the span of this is the column space of A, so the span of this is the column space of A. o in the last video showed you that these guys are linearly independent, and now ve showed you that the span of these guys is the column space of A. o now you should be satisfied that these vectors that are associated let me do it in a blue color that that column vector, this column vector, and this column vector, that are associated with the pivot columns in the reduced row echelon form of the matrix, do indeed represent a basis for the column space of A. Anyway, hopefully you didnt find that too convoluted.","The span of this is equivalent to the span of this, the span of this is the column space of A, so the span of this is the column space of A. o in the last video showed you that these guys are linearly independent, and now ve showed you that the span of these guys is the column space of A. o now you should be satisfied that these vectors that are associated let me do it in a blue color that that column vector, this column vector, and this column vector, that are associated with the pivot columns in the reduced row echelon form of the matrix, do indeed represent a basis for the column space of A. Anyway, hopefully you didnt find that too convoluted. And then you can get a linear combination of the vectors that are associated with the pivot columns. o given that, ve hopefully shown you, or least helped you see or made you comfortable with the idea, that the vectors let me do them in a nice vibrant color these magenta color vectors here that are associated with the free columns or with the free variables, the free variables were x3 and x5, those were these columns right here, that they can always be expressed as linear combinations of the other columns. Now the next requirement for a basis, we checked this one off, is to show that a1 a2 and an, that their span equals the column space of A. Now the column space of A is a span of all five of these vectors, so had to throw a3 in there and a5. Well, of course theres always some x1, x2 that satisfy this, we just have to substitute our free variables, x3 is equal to minus 3 and x5 is equal to 0, into these equations that we get from our system when we did it with the reduced row echelon form. And then the span of these three guys would be the same as the span of these five guys, which is of course the definition of the column space of A. o lets see if we can do that. You can always express the vectors that are associated with the nonpivot columns as linear combinations of the vectors that are associated with the pivot columns. o if we go to our original matrix, you can always construct one of the vectors that are associated with the free columns. You can always construct one of the free vectors using the linear combination of the ones that were associated with the pivot columns before. Now we know that when we put this into reduced row echelon form the x variables that are associated with the pivot columns are so what are the x variables associated with the pivot columns? Now in order to show that can always do this, we have to show that for this combination theres always some x1, x2, and x4 that satisfy this. Then the span of a1, a2, a3, a4, and a5 doesnt need the a3 and the a5 terms, that we can just reduce it to this. o to show that just these three vectors by themselves span our column space, we just have to show that can represent a3 and a5 as linear combinations of a1, a2, and a4. o given that, weve shown you that these free vectors, and m using my terminology very loosely, that these ones that are associated with the non pivot columns can be expressed as linear combinations of these guys. And showed you in the last video that if we know that these are linearly independent, we do know that they are, given that R has the same null space as A, we know that these guys have to be linearly independant, did that in the very last video.",0.3050314465408805
18,18,"Lets say had the set of vectors dont want to do it that thick. Lets say one of the vectors is the vector 2, 3, and then the other vector is the vector 4, 6. And just want to answer the question: what is the span of these vectors? And lets assume that these are position vectors. What are all of the vectors that these two vectors can represent? Well, if you just look at it, and remember, the span is just all of the vectors that can be represented by linear combinations of these. o its the set of all the vectors that if have some constant times 2 times that vector plus some other constant times this vector, its all the possibilities that can represent when just put a bunch of different real numbers for c1 and c2. Now, the first thing you might realize is that, look, this vector 2, this is just the same thing as 2 times this vector. o could just rewrite it like this. could just rewrite it as c1 times the vector 2, 3 plus c2 times the vector and here, instead of writing the vector 4, 6, m going to write 2 times the vector 2, 3, because this vector is just a multiple of that vector. o could write c2 times 2 times 2, 3. think you see that this is equivalent to the 4, 6. 2 times 2 is 4. 2 times 3 is 6. Well, then we can simplify this a little bit. We can rewrite this as just c1 plus 2c2, all of that, times 2, 3, times our vector 2, 3. And this is just some arbitrary constant. ts some arbitrary constant plus 2 times some other arbitrary constant. o we can just call this c3 times my vector 2, 3. o in this situation, even though we started with two vectors, and said, well, you know, the span of these two vectors is equal to all of the vectors that can be constructed with some linear combination of these, any linear combination of these, if just use this substitution right here, can be reduced to just a scalar multiple of my first vector. And could have gone the other way around. could have substituted this vector as being 1/2 times this, and just made any combination of scalar multiple of the second vector. But the fact is, that instead of talking about linear combinations of two vectors, can reduce this to just a scalar combination of one vector. And weve seen in R2 a scalar combination of one vector, especially if theyre position vectors. For example, this vector 2, 3. ts 2, 3. t looks like this. All the scalar combinations of that vector are just going to lie along this line. o 2, 3, its going to be right there. Theyre all just going to lie along that line right there, so along this line going in both directions forever. And if take negative values of 2, 3, m going to go down here. f take positive values, m going to go here. f get really large positive values, its going to go up here. But can just represent the vectors, and when you put them in standard form, their arrows essentially would trace out this line. o you could say that the span of my set of vectors let me put it over here. The span of the set of vectors 2, 3 and 4, 6 is just this line here. Even though we have two vectors, theyre essentially collinear. Theyre multiples of each other. mean, if this is 2, 3, 4, 6 is just this right here. ts just that longer one right there. Theyre collinear. These two things are collinear. Now, in this case, when we have two collinear vectors in R2, essentially their span just reduces to that line. You cant represent some vector like let me do a new color. You cant represent this vector right there with some combination of those two vectors. Theres no way to kind of break out of this line. o theres no way that you can represent everything in R2. o the span is just that line there. Now, a related idea to this, and notice, you had two vectors, but it kind of reduced to one vector when you took its linear combinations. The related idea here is that we call this set we call it linearly dependent. Let me write that down: linearly dependent. This is a linearly dependent set. And linearly dependent just means that one of the vectors in the set can be represented by some combination of the other vectors in the set. A way to think about it is whichever vector you pick that can be represented by the others, its not adding any new directionality or any new information, right? n this case, we already had a vector that went in this direction, and when you throw this 4, 6 on there, youre going in the same direction, just scaled up. o its not giving us any new dimension, letting us break out of this line, right? And you can imagine in three space, if you have one vector that looks like this and another vector that looks like this, two vectors that arent collinear, theyre going to define a kind of twodimensional space. They can define a twodimensional space. Lets say that this is the plane defined by those two vectors. n order to define R3, a third vector in that set cant be coplanar with those two, right? f this third vector is coplanar with these, its not adding any more directionality. o this set of three vectors will also be linearly dependent. And another way to think about it is that these two purple vectors span this plane, span the plane that they define, essentially, right? Anything in this plane going in any direction can be any vector in this plane, when we say span it, that means that any vector can be represented by a linear combination of this vector and this vector, which means that if this vector is on that plane, it can be represented as a linear combination of that vector and that vector. o this green vector added isnt going to add anything to the span of our set of vectors and thats because this is a linearly dependent set. This one can be represented by a sum of that one and that one because this one and this one span this plane. n order for the span of these three vectors to kind of get more dimensionality or start representing R3, the third vector will have to break out of that plane. t would have to break out of that plane. And if a vector is breaking out of that plane, that means its a vector that cant be represented anywhere on that plane, so its outside of the span of those two vectors. Where its outside, it cant be represented by a linear combination of this one and this one. o if you had a vector of this one, this one, and this one, and just those three, none of these other things that drew, that would be linearly independent. Let me draw a couple more examples for you. That one might have been a little too abstract. o, for example, if have the vectors 2, 3 and have the vector 7, 2, and have the vector 9, 5, and were to ask you, are these linearly dependent or independent? o at first you say, well, you know, its not trivial. Lets see, this isnt a scalar multiple of that. That doesnt look like a scalar multiple of either of the other two. aybe theyre linearly independent. But then, if you kind of inspect them, you kind of see that v, if we call this v1, vector 1, plus vector 2, if we call this vector 2, is equal to vector 3. o vector 3 is a linear combination of these other two vectors. o this is a linearly dependent set. And if we were to show it, draw it in kind of two space, and its just a general idea that well, let me see. Let me draw it in R2. Theres a general idea that if you have three twodimensional vectors, one of them is going to be redundant. Well, one of them definitely will be redundant. For example, if we do 2, 3, if we do the vector 2, 3, thats the first one right there. draw it in the standard position. And draw the vector 7, 2 right there, could show you that any point in R2 can be represented by some linear combination of these two vectors. We can even do a kind of a graphical representation. ve done that in the previous video, so could write that the span of v1 and v2 is equal to R2. That means that every vector, every position here can be represented by some linear combination of these two guys. Now, the vector 9, 5, it is in R2. t is in R2, right? learly. just graphed it on this plane. ts in our twodimensional, real number space. Or guess we could call it a space or in our set R2. ts there. ts right there. o we just said that anything in R2 can be represented by a linear combination of those two guys. o clearly, this is in R2, so it can be represented as a linear combination. o hopefully, youre starting to see the relationship between span and linear independence or linear dependence. Let me do another example. Lets say have the vectors let me do a new color. Lets say have the vector and this one will be a little bit obvious 7, 0, so thats my v1, and then have my second vector, which is 0, minus 1. Thats v2. Now, is this set linearly independent? s it linearly independent? Well, can represent either of these as a combination of the other? And really when say as a combination, youd have to scale up one to get the other, because theres only two vectors here. f am trying to add up to this vector, the only thing have to deal with is this one, so all can do is scale it up. Well, theres nothing can do. No matter what multiply this vector by, you know, some constant and add it to itself or scale it up, this term right here is always going to be zero. ts always going to be zero. o nothing can multiply this by is going to get me to this vector. Likewise, no matter what multiply this vector by, the top term is always going to be zero. o theres no way could get to this vector. o both of these vectors, theres no way that you can represent one as a combination of the other. o these two are linearly independent. And you can even see it if we graph it. One is 7, 0, which is like that. Let me do it in a nonyellow color. 7, 0. And one is 0, minus 1. And think you can clearly see that if you take a linear combination of any of these two, you can represent anything in R2. o the span of these, just to kind of get used to our notion of span of v1 and v2, is equal to R2. Now, this is another interesting point to make. said the span of v1 and v2 is R2. Now what is the span of v1, v2, and v3 in this example up here? already told you. already showed you that this third vector can be represented as a linear combination of these two. ts actually just these two summed up. can even draw it right here. ts just those two vectors summed up. o it clearly can be represented as a linear combination of those two. o whats its span? Well, the fact that this is redundant means that it doesnt change its span. t doesnt change all of the possible linear combinations. o its span is also going to be R2. ts just that this was more vectors than you needed to span R2. R2 is a twodimensional space, and you needed two vectors. o this was kind of a more efficient way of providing a basis, and havent defined basis formally, yet, but just want to use it a little conversationally, and then itll make sense to you when define it formally. This provides a better basis, or this provides a basis, kind of a nonredundant set of vectors that can represent R2. While this one, right here, is redundant. o its not a good basis for R2. Let me give you one more example in three dimensions. And then in the next video, m going to make a more formal definition of linear dependence or independence. o lets say that had the vector 2, 0, 0. Let me make a similar argument that made up there: the vector 2, 0, 0, the vector 0, 1, 0, and the vector 0, 0, 7. We are now in R3, right? Each of these are threedimensional vectors. Now, are these linear dependent or linearly independent? orry, are they linearly dependent or independent? Well, theres no way with some combination of these two vectors that can end up with a nonzero term right here to make this third vector, right? Because no matter what multiply this one by and this one by, this last term is going to be zero. o this is kind of adding a new direction to our set of vectors. Likewise, theres nothing can do theres no combination of this guy and this guy that can get a nonzero term here. And finally, no combination of this guy and this guy that can get a nonzero term here. o this set is linearly independent. And if you were to graph these in three dimensions, you would see that none of these these three do not lie on the same plane. Obviously, any two of them lie on the same plane, but if you were to actually graph it, you get 2, 0. Let me say that thats xaxis. Thats 2, 0, 0. Then you have this, 0, 1, 0. aybe thats the yaxis. And then you have 0, 0, 7. t would look something like this. o it almost looks like, your threedimensional axes, it almost looks like the vectors i, j, k. Theyre just scaled up a little bit. But you can always correct that by just scaling them down, right? Because we care about any linear combination of these. o the span of these three vectors right here, because theyre all adding new directionality, is R3. Anyway, thought would leave you there in this video. realize ve been making longer and longer videos, and want to get back in the habit of making shorter ones. n the next video, m going to make a more formal definition of linear dependence, and well do a bunch more examples.","o we can just call this c3 times my vector 2, 3. o in this situation, even though we started with two vectors, and said, well, you know, the span of these two vectors is equal to all of the vectors that can be constructed with some linear combination of these, any linear combination of these, if just use this substitution right here, can be reduced to just a scalar multiple of my first vector. Anything in this plane going in any direction can be any vector in this plane, when we say span it, that means that any vector can be represented by a linear combination of this vector and this vector, which means that if this vector is on that plane, it can be represented as a linear combination of that vector and that vector. And linearly dependent just means that one of the vectors in the set can be represented by some combination of the other vectors in the set. The span of the set of vectors 2, 3 and 4, 6 is just this line here. And draw the vector 7, 2 right there, could show you that any point in R2 can be represented by some linear combination of these two vectors. Well, if you just look at it, and remember, the span is just all of the vectors that can be represented by linear combinations of these. o if you had a vector of this one, this one, and this one, and just those three, none of these other things that drew, that would be linearly independent. But then, if you kind of inspect them, you kind of see that v, if we call this v1, vector 1, plus vector 2, if we call this vector 2, is equal to vector 3. o vector 3 is a linear combination of these other two vectors. Lets say one of the vectors is the vector 2, 3, and then the other vector is the vector 4, 6. And you can imagine in three space, if you have one vector that looks like this and another vector that looks like this, two vectors that arent collinear, theyre going to define a kind of twodimensional space. could just rewrite it as c1 times the vector 2, 3 plus c2 times the vector and here, instead of writing the vector 4, 6, m going to write 2 times the vector 2, 3, because this vector is just a multiple of that vector. o this green vector added isnt going to add anything to the span of our set of vectors and thats because this is a linearly dependent set. But the fact is, that instead of talking about linear combinations of two vectors, can reduce this to just a scalar combination of one vector. And if a vector is breaking out of that plane, that means its a vector that cant be represented anywhere on that plane, so its outside of the span of those two vectors. o, for example, if have the vectors 2, 3 and have the vector 7, 2, and have the vector 9, 5, and were to ask you, are these linearly dependent or independent? o both of these vectors, theres no way that you can represent one as a combination of the other. And think you can clearly see that if you take a linear combination of any of these two, you can represent anything in R2. o its the set of all the vectors that if have some constant times 2 times that vector plus some other constant times this vector, its all the possibilities that can represent when just put a bunch of different real numbers for c1 and c2. o the span of these, just to kind of get used to our notion of span of v1 and v2, is equal to R2. And if we were to show it, draw it in kind of two space, and its just a general idea that well, let me see.",0.2690824980724749
19,19,"BEN HARR: Hi, and welcome back. Today were going to do a problem about the four fundamental subspaces. o here we have a matrix B. B is written as the product of a lower triangular matrix and an upper triangular matrix. And were going to find a basis for, and compute the dimension of, each of the four fundamental subspaces of B. ll give you a minute to try that on your own, to hit pause, and then ll be right back in just a minute and we can do it together. OK. Were back. Now, the first thing to notice is that this is an L decomposition of B. o we have L here and here. Now lets go one space at a time. Lets start with the column space. And first, lets just say what the dimension of the column space is. OK, so lets look at our matrix. How many pivots do we have? We have two pivots, so the column space has dimension 2. This is the number of pivots. Good. Now, how do we find a basis for the column space? How do we find that basis? Well, in lecture, Professor trang had started with a matrix B. He did elimination on it, and then he took the pivot columns in the original matrix. And thats great. That works. You can also take the pivot columns in the L matrix. You can see by multiplying this out that it will amount to essentially the same thing. o a basis for this column space, can just take these two pivot columns of my L matrix, . Good. OK. o theres the dimension of and the basis for the column space of B. Next, lets do the null space together. OK. Whats the dimension of the null space? Well, the dimension of the null space is always the number of columns minus the number of pivots. Right? ts the number of free variables. o here, thats just one. Good. And how do we find this one vector in the null space? Well, what we do is we can just plug in 1 for our free variable, and we can backsolve to get the other two. o this equation tells me that my second number is 1, and this equation tells me that that third variable 3/5, if can fit it in here. Thats a 3/5, if its difficult to see that on the tape. Now, lets move on, next, to the row space. Next is the row space. o how do we find the dimension of the row space? m going to write row space as column space of B transpose. How do we find that? Well, remember that one of our big facts in this class is that the dimension of the row space is the same as the dimension of the column space. ts just the number of pivots. o thats good. ts 2. And how do we find a basis for the row space? There are a couple ways of thinking about this. One way to think about it is: we got this upper triangular matrix from B by doing elimination. And elimination doesnt change the row space. o can just use the two pivot rows of the matrix . Basis for my row space here is: just put these two pivot rows together, and get a basis for this row space. The last one is always the toughest and the trickiest. We have to do the left null space or the null space of B transpose. First, lets compute its dimension. Whats the dimension of this left null space? Well, theres a similar formula to the one we used when we were computing the dimension of the null space. ts just the number of rows minus the number of pivots. o there are three rows. Our matrix is 3 by 3. And there are two pivots. o this is just onedimensional, again. We need to compute, now, this left null space. Let me go back to our original matrix. The way to do this is to take B equals L*, and invert L, and get E*B equals . o we need to move L over to the lefthand side. f we do that m just going to write that down here. o whats the inverse of the L matrix? We just get 1, 2, 1, 0, 1, 1, times B, is our matrix, this upper triangular matrix. Now that moved L over to the other side, can read off the vectors in my left null space. Now, m looking at not my pivot variables but my free variables, because its some sort of null space. but want to look at this E matrix. o the third row of this E matrix, the third row corresponds to the three row here and when multiply this by B, just get zeros, so this is in the left null space. A basis for this left null space is see if can fit it here just this . Good. o weve found the dimension of and basis for all of the four fundamental subspaces. Before move on, just want to recall which of the L matrix or the matrix we used for each of these subspaces. o for the column space, we used the pivot columns of the L matrix. For the null space, we looked at the matrix. For the row space, we also looked at the matrix. And for the left null space, we needed to invert the L matrix and look at the free row. Were done with the problem. But the last thing thats useful is to draw a picture, which have right here. know in lecture Professor trang has drawn you some sort of cartoon pictures of what these subspaces look like. But here want to try to actually draw them in a special case. o if you can read my drawing here, what do we have? We have the row space here, and the null space here. Right? And so B maps this picture into this picture. The null space here all the scalar multiples of this vector all go to 0, because theyre in the null space. Thats exactly what B takes to 0. B takes everything else, including the row space, into this column space. And what does B transpose do? Well, B transpose kills this left null space, kills this vector, and it take everything else into the row space, into the column space of B transpose. OK. Thanks for doing this exercise together. hope this picture is helpful.","o theres the dimension of and the basis for the column space of B. Next, lets do the null space together. o how do we find the dimension of the row space? Well, remember that one of our big facts in this class is that the dimension of the row space is the same as the dimension of the column space. And for the left null space, we needed to invert the L matrix and look at the free row. We have the row space here, and the null space here. o for the column space, we used the pivot columns of the L matrix. o the third row of this E matrix, the third row corresponds to the three row here and when multiply this by B, just get zeros, so this is in the left null space. We have to do the left null space or the null space of B transpose. And how do we find a basis for the row space? Well, the dimension of the null space is always the number of columns minus the number of pivots. And how do we find this one vector in the null space? For the null space, we looked at the matrix. And first, lets just say what the dimension of the column space is. Whats the dimension of the null space? And were going to find a basis for, and compute the dimension of, each of the four fundamental subspaces of B. ll give you a minute to try that on your own, to hit pause, and then ll be right back in just a minute and we can do it together. Now, how do we find a basis for the column space? Whats the dimension of this left null space? For the row space, we also looked at the matrix. This is the number of pivots. Well, theres a similar formula to the one we used when we were computing the dimension of the null space. Now, the first thing to notice is that this is an L decomposition of B. o we have L here and here.",0.3545454545454545
20,20,"o now that weve spent some time thinking about what a differential equation is and even visualizing solutions to a differential equations using things like slope field, lets start seeing if we can actually solve differential equations. As well see, different types of differential equations might require different techniques and some of them we might not be able to solve at all using analytic techniques. Wed have to resort to numeric techniques to estimate the solutions. But lets go to what would argue as the simplest form of differential equation to solve and thats whats called a eparable. eparable differential equation. And we will see in a second why it is called a separable differential equation. o lets say that we have the derivative of Y with respect to X is equal to negative X over Y E to the X squared. o we have this differential equation and we want to find the particular solution that goes through the point 0,1. encourage you to pause this and ll give you a hint. f you can on one side of this equation through algebra separate out the Ys and the DYs and on the other side have all the Xs and DXs, and then integrate. Perhaps you can find the particular solution to this differential equation that contains this point. Now if you cant do it dont worry because were about to work through it. o like said, lets use a little bit of algebra to get all the Ys and DYs on one side and all the Xs and DXs on the other side. o one way, lets say want to get all the Ys and DYs on the left hand side, and all the Xs and DXs on the right hand side. Well, can multiply both sides times Y. o can multiply both sides times Y that has the effect of putting the Ys on the left hand side and then can multiple both sides times DX. can multiple both sides times DX and we kind of treat, you can treat these differentials as you would treat a variable when youre manipulating it to essentially separate out the variables. And so, this will cancel with that. And so, we are left with Y, DY. Y, DY is equal to negative X. And actually let me write it this way. Let me write it as negative X, E. Actually, might need a little more space. o negative X E to the negative X squared DX. DX. Now why is this interesting? Because we could integrate both sides. And now this also highlights why we call it the separable. You wont be able to do this with every differential equation. You wont be able to algebraically separate the Ys and DYs on one side and the Xs and DXs on the other side. But this one we were able to. And so thats why this is called a separable differential equation. Differential equation. And its usually the first technique that you should try. Hey, can separate the Ys and the Xs and as said, this is not going to be true of many, if not most differential equations. But now that we did this we can integrate both sides. o lets do that. o, ll find a nice color to integrate with. o, m going to integrate both sides. Now if you integrate the left hand side what do you get? You get and remember, were integrating with respect to Y here. o this is going to be Y squared over two and we could put some constant there. could call that plus one. And if youre integrating that thats going to be equal to. Now the right hand side were integrating with respect to X. And lets see, you could do substitution or you could recognize that look, the derivative of negative X squared is going to be negative two X. o if that was a two there and if you dont want to change the value of the integral you put the 1/2 right over there. And so now you could either do substitution explicitly or you could do it in your head where you said is equal to negative X squared and then D will be negative to X, DX or you can kind of do this in your head at this point. o have something and its derivative so really could just integrate with respect to that something too with respect to that . o this is going to be 1/2. This 1/2 right over here. The antiderivative. This is E to the negative X squared and then of course, might have some other constant. ll just call that two. And once again, if this part over here what just did seemed strange, the substitution, you might want to review that piece. Now, what can do here? Well have a constant on the left hand side. ts an arbitrary constant. We dont know what it is. havent used this initial condition yet we could call it. o, let me just subtract one from both sides. o if just subtract one from both sides have an arbitrary so this is gonna cancel, and have two, sorry. Let me. o, this is one. o these are going to cancel and two minus one. These are both constants, arbitrary constants and we dont know what they are yet. And so, we could just rewrite this as on the left hand side we have Y squared over two is equal to on the right hand side. ll write 1/2 E. Let me write that in blue just because wrote it in blue before. 1/2 E to the negative X squared and ll just say two minus one. Lets just call that . o if you take the sum of those two things lets just call that . And so now, this is kind of a general solution. We dont know what this constant is and we havent explicitly solved for Y yet but even in this form we can now find a particular solution using this initial condition. Let me separate it out. This was a part of this original expression right over here but using this initial condition. o, it tells us when X is zero, Y needs to be equal to one. o we would have one squared which is just one over two is equal to 1/2. E to the negative zero squared. Well, thats just going to be e to the zero is just one. This is gonna be 1/2 plus and just like that were able to figure out if you subtract 1/2 from both sides is equal to zero. o the relationship between Y and X that goes through this point, we could just set is equal to zero. o thats equal to zero. Thats zero right over there. And so we are left with Y squared over two is equal to E to the negative X squared over two. Now we can multiply both sides by two and were going to get Y squared. Y squared. Let me do that. o were gonna get Y squared is equal to E to the negative X squared. Now, we can take the square root of both sides and you can say, well look, you know, Y squared is equal to this so Y could be equal to the plus or minus square root of E to the negative X squared. Of E to the negative X squared. But they gave us an initial condition where Y is actually positive. o were finding the particular solution that goes through this point. That means Y is gonna be the positive square root. f this was a point zero negative one then we would say Y is the negative square root but we know that Y is the positive square root, its the principal root right over there. o let me do that a little bit neater so we can get rid of, whoops. thought was writing in black. o we can get rid of this right over here. Were only going to be dealing with the positive square root so we could write Y is equal to E to the negative X squared to the 1/2 power and that of course is equal to E to the negative X squared over two. o this right over here is or Y equals E to the negative X squared over two is a particular solution that satisfies the initial conditions to this original differential equation. o just like that. Because we were able to just as a review, because this differential equation was setup in a way or because we could algebraically separate the Y, DYs from the Xs, DXs, were able to just separate them out algebraically, integrate both sides and use the information given in the initial condition to find the particular solution.","Were only going to be dealing with the positive square root so we could write Y is equal to E to the negative X squared to the 1/2 power and that of course is equal to E to the negative X squared over two. Now the right hand side were integrating with respect to X. And lets see, you could do substitution or you could recognize that look, the derivative of negative X squared is going to be negative two X. o if that was a two there and if you dont want to change the value of the integral you put the 1/2 right over there. o lets say that we have the derivative of Y with respect to X is equal to negative X over Y E to the X squared. Now, we can take the square root of both sides and you can say, well look, you know, Y squared is equal to this so Y could be equal to the plus or minus square root of E to the negative X squared. And so we are left with Y squared over two is equal to E to the negative X squared over two. o this right over here is or Y equals E to the negative X squared over two is a particular solution that satisfies the initial conditions to this original differential equation. And so, we could just rewrite this as on the left hand side we have Y squared over two is equal to on the right hand side. This is gonna be 1/2 plus and just like that were able to figure out if you subtract 1/2 from both sides is equal to zero. o we have this differential equation and we want to find the particular solution that goes through the point 0,1. encourage you to pause this and ll give you a hint. o were gonna get Y squared is equal to E to the negative X squared. Because we were able to just as a review, because this differential equation was setup in a way or because we could algebraically separate the Y, DYs from the Xs, DXs, were able to just separate them out algebraically, integrate both sides and use the information given in the initial condition to find the particular solution. f you can on one side of this equation through algebra separate out the Ys and the DYs and on the other side have all the Xs and DXs, and then integrate. Well, thats just going to be e to the zero is just one. Of E to the negative X squared. o this is going to be Y squared over two and we could put some constant there. 1/2 E to the negative X squared and ll just say two minus one. o we would have one squared which is just one over two is equal to 1/2. And so now you could either do substitution explicitly or you could do it in your head where you said is equal to negative X squared and then D will be negative to X, DX or you can kind of do this in your head at this point.",0.3622994652406417
21,21,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: o today were going to continue our course on the graph theory. ts going to be a mixture of all kinds of topics. We first start off with Euler tour, and then we get into directed graphs and cover the definitions. And well talk about a special type, which are called tournament graphs. And well do a whole bunch of proofs, and hopefully you will all contribute to make this work and think about how to do this. o next week we will continue with graph theory, and we will discuss a very special type. We will use directed graphs in communication networks. And on Thursday, well actually use these special types of graphs that well talk about in a moment, DAGs. o lets talk about Euler tour. Euler, a famous mathematician, he asked the question like he lived in Konigsberg. And there were seven bridges. He was wondering, can you traverse all the bridges exactly once. o he would start walking and try to cross all those bridges. And apparently this little islands in sort of the river as well, and so on. o how can you do that? This is actually the birth of graph theory. And this particular problem is named after him. o what is an Euler tour? ts actually defined as a special walk. ts a walk that traverses every edge exactly once. o it turns out the you can actually characterize these types of graphs. o we are talking here about undirected graph, so continuation of last time. o the edges that we consider have no direction. Well come back to that in a moment. We will start defining those later. o Euler tour is a walk that traverses every edge exactly once. And at the same time, its also a tour. o that means that it actually starts and finishes at the same vertex. Now it turns out that undirected graphs, the graphs that weve been talking about, those that Euler tours can be easily characterized. And thats the theorem that were going to prove next. The theorem states that actually, if a connected graph has an Euler tour, if and only if, every vertex of the graph has even degree. o thats a very nice and simple and straightforward characterization. o how can we prove this? o lets have a look. o first of all, we have an if and only if statement. o we need to prove two directions. We need to proof that if have a connected graph that has an Euler tour, need to show that every vertex has even degree. And also the reverse if have a graph for which every vertex has even degree, need to show that it has an Euler tour. o the proof consists of two parts. o lets first do this implication, where we assure that we have connected graph that has an Euler tour. o assume we have a graph. o we have G with the vertex at V, edge is at E, and it has an Euler tour. o what does it mean? Well, its really means that we can walk from some start vertex, V0. We can go all the way around to V1, and some more, and so on, to V, say, k minus 1. And then end vertex is the same as the start vertex. o we have a walk that goes all around the whole graph. o every edge in this walk are exactly the edges that are in the graph. And each edge in the graph is offered exactly once. o what does it mean? o lets write this down actually. o since every edge is traversed once every edge in E is traversed once what can we conclude from that? an we say something about, given such as walk of length k, what can we say about the number of edges, for example? What can we say about the degree of the vertices? Because thats what you want to show, right? You want to show that every vertex has even degree. Does anyone know how to go ahead here? o we know that every single edge in E is referred ones within this whole walk. o what kind of properties can we derive? ADENE: PROFEOR: aybe someone else can pick up here. o every vertex that have here, will enter it. But also leave it. o if see a vertex in here, can see at least two contributing edges that are coming from this. ADENE: Then you know that that note has at least degree 2, but it cant be more than 2 because otherwise that would be the endpoint. PROFEOR: Ah. But is it true though that so for example, this particular note here may repeat itself somewhere else. only have a condition on the Euler tour that every edge occurs exactly ones. o how could formally sort of continue to prove? ADENE: Well, it will intersect itself again, and youll leave again and it will still have the even number. PROFEOR: Yeah. Thats true. Thats right. Any other additions? ADENE: You can define the number of edges that it will test by how many you enter it, so the number of edges that it has is twice the number of traverse once, then every time you enter a node you have to leave it. o you can say that youre never going to leave a node for a node that it already left for from there. And youre never going to enter from a node you already entered from or left to from there. o thats how you can say that youre only going to increment your degree by 2. PROFEOR: Thats true. o what hes essentially saying is that every edge as start to count here, in this particular tour, well theyre all different. o they all counts towards a degree of the node. o everything that you have been saying is right on. What we can conclude here is that if you look at a vertex, say vertex for example, somewhere over here well, it may repeat itself over here. And it has an incoming and outgoing edge, incoming and outgoing edge. And may have it somewhere else, but say its just at those two. Well, know that all the edges are different. o can clearly count this now. can say that the degree of must be equal to the number of times that actually appears in this tour. o in the tour from V0 all the way to Vk minus 1. And then have to multiply times 2. And what said, its exactly what you have been saying, all the edges occur exactly once of the whole graph. o just count the number of times see in here. come the number of edges that go into it and leave from it. Well, thats twice times the number of times that actually appears in this tour. o this implication is pretty straightforward because now we actually note that the degree of is even. o thats great. o lets talk about the other implication and see whether we can find a trick to make that happen. o what do we need to start off with? Well, now we have to assume that every vertex has even degree. o lets write this down. o say we have the graph. And for this graph we actually issue that the degree of every vertex V is even. Well, what can we do? Well, this is sort of creative trick, so let me just continue here. o what we have is if you start to consider a walk, W, that touches, say, a lot of vertices in such a way that W is actually the longest walk. o thats often what you do in graph theory. You think about, say, the shortest path or whatever with a certain property, or the longest walk, or whatever with a certain property. o thats what youre doing here. o let W be the longest walk. And well, we want to prove something about an Euler tour. An Euler tour has a very specific property that all the edges occur only once. o it makes sense to look at walks in which all the edges of which it walks are unique, are occurring only once. o were interested in the longest walk that traverses no edge more than once. o thats important. Now, lets think about this a little bit. o the first property that you may want to consider it you really want to show that W is going to be an Euler tour. Thats what were going to try to prove here. o the first thing we may want to show is that actually it goes all around. o V0 is equal to Vk. That will be great if we can do this. o we want to show that this is true. o what would happen if this would not be the case? o suppose these two would not be equal to one another. Well, actually, m skipping a little bit ahead here in the proof, notice. o this will be the conclusion of another observation. o let me start with the first observation. o first of all, let us consider this end node over here, this end vertex, Vk. Now if there is an edge that is not covered in this walk and can lengthen the walk, right? o if have, say, another edge, which looks like Vk, an edge from Vk to some , where this edge is not in the walk. Or the other one is not in the walk. Well, it doesnt matter in an indirect graph, of course. o if this is not in the walk, then what can do? can just lengthen it, right? can just say V0 goes to V1 goes to Vk goes to . Now have a longer walk. And thats a contradiction. o know that all the edges in which Vk is participating are actually covered by the walk. o thats a great property. o lets write this down. o all edges that are incident to Vk are actually used traversed in the walk, W. o now lets have a look whether we can show that its a walk that goes all the way around. o lets try to prove this statement. o let me repeat this over here. o what we want to show is that Vk equals V0. o given this statement, do you have any idea how we could possibly prove this, also given what we have been doing over here? uppose this would not be the case, right? o suppose the start and the end node in this walk are not equal to one another. o what can we do here? o maybe theres some suggestions? already had you once. aybe someone else? ADENE: t has to attach to another vertex. And you know that all the degrees are even. o it means that Vk must attach to V0. PROFEOR: o what youre saying, first of all, is that if this would not be the case, then you know that the degrees must be even. But suppose this would not be the case. That would mean actually that the degree would be odd, right? OK. o lets think about that a little bit. o lets write this down. o otherwise the Vk has odd degree. But the only node has odd degree in this particular walk, W. o thats what we do now. Because we really consider this walk. o we see that if have V0 unequal to Vk and Vk maybe repeated in this walk a number of times, each time it is repeated it has an incoming and an outgoing edge. And all these edges are different because they do not occur more than once. o whenever Vk enters in this middle part over here its partaking over here it gives an even contribution to the number of edges in this walk, and it has one extra incoming edge over here. o if V0 is not equal to Vk, we will have an odd degree of edges in W, in which Vk is participating. o now we can go ahead and we can have a look over here because we just showed here that all the edges incident to Vk are actually used in W. o we can now conclude that this means that Vk also has odd degree in G because all the edges in G, in which Vk is participating, are actually used in the walk. o hope that is clear. o lets write this down. o well use the first statement over here, so by 1. We have that Vk has odd degree in G. And now we come to what you were saying. We have assumed over here that the degree is even. o this cannot be the case, so thats a contradiction. o we know that the otherwise situation cannot occur. o we know that Vk must be equal to V0. o thats proving that we actually have a walk that goes all around. Now, we are not yet done. Because why would this be an Euler tour? For an Euler tour, we really want that every single edge that G has is actually used in the tour. o we need much more. o lets use this board. o suppose that W is not an Euler tour. Were going to use contradiction. But we already have shown this particular property, so we can use this. o suppose W is not an Euler tour. Well, we know that G is connected. o thats a good thing because that means that if it is not an Euler tour, there is an edge that is not used in W, in the walk, but it is incident to some vertex that is used in W. o this is where the connectivity here comes in play. Now let us call this edge so let Vi be this particular edge. o is not used in the walk well, its maybe used in the walk, but the edge is not used in the walk. And Vi is part of the walk. o what can we do here? o does anybody see what we could do here? an we somehow get a contradiction? o when we started out here, we actually assumed that W is the longest walk in which the edges do not occur more than once. an we create a longer walk by simply using this? Now, notice that we did prove that V0 goes all around to Vk, and so on. o can we find a longer walk that uses this extra edge? Any ideas? What would happen if we, for example, started a walk with ? Lets see how that would work out. We can have . We walk to Vi. Well, lets just simply start walking all around. o essentially we have Vi over here and over here. o we could do is we can start walking like this and then end over here. And notice we have used one extra edge. o we get a longer walk. o we go to Vi plus 1, and we go all the way up to Vk, which is equal to V0. And then we go up to V1, all the way up to Vi. This is a longer walk. And therefore we have a contradiction. And now the proof is finished because that means that this assumption was wrong. o W actually is an Euler tour. o now we have shown the existence of an Euler tour, and that was the other direction of the theorem. o lets now start with a new topic on directed graphs. And later on, we will talk about how Hamiltonian paths. And these are different from an Euler tour in that in an Euler tour, every edge is used exactly once. n a Hamiltonian path, we will have that every vertex is used exactly once. o well do that a bit later. o what are directed graphs? Directed graphs are graphs where we have edges that have a specific direction. o we can walk from one vertex to the other. Let me just depict an example. For example, we could have something that looks like this. We have V1. We have V2, V3. We may have an edge that goes like this with an arrow. We use arrows in this case. can go to V2 and V3. From V2 may be able to go to V3, and from V3 to V2. o this will be a directed graph, as an example. We also call these digraphs. And as you can see, we may have an edge that goes from V2 to V3 and also one that goes from V3 to V2. o these are two separate edges. o every edge has a direction. And we usually say that if we have an edge that points from V2 to, say, V3, then we call this the tail and over here we have the head. o this is some notation that you may use. We also have now a different notion for the degree of a vertex because we have different types of edges, essentially. Take, for example, V2. You have incoming edges and outgoing edges. o thats why were going to talk about an indegree and an outdegree. o, for example, the indegree of V2 is equal to, well, have one, two incoming edges. And the outdegree of V2 is actually something different. ts just one outgoing edge. o this is equal to 1. o this is some notation. o lets talk about walks. Lets figure out where we can enumerate walks in a directed graph and compute those. o how many walks do have, say, of length k that go exactly from, say, V1 to V3. How can compute this? o d like to introduce adjacency matrices. Youve probably seen them before. But lets go over this once more because induction is also important. o lets do the theorem. o the theorem is this. uppose we have a graph. And suppose it has end nodes. And we have the vertices V1 up to Vn. And now we let the matrix that contains the entries aij denote the adjacency matrix for G. And what does it mean? ts actually means that in this case, we say that aij is equal to 1 if we actually have an edge that goes from Vi to Vj so this is an edge and 0 if this is not the case. o this is the adjacency matrix. And now we can state something about the number of directed walks in a directed graph. o it turns out that this is easily computed by taking powers of the adjacency matrix. Let aij with the superscript k be equal to the number of directed walks of length k that go from Vi to Vj. o you want to compute this and it turns out that wait a minute. o what did do here? o actually let p kij be the number of directed walks of length k from phi i to phi j. Then what you can show is if you look at matrix A to the power k, this actually is the matrix that contains all these numbers. o let me give a few examples. o lets take the matrix over here. Look at this ajc matrix, take a few powers and see what happens. o the matrix looks like this. f you just label the rows by, say, V1, V2, and V3. The columns by V1, V2, and V3. Well, V1 has an edge to V1. V1 has an edge to V2 and also one to V3. V2 has only one edge to V3, so we have zeros here. And we have this. o this is the matrix A. For example, if you compute a squared. o how do we do this? How many of you know about matrix multiplication? Everybody knows? Well anyway, so lets assume that you do know actually. o thats pretty simple. o you take the column. You take the inners product with the first row, and so on. And you can easily compute this otherwise you may want to do it yourself later on. And you get this particular matrix. And, for example, a to the power of 3 is something very similar 1, 3, 3, 0, 0, 1, 0, 1, 0. Well, it turns out that if you look at this graph and we, for example, want to know the number of walks of length 3 that go from V1 to, say, V2, lets see whether we can see those over here. Well, if travel from V1 to V2 in three steps, can go like this. can go one, two, three. o thats one option. can go one, and another one time, so its a second step, and three. can also go directly over here and go back here, and back over here. o it turns out thats can compute this. o how do we prove this? Well use induction. And the steps are pretty straightforward. o lets first define, because we want to prove that kth power of a is equal to that matrix up there, where all the entries represent the number of walks. o lets say that aijk, that this denote the i jth entry in a to the power k. And you want to show that this number is actually equal to pijk for all the entries. o if youre going to use induction, it makes sense to assume that the theorem is true for k. o the induction hypothesis could be something like this. The theorem is true for k. And this is really the same as stating that all these entries for all i and j aij is equal to the pijk. o this is what you want to prove. And this is pretty straightforward because now what we can do is we can start to look at how we compute walks of length k, or k plus 1. o lets first do the base case. Thats how we always start. The base case is k equals 1. And we have essentially two options. o we want to prove this, so take an i and a j, whatever you want. o suppose we has an edge, Vi, that goes to Vj. Well in that case, we know that the number of walks of length 1, from i to j, is exactly 1 because theres this edge. o this is equal to 1. But this is also the definition of my adjacency matrix. o can just write out here aij 1. o thats great. This case definitely works. Now, if there is no edge of this type, then you know that in one step, we can never achieve Vj from Vi because theres no edge. o we know this is equal to 0. Theres no such walk. And this is, by definition of the adjacency matrix, also equal to the aij. o this works. o the base case its easy. And induction step always starts by issuing pk. As you can see, these types of proofs always have the same structure. n this case, let me again assume pk. We want to prove pk plus 1. o what you want to know is whats the number of walks of length k plus 1. o how can we express those? pij k plus 1. How can we use this assumption over here? Do you have an idea so we can ADENE: Any walk of length k plus 1 can be got by taking one of lets say from Vs to Vx. You go from Vs to V in k steps and then V to Vf in one step. PROFEOR: Thats true. We could do that. o lets write it down. o what youre essentially saying is that you can enumerate all the walks by going of length k plus 1 by first going from i in, say, k steps, to whatever V, and then in one step to, say, Vi to V in k steps and then in one step to Vj. o lets write this down because V can be anything. o what to do is we have a sum over all the Vs such that well, we will use indices here let me do that a little bit differently. o lets say have an h over here. o all the indices h such that Vh to Vj is actually an edge in G. o then we can write out here that we go in k steps to Vh. o how many walks are there? Well, we can use the induction hypothesis now, right? o we can say you go from i to h in k steps. And then, well, we can use this particular edge to complete it to a k plus 1th walk from i to j. o now what is this equal to? an we simplify this sum? We can, right, because we know that theres an edge if and only if the adjacency matrix has a 1 in a particular position. o we could rewrite it and sum over all h from 1 to n. And write pij k times, and then aj oh, this should be an h. o we have the same over here. And then we have one edge from h to 2j. o only count this number over here if this is equal to 1, and that happens exactly if theres an edge. do not count this if theres a 0 over here, that is as if theres no edge. But now we can use induction hypothesis because know that these numbers are equal to the as. o we rewrite this. And we see that we get aih k. o this is where we use the induction step. o its like we assume pk over here. And need to finish this formula. o we have this. Now, by the definition of matrix multiplication, we actually see that this is equal to a k plus 1 ij to the ijth entry in the k plus oneth power of the adjacency matrix. Here we have this represents the matrix of the kth power. This represents the matrix of a. o we multiply essentially a to the power k times a and get a to the power k plus 1. And thats what we see here. o this is the induction step, and so this proves the theorem up here. o lets talk about a few more definitions concerning directed graphs. And then you go a step into a very special type of graph, which are the tournament graphs up here. One of the things that we have in undirected graphs, so where we have no directed edges, we talked last time a lot about cyclicity and stuff like that. And we talked about acyclic connected graphs. And as we categorized those, so we defined them as trees, and they have a very special structure. o what would happen here, if we look at a directed graph, and we wonder, what does it mean to be connected. an we really talk about that? What does that mean? For example, if look at this particular example graph over here, can say, well, V1 has an edge that points towards V1 itself, and towards V2, and towards V3. o maybe would call this graph connected. But if look at V2, only see an edge that goes from V2 to V3, and not to V2. o maybe do not call it connected. o thats why we define a stronger notion for a digraph. o a digraph, G, VE is called strongly connected if we know that for every pair so if for all vertices and V in the vertex set there exists a directed path that starts in and ends up in V in G. o this is what we would call strongly connected. o now in undirected graphs, we had connectivity and then we said, well, if a connected, undirected graph has no cycles. We have trees and they have special properties and all that. o what about this over here? uppose you have an acyclic strongly connected digraph. What does that look like? o lets give an example. ts not completely clear what kind of structure it has. o for example, may have a graph that looks like this, for example. ay this is an acyclic graph. but t does not have at all a tree structure. o actually, the type of graph the we have here is called a directed acyclic graph. As you can see, there are no cycles because only go forward, essentially. can never go backward in this particular way that depicted the graph. o this is an example for directed acyclic graph. But it doesnt look like a tree at all. o its worth to define it separately, and we will use this on Thursday when well talk about partial orderings. And it turns out that, as you can see here well, at that case, a directed acyclic graph has really nice properties, and one of them is that you can order these vertices in such a way that you go from, say, left to right in a directed fashion. And that will lead to partial ordering. o thats something that we will talk about as well. o whats the definition? A directed graph is called a directed acyclic graph, and we appreciate this by DAG. We call them DAGs. Theyre used everywhere, actually. f it does not contain any directed cycles. o these kinds of graphs are used in scheduling and optimization, and we will use them next week in the lecture on partial orderings. Now we have done a lot of definitions concerning directed graphs. o now lets talk about these very special ones, tournament graph, and see whether we can prove a few really nice theorems about them. Lets see whether we can figure that out together. o what is a tournament graph? n a tournament graph, have a bunch of vertices. And essentially we want to represent like a tournament. o every vertex, say, represents a team. And a team can play against another team, and beat them or lose against the other team. o we want to use the directed edges to indicate who is winning from home. And such types of graphs have very special property. o let me first depict one. o for example, we have E goes to A, goes through B, incoming edge from , one coming from B. And over here we have this directed edge. We have this one. We have this one. Let me see dont make any mistakes here. And this one. And over here is another one. o what do we see in this graph? We see that either say, team , beats team V. And that means that we have a directed edge from pointing at V. Or its other way around. V actually beats , and we have a directed edge from V to . Lets have a look at this graph and see how this works. o for example, we have that B is beating E, and E is beating A, and so on. o lets have a look. aybe we can figure out whos the best player in here. o this is sort of a general question if who would like to answer. aybe you cannot answer this. o lets have an example. Has anybody seen an example where we start with A, then we may beat another vertex, and maybe another vertex, and so on, until we have covered all the different vertices. Do you see a path that works like that? And that could gives us an ordering on who is the best player, like the one at the top, like A, is able to, for example, beat B. And B is, for example, able to beat D and this one, E, and this one, . o you would say, OK, thats great. Now know that this one is the strongest player. But theres a little problem here, right? Because can produce many such paths. And actually, if look at , then beats A as well. o thats kind of weird. o wait a minute. We have that theres a directed edge from to A. ts like teams beats one another. And its not very clear how we can talk about a best player. Well, we would have a best player if one player sort of wins from everybody else. But theres many examples here. o lets look at another walk. For example, can go to B, to D, and then to E, and then to A. o there are many possibilities here. o this leads us to a concept. And we call thats a directed Hamiltonian path. And were going to show that, in a tournament graph, you can always find such a directed Hamiltonian path. o whats a Hamiltonian path? This is actually an example of it. Theres a walk that goes around the graph and visits every vertex exactly once. o were going to prove that a tournament graph has this beautiful property. o lets first write out a definition of this. A directed Hamiltonian path is a directed walk that visits every vertex exactly once. o as said already, here we have such an example. We can go from A to B, to D to E, to . aybe there are even other examples. did not actually see them. aybe you can have a look at them as well. o maybe theres something that starts with B going to E maybe. Thats a very different direction, like this. There will be one as well, and so forth. o whats the theorem that you want to prove? The theorem in that you want to show that every tournament graph actually contains such a directed Hamiltonian path. o lets have a look at how we can prove this. What kind of proof technique are we going to use? Do you have an idea? Well, usually we use induction so thats what were going to do here as well. But what kind of induction hypothesis can we do? o what would we induct on, you think? omeone else? aybe someone up there? ADENE: The number of nodes. PROFEOR: The number of nodes. o we use induction on the number of nodes. And why would that be of interest? o lets have a look at how we can think of that. o we start thinking about such a problem, this really sort of one parameter here thats the number of nodes in a tournament graph. also have edges. But if think about edges, then, the edges is always directly related to the number of nodes in a tournament graph. o really have just that one parameter. o it makes sense to use induction on number of nodes. o induct on n, where Pn is going to be that every and essentially, the theorem holds true for a tournament graph on n nodes so every tournament graph on n nodes actually contains a directed Hamiltonian path. o this is semi induction hypothesis. o when you think about that, well, feel pretty confident because if look at the base case thats how we always start well, n equals 1. f n equals 1, have just a single vertex. Theres no edges. Everything is fine because the single edge is a directed Hamiltonian path. o this is great. o this works. o what about inductive step? Now with inductive step, we always perceived in the same way. We start to assume that Pn is true. Essentially, the theorem holds for a tournament graph on n vertices. Actually, let me keep this over here. o now lets just think about how we can prove this induction step. need to prove P of n plus 1. o how do start? have to start with a tournament graph on n plus 1 vertices. And somehow got to be able to use this property because thats what assume. And the property only holds for a tournament graph on end points. o what will be a really good strategy to do here to sort of proceed our proof? o let me first write out what we want to do here. aybe you can think about how to advance here. o we have shown Pn. Now we want to actually prove something about tournament graphs on n plus 1 nodes, so lets consider one. onsider a tournament graph on n plus 1 nodes. Now how can use my induction hypothesis? o start with this, and want to use something that talks about the tournament graph on n nodes. o how could proceed here? Any suggestions? o what do you usually do, right? f have like an n plus 1 nodes, have to somehow look at least maybe there exists a subgraph in this bigger graph. o this is really how you always think about these types of proofs or also other problems. o there must be some kind of subgraph that already has this property. Well, lets take out one node and see what happens because then we have one node less and maybe we will be able to apply our induction step. o lets take out one node V. And what can we say about remaining graph if we take out one node? For example, if take out the node E over here and just look at all the rest, can still see that for all the other nodes either, say, beats V or V beats . o still have an edge in one direction between each two nodes. o actually still have a tournament graph, so thats great. o this gives a tournament graph. o essentially, so far, we really havent done anything creative or anything that we had to make a big leap in order to prove this theorem. We started out with, if you want to prove something like this you have to really look at the number of vertices. And then we start to write down this stuff over here that makes total sense. And then we are going to figure out where we can use this induction step. And so we just take out one node. And yes, it is a tournament graph on n nodes. o this is very systematic. Thats what try to get at here. o by the induction step because by the induction hypothesis, we know now that we actually have a directed Hamiltonian path. o let V1 to V2 to Vn be such a path. o now that we can use this, we apply it and we get a path. Now what do we want to do? We want to show that we can create a new path, which is also a directed Hamiltonian path, but now one that also includes V in the bigger graph. f we can do that, we are done. o now we have to start really looking at how we can make that happen. n order to do this, we have to see how we can somehow plug V, the vertex that we have removed, in this path over here. f you can do this, we are in really good shape. o far, we havent used at all so theres also something that you can look at if you start solving these types of problems we havent used at all the property that the tournament graph has, which just wiped out. o lets figure out what we can do here. We should be able to use something. o of course, we have a few simple cases. For example, if V has a directed edge into V1, and have a Hamiltonian path that goes from V to V1 to V2 all the way to Vn, and then cover all the vertices exactly once. And will have a direct Hamiltonian path. o this is great. o this is definitely easy. o this is case one. n case two, suppose that V1 has a directed edge to V. And o now we have a little problem because somehow there is an edge like this to V, but they cannot go like this. This is not a Hamiltonian path. need to have a directed walk that covers all the vertices exactly once. o now we have a little problem. o now we have to start really thinking about how we can solve this. o are there any suggestions to make this happen? o lets think a little bit about this. o somehow if start thinking about this, would like to plug V somewhere in this sequence. That will be like a pretty obvious way to go ahead. ADENE: PROFEOR: Yeah. Thats true. o for example, may have that for example, V2 beats V as well. But suppose that have V3 over here, and this one beats V3. Then, as you say, could sort of plug V in here in this sequence and may have a longer sequence. Or if this is not the case, then maybe the next one, V4, may have the property and can plug V in here. o essentially what want to show is that can plug V somewhere in the middle of two of these Vis. And the property that you were using is actually that you said, well, know that V2 either beats V, or V beats V2. o thats the property of the tournament graph that were going to use here in order to prove this. o yes, thats a great observation. o the way you formulated it, we can maybe use induction, for example, to prove this. We can sort of go recursively through this until you find the right spot. aybe they can immediately precisely indicate such a spot. o how do we usually do that? f we are thinking about other theorems that we have tried to prove. ADENE: Whats the smallest value of i such that V beats Vi? PROFEOR: OK. o whats the smallest value of i where V beats Vi. o usually we have words in our mind like largest or smallest, et cetera, and sort of an extreme precision. And then we like to find out that we can make it happen. And then we say, well, if something goes wrong, we violate that smallest condition. o lets do that here. o lets indicate a specific spot. Lets considered the smallest i such that V beats Vi. Well, lets have a look how this would work. o this is a little bit of a different proof than what had but this should work fine as well. o lets just see how it works. o we have V1. First of all, we notice that, of course, if i equals 1, which that cannot be the case, so we know that i is larger than 1. o theres really somewhere a Vi minus 1 we have to check that, right? f that exists, this index is not equal to zero that goes to Vi and then goes all the way up to Vn. o now we say we can use this, that V actually beats Vi. Now whats about maybe someone else can help me here would like to have that Vi minus 1 beats V. That would be fantastic because then have a path that goes from V1 all the way up here, goes here, goes there, and all the way up to here. o we have a directed Hamiltonian path that covers all the vertices exactly once, and thats what you want to prove. But why would there be an edge that goes this way? o how do we reason about this? omeone else? ADENE: Because Vi is the smallest number that V beats, then anything smaller than i must have beat V. PROFEOR: Exactly. o if V would beat Vi minus 1, that would contradict the smallest property over here. o thats a contradiction. Now we use a property of the tournament graph. o now know that Vi minus 1 must beat V. o really have an edge over here. o that works. o thats the end of the proof. Now another version could have been in which we would do something similar like this. But we could also have used, say, the largest i just something that you may want to look at. You can also use the largest i such that Vi beats V. We have a completely symmetrical argument here, but you could use this solution as well. o m just trying to sketch here the way of thinking that you may want to consider in these types of problems. o why would this work, by the way? Well, we have the same kind of argument like this. We plug V right after Vi. We know theres an edge from V to Vi plus 1. Why? f its not the case, there will be a large index, i, that contradicts our assumption that we have the largest i already. ts a tournament graph, so we know that theres an edge from V to Vi plus 1. And we get a directed Hamiltonian path as well. o you may want to look at that as well. o this is about tournament graphs. o lets talk about an interesting tournament graph with a funny game. And this is actually a chicken tournament, like the chickens here represent the vertices and they are pecking one another, but in a certain rule that defines a chicken to be the king chicken. o lets see how that works. o thats a great application of graph theory. o what do we have? We have that either a chicken, , pecks a chicken, V. And we said that has a direct edge to V, so were actually defining a tournament graph here. Or we have a chicken, V, that pecks a chicken, , and we get V has a direct edge to . o we have a tournament graph. But now we define something new. We say that virtually pecks V if one of the two conditions holds one of these two conditions either , of course, pecks V. Thats great. Hes in good shape. Or there exists another chicken, W, such that actually pecks W and W, in turn, pecks V. o this is very special kind of first relationship. o we are wondering now can we now define something is there a question? ADENE: in between? To be virtually pecked, is one chicken in between and the other one? PROFEOR: Well, there can be multiple chickens in between here. have several friends who help me out pecking someone else. o when we were looking at these tournament graphs, we were wondering, can we really indicate a winning player? Well in this case, if you start to talk about virtual pecking, we look at the pecking order. Then we can actually define something like a chicken king. o let me write that down. Let me first explain what mean here. And give an example of a graph. o a chicken that is able to virtually peck everyone else, we will call a king. o chicken that virtual pecks every other chicken is called a king chicken. o lets give an example of a graph. o, for example, suppose have four chickens that know how to pick one another in this order. o who in the pecking party here is going to be king? Do you see some solutions here? o take, for example, this one. o this one pecks this one. Because we talk about virtually pecking, it can also peck this one over here. t does, right? t pecks this one and this one helps out, and can peck both this one and this one. Thats cool. o this one is king. And this one, actually lets have a look. t pecks this one this one, and it virtually also pecked this one. Yay. He has a friend over here that is doing that for him. o this one is actually also a king. o you can have multiple king chickens in here. What about this one? The same story pecks this one, pecks this one, and virtually wait a minute. The one on the left oh yeah, over here. This, this. o this one is king as well. Now what about this one? Well, it can peck this one. And then in one more step from here, because theres only one outgoing edge, it can virtually peck this one, but not this one. o this one is definitely the loser of the four. o hes not the king. o now what we want to prove this a theorem sort of transidentify one of the chickens that we know for sure is going to be king. o you can have multiple kings. But maybe theres one chicken that, from our intuition, we may feel is definitely going to be king. o what will be a good intuition? o were talking here about virtual pecking. We have this definition. o what kind of node in a tournament graph essentially would be can we know for sure that its a king? Do you have an intuition for a theorem that you may want to prove? o thats often what we do in mathematics. We have some kind of funny new structure, and then we want to find out whether we can prove interesting properties about it. o we start to search for actual nice properties and theorems. o in this case, it makes sense that the vertex that has the most outgoing edges may be always king. an we prove this? o thats what were going to do. o thats the theorem. And lets see whether we can do this in an elegant way. o the theorem is that even though there are multiple kings as indicated in that particular example, that may happen but certainly know that the chicken that has the highest outdegree is definitely a king. And the way were going to prove this is by contradiction. Lets assume thats not the case. That must be really, really weird. f you are the one who has the largest outdegree, it means that you can directly, just by yourself, peck the most others. o suppose youre not king. o by contradiction, first of all, let have the highest outdegree. And we want to show that this is king. o lets assume the contrary. o lets suppose that is not king. o what does that mean? o lets look at a definition over there and see what it means that is not king. o that means that both those conditions are violated because if one of those two holds, know there must be one vertex, V, such that does not virtually peck V. o know that. o lets see what that implies. o know that there must be a V such that does not virtually peck V. o maybe can help me out. What does that mean? t means that both these conditions are not true. o lets look at the first. o pecks V. f thats not true, and were in the tournament graph, we know that V must peck . o we have this. and we also know that the other condition, the second one over here, does not hold. o whats the negation of this second condition? aybe you can help me out. o we have here, there exists a W such that pecks W and W pecks V. o how do we negate that logical expression? ADENE: PROFEOR: For all W, whats over there is not through true. o can we formulate that a little bit better? o its not true that pecks W and W pecks V. o that means that either pecks W is not true, or W pecks V is not true. o lets write that down. Lets just write it all out. o not pecks W or not W pecks V. Well, how can we rewrite this? Well, its a tournament graph, so we know that W pecks . Or this particular condition, which is V pecks W. o what do have here? s this going in the right direction? Well, want to prove something about if use contradiction and suppose that is not the king, ve assumed that has the highest outdegree. o want to show that somehow violated. o somehow m able to construct some vertex, V. And by negating that is not a king it seems that this vertex, V, makes a really good candidate to show that theres a higher degree outdegree than . o lets see whether we can do this. We can rewrite this logical expression once more. We can also say that, well, if pecks W so this is not true then it must be true that this one holds because if thats not the case, then this condition is not true. o if pecks W, then this is not true. o then it must be the case that that is true. o V pecks W. o now lets have a look at V. We noticed that for all outgoing edges from , there exists a similar outgoing edge for V. But V has one more outgoing edge. V, actually, is an outgoing edge to . o what do we see here? That the outdegree of V is actually at least the outdegree of which is this particular condition over here that we show here, for all the W we have that this is true plus and we have an extra one. ts this one. Oh, but now we have a contradiction because we said that has the highest degree. But it turns out that you have constructed one that has a higher outdegree. o thats a contradiction. That means that our original assumption is actually wrong. o suppose that is not the king was a wrong assumption, and must be king. o thats the end of this proof. This is the end of this lecture. o see you tomorrow at recitation and next week we will continue with communication graphs and partial orderings.","That the outdegree of V is actually at least the outdegree of which is this particular condition over here that we show here, for all the W we have that this is true plus and we have an extra one. Well, it turns out that if you look at this graph and we, for example, want to know the number of walks of length 3 that go from V1 to, say, V2, lets see whether we can see those over here. ts actually means that in this case, we say that aij is equal to 1 if we actually have an edge that goes from Vi to Vj so this is an edge and 0 if this is not the case. n order to do this, we have to see how we can somehow plug V, the vertex that we have removed, in this path over here. And we have this. o now we can go ahead and we can have a look over here because we just showed here that all the edges incident to Vk are actually used in W. o we can now conclude that this means that Vk also has odd degree in G because all the edges in G, in which Vk is participating, are actually used in the walk. And we usually say that if we have an edge that points from V2 to, say, V3, then we call this the tail and over here we have the head. And this is pretty straightforward because now what we can do is we can start to look at how we compute walks of length k, or k plus 1. o lets first do the base case. o we have this. o we have this. And it turns out that, as you can see here well, at that case, a directed acyclic graph has really nice properties, and one of them is that you can order these vertices in such a way that you go from, say, left to right in a directed fashion. o thats the property of the tournament graph that were going to use here in order to prove this. Well in that case, we know that the number of walks of length 1, from i to j, is exactly 1 because theres this edge. o now what we want to prove this a theorem sort of transidentify one of the chickens that we know for sure is going to be king. And then, well, we can use this particular edge to complete it to a k plus 1th walk from i to j. o now what is this equal to? We have this one. We have this one. o thats a good thing because that means that if it is not an Euler tour, there is an edge that is not used in W, in the walk, but it is incident to some vertex that is used in W. o this is where the connectivity here comes in play. o if this is not in the walk, then what can do? And we want to show that this is king. o lets say that aijk, that this denote the i jth entry in a to the power k. And you want to show that this number is actually equal to pijk for all the entries. We can also say that, well, if pecks W so this is not true then it must be true that this one holds because if thats not the case, then this condition is not true. n case two, suppose that V1 has a directed edge to V. And o now we have a little problem because somehow there is an edge like this to V, but they cannot go like this. o what we have is if you start to consider a walk, W, that touches, say, a lot of vertices in such a way that W is actually the longest walk. o we have a directed Hamiltonian path that covers all the vertices exactly once, and thats what you want to prove. We started out with, if you want to prove something like this you have to really look at the number of vertices. o all edges that are incident to Vk are actually used traversed in the walk, W. o now lets have a look whether we can show that its a walk that goes all the way around. can say that the degree of must be equal to the number of times that actually appears in this tour. o what to do is we have a sum over all the Vs such that well, we will use indices here let me do that a little bit differently. Now if there is an edge that is not covered in this walk and can lengthen the walk, right? o what do we see in this graph? Now whats about maybe someone else can help me here would like to have that Vi minus 1 beats V. That would be fantastic because then have a path that goes from V1 all the way up here, goes here, goes there, and all the way up to here. PROFEOR: o what youre saying, first of all, is that if this would not be the case, then you know that the degrees must be even. And for this graph we actually issue that the degree of every vertex V is even. o lets have a look at how we can prove this. o actually let p kij be the number of directed walks of length k from phi i to phi j. Then what you can show is if you look at matrix A to the power k, this actually is the matrix that contains all these numbers. o we want to show that this is true. Or if this is not the case, then maybe the next one, V4, may have the property and can plug V in here. First of all, we notice that, of course, if i equals 1, which that cannot be the case, so we know that i is larger than 1. o theres really somewhere a Vi minus 1 we have to check that, right? And as you can see, we may have an edge that goes from V2 to V3 and also one that goes from V3 to V2. o the first property that you may want to consider it you really want to show that W is going to be an Euler tour. o now that we can use this, we apply it and we get a path. And that could gives us an ordering on who is the best player, like the one at the top, like A, is able to, for example, beat B. And B is, for example, able to beat D and this one, E, and this one, . And this one, actually lets have a look. o lets have a look at how we can think of that. o now we say we can use this, that V actually beats Vi. o we could do is we can start walking like this and then end over here. And we see that we get aih k. o this is where we use the induction step. Lets have a look at this graph and see how this works. And thats what we see here. For example, if V has a directed edge into V1, and have a Hamiltonian path that goes from V to V1 to V2 all the way to Vn, and then cover all the vertices exactly once. o what youre essentially saying is that you can enumerate all the walks by going of length k plus 1 by first going from i in, say, k steps, to whatever V, and then in one step to, say, Vi to V in k steps and then in one step to Vj. We can have . o this is what you want to prove. o all the indices h such that Vh to Vj is actually an edge in G. o then we can write out here that we go in k steps to Vh. o we have G with the vertex at V, edge is at E, and it has an Euler tour. That will be great if we can do this. o what can we do here? o what can we do here? o lets first do this implication, where we assure that we have connected graph that has an Euler tour. o lets first define, because we want to prove that kth power of a is equal to that matrix up there, where all the entries represent the number of walks. We have that Vk has odd degree in G. And now we come to what you were saying. o we know this is equal to 0. o suppose the start and the end node in this walk are not equal to one another. o for example, we have E goes to A, goes through B, incoming edge from , one coming from B. And over here we have this directed edge. o we could rewrite it and sum over all h from 1 to n. And write pij k times, and then aj oh, this should be an h. o we have the same over here. o pecks V. f thats not true, and were in the tournament graph, we know that V must peck . What can we say about the degree of the vertices? The theorem is true for k. And this is really the same as stating that all these entries for all i and j aij is equal to the pijk. o what do we have? o what would happen here, if we look at a directed graph, and we wonder, what does it mean to be connected. o say we have the graph. o we start thinking about such a problem, this really sort of one parameter here thats the number of nodes in a tournament graph. We want to prove pk plus 1. o what you want to know is whats the number of walks of length k plus 1. o how can we express those? o only count this number over here if this is equal to 1, and that happens exactly if theres an edge. Well, want to prove something about if use contradiction and suppose that is not the king, ve assumed that has the highest outdegree. o we can walk from one vertex to the other. an we say something about, given such as walk of length k, what can we say about the number of edges, for example? o every edge in this walk are exactly the edges that are in the graph. For an Euler tour, we really want that every single edge that G has is actually used in the tour. o actually, the type of graph the we have here is called a directed acyclic graph. ADENE: You can define the number of edges that it will test by how many you enter it, so the number of edges that it has is twice the number of traverse once, then every time you enter a node you have to leave it. We can, right, because we know that theres an edge if and only if the adjacency matrix has a 1 in a particular position. V actually beats , and we have a directed edge from V to . We want to show that we can create a new path, which is also a directed Hamiltonian path, but now one that also includes V in the bigger graph. But the only node has odd degree in this particular walk, W. o thats what we do now. This is actually an example of it. o the way you formulated it, we can maybe use induction, for example, to prove this. Do you have an idea so we can ADENE: Any walk of length k plus 1 can be got by taking one of lets say from Vs to Vx. o we go to Vi plus 1, and we go all the way up to Vk, which is equal to V0. And this one. o now we have shown the existence of an Euler tour, and that was the other direction of the theorem. Now, by the definition of matrix multiplication, we actually see that this is equal to a k plus 1 ij to the ijth entry in the k plus oneth power of the adjacency matrix. o now we have to start really thinking about how we can solve this. And then in one more step from here, because theres only one outgoing edge, it can virtually peck this one, but not this one. One of the things that we have in undirected graphs, so where we have no directed edges, we talked last time a lot about cyclicity and stuff like that. What we can conclude here is that if you look at a vertex, say vertex for example, somewhere over here well, it may repeat itself over here. Well, what can we do? And then we are going to figure out where we can use this induction step. o a digraph, G, VE is called strongly connected if we know that for every pair so if for all vertices and V in the vertex set there exists a directed path that starts in and ends up in V in G. o this is what we would call strongly connected. o if youre going to use induction, it makes sense to assume that the theorem is true for k. o the induction hypothesis could be something like this. o start with this, and want to use something that talks about the tournament graph on n nodes.",0.206054279749478
22,22,"DAVD ALAN: This is 50, and this is the start of week one. And what we thought wed do is pull back the curtain of a device that many of you have probably walked past now for years, but perhaps never quite known what it is. o this device will probably look familiar to most of you by now. A triumph of mathematical and mechanical skill is this great new automatic calculator at Harvard niversity. ntricate problems in mathematics put through the machine in coded form on tape are accurately solved in a minute fraction of the time required for human calculation. Designed to expedite all forms of mathematical and scientific research, the giant mechanical brain will work for the nited tates Navy until wars end. DAVD ALAN: o thats what you see in the cience enter, which is where that machine is now housed. You actually only see part of it. Only a portion is currently on display, and thats a device called the ark , and it was essentially a really huge calculator. t was succeeded by the ark , and its from the ark that we actually get a bit of lexicon. This is the first recorded instance of something youre about to become all too familiar with over the course of the semester namely, a bug. o it turned out at one point the ark device was not functioning properly, and upon closer inspection, there was indeed a bug a moth that moth inside. And ever since, we have referred to mistakes in programs as bugs. o more on that to come. peaking of video as well, if you havent already, know that 50 is obviously filmed. The lectures typically go up right after class in sort of a prerelease unedited format, and then a day or two later, we put up a higher resolution and higher quality version. You may recall Vanessa, who had the fortune of playing with Google Glass, which remembered to charge this time. o if you want to come up after class and play, they should work today. And if youd like to see what Vanessa saw the other day, what youll see here let me raise the volume here on my laptop. DAVD ALAN: All right, so the reason that it didnt work for most of you at the end of class was because proceeded to record an hour of that footage pointing at myself after put the glasses down. o today, ll leave them off, but thank you to Vanessa for being such a good sport. eanwhile, a couple of announcements before we forge ahead today. o one, 50 has a tradition of doing what we can to try to make a very large class feel smaller. And toward that end, most every Friday, we gather at a restaurant in the square, Fire and ce, with 30 or 40 of your classmates, myself, some of the teaching staff, and we just have a very casual lunch. We often invite friends from industry, alumni of the class, alumni of the college, really just to chat each other up, talk about life after college, life in college, and the like. o if you would like to partake in this first such lunch this Friday at 1:15 P, head to that RL at some point. pace is limited, so well do first come first serve. But well do this again on a recurring basis, so not to worry if you dont make it into this first batch. Now, in terms of the resources provided curricularly by the course, theres a whole slew, and if you havent pulled up the courses website already, in particular, under lectures, youll find that everything we did last week is there, both in video and some sort of electronic form. But beyond the videos, youll find that there are now full text transcripts, for instance, of every lecture. o if we actually go back to this screen here with Vanessa, and navigate to the bottom corner of the screen, youll find that actually, not for Fridays lecture, but if we go back to Wednesday, since this feature takes a few days, youll find that you can actually pull up, for better or for worse, every word that or a volunteer on stage said. And more than just that. You can actually search it, you can click on any of those sentences, jump to that point in the video, all toward and end of making the material all the more navigable. But if youd rather not follow it in such detail, youll find, for instance, this little menu here for 1.5x speed, 2x speed, or i f already speak too fast for you, 0.75x speed. o realize all of that is available there for you. But beyond that, for lectures, realize that we also make these resources available as well. lides, example code, anything that do on stage or in advance of class, well post there so that you can play along at home or in anders. But what we also thought wed do this year for the first time is also provide you with walkthroughs of these examples. ncreasingly, as ve reflected on the value of 50s lectures, ve begun to question just how useful it is for you guys to sit there, for me to stand here, and for me to talk at you, particularly as the material, the examples, get more complex. Because invariably, after some number of minutes, someone will zone out, and then you miss some key insight of some example, and then youre essentially gone for the remaining portion of class, which is not the best use of your time, or, really, ours collectively as a class. And so what we thought wed try to do, particularly for some of the more complex examples that might not do justice to in class, that we might not have time for in class, or you might just zone out during, is well produce a series of walkthroughs of these examples so that if you go to, for instance, last Fridays video page here under examples, youll see that for Friday, theres now this link to walkthroughs. And the format of these will change over time. For now, were using a simple YouTube playlist. But what did with our production team afterwards was walk through each and every one of those examples again, hopefully pointing out in much greater clarity exactly what the takeaways could be or should be from each. o youll find, for instance, from last week, all of the scratch examples we went through are done linearly in that fashion. o feel free to engage or not engage in this material. Really a theme of this particular course is that theres probably more in the way of resources than you could possibly absorb over the course of a semester. But thats deliberate. ts meant to be so that you as individual students can selfselect based on your learning style. o if lectures work for you, great. f sections work for you, great. f walkthroughs work for you better, great. t really will be up to you to choose what resources work best for you. Finally, thanks to an alumnus of the course, we also provide a canonical set of scribe notes. o rather than have you guys heads down in lectures scribbling down things that come up in class, we, the staff, will provide you with what we feel are a canonical set of notes to free you of that distraction, so that if you are here and engaged, you are truly engaged with whats going on, with your classmates are doing up on stage, with whats up on the screen, and not simply writing down verbatim what happened to have been said. o realize all of those resources are available to you. ectioning, in answer to an FAQ, will begin this Wednesday. We essentially wait until after most other classes are done so that we can minimize the number of changes that we need to do. But coming up this weekend, starting unday, will be a one time instance of what we call super sections. These will be filmed for those who cant attend, and essentially, the courses heads will walk us through some of the portion of the course, toward an end of problem set one, which is going to be our first based problem set. And these will just be optional sections in anticipation of what will become recurring sections led by the courses 50 plus teaching fellows weekly. On undays or ondays or Tuesdays, we have 90 minute sections in a very traditional sense, which will be opportunities for hands on and more intimate review of the courses material. Without further ado, let me to introduce the courses heads, who are behind the scenes with me, making everything happen. f those here today could join me, Rob and Lauren and Joseph and Lucas, all of whom have been with the course for some time. RJ is the third such member of our team. He couldnt be here today, but he asked me to show a photo of him. m not sure he wanted this one, but there he is. And let me just allow the team to say hello and introduce themselves so that you get to know them as well as your own teaching fellow this term. LAREN ARVALHO: Hi. y name is Lauren arvalho. m a resident tutor in Leverett House. m also super excited to be a head TF for 50 this year. 50 is a very challenging class as well as a very large class, so if you have any concerns, if you feel like youre falling behind, feel free to reach out to any of us at heads@cs50.het, or to me, if youd like to play with my golden retriever puppy. Heres Lucas. LA FRETA: Hey, guys. y name is Lucas Freitas. m a junior computer science and linguistics. m actually from Brazil, and also realize east Asian studies. o if you have any questions about computer science or language or anything, just let me know. Or 50, especially. JOEPH ONG: Hi. m Joseph. m a senior studying computer science in Elliott House. o, lets see. Youll see me around carrying one of these huge cameras. m the resident staff photographer for 50 as well, and students often sometimes mistake me for an Asian tourist. o if you see me with a camera, dont run away. Just smile and dont be shy. And hope you enjoy 50. enjoyed it very much when took it, and thats why ve been TFing for these three years. ROB BOWDEN: Hi. m Rob. This is my fourth semester with 50. just graduated in ay. was in Kirkland. m excited for this semester, and hope you are too. DAVD ALAN: Thank you to this years heads, and to RJ as well. o a few final notes on resources. One, p set 0 is already up. This one exists only in standard edition, which is meant for the entirety of the class. tarting with p set 1, there will be hacker editions of most problem sets, which, again, will cover the material with a bit more of a challenge, while still touching on some of the same topics. Do take a look at the courses website for that specification for p set 0. Office hours, too, will begin this week, tonight, tomorrow, Wednesday, and Thursday in various dining halls on campus. heck out cs50.net/ohs for the office hours for the course, and realize this will be very casual opportunities, certainly, at the start of the semester, to come by with your laptop during brain break. Bring some friends and chat up the courses TFs and As with any questions that you might have. And cant emphasize enough, even for scratch and problem set 0, there really is no dumb question. There is no student too uncomfortable to ask questions. Please feel free to take advantage of this resource. t will be there recurringly throughout the semester, as will 50 Discuss. This is the courses online discussion forums at that address there. What well also do during lectures is also monitor this, thanks to the courses teaching team. And so if you have a question during lecture, because said something poorly or not at all, by all means, ask that in real time if you have your laptop or phone on the website, and well do our best in near real time to respond to that. Are there any questions about 50? Then one last notes of mine. That issue of AT NAT. As you finalize your study cards, if you havent already, realize that the overarching vision of AT NAT, available for all students, is really to help take the edge off of a course like this, so to speak, so that if you find yourself late at night, 2A, 3A, really banging your head up against the wall where you know youre 99% of the way there to completion, but you have so many other things to do, think a better use, often, of students time is to indeed move on to those other things so that you can optimize your time all around. And AT NAT is one mechanism via which you can take comfort in the fact that 99% of the way there is still pretty darn good. imilarly, too, if youve come into the course with no prior background or not nearly as much background as you think your classmates might have, true or false, realize that this is an opportunity to get your hands dirty with the course, put your toes in the water, so to speak, much like myself did years ago with pass/fail. As said last week, had this course, like others, not been available pass/fail at the time, probably would not have ever stepped foot in the class, even though did, five weeks later, change my mind, which you may as well, and switch over to or from letter graded status. All right. o we looked at things like this on Friday, and were very quickly going to transition to something that looks more cryptic today. But its nonetheless the same fundamental idea. What was the general term that we used to describe a puzzle piece that looked like this? o, a statement. And you can call this any number of things. But well just call it a statement, and a statement just tells the program or in the case of cratch, the sprite to do something. ay hello in this case. omething like this we instead called what? Yeah, so a Boolean expression. A Boolean expression is just something that is true or false, and so cratch draws them with this shape, but also with a question mark to convey the idea that the answer to this question is either true or false, yes or no one, 1 or 0. And we use these Boolean expressions inside of constructs like these, which we called what? o, condition or branch. And the condition you see here in cratch has a little placeholder for a puzzle piece of that shape, the purpose of which is so that you can drag it and drop it on top and then dictate to the program if this Boolean expression is true, do this set of instructions, this set of statements, else do this other set of instructions. And recall that you can nest these things. Even though theres not much room visually in that puzzle piece, cratch will grow and shrink to fit whatever puzzle pieces you drag and drop in there so you can actually nest this and have a three way fork in the road. f, else if, else. And you can even go beyond that by nesting further and further as needed. o lastly, we saw constructs like this, otherwise known as a loop. And this is just something that does something again and again and again. n this case, it happens to be a finite number of times, but we also saw an example where the number of times came from a variable, so it could change. And we also saw another block altogether called a forever block that allowed us to loop infinitely long. o one other construct that cratch 2.0 has and those of you more comfortable with prior background are already well familiar with this construct these things we called what? o we called these functions, otherwise known as procedures. Theres a slight semantic difference, but well call them just functions. And a function, in the case of cratch, is a custom puzzle piece that you yourself can create so that you have a new puzzle piece that, in this case, would be called cough, that appears among all of the other available puzzle pieces in cratch. Even though T didnt invent this puzzle piece, you did. o a function allows you to create new behavior, give it a function name, and then call it, so to speak. Really use it in programs again and again and again without having to wait for someone like T down the road to invent new functionality from cratch. You yourself can build these puzzle pieces yourself, and then reuse them in your own programs. And well see a recurring example of that today and onward. o today, we transition now to something more arcane. But at the end of the day, something more powerful and expressive and more representative of the path that well be on all semester throughout a number of languages that of code or source code. And source code, it turns out, is not what a computer typically actually runs. ource code looks something like this. o this is, again, perhaps the simplest program we can write in a language called . We will start to tease this apart before long, and if this really does look like Greek to you right now, trust me. Within just a week or twos time, this will be all too familiar and actually quite representative of increasingly complex examples with which youll also get more comfortable. But you cant just run this kind of code usually. You have to turn it into something that the computer itself understands. And so for that, we need something were going to call a compiler. A compiler is a program that takes source code, like you just saw, as input, and it produces zeroes and ones as output, otherwise known as object code. And its those zeroes and ones that might look quite like this that ultimately are understood by your ac or your P. f youve ever heard the marketing expression ntel nside, that just means that a company called ntel has manufactured the brains of your computer otherwise known as the P, central processing unit and thats just the thing that understands patterns of zeroes and ones. And so by converting source code into object code through this process here that well do with a couple of commands in just a bit, you are creating patterns of zeroes and ones that the employees at ntel have decided represent certain statements. Now, dont quite know which is which by just glancing at these zeroes and ones. ost humans these days dont. But somewhere in there is a pattern of zeroes and ones that represents the statement print. omewhere in there could be a different set of zeroes and ones that represent the notion of forever or repeat 10 times or even meow, if its actually a program that can include some sounds. o in short, humans have just decided, much like we did for A for letters of the alphabet last week humans have decided that even more complex patterns of zeroes and ones represent more complex behavior like printing or saying or meowing. And so for today, and largely onward in the semester, well take for granted that someone has figured out how to do that mapping. But we, consistent with this idea of layering on top of the work of people whove come before us, will take for granted the fact that this is going on underneath the hood. But were much more interested in building things that are more interesting on top of all this. And so indeed, the first program we wrote in cratch was this super simple one, Hello World. And you can think of this as being the main program that governs cratchs behavior as of last Friday. Today, were going to start to translate cratch puzzle pieces into source code in this language called so that what looked like this on Friday, starting today onward, is now going to look like this. Admittedly more cryptic. ts pretty distracting and sort of mind numbing to see all of the quotes and the semicolons and the parentheses and so forth. But if you start to ignore anything you dont understand, and really look at the essence of the white text on the screen, surely you see, like do, ""hello, world."" That lines up with the purple statement up top, and it turns out that the keyword main is going to line up conceptually with when green flag clicked. o in other words, whereas in cratch, we implement this Hello World program with those two puzzle pieces, in , were going to implement it with these four somewhat cryptic lines. But as we dive deeper into this, youll see that each one of those white characters on the screen actually has some significant meaning, and most of our programs are going to look structurally quite the same. o a statement, to be clear, what looked like ""say hello world"" on Friday is going to start looking like this on onday. printf (""hello, world""). And theres some weird backslash n, theres parentheses, theres semicolons. But at the end of the day, the key takeaway today is that all were doing is translating things from picture form to text form. o a loop that looked like this last week is going to now look something like this. And let me disclaim, just like in cratch, theres different ways to achieve the same goal. ame in . m not showing you the only way to do things, but one way to translate this forever block to a loop in . This one here, repeat 10 times. This one really kind of bends the mind the first time you see it. But to implement that henceforth, were going to use a for loop, so to speak. And clearly theres some syntax thats a little complex there with and equal and 0, but once we dive into that today onward, youre just going to see that thats all simply doing some basic arithmetic to get us from 0 on up to 9 or 10 in this case. Lastly, with variables, youll recall that a variable is a piece of storage. Well, what we had on the left there last week is going to look a little something like this this week. But again, more on that to come. We had Boolean expressions, which last week, looked like this. This week onward, theyre going to start to look a little like that. And this ones actually a little reassuring. ts actually almost as simple as the green blocks. But here we have, again, some new syntax with ampersands, but you can perhaps guess whereas in cratch, you say AND, apparently in , were going to say ampersand ampersand. But all of this will become natural before long. onditions. Last week, it looked like this on the left. This week, on the right, its going to look like that. But again, the key takeaway here is that as you see things like that on the right, and especially if less comfortable, its sort of over your head, you get a little overwhelmed that really have no idea how could possibly come up with that from scratch no pun intended just realize that its really the same idea that probably felt so much more natural the previous week. And even if you havent dived into problem set 0, daresay youll find that the puzzle pieces are fairly intuitive, or at least become so over time. o lets dive in. Lets get our hands dirty here with this first simple program. And simple is, of course, relative. To do this, m going to change screens to that device called the 50 appliance. The 50 appliance is a piece of software thats running inside a window on my ac, and you can also run it in a window inside of your P, that allows me to have the same exact computer environment as every TF, as every A, as every student in the class. Because the 50 appliance is literally another operating system called Linux specifically Fedora Linux and we, the staff, have configured the installation process for this operating system in such a way that one, theres a little 50 logo in the middle, two, theres a custom menu at the bottom, and three, pedagogically, weve simplified the user interface as much as possible so that when you first boot this thing up on your own ac or P, you have the same environment as everyone else, and you have all of the tools that you need for the course without having to figure out how to install each and every tool individually. o this looks relatively like ac O or Windows. n the bottom left hand corner, you essentially have a tart menu of sorts, or an Apple menu. And then you have what we suspect will be your three most common icons. One is hrome. Theres a browser inside of the appliance. Two is a program called gedit, which we saw briefly last week, which is just a graphical editor. And three is a Terminal program, which is a black and white window from yesteryear that allows us to type more arcane but more powerful commands at our keyboard. o m going to go ahead and click on gedit, and rest assured that problem set 1, to be released on Friday, will walk you through all these same steps. Notice that have a pretty simple user interface with three parts. On the left, have a big empty window called source code. Thats where m going to see a summary of any code that write. On the right hand side, have a big tab, currently unsaved, and thats where my codes going to go. And on the bottom, have an embedded Terminal window, a black and white prompt at which can type commands to compile and to run my programs that is, to make and to run my programs. o lets start simple. Let me go up to File, ave. m going to go ahead and go into jharvard. o the appliance, irrespective of what your name is, belongs to a guy named John Harvard, who has a jharvard home directory inside of which all of your files will go. ts your own copy of his account, so were not all sharing the same hard drive. You just have the illusion of John Harvards own. n advance, actually installed Dropbox. The appliance comes preprepared with Dropbox so that if anything goes wrong during the semester, if youve configured your appliance with a free Dropbox account, all of your files will be automatically backed up so you can very easily recover them. And m going to go into my Dropbox directory, and theres already going to be a couple other things there. But m going to go ahead and go to File, ave. m going to go into jharvard, Dropbox, and up here, m going to give my program a super simple name, hello.c, and then save. o have the same interface now. The only thing thats changed is the tab up top. o m very quickly going to recreate that program from the slides a moment ago. o into main(void), and then printf, ""hello, world,"" a somewhat cryptic backslash n, close quote, close parenthesis, semicolon, and then trl to save. And notice as an aside what just appeared here on the top left. And this is not useful today, but will be useful over time. You just see, again, a summary of the code that youve written. And because wrote something called main, thats why the word main popped up over there. But for the most part, we dont need that today. All right, so claim that this is my first program written in a programming language called . We know from Friday that what this thing is going to do very uninterestingly when run it is just display in other words, print, as a computer scientist would say hello world on the screen. o to do this, youll notice that have to go down here to my terminal window, and now have a blinking prompt. But this is now a keyboard version of my John Harvard home directory. ts a keyboard way via which to navigate my hard drive. o cant actually type like did last week make hello, because its going to say no rule to make target somethings wrong. dont quite know what that message means, but thats because by default, when this black and white window is opened, m inside of John Harvards home directory. But where did save that program? n my Dropbox folder. Now, all of us have grown up with computers where you just double click on a folders icon, it opens up, and voila. Thats where the file called hello.c would be. o in fact, lets do that. Let me go ahead and minimize gedit just like you would on Windows or ac O. Let me go into the Home folder at top left. ll see here a bunch of folders. Heres my Dropbox folder. The checkmarks means its been synced. And theres a few things in here, but notice that hello.c is indeed right there. And so in ac O or Windows, normally, d double click that, it opens up in whatever program, and m good to go. But now, at least starting today, we need to take a step backwards just to give you some basic tools for your tool kit with which to do more powerful things before long. o at this prompt, have to do the equivalent of double clicking the Dropbox folder. And to do that, m going to type D for change directory, Dropbox, enter. And now notice the parentheses at my socalled prompt now says that m in tilde, the little squiggly symbol at the top left of most keyboards, /Dropbox. This is just shorthand notation for saying human, you are now inside of the Dropbox folder, just as though you had double clicked on it. o now if go ahead and type make hello, enter, see a very cryptic command, but not an error message, it seems, and then another blinking prompt. And if you recall, ever so quickly on Friday, we did this. ./hello. What youre about to see is the result of my running this program, or, in ac O and P terms, if d normally double click on a program a .app program or a .exe, it opens a window and runs. Typing ./hello is just like double clicking a program to run it, but using my keyboard. Enter, hello world. And thats it. o lets tease this apart to so it doesnt feel too much like Greek. o ./ means what? Anyone whos ever navigated a keyboard environment like this, whats dot referring to? The current directory. o the current directory is just a synonym for a folder. o by saying dot slash, am saying, look for a program called hello in the current directory in the current folder. n other words, my Dropbox folder. And thats why hitting Enter thereafter actually runs that program. f now type L, notice that see everything in my Dropbox folder. Getting tarted.pdf, which Dropbox gives you for free, my photos directory, a public directory, nsaved Document 1 which made by mistake a moment ago, foo.c, which made earlier today, and then two lines hello.c and hello. Hello.c is the program wrote with gedit, and what is hello in green? The hello in green is the executable. n other words, when ran this command a moment ago let me roll back in time when ran make hello, thats the middle step here. And it took hello.c as input AKA source code and it produced a file called hello as output, which contains all of these zeros and ones. And thats why, in the end, can actually run a command like ./hello. Because of ntel nside, my operating system, Linux, opens up those zeros and ones, feeds them into the brains of my computer the P and that P knows, oh, heres a pattern of zeros and ones that says print the following thing on the screen. All right, so a quick summary of some of these commands so that we can start taking them for granted. Let me skip ahead to where we left off. And youll recall that we had D at our back end a moment ago. These are, dare say, for todays purposes maybe this week the only commands we might need to type at the command prompt besides make. o D stood for what? hange directory. ts just the equivalent of double clicking on a folder. And as an aside, if you get lost ever inside of your appliance, such that Dropbox want to get back to my jharvard directory when in doubt, D, enter, with no other words, and it just zips you back to where you started, which is a nice little shortcut. o, list, right? Back in the day, when humans were coming up with these commands, they were trying to be efficient, and rather type out LT enter, they decided L. Thats fine. t sounds enough like list. Lets just type L enter, and that lists the files or folders in my current folder. mkdir, you can probably guess. ake directory. o you dont go to some file menu and choose New Folder in this environment. Rather, you type the command mkdir space and then the name of the folder that you want to create, and it will be done. rm, you can probably guess. Remove or delete. o if you want to delete a file, youll see in the problem set how you can do this. And rmdir, remove directory. o, again, ll tend to fly through some of these details, one, because theyre not all that much fun to spend time on, but two, well provide you in much greater detail in any of the courses problem sets, particularly the standard editions, any of the steps that you might need to type so that you get more and more comfy with these things. But now let me tease apart one other thing. When we typed make hello a moment ago, we saw a cryptic sequence of commands. We saw, again, make hello. OK, lets do this. was about to make a mistake, but now admit was about to and do it deliberately. What did do wrong just now? Yeah, so m not in the Dropbox directory, because a moment ago typed D and just hit enter. That zoomed me back to where started, which is not where my file is, so really need to do D Dropbox enter, and now can do rm hello enter, remove regular file hello. dont know why its so cryptic. That just means are you sure you want to delete hello? You can type y for yes, enter, and now its gone. And now if type make hello again, its going to recreate my object code, recreate those zeros and ones. But what is the deal with this very long command? Well, well tease this apart before long, but make told a bit of a white lie before is not a compiler. ake is a program that automatically figures out how to run a compiler for you. o the command that you really should be running in order to convert hello.c into hello actually looks a little more like this. And God forbid we have to actually remember to ever type that whole sequence of commands out. The purpose of make in life is to remember that degree of complexity for us and automate what would otherwise be very tedious steps. But can approximate that. o per this little cheat sheet here, can approximate what that command is doing by simply typing clang, which is the name of the compiler well be using this semester. f any of you have ever done iO development for the iPhone or iPad, lang is the program youve used within xcode to make your apps. lang, though, is just another program whose name we can type at the command prompt, and by introducing this now, can introduce something thats a little representative of commands to come. o let me do this manually. o rm hello, and now notice in case , just as a sanity check ./hello, what do you think should see if do this? ome kind of error. No such file directory. o that ones actually pretty direct. o now let me clear the screen. ontrol just clears the screen to remove some distractions. And now m going to manually type clang o hello hello.c. o what is this about to do? Exactly the same thing that make did for us, but did throw away some of the superfluous words. lang is the program thats going to take a .c file as input and produce zeros and ones as output. But o henceforth will call a switch or a flag. This is just techie jargon for saying this is a word you type at a prompt that somehow influences the behavior of the program youre running. o does anyone want to guess what o conveys? Output. ts just someone decided o means output a file called hello and take as input whatever the last word on the prompt is, hello.c, so that if hit Enter now, nothing seems to happen. And oddly enough, in this world of programming, if nothing happens, thats probably a good thing because theres no error message on the screen. Hello, Enter. y program has now run. But can do stupid little things like this. clang o, hihihi, hello.c, enter, and now have an identical program named hihihi. t behaves the same, but the name is different. o this is only to say that if you want to influence the behavior of a program, were going to increasingly see things called command line arguments. This is a line. Youre typing commands. Ergo, command line. And command line arguments are just words or little pieces of syntax that somehow influence the behavior of programs that we are running. All right, that is perhaps the most boring program we can write and see. Lets now do something more interesting. Let me go ahead and create a new file. m going to go ahead and save this in John Harvards home directory as, lets call it custom.c, to be a little custom program. m going to zoom in at my top of the tabs here and start again. nclude standard stdio.h. ore on that in a moment. int main(void), more on that in a moment. printf (""hello, David""). uch more interesting, right? o now if not because of my name, just because its different make custom enter. ustom, hello, David. But even that, too, is a white lie. This program might as well be called david.c, because its not custom behavior at all. Really, want a program that asks the user for their name. o how do do that? Well, let me scroll up over here. And before use a statement in cratch, before use printf, otherwise known as a function, let me first ask the user for some input. o to do this, need to do a couple of things. One, need a place to store the users name after ve asked him or her for it. What construct do need to use probably to store a value like that? A variable. o unlike cratch, where you just get a nice puzzle piece, here, we have to use, obviously, words to express ourselves. And henceforth, anytime you want a word or a phrase, in programming, were going to call that a string. o a string is a word or a phrase. ts a sequence of zero or more characters, more formally. And if you want such a variable to exist, you have to say give me one. And the way by which you do that is to say string s semicolon, where s can be anything you want. n fact, s is a little underwhelming. Let me go ahead and more clearly call it name. Give me a string called name. But now, what is the persons name by default? Well, if the human doesnt type anything in, obviously, theres going to be no value in name. And it turns out in , if you dont give a variable a value, its going to have a garbage value. t might have some random zeros and ones, so youre just going to see garbage, truly, on the screen if you try to display it. But can fix that by saying, actually, store inside of name the following value. The way you store values in variables is if you have a variable name on the left, you have to somehow put what you want to store in it on the right, and the syntax for thats going to be as follows. Weve not seen this before, but take on faith for a moment that there exists another statement in the world that m going to again call function, and this function is called get string. And it does literally that. t somehow gets a sequence of characters from the human and then does something with it. And this does look like cryptic syntax. Normally, if youre using equal signs youre in the world of, say, algebra, and youre actually saying x equals 1. Here were not saying that name equals get string. Were saying store in name whats on the right hand side of that equals sign. The equals sign represents the assignment operator. ts the syntax that says put whats on the right hand side inside whats on the left. o get string is a new piece of functionality that not only does something asks the user for a string it also returns it, so to speak. o lets see whats going to happen here. m going to go ahead now and zoom out, and m going to get a couple of mistakes at my prompt. m going to type make custom, enter. Wow. A whole bunch of mistakes. All right, so this is kind of overwhelming, certainly at first, but it turns out all of these very admittedly cryptic error messages are referring to some patterns that well start to see over time. o this one ll pluck off, because its first. se of undeclared identifier string. That just means that gedit, or lang, in this case, doesnt know what mean by string. And thats because in , the programming language, there actually is no variable of type string. does not know that strings exist. But we, 50 staff, do. And so in advance of class, we created a file called 50.h. And inside of this file and more on the syntax to come inside of this file is a definition of what just defined verbally as a string. o simply by concluding this one line of code in my program, am now teaching gedit and make and lang all in turn what a string is. dont know how yet. n a few weeks, well peel back that layer. But for now, know that this teaches the compiler what a string actually is. o now let me clear my terminal down here. Let me rerun make custom, and solved all of those problems. o this, too, is going to be a recurring theme. only wrote a 9 line program, and yet had 18 lines of errors. Thats kind of intimidating. But realize that they often cascade such that one little mistake triggers the confusion for the compiler, and it looks much worse than it is. o now an astute observer will know that m getting a string from the user, but m not actually doing what with it? m not actually doing anything with it. o if actually run custom again, its going to just sit there. Whats going on? y program seems to be stuck on what line, would you say? o its actually stuck on line 7. And weve not seen this before, but on line 7, theres, again, this statement, get string. And it does what it says. ts waiting now for the human to take the string. This is completely unintuitive to me, the human, because havent been told what to do. But let me go ahead on faith and let me pretend to be Rob. Enter. Thats not the program intended. o we have our first blog. But lets try to reason through why this is the case. First, let me fix this first problem. Let me add a line here that says printf, what is your name? Lets just do close quote, close parenthesis. Right. didnt recompile it. o in a language like , because you change the source code does not mean that you have changed your object code to generate new object code, new zeros and ones. You have to recompile. f now rerun custom, enter, ah, a little more clear. y name is Rob. till buggy. And d argue theres a couple bugs now. This just looks hideous, frankly. Like, minimally, should have a space there, or move the cursor to the new line. And actually, new line. Thats kind of a key phrase here that weve seen but not defined. The way in which you get a text to move onto the next line, you dont do this. This is confusing to the computer. This looks like two separate lines of code, neither of which is perfectly correct. You instead have to be more explicit and say, give me a new line, which we represent in c with backslash n. This looks better. ts still pretty primitive, but its also still buggy. o theres one even if youve never programmed before, you can probably take a guess at why this program is still flawed. What do need to do that havent done yet? o have to actually do something with name. Just like in scratch, you might actually assign a variable like n or counter like we did last week a value. But if you dont do anything with it, nothings going to happen. All right, so let fix. Let me type my variable there, and let me go down here. Let me recompile, let me rerun, Rob. Now m just an idiot, right? o this is not really getting me any further. But that was a perfectly natural instinct, d argue, right? f the variable is called name, and want it to go there, why dont just type it there? Well, of course, we have to distinguish now between what is a string? Notice that quote unquote here, hello name? This actually itself is a string. ts hard coded. ts not from the human. ts from me, the original programmer, but its still a string. o if you literally write NAE inside of a string, whats going to get printed? Well, NAE. We instead need to tell printf, dont print out literally name. Print out the value of name. And to do that, we do the following. And this is just a human convention. We instead say %s, and that stands for string. And then at the end of my close quotes, m going to put a comma and pass a second argument into this statement. m going to type name there. o now notice we have a slightly new syntax. We still have two parentheses, and henceforth let me say that the stuff between parentheses represents arguments into a function inputs that are somehow going to influence its behavior. o by that logic, how many arguments does printf seem to be taking at this moment? o it seems to be taking two, and thats indeed implied by the comma here. o this is one argument in between quotes. Even though it has a comma in it, everythings in quotes, which means its one long string. omma, variable name. All right, so you have to recompile. o again, easy mistakes to make early on again and again. o now rerun custom, Rob, enter, and voila. We now have a custom program. o now have a program that using a few characteristics that are worth noting. One, m using 50.h, otherwise known as the 50 library. And inside of the 50 library are functions that other people wrote namely the staff for you to use, and get string is one of them. nside of line 2 is stdio.h, and why is this there? Take a guess. What exists inside of standard O parent? Printf. o printf, you dont see it anywhere else on the screen. t must come with the computer somehow. Where does it come from? ts inside of its declared, so to speak, inside of a file called stdio.h. Now, for this stuff, well come back to eventually, because its a bit distracting for now. This refers to our return type. This refers to an argument. But for today, were focused just here on these lines. o string name, just to recap, what did this do for me? o it declared a variable of type string. o its meant to be for words, not for numbers. Just as an aside, if did want it to be a number, would say something like int. f wanted to be a floating point value, something with a decimal point, would say float. But for today, ve just said string. And ve given this variable a name of name, but could have called it anything. n fact, originally, called it . This here is just another statement. How many arguments is this statement taking? o its just taking one, and that is simply an aesthetic detail of displaying something on the screen to me. What is this line 8 doing in laymans terms? ts getting a value from the user namely, a string and whats it doing with it? ts essentially handing it over to the guy on the left hand side of the equals sign so that in this case, the name variable can actually store it. And then printf is another instance of a function. And we say to call a function. To use a function is to call a function. This thing apparently takes two arguments. This one, comma, this one. This one itself is just a string that contains a placeholder. %s means put another string here. And so by putting name after the comma, thats inserting that thing for me. o can do other things still. Let me go ahead now and do a bit of a silly program, but let me delete this, and let me create a loop. While true. The symbol true is obviously always going to be true. o what kind of loop am inducing by writing a line like this? o an infinite loop, right? Just while true. Theres no way can change the word true to be anything else, so this is just going to run forever. o if youve ever actually had a program in your ac or P that seems to have locked up, or its doing something and something, and you just cant get the thing to quit unless you reboot your computer or pull up the Task anager or the like, heres such an example. can say, "" am a buggy program."" lose quote, close parenthesis, semicolon. o again, notice the patterns. Even though some of our syntax is new, the keyword while, the keyword true, notice have the same kind of curly braces. ve got a semicolon and parentheses. o now lets go ahead and compile this. ake custom, custom. o at this point, you do not need to restart your appliance, all right? o the easy lesson here is with two fingers, ontrol , well quit that program. But you can see exactly why this thing was repeating itself again and again. can do something a little more complex as a teaser. m going to go ahead and say the following. For int, which is an integer, i is just a generic name we give to most variables in programming when youre just counting, equals 0. Let me go ahead and say while i is, lets say, less than 100, i plus plus. Well back to this syntax, but this just means increment i again and again. And what do want to do here? Printf, "" can count to i."" This is a buggy program. What did actually intend here? Percent not s, but you wouldnt guess this. %d is a decimal integer. omma i. Let me roll back. Let me now recompile with make. Let me now rerun with custom. can count really damn fast when write a program like this. Now lets do something a little unintentional. How about lets do this while i is greater than or equal to 0. Whats this going to induce? o logically, this is not so wise, because now if rerun this, recount, can now count really, really high. nfortunately, about as high as can count is 4 billion, so this is going to take a while. o why dont we leave this as our cliffhanger, promising that on Wednesday, well see if this program is done. Two, well introduce how you yourself write your own function so that very quickly, by Wednesday, well, as we transition from cratch into , start writing ever increasingly complex programs that do much, much more than this. We will see you then. PEAKER 1: At the next 50, Puzzle Day was a success.","And the condition you see here in cratch has a little placeholder for a puzzle piece of that shape, the purpose of which is so that you can drag it and drop it on top and then dictate to the program if this Boolean expression is true, do this set of instructions, this set of statements, else do this other set of instructions. The way you store values in variables is if you have a variable name on the left, you have to somehow put what you want to store in it on the right, and the syntax for thats going to be as follows. Because the 50 appliance is literally another operating system called Linux specifically Fedora Linux and we, the staff, have configured the installation process for this operating system in such a way that one, theres a little 50 logo in the middle, two, theres a custom menu at the bottom, and three, pedagogically, weve simplified the user interface as much as possible so that when you first boot this thing up on your own ac or P, you have the same environment as everyone else, and you have all of the tools that you need for the course without having to figure out how to install each and every tool individually. And so what we thought wed try to do, particularly for some of the more complex examples that might not do justice to in class, that we might not have time for in class, or you might just zone out during, is well produce a series of walkthroughs of these examples so that if you go to, for instance, last Fridays video page here under examples, youll see that for Friday, theres now this link to walkthroughs. As you finalize your study cards, if you havent already, realize that the overarching vision of AT NAT, available for all students, is really to help take the edge off of a course like this, so to speak, so that if you find yourself late at night, 2A, 3A, really banging your head up against the wall where you know youre 99% of the way there to completion, but you have so many other things to do, think a better use, often, of students time is to indeed move on to those other things so that you can optimize your time all around. o, again, ll tend to fly through some of these details, one, because theyre not all that much fun to spend time on, but two, well provide you in much greater detail in any of the courses problem sets, particularly the standard editions, any of the steps that you might need to type so that you get more and more comfy with these things. But again, the key takeaway here is that as you see things like that on the right, and especially if less comfortable, its sort of over your head, you get a little overwhelmed that really have no idea how could possibly come up with that from scratch no pun intended just realize that its really the same idea that probably felt so much more natural the previous week. o this is only to say that if you want to influence the behavior of a program, were going to increasingly see things called command line arguments. o rather than have you guys heads down in lectures scribbling down things that come up in class, we, the staff, will provide you with what we feel are a canonical set of notes to free you of that distraction, so that if you are here and engaged, you are truly engaged with whats going on, with your classmates are doing up on stage, with whats up on the screen, and not simply writing down verbatim what happened to have been said. Rather, you type the command mkdir space and then the name of the folder that you want to create, and it will be done. ncreasingly, as ve reflected on the value of 50s lectures, ve begun to question just how useful it is for you guys to sit there, for me to stand here, and for me to talk at you, particularly as the material, the examples, get more complex. And the way by which you do that is to say string s semicolon, where s can be anything you want. Weve not seen this before, but take on faith for a moment that there exists another statement in the world that m going to again call function, and this function is called get string. And clearly theres some syntax thats a little complex there with and equal and 0, but once we dive into that today onward, youre just going to see that thats all simply doing some basic arithmetic to get us from 0 on up to 9 or 10 in this case. But well just call it a statement, and a statement just tells the program or in the case of cratch, the sprite to do something. o if we actually go back to this screen here with Vanessa, and navigate to the bottom corner of the screen, youll find that actually, not for Fridays lecture, but if we go back to Wednesday, since this feature takes a few days, youll find that you can actually pull up, for better or for worse, every word that or a volunteer on stage said. And inside of this file and more on the syntax to come inside of this file is a definition of what just defined verbally as a string. We know from Friday that what this thing is going to do very uninterestingly when run it is just display in other words, print, as a computer scientist would say hello world on the screen. This week, on the right, its going to look like that. Today, were going to start to translate cratch puzzle pieces into source code in this language called so that what looked like this on Friday, starting today onward, is now going to look like this. And as an aside, if you get lost ever inside of your appliance, such that Dropbox want to get back to my jharvard directory when in doubt, D, enter, with no other words, and it just zips you back to where you started, which is a nice little shortcut. But if you start to ignore anything you dont understand, and really look at the essence of the white text on the screen, surely you see, like do, ""hello, world."" And inside of the 50 library are functions that other people wrote namely the staff for you to use, and get string is one of them. ts essentially handing it over to the guy on the left hand side of the equals sign so that in this case, the name variable can actually store it. Well, what we had on the left there last week is going to look a little something like this this week. And to do that, m going to type D for change directory, Dropbox, enter. But at the end of the day, something more powerful and expressive and more representative of the path that well be on all semester throughout a number of languages that of code or source code. f the variable is called name, and want it to go there, why dont just type it there? o if youve ever actually had a program in your ac or P that seems to have locked up, or its doing something and something, and you just cant get the thing to quit unless you reboot your computer or pull up the Task anager or the like, heres such an example. And a function, in the case of cratch, is a custom puzzle piece that you yourself can create so that you have a new puzzle piece that, in this case, would be called cough, that appears among all of the other available puzzle pieces in cratch. And to do that, we do the following. But we, consistent with this idea of layering on top of the work of people whove come before us, will take for granted the fact that this is going on underneath the hood. But for now, know that this teaches the compiler what a string actually is. And on the bottom, have an embedded Terminal window, a black and white prompt at which can type commands to compile and to run my programs that is, to make and to run my programs. lang, though, is just another program whose name we can type at the command prompt, and by introducing this now, can introduce something thats a little representative of commands to come. That zoomed me back to where started, which is not where my file is, so really need to do D Dropbox enter, and now can do rm hello enter, remove regular file hello. And let me just allow the team to say hello and introduce themselves so that you get to know them as well as your own teaching fellow this term. You can actually search it, you can click on any of those sentences, jump to that point in the video, all toward and end of making the material all the more navigable. And so if you have a question during lecture, because said something poorly or not at all, by all means, ask that in real time if you have your laptop or phone on the website, and well do our best in near real time to respond to that. DAVD ALAN: This is 50, and this is the start of week one. The 50 appliance is a piece of software thats running inside a window on my ac, and you can also run it in a window inside of your P, that allows me to have the same exact computer environment as every TF, as every A, as every student in the class. ts just someone decided o means output a file called hello and take as input whatever the last word on the prompt is, hello.c, so that if hit Enter now, nothing seems to happen. o to do this, youll notice that have to go down here to my terminal window, and now have a blinking prompt. And its those zeroes and ones that might look quite like this that ultimately are understood by your ac or your P. f youve ever heard the marketing expression ntel nside, that just means that a company called ntel has manufactured the brains of your computer otherwise known as the P, central processing unit and thats just the thing that understands patterns of zeroes and ones. This is just techie jargon for saying this is a word you type at a prompt that somehow influences the behavior of the program youre running. o if you want to delete a file, youll see in the problem set how you can do this. But as we dive deeper into this, youll see that each one of those white characters on the screen actually has some significant meaning, and most of our programs are going to look structurally quite the same. o a loop that looked like this last week is going to now look something like this. o simply by concluding this one line of code in my program, am now teaching gedit and make and lang all in turn what a string is. We will start to tease this apart before long, and if this really does look like Greek to you right now, trust me. o what is this about to do? This is just shorthand notation for saying human, you are now inside of the Dropbox folder, just as though you had double clicked on it. o we looked at things like this on Friday, and were very quickly going to transition to something that looks more cryptic today. And what we thought wed do is pull back the curtain of a device that many of you have probably walked past now for years, but perhaps never quite known what it is. What youre about to see is the result of my running this program, or, in ac O and P terms, if d normally double click on a program a .app program or a .exe, it opens a window and runs. All right, so this is kind of overwhelming, certainly at first, but it turns out all of these very admittedly cryptic error messages are referring to some patterns that well start to see over time. m going to go ahead and save this in John Harvards home directory as, lets call it custom.c, to be a little custom program. o string name, just to recap, what did this do for me? And so by converting source code into object code through this process here that well do with a couple of commands in just a bit, you are creating patterns of zeroes and ones that the employees at ntel have decided represent certain statements. To do this, m going to change screens to that device called the 50 appliance. Hello.c is the program wrote with gedit, and what is hello in green? A Boolean expression is just something that is true or false, and so cratch draws them with this shape, but also with a question mark to convey the idea that the answer to this question is either true or false, yes or no one, 1 or 0. o per this little cheat sheet here, can approximate what that command is doing by simply typing clang, which is the name of the compiler well be using this semester. m going to go into jharvard, Dropbox, and up here, m going to give my program a super simple name, hello.c, and then save. o its just taking one, and that is simply an aesthetic detail of displaying something on the screen to me. And thats because in , the programming language, there actually is no variable of type string. Now, in terms of the resources provided curricularly by the course, theres a whole slew, and if you havent pulled up the courses website already, in particular, under lectures, youll find that everything we did last week is there, both in video and some sort of electronic form. o the command that you really should be running in order to convert hello.c into hello actually looks a little more like this. But well do this again on a recurring basis, so not to worry if you dont make it into this first batch. And if you want such a variable to exist, you have to say give me one. m going to go ahead now and zoom out, and m going to get a couple of mistakes at my prompt. Theres no way can change the word true to be anything else, so this is just going to run forever. ts pretty distracting and sort of mind numbing to see all of the quotes and the semicolons and the parentheses and so forth. And m going to go into my Dropbox directory, and theres already going to be a couple other things there. And so for that, we need something were going to call a compiler. DAVD ALAN: All right, so the reason that it didnt work for most of you at the end of class was because proceeded to record an hour of that footage pointing at myself after put the glasses down. And theres a few things in here, but notice that hello.c is indeed right there. o rm hello, and now notice in case , just as a sanity check ./hello, what do you think should see if do this? Let me go ahead now and do a bit of a silly program, but let me delete this, and let me create a loop. f those here today could join me, Rob and Lauren and Joseph and Lucas, all of whom have been with the course for some time. Even though theres not much room visually in that puzzle piece, cratch will grow and shrink to fit whatever puzzle pieces you drag and drop in there so you can actually nest this and have a three way fork in the road. t might have some random zeros and ones, so youre just going to see garbage, truly, on the screen if you try to display it. o a statement, to be clear, what looked like ""say hello world"" on Friday is going to start looking like this on onday. o at this prompt, have to do the equivalent of double clicking the Dropbox folder. These will be filmed for those who cant attend, and essentially, the courses heads will walk us through some of the portion of the course, toward an end of problem set one, which is going to be our first based problem set. You instead have to be more explicit and say, give me a new line, which we represent in c with backslash n. This looks better. And so for today, and largely onward in the semester, well take for granted that someone has figured out how to do that mapping. o to do this, need to do a couple of things. m going to zoom in at my top of the tabs here and start again. imilarly, too, if youve come into the course with no prior background or not nearly as much background as you think your classmates might have, true or false, realize that this is an opportunity to get your hands dirty with the course, put your toes in the water, so to speak, much like myself did years ago with pass/fail. The way in which you get a text to move onto the next line, you dont do this. o get string is a new piece of functionality that not only does something asks the user for a string it also returns it, so to speak. This is the first recorded instance of something youre about to become all too familiar with over the course of the semester namely, a bug. And if youd like to see what Vanessa saw the other day, what youll see here let me raise the volume here on my laptop. And AT NAT is one mechanism via which you can take comfort in the fact that 99% of the way there is still pretty darn good. All right, that is perhaps the most boring program we can write and see. But what we also thought wed do this year for the first time is also provide you with walkthroughs of these examples.",0.1057880328509972
23,23,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu PROFEOR: All right today we begin a topic dear to my heart. ts a problem thats still pretty open, but ve worked on a lot. Dynamic optimality. entral question here is, is there one binary search tree thats at least as good as all other binary search trees? s there one binary search tree to rule them all? ts kind of a fundamental question. Goes back to the 80s. n particular splay trees, which you have probably seen before. Well review them briefly here. And theres a lot of technology built up, which well talk about this lecture and next lecture, about how to tackle this problem. ts still active area of research. o before get to well the central question here is, is there one best binary search tree? ts a natural question because we use binary search trees all the time. Already in this class, probably every lecture, weve used a binary search tree for something. And we all know how to do order log in balanced binary search trees. And the question becomes, is this the best you can do? To make this more formal of a problem, well, yeah. want to define binary search tree as a model of computation. OK weve talked about pointer machine model. Binary search tree model is a more restricted version of pointer machine. o its less than pointer machine. Just so its clear what the name of this game is, we require the data to be stored in a binary search tree as its keys. And then we allow basic operations. Let me make sure get the right list. You can follow pointers, and were going to assume our BT has left triad pointers, right triad pointers, and parent pointers. You can do each of these in constant time. And then the interesting one, interesting thing you can do, is rotate a node and its parent. o sure youve seen rotations before. f this is my node x, and do rotate x, its going to be my definition of rotation, x moves to the root. ts in this case because x is to the right of its parent it remains now to the right of its, what now becomes its child. These subtrees come along for the ride. This preserves the binary search tree property. And you could get back by rotating y. OK by this definition of rotate we dont need to distinguish between left and right rotations. We always name the child and its parent is well defined. Thats who we rotate with. o its just flipping this edge. o these are the operations you can do, and in this model really the only thing you can do is a search. uppose want to search for a key. The model is to do a search start at the root of the tree, of the binary search tree. can do these walks. could do rotations. And must visit if m going to search for x, must visit the node with key equal to x. And were going to assume throughout these lectures that searches are always successful. Youre only searching for keys that are in the tree. Doesnt make that much of a difference if youre searching for keys that are not in the tree. Think of it as searching for the predecessor or the successor. Not a big deal. And so for simplicity, were just going to assume successful searches. Were also going to assume no insertions and deletions in this model, because they just make problem a little messier. ome papers have considered insertions and deletions, and to the large extent things just work. o, were going to focus on searches. earches is interesting enough for this problem. uppose have n items. Now the model is clear. store them in a binary search tree. These are the only things m allowed to do. ould imagine beating log n? Well, no in the worst case. This is best possible in the worst case. Why? Because your tree has to have depth at least log n if youre going to store n things, and if its binary. o your adversary could always be, could always just look at your tree and say, Oh m going to pick anybody whos a log in depth or lower, and if youre going to start at the root and walk left and right to get there, youre going to have to pay log n time every single, every single operation. o actually theres a lot of sequences, in fact, most sequences. You take an average sequence, this will be the case. But well see formal ways to prove that next class. But, easy adversary argument tells you you need to go down, log n in the worst case. But, thats the worst case. The name of the game in dynamic optimality is to consider every case separately, and do as well as you can all the time. n general, the answer to this question is that, is log n best possible? Depends on the access sequence. Depends on what youre searching for. o, lets say were, to make things a little clean, lets assume that the key is were storing are the numbers, integers, one up to n. o in particular theres n of them. And really we just care about their order, so m going to relabel them to be one up to n. And say we search for x 1 first, then we search for x 2, and so on, up to x m. o, these indices are time, basically. Over time, and you can plot this if you like. Here is time. Here is space. o at the first time unit we search at x 1, then some other place, then some other place. We can have repeats, whatever. OK. ome search sequences are going to be slow, theyre going to require log n time per access. n particular, if always, if the adversary chooses x i to be the deepest node in the tree repeatedly. But some search sequences are going to be fast. Any suggestions, if you dont have the notes in front of you, of access sequences that we can do in less than log n time per operation. Lets A constant. Theres a lot of possible answers, think. Yeah? ADENE: N divided by two. PROFEOR: N divided by two. What do you mean. ADENE: Like its going to be PROFEOR: Oh, if youre searching for the median in a perfectly balanced tree? Yeah, thats true. o if youre always searching for the root of the tree, in particular, thats going to be good. Thats hard to, that depends on the tree though. d like a particular access sequence that can always serve as well, with some tree. ADENE: n order walk. PROFEOR: n order walk. Yeah, good. o this has a name. equential access property. f you want to access the elements in order, so one through n, this should only cost constant amortized. Because for any tree you can do and in order walk in linear time. And so per search, if youre careful about how you do this, now this is not totally obvious in the model that ve set up, because said search always starts at the root. f you let your search start where you left off, then you can definitely do this. t turns out you can assume this or not, it doesnt make a difference. Thats maybe a fun exercise. But if, you can essentially reroute the tree to be wherever you left off. m not going to prove that here. OK, but in particular, some trees have this property. ome dont. mean if you use a red black tree, and you search for one, then search for two, then search for n, youre not going to get constant amortized, because it doesnt realize this is happening. But if youre clever, of course, this can be achieved. Easy way to see how to achieve it is, this is your binary search tree. You access one, then you do a rotation with the right child of one. o then you have this binary search tree, and then you access two, and then you do a rotation. This is basically a way to make a deck. View a, use a link, simulate a linked list in a binary search tree. OK, so it definitely can be done by some binary search tree. A stronger version of the sequential access property, its a little more interesting, is dynamic finger property. This is something that should hold for every access sequence, and it gives you a measure of how expensive an access sequence is. o were doing x one x two up to x m. And we say, if we measure the key distance between our current access, x i and the previous access, x i minus 1. f that equals k, then ideally we should be able to do log k amortize per operation. o in the worst case this is going to be log n, but if look at my spacetime diagram here, if for example, if do sequential access, then this number is always one, and so get constant time. f do something like this, where my spatial distance is not changing too much, and this is the reason numbered the keys like this. What really mean is, in the sorted order of the keys, how do the ranks differ. But if number the keys one through n, thats just their absolute difference. o anything like this, dynamic finger properly tells you youre doing well. Dynamic finger property can be achieved by a binary search tree, but its quite difficult as you might imagine. Or its difficult, lets say not necessarily super hard. f you dont need to do it in a binary search tree model, an easy way to do it is with whats called a level linked tree. o a level link tree looks like this, and add pointers between adjacent nodes in the same level. This is a pointer machine data structure, its not a binary search tree, because binary search trees we only allowed to do walk left, walk right, walk parent, But its an easy way to do dynamic finger. You start from somewhere, you basically walk up to the right level, move over, walk back down, and you can do a search for x i from x i minus one relatively quickly. t turns out this can be stimulated by a binary search tree, but we wont go into it here. Lots of fun puzzles here. Lets move on to some more bounds or properties. o, next property is called the entropy property, or entropy bound. And it says if key k appears p sub k fraction of the time, then d like to achieve a bound of entropy p k log one over p k per operation. This is, everything here today will be amortized. o, if you havent seen entropy, thats the definition. ts the entropy of this distribution. Were thinking of these as probabilities, although theyre not really probabilities. Theyre, because this is over a particular sample. We have this sequence x1 to x n, Theres no randomness here, but we just measure what is the fraction, what is sort of the observed probability for each of these x is. And then just say, what is the, what is the entropy of that distribution? n the worst case its log n, if say everybodys equally likely, everyones accessed once, or the same number of times. But if its highly skewed, like if only one element is accessed, the entropy will be constant. f a small number of elements are accessed, entropy will be constant. o this can be achieved, and in fact, if you disallow rotations, so if change the model to forbid rotations, all get to do is set up some binary search tree, and then have to walk left, walk right, walk parent, from in that binary search tree to do this. o if get to see the x is then get to build a binary search tree. One binary search tree. This is whats called the static optimal, when youre not allowed to change the tree as youre doing searches. Entropy is the best bound you can get. And roughly speaking, key k appears at height, or depth, log one over p k in the tree, maybe plus one. And you can show theres always a tree where every node ends up at depth log one over p k. Again, not hard, but we wont do it here. OK, a related property, its called the working set property. Little bit harder to state. For each search we do x i, were going to see when thats some key, when was the last time that key was accessed. o, in our spacetime diagram we have some access at time i, we want to look backwards in time to the last time that key was accessed. ay that was, well some other time j, and in this interval we want to know, how many distinct keys were accessed. o its at most, i minus j, but maybe there were some repeated accesses, we only count those as one. o how many different keys were accessed during that time interval? laim is, we only have to pay log of that. o what this means, what this implies, is in particular, if youre only accessing say k different elements at all, and you ignore all the other elements, then youll only pay log k per operation. Thats why its sort of a working set. f youre focusing your attention on k elements for a while, you only pay log k. But in general, for any access sequence you can compute, what is the working set costs, which is you sum over all the is of log t i, that is your total cost for the access sequence. Divide by n, thats the amortized cost. OK? Thats another nice property, not so obvious is that working set implies the entropy bound, so this is a stronger property. guess dynamic finger implies sequential access, but theres no relation between working set and dynamic finger. n fact, theyre kind of transposes of each other. Working set is about, if you access a key that was accessed recently, then its fast. Dynamic finger is saying, if you access a key that is close in space to the previous access, so here were looking at a key and basically in the very previous time step were looking at how far away it was vertically, here were looking at how far away it was horizontally. o theyre almost duals of each other. Be nice if you could have one property that included both dynamic finger and working set. should mention, again this is not obvious how to do it with a binary search tree, but it can be done. Leave it at that. Well see eventually some trees that have it. For now just want to cover what are the conceivable goals we should aim for. All right, log n is too easy, so we need more challenging goals. o the next property, it was introduced, this is were getting into more recent territory, 2001, its called the unified property. Natural name, and it tries to unify dynamic finger with working set. And the rough idea is, that if you access a key that is close in space to a key that was accessed recently in time, then your access should be fast. o, heres the formal statement. o heres the unified bound. Lets draw a picture. o here were accessing x i at time i, sorry j. hange of notation. Time j. We want to evaluate the cost of x j, and were basically going to look in guess one way to think of it is this cone. 90 degree cone, where these are 45 degree angles. And if theres something and look at the first thing that you hit in that cone. s that the right way to think of it? aybe not. ts like youre growing a box, this is probably more accurate, and you find the first key that you hit in that box. ts a little more subtle than that because were only counting distinct keys, and lets say you find this key, and this key is good because its temporal distance, this time, is only this big. ts spatial distance is only this big. What were going to pay is, log of the sum of the spatial distance, and the temporal distance. And temporal distance is measured as the number of distinct keys accessed in this time interval. o, as long as there is some key in the recent past that is close in space, you take the min over all such keys, magically a unified structure has to find what is the most recent close item and search from there, and achieve log of that. Theres a plus 2 just to make sure if these things are zero, you still get a constant. Always have to pay constant time. OK, so thats a unified property. Fairly natural generalization of dynamic finger and working set. mean you could change this plus to a product or a max, doesnt make a difference. Yeah. adly we dont know whether there is any binary search tree that achieves the unified property. What we know is that this can be done, this property can be achieved by a pointer machine data structure, which is more powerful than a binary search tree. But we dont know whether its possible to achieve by a binary search tree. Best bound so far, is that you can achieve this bound plus log log n. o as long as this never gets too small, as long as this quantity never gets smaller than log n, then this thing will be log log n, and its fine. But in particular, if this is constant, then its only achieving log log n, so its not theres an additive log log n per operation loss. Thats the best binary search, unified binary search tree known. OK, so this is all good. And this is sort of where the, what we call analytic bounds, here ends. There are various attempts to in general we want to characterize, what is the optimal thing we could hope for. When can we do better than log n? These are all specific cases where we can do better than log n, but its not a complete characterization. There are sequences that have large unified bound, but have no yet they can be accessed more quickly by the optimal binary search tree. And so, while it would be nice to characterize with some clean algebraic, whatever, most of the work, beyond what ve told you about here, is just trying to figure out what opt is. nstead of trying to write it down, try to compute it. nstead of trying to define something that happens to match the optimal, lets just go for optimal. And this is the notion of dynamic optimality. And in modern terminology this would be called constant competitive, but the paper then introduced dynamic optimality preceded competitive analysis, so it has another name. What wed like, is that the total cost of all your accesses, so this is like amortized cost, is within a constant factor of the optimal. Whats the optimal? This is the over all binary search trees, mean to be precise should talk about binary search tree algorithms, meaning you specify somehow how to do a search, and it may involve rotations and walks. o, you know, it could be red black tree, it could be an AVL tree, could be a alpha tree, anything that can be implemented in this way. Those are all kind of boring. ore sophisticated is something like a splay tree. deally, you take the min over all min over all binary search tree algorithms. The cost of that algorithm, on that access sequence x, and you want the total cost of your algorithm on x to be within a constant factor of the optimal algorithm on x. x is a vector. o this is what you call the offline optimal, because here you basically get to choose the binary search tree algorithm to be customized to x, and yet somehow you want to achieve dynamic optimality. Open question, is this possible? And, of course, were only interested in online solutions. o, you want to build a binary search tree that doesnt know the future, it doesnt know what accesses are to come, but it has to be within a constant factor of the offline solution that does know. And we dont know whether this is possible. Another interesting question, is whether its possible for a pointer machine, because save for the unified property we know how to get it for a pointer machine. And theres two versions of this question. You can consider, is there a pointer machine that matches the optimal binary search tree, or you could ask, is there a pointer machine that matches the optimal pointer machine. Thats a little less defined, although there are some have some ideas on how to define that problem, maybe will work on it. But all versions of this problem are open, basically. o, this may seem rather depressing. What else am going to do for a lecture and a half? But actually, theres a lot of study of this problem, and we do know some good things. For example, we can get log log n competitive. o, not within a constant factor, but we can get within a log log n factor of the optimal. o, thats pretty good. An easy result is to get log n competitive. Any balanced binary search tree is log n competitive, because best case you could hope for is constant, the worst case you can hope for is log n. o at least within the log factor, but we can do exponentially better. Well do that next class. Before we go there, want to tell you about two structures, to binary search trees that we conjecture are dynamically optimal, but we cant prove it. o first one, and the classic one, is splayed trees. dont want to spend too much time on splay trees, but just to let you know what they are if you dont already know. f you want to search for x, you search for x. You do a binary search for x in the tree. You locate x, and then you splay trees always move x to the root. o, this is what we call a selfadjusting tree. t changes the structure of the tree as you do searches, not just when youre doing updates, and there are two cases. f you look at x, and its parent and its grandparent, if theyre oriented the same way. o, here its two left pointers, could be two right pointers. Then you flip them. o we rotate y, and then rotate x, so its in this model. And we get x y z in the other order. Theres pretty much only one way to move x to the root in that picture. Then the other case is the zigzag case, y, w, x. o, here the two parent pointers are in different directions, one is left, one is right. Theres a symmetric picture. n this case, we rotate in the other order. o, we rotate x, and then we rotate y. And in that case you get x nice, x, w, y, and the subtrees hang off. m not labeling the subtrees, but they have to be labeled in the same order, left to right. OK, this is splay trees. You do this for x. x is the thing you search for, now x is up here, then you repeat. You look at its two parents, its one of these two cases, you do the appropriate two rotations. ntil x is either the root, and youre done, or its one child from the root, and then you do one rotation to make it the root. eems simple enough. ts slightly more sophisticated than an algorithm known as move to root, which is just rotate x, rotate x, rotate x, which would eventually propagate it up. ove to root is a really bad algorithm. t can show its can be a factor of square root of n. t can be square root of n per operation if youre unlucky. play trees are always, at most log n amortized per operation, although thats not at all obvious. Rough intuition, is if you look at the path, the route to x path, half of the nodes, at most half of the nodes go down when you splay. o why is that? Here, see y stays at the same level. z goes down, but x goes up. Here, w stays at the same level, x goes up, y goes down by one, x goes up by two, so you might call that a net improvement. But in general you dont mess up the path too much. Half the items stay where they are, and so its something like your bisecting if you repeatedly search for x, well m not really searching for x, but if you repeatedly search for things in that path, youre kind of cutting the path and half repeatedly. tll look kind of logarithmic, thats a very vague argument. And in the advanced algorithms, its proved why this is log n amortized. Question? ADENE: PROFEOR: Rotate x, rotate x, think youre right. Yeah, y would go somewhere else, thanks. Yeah, so here it looks like move to root, here it doesnt. Good, and theres lots of things known about splay trees. o for example, they satisfy the sequential access property. Theres an entire paper, combinatoric on 1985 Tarjan believe, proving splay trees have the sequential access property. Not at all obvious, but its true. They have the working set property, and therefore they have the entropy property. Thats in the original splay tree paper. ts not that hard to prove. Once you prove log n, with a little bit more effort you can prove the working set property. We wont do it here. ts a cool amortization. play trees also have the dynamic finger property. This is a series of two papers, over 100 pages long. Very difficult to prove. n general, splay trees are hard to work with, because its hard to keep track of whats going on. You can define potential functions, and that will prove this, and with a lot of effort it will prove this. We dont know whether splay trees satisfy the unified property, and more to the point, we dont know whether splay trees are dynamically optimal. This is the big question, and in the original splay tree paper it asked this question, are splay trees dynamically optimal. till open, wish knew the answer, but we dont. o what the rest of todays lecture is going to be about is a geometric view, which ve kind of been hinting at with this picture, but its, theres more to it than just plot the points of searches. Theres a way, not only to see what the searches are, but to see what the binary search tree algorithm is. o, at this point binary search tree algorithms are kind of abstract. mean, weve kind of drawn pictures, and how to do specific cases, you know how red black trees or something work. But, you know, its complicated to write down one of these algorithms. How would we think about all of them? What would you think about the optimal one? Turns out theres a geometric view, in which its actually very easy to see what the algorithm is, but it takes a little bit of work to prove that its the right thing. And it suggests a totally obvious offline optimal algorithm, which we dont know is optimal, but we think is offline optimal. ts like the obvious thing to do, which well get to, and we can actually make it online. o, this is this gives us a binary search tree that is so obviously optimal, where splay trees is kind of vaguely feels good, this is really obvious, yet we cant prove it. o its like were one step closer, but not quite there. o thats where were going. o this geometric view was the topic of PhD thesis, here at T. And its also with John acono, who was an undergrad here, and who was an undergrad and a PhD here. t was published three years ago, 2009. o, heres the idea. Access sequence, this is the sequence of searches you do. Thats very easy, you just map it to the set of points where, guess m going to switch things on you. orry about that. Xaxis is going to be space, yaxis is going to be time. And m going to look at a specific example of this. ort of pinwheel. Four accesses at the first time we access key three, then we access key one, then key four, then key two. Weve been drawing this picture throughout the lecture. Now the interesting part, is if you look at a binary search tree that actually accesses these elements, heres the exciting part. Were going to map this to a point set, which is which nodes get touched during a search, during each search. OK, this is the fun part. o, what does touched mean? erased the model, but in general with a binary search tree, we only have one pointer into the tree. t starts at the root, it can go left, it can go right, it can go to the parent. Every node that gets visited during it can do rotations, every node that gets visited by walking, thats what call a touched node. Just look at all of them. o for example, in this picture color suppose, maybe were lucky, and key three is at the root. o thats the only thing touch. f thats the case, when access one, its definitely not the root. o, in particular, definitely have to have touched that node. And m just going to fill in a reasonable choice. happen to know, this is a valid binary search tree. Not so obvious, but you can find one. m using here, the greedy algorithm actually, m cheating. Well get to what that is. But in general, some point sets, like this point set, are not valid. Theres no binary search tree that has this touch pattern. You just cant jump around, teleport, without getting somewhere. But, this one is valid. o, which ones are valid? Oh sorry need one more point. Which ones are valid, which ones are invalid? How did know that that was invalid, and now this one is valid? Well, theres a theorem tell you. Then what was just doing there will become clear. o, want to know when is the point set a valid BT execution, and claim it is, if and only if, in every rectangle spanned by two points not on a common horizontal or vertical line, in every rectangle, there theres another point. o, let me draw a picture. have some point set. Well look at this one in a moment. n general, take any two points that are not horizontally or vertically aligned, that spans a rectangle. n other words, the rectangle with those two as opposite corners. Theres got to be another point in there, somewhere. ould be on the boundary, could be interior. OK, in fact, you can pump this a little bit. o once find some point, lets say on the interior, then my picture looks like this. aybe ll just draw it as a circle. Well, heres another rectangle. That one has to have a point inside it. Heres another rectangle, that one has to have a point inside it. You could keep going, until you know maybe d put one here, maybe put one here. OK if update the picture then, still have a little rectangle like that, little rectangle like that, little rectangle like this. At this point, this point is no longer involved in any rectangles essentially, because theres no rectangle between these two points because theyre horizontally aligned. could finish this off, finally, by adding points, lets say at the corners. Once add points at the corners this will be satisfied. We call this property arborally satisfied. Gotta look up the spelling. Arboral satisfaction. Arboral means having to do with a tree. OK, so this is saying that point set is treelike, if it has this property. OK this picture is getting a little messy, but if have any monotone path, like this, this is satisfied. f you look at any rectangle, say this one, it has points other points in it. OK, if look at, dont know, this one, it has another point in it. But look at this one, that one doesnt count because its just a vertical line. o it has to have nonzero area. f you look at any two points, this one and this one, they span a rectangle, theres another point in there. o, monotone paths are good. This point set. f drew it correctly, is good. look at any rectangle here, its got another point in it. Takes some practice to be able to see all the rectangles, but check it. Eventually this one is good. This ones obviously bad. t has lots of wrecked we call these empty rectangles, unsatisfied rectangles, that have no extra points in them. o, this is a valid BT execution, this is not. Lets prove this theorem its actually not that hard. The consequence of the theorem, is it tells us what we need to do. We are given a point set like this, we want a point set like this. We can define the cost of a binary search tree to be how many nodes does it touch. Thats within a constant factor of how many operations you do. f you dont want to, its pointless to touch a node more than a constant number of times. You can prove that. o, really we just care about how many nodes get touched. o, what we want to do is find a minimum superset of this point set that is satisfied. Were given a set to represent or access sequence that is not satisfied. We need to act we need to touch those points. Thats the definition of search. And now wed just like to also touch some other points that make it satisfied. This is a geometric interpretation of dynamic opt. an you find the offline optimal, is whats the minimum number of points to add in order to make your point set arborally satisfied. We dont know whether that problem is NP hard. Probably it is. We dont know how to compute it. We dont know how to find a constant factor approximation, unfortunately. Yeah? ADENE: o, this is for and access sequence, so every time corresponds to just seeking like one individually thing in the tree? PROFEOR: Right. ADENE: OK, that makes more sense. PROFEOR: Yes, so the input there is a unique thing, a key, that gets accessed at each time. o if you draw any horizontal line, in the input point set theres only one point in each horizontal line. There can be multiple points in each vertical line, which would mean that key gets accessed more than one time. Thats the obvious interpretation terms of binary search tree, turns out you dont need to assume either of those things. You could allow a sort of multisearch, where say during this round you have to access key 5 and 7, dont care in what order. Whereas normally say, you have access key 5, then you have to access key 7. You could do a multisearch, where theres multiple points in a single row. All of the things ll say work, it doesnt really make a difference as the claim. You could also assume that no key is accessed more than once, so then theres only one those would be the opposite extreme. You can assume theres only one key per column. That turns out not to make much difference. f theres multiple keys in the same position, just spread them out a little and youll get roughly the same cost. o, good question. But in the natural interpretation theres one per row, multiple per column, but it doesnt, neither one matters. OK. o this becomes the problem, go from this to this, with a minimum number of added points. We do know a log log n approximation to that problem. Thats the best we know. You just take this binary search tree, and apply this transformation, and it tells you if you have a binary search tree, you can turn it into a way to satisfy a point set. But this is actually the best approximation algorithm that we know. But as youll see, the geometric view offers a lot of insight. Gives us a lot of power, and in some sense we use it to construct this. OK, so lets prove the theorem. o theres two directions. Well start with the easy direction. f you have a binary search tree, then it must be arborally satisfied. Then well have to do the reverse, actually build a binary search tree out of a point set. Thats kind of the harder step. o, lets say we have two points. Lets say this is that time i, we access key x, and this one is at time j, we access key y. Lets suppose y is greater than x, theres the symmetric picture. We want to argue that there is some other point in this rectangle. OK, heres the plan, let a be the lowest common ancestor of x and y. x is a key, y is a key, they are nodes in the tree. This is a changing quantity. As the tree wiggles around those rotations, the least common ancestor changes. But at every time, there is, it has some of these common ancestor. ight be x, might be y, might be some other node. OK? want to look at a couple of things. Lets say, right before x is touched at time i, or guess right before time i would be the proper phrasing. o were at this moment, want to know does a equal x? f a does not equal x, then m going to use the point a comma i. OK? Least common ancestor has the property that x is less than or equal to a, is less than or equal to y. ommon ancestor, least common ancestor will be between x and y. And we also know that its an ancestor of x, and its an ancestor of y. o, if youre going to touch x, you must touch a. f youre going to touch y, you must touch a. o a comma i is going to be some point here, which is what we need. Except, if a equals x, sorry, what the heck did do? Transposed again. OK, this is time i, time j, x, y, sorry about that. OK, heres time i, here is a. o, if a does not equal x, we know at time i, we must access all ancestor of x, we must touch all ancestors of x, before we touch x. Therefore, this is a point and were done, unless a equals x at this time, because then its the same point. We didnt get a third point, we at least need to find some different point other than these two. OK, so that would be good. We can so it must actually be that at this time, at time i, a is right here. Lets then look at time j. ame deal, at time j, heres y that gets access. We must also access a. Now it could be its equal to y, or it could be somewhere else. t could be here, or could be still here. n those cases were happy, but this one also could have been at the corner. The case were not happy is when b equals x, sorry, a equals y. Getting my letters mixed up. As long as a does not equal y, then we can use the point a comma j, that must be, these are always in your execution, and if a does not equal y, then were, thats a third point and were done. o were left with one more case, which is that at time i, a is here, heres a, and at time j, a is here. Question is, what happened in between? a changed. For a to change, something in here has to get rotated. Thats the mean heres the picture, you have a, actually picture is at the beginning a is x. o, here we have x, and then y is an ancestor, sorry, y is a descendant of x, because this is a. And then somehow we have to transition to the reverse picture, which is that y is the least common ancestor, x. And for this to happen, somebody here has to get rotated. guess, in particular x. At some point x had to overtake y, had to be moved up. o, at some point x was rotated, and that would correspond to a point here. o else x must be rotated at some time k, where k is between i and j. And if we set it up right, guess because here a was still x, and here a is y, then it must be strictly between. aybe like this would be what we want. And so then we use the point k comma x. s that more or less clear? This was the easy case. guess it depends what you consider easy. Here were just, were given a search tree and the short version is, look at the least common ancestor. ts got to move around at some point, or not, either way youre happy. o the least common ancestor gives you the points you care about. o for that we just needed the least common ancestor idea. For the other direction, if were given a point set that corresponds to, that has this satisfaction property, we have to somehow build from scratch a binary search tree. How the heck are we going to build a binary search tree? Well, with treaps. o, this is the other direction. Treap is a portmanteau of tree and heap. nderline it correctly, tree, heap. o, its simultaneously a binary search tree and a min heap, in this case. ts a binary search tree because it has to be. ts a binary search tree on the keys. ts going to be heap on a different set of values. o, binary search tree on the keys, and its heap ordered, min heap ordered, by next access. didnt say was going to give you an online algorithm, this is an offline algorithm. o it looks if look at a key, and its going to be accessed next, it better be at the root. Next access will always be at the root, if youre heap ordered. f youre a min heap by next access. This is great, that means the thing youre searching for is always at the root, and its basically free to touch. But you may choose to touch other nodes. And in fact, were told how to touch notes. Were given a pattern that says, well, at this time you have you will touch this node, this node, and this node. By that definition, this one will be at the root, but youre going to touch these nodes, and possibly you could rotate them. hange the tree. Youll have to, actually, because next access time is constantly changing. And as soon as you access an item, its next access goes in sometime in the future, possibly infinity, and so youd like to start pushing it down in the tree. o, what we need to do is show that we can maintain its always going to be a binary search tree, because we only do rotations. We have to somehow use rotations to maintain this heap order property, and only touch the nodes that were supposed to touch. Thats the challenge. o, should mention, this is not a uniquely defined tree. sually treaps are unique, if you specify a key order, and you specify a heap order, there is exactly one tree that satisfies both of them. But here its not quite unique, because the next access time there are many keys that are going, sorry, next touch. Next touch time. o, for example, at this moment all three of these nodes are going to be accessed at the same time. And so you dont know, or m not specifying how theyre supposed to be heap ordered in the tree. Just break ties arbitrarily, it doesnt matter. OK. o, lets look at a time i. When we reach that time i, the nodes to touch form and connected subtree containing the root. Because according to heap order, theyre all, they all want to be at the root. o we break ties arbitrarily, somehow, you know all the nodes that were supposed to touch, live in some connected subtree of the root. orry, some connected subtree containing the root. Everything down here has a later next touch time, and so theyre below, by definition of heap order. OK, now one of these is the one we actually want to access. But we need to touch all of them, so touch all of them, you know, navigate. Walk left, right, whatever. The big question is, what should we change this tree into? d like to change it to some other top structure. cant touch anything down here, m only allowed to touch these points. only want to rotate these somehow. Theres a convenient theorem, if you have one tree and you want to convert it into another, you can always do it in a linear number of rotations. o, rotations are basically free and this model, its just about how many nodes we touch. Were told which nodes to touch. We want to somehow rearrange them to restore this heap order property. Right? As soon as we touch all these nodes, their next touch time will be sometime in the future. We need to rearrange the tree to still be heap ordered by that new next touch time. OK, heres what we do. mean theres only one thing to do. Rearrange those nodes in this connected subtree to be a local treap by the new next touch time. These are the only nodes to get a new next touch time, so its more or less unique how to rearrange them. Do that. Now the hard part is to argue that the whole thing is now a treap. Why was it enough to only modify these nodes? aybe you set one of these nodes to have a very large next touch time, so its got to be really deep down there. And you cant afford to push it down deep, because youre not allowed to touch any of these nodes. Looks worrisome, but turns out, it just works. o, if there were a failure, picture would be like this. We rearrange these nodes perfectly, in particular, lets look at some node x that has a child y, that was not touched. o x was touched, all of its ancestors were touched, but y was not touched. o we know that the next touch time of x is greater than or equal to the next touch time of its parent, of its ancestor, up to the root. The worry would be that the next touch time of x is greater than the next touch time of y. o suppose next touch of x is greater than next touch of y. This would be a problem, because then you would not be a heap. o, claim is we get an unsatisfied rectangle based on x and its next touch time, and y and its next touch time. o lets draw the picture. Heres time, heres space, m going to assume by symmetry x is to the left of y and and now were supposing the next touch time of y is earlier than the next touch time of x. o it looks like this. o this is next touch time of x, this is the next time of y, and claim that there are no other points in here. That would be a contradiction, because we assume that the thing is satisfied. To prove this, need to go back a little bit to the definition, over here. There are actually a couple of different ways to think about satisfaction, which was getting at here, but didnt solidify. o, said OK, if you have two points and that rectangle is satisfied, there is some point, possibly in the interior, in that rectangle. But if its interior, then can keep going. f keep going, in the end can conclude that not only is this rectangle nonempty, but there has to be a point on one of these two sides, because if choose any other point, get a smaller rectangle. Eventually, have to get one on one of those two sides. ould be at the corner, or this corner, but one of those two sides has to have a point on it. Also, one of these two sides has to have a point on it. t could be both of these constraints are met at once by having one point in the corner. Well thats a somewhat stronger formulation, but equivalent formulation of the satisfaction property. o in particular over here, it should be the case that theres a point, either here or here, and there should be a point, either here or here. claim that one of those is violated. You think d know this stuff, wrote the paper, but its all these subtle details, easy to get wrong. think what want to look at is, now, which is the moment were drawing this diagram, versus the next time of y. orry, so this is x comma now. Thats what my diagram looks like there, so think thats what mean. o ignore this picture. What we learn, what we know is the next access to x, is sometime in the future. Thats what were told here. Next touch of x is greater than next touch of y. o next touch of x is up here, which means this is empty. OK, if thats empty, this better not be empty. All right thats what we claim, one of these two has to have a point in it. This ones empty, so this better not be empty, but claim this is empty. uppose its not, suppose it is, suppose its not empty Wait, no, one of these. uppose its not empty, should be the correct scenario. Look at the left most point in this range. This guy. ts a point between x and y, in terms of key value. o, in this picture, where could it be? Where are the points between x and y in this diagram? They have to be in the left subtree of y. Right? The only points in a binary search if x and y are if y is a child of x, the only points in the binary tree that are between x and y, or the left subtree of y, or theres a symmetric case, but in this picture left subtree of y. But put an x here. That means that whoever were looking at, some point between x and y, has to be in this top tree. ontradiction, done, OK? f this guy is in there, then y was also in there, which meant there was a point here, and thats what were assuming did not happen. This is supposed to be an interface between inside the set of touched nodes, and outside the set of touched nodes. o there cant be any points in here, which means this is empty, which means you werent satisfied. OK, maybe should write down the words to go with that argument, but o, this part is empty by next touch of x being greater than next touch of y. And this part is empty else, or its empty because any key between x and y is in the left subtree of y, which would imply if its touched then so is y. But y cannot be touched, by assumption. o thats the end of that proof. A little bit longer, but hopefully its pretty clear at this point. Question? ADENE: ant there be something like a descendent of x, but an ancestor of y, instead of being in a subtree with y? PROFEOR: Would, so you could say it could be in between here. t could be a descendant of x, but an ancestor of y, but were assuming here that this was a child relation. y was a child of x. This was a didnt say that at the beginning. On a claim thats a global treap, if its not a global treap, there is some edge thats violated, that does not have heap order property. o assuming this was an edge, x is OK, it was in the local treap, y somehow is going to be bad, because its next touch time was, should be higher. hould be above. Other question? ADENE: o that you drew, and all the everything in that subtree root that gets taken to Like the picture on the right is also, its the same subtree right? But after you change their PROFEOR: These two subtrees have the same set of nodes, theyve just been rotated somehow. ADENE: OK, but then how do you but didnt we change the next touch times of all the nodes in there? o how you do know if the root is still going to be some guy in that subtree? PROFEOR: Buy this argument. o the question is say, after we do this, we make some root, which is going, the root is going to be among all the nodes that just got touched now, who is going to be touched next? Thats who we put at the root. And the claim is, that is globally the node that will be touched next. Why? By this argument. f there were some other node down here that has a smaller next access time, then but we know that it was heap ordered before, so all of these guys are heap ordered. o, this guy would then have the minimum. And then we look at that interface, and the claim is by the satisfaction property, actually this point should have been in the set. o, what this tells you is the guy that has to be accessed next, in particular, must be in your set. ust be touched now. Kind of magical. Well actually, its because theyre tiebreaking think, that this works out. Anyway, another question? ADENE: o, like when youre rearranging the in the sort of subtree on top PROFEOR: Yeah, were doing this transformation of the top tree into the local tree by rotations. mentioned ADENE: ts always PROFEOR: Right, so were staying within the binary search tree model, and in particular then we stay a binary search tree. o, we cant mess things up. And theres a nice theorem that if you have two binary search trees on the same, keys theres a way to get there with the linear number of rotates, so thats for free. o, our new cost model is just to count points. The cost of this access is one, the cost of this access is two, two, three. f we just count how many nodes are touched, then the cost of the binary search tree is equal to the cost of the minimum satisfied superset of your point. o this is the problem now. adly we dont know how to solve the problem. We do know some things. dont think ll go through an example of this, its not very exciting. You could run through this picture, and see how the binary changes. Actually the binary wont change it all here, so its a little anticlimactic of an example. You can see it in the notes. want to get to the greedy algorithm, so lets go here. Last bullet. The idea is to imagine your points are added one at a time, bottom up. o m going to do the same example. First we add this point. Thats a satisfied set, done. n general add necessary points on the same row as the search. OK, theres nothing to add here. Next point we add is over here. ve got the orientation correct, yeah. o this was three, this was one. s this satisfied? No, theres a bad rectangle here. Theres two obvious ways to satisfy the rectangle, could add a point here, or could add a point here. m going to add a point here, because thats the row, this is current time. OK, its like a sweep line algorithm. We go up, next point is over here. Thats the Next position to sweep line. Now theres a bad rectangle. We fix it by adding this point. Now were good, all rectangles are satisfied. Next level is, theres a point here. Now theres two bad rectangles. This one, m going to add a point here, and this one, m going to add a point here. Now were good. And that should be what did here. Yeah. o thats how found that point set, and in general claim is this is a good algorithm. t seems, in fact, pretty obvious. There was a choice of course, we could satisfy this corner or this corner. Or some monotone path in between. But the claim would be that doesnt make that big a difference. Doing things later is always better. ts kind of a lazy property. Or you could think of this Originally actually this algorithm goes back in tree land. And in tree land, if you follow through this reduction, which we did, you know if you convert this into a treap, what this is saying is, look you search for an item, you follow a path. Take all the nodes on the path that you follow, rearrange them optimally for the future, build a heap based on the next access time, next touch time, whenever that happens to be. Next access time actually, in that case. That is equivalent to this algorithm. That seems like the right thing to do offline. You visit your item, you do the minimum amount you have to do, which is following the search path. You rearrange those items to be optimal for the future. ts an offline algorithm, seems like a really good one. The only thing its missing out on is maybe you should go off the path and bring other guys closer to the root. But if you believe, which we dont know how to prove, if you believe that theres no it doesnt really buy you anything to go off the path now, you can always do it later, then this algorithm is optimal offline. And in terms of the point set its in the point set of view its kind of nice because it almost looks online, right. You only have to look at each time and add the points at that time that are useful, in terms of the past. You only had to satisfy the rectangles of the past. o this is where things get interesting, because in terms of a tree view, with this transformation, this looks like an offline tree because it needs to know the future. And it does if you want to build a heap. OK, but if you look at in the geometric view, suddenly it looks online, because youre only looking at the points and all the points youve accessed in the past. o, in fact, theres a way to make this online. This is where things get interesting. There is a transformation that if you have an online satisfying point set, meaning you can decide what points to add based only on the past, not on the future. o its a fancier version of this transformation, you get an actual online binary search tree. o, this algorithm goes back to the 90s think. Actually 88, its in a thesis 1988 as rediscovered by my PhD adviser in 2000. ts totally natural algorithm, but they thought it was an offline algorithm. With this view, its an online algorithm. Let me quickly convince you, or sketch to you, how we make this online. o we do the same thing, except we dont know how to heapify, because we dont know the next access times. But we know whatever we touch, its some connected subtree of the root. We know what we touch. We touch whatever greedy tells us to touch. Well touch all these guys, whatever. How do we rearrange them? We dont rearrange them. We store them into something called a split tree. plit tree has the feature, its a tree. Binary search tree, and if you ask for some item x, you can move x to the root. o then you have a left subtree of x, right subtree of x, and then delete x, and now youre left with things that are less than x, and things that are greater than x. Things are greater than x. And you can do all this in constant amortize time. This is what we need, if you think about here. m going to take all these items that were touched, throw them into a split tree. y resulting structure will be a tree of split trees. o think of it as, when touch all these items just sort of throw them all in the root, but that root is represented by a split tree. And then hanging off here, there are other split trees which may have several keys in them. When do, when now touch a node, you can show that if m trying to touch some node here, cant just magically touch a node here. ve got to, had to have followed in the actual tree, whatever the optimal tree is, had to follow some route to leave path. o in fact, the predecessor and successor here had to have been touched before touched this one. Which means m going to split those nodes. o, when actually this is basically lazy evaluation. When actually access something in here, that means want to pull it to the root. wanted to pretend that it was at the root, thats what the treap would have done. o basically pull it up, be a root, split, because now theres two structures left where dont know their orders, but know that this item was first. And so end up with dont actually remove it, but remove it from the split trees, so make it look like this. And heres the guy where wanted to access something. o, split this root into two split trees, and have an individual node up here. f can do this in constant amortize time, its as if this node was at the root in the first place. And so simulate this perfect treap order, but using a data structure, split trees, which can actually be solved. How do you solve split trees? You just do the obvious thing, more or less. deas, you know red black trees, take your favorite balanced binary search tree, red black trees work fine. f youre given a node, you can split there. You basically just carve it in half. How much does it cost to split? Well if youre a little bit clever, let me go to a board of cleverness. The one thing we have to optimize for is, what if youre splitting like right here, very close to the left? f youre clever, youll search from here, and cut off this part in time basically proportional to the height of that tree. OK, you might get some propagation up here, but thats very small amortized. OK, on the other hand, if you search over here, youd like to spend only time proportional to this. o in general, if you just search in parallel here, you can split in lets say this has size n one, and the rest has size n two. You can split in order log the min of n one and n two. And you can show that if you just thats straightforward splitting, thats really easy to do. You can show that that implies constant amortized, because either youre cutting off a very little nibble and the cost is small, or youre cutting things more or less in half, but that cant happen very much. Then you charge to the fact that log n is going down by one, and overall you get a linear cost to splitting nodes. ts an amortization. Now the trouble is this is not a binary search tree. How in the heck do have to pointers that in parallel search? Well you have to take these two halves of the tree and interleave them to make them a binary search tree. ts kind of awkward, but just put it, mash it all together. Fold it in half, basically and turn it upside down, and youve got a binary search tree. o theres some messy stuff there, but thats just a handwavy argument that you can make greedy online. Next class well talk about lower bounds, which almost proved the greedy is optimal. But not quite.","Thats the mean heres the picture, you have a, actually picture is at the beginning a is x. o, here we have x, and then y is an ancestor, sorry, y is a descendant of x, because this is a. And then somehow we have to transition to the reverse picture, which is that y is the least common ancestor, x. And for this to happen, somebody here has to get rotated. The model is to do a search start at the root of the tree, of the binary search tree. o this is next touch time of x, this is the next time of y, and claim that there are no other points in here. The only points in a binary search if x and y are if y is a child of x, the only points in the binary tree that are between x and y, or the left subtree of y, or theres a symmetric case, but in this picture left subtree of y. But put an x here. o this can be achieved, and in fact, if you disallow rotations, so if change the model to forbid rotations, all get to do is set up some binary search tree, and then have to walk left, walk right, walk parent, from in that binary search tree to do this. o the question is say, after we do this, we make some root, which is going, the root is going to be among all the nodes that just got touched now, who is going to be touched next? o in the worst case this is going to be log n, but if look at my spacetime diagram here, if for example, if do sequential access, then this number is always one, and so get constant time. o we know that the next touch time of x is greater than or equal to the next touch time of its parent, of its ancestor, up to the root. Time j. We want to evaluate the cost of x j, and were basically going to look in guess one way to think of it is this cone. OK, heres time i, here is a. o, if a does not equal x, we know at time i, we must access all ancestor of x, we must touch all ancestors of x, before we touch x. Therefore, this is a point and were done, unless a equals x at this time, because then its the same point. o this is what you call the offline optimal, because here you basically get to choose the binary search tree algorithm to be customized to x, and yet somehow you want to achieve dynamic optimality. And then we look at that interface, and the claim is by the satisfaction property, actually this point should have been in the set. Heres time, heres space, m going to assume by symmetry x is to the left of y and and now were supposing the next touch time of y is earlier than the next touch time of x. o it looks like this. Least common ancestor has the property that x is less than or equal to a, is less than or equal to y. ommon ancestor, least common ancestor will be between x and y. And we also know that its an ancestor of x, and its an ancestor of y. o, if youre going to touch x, you must touch a. f youre going to touch y, you must touch a. o a comma i is going to be some point here, which is what we need. o, what we need to do is show that we can maintain its always going to be a binary search tree, because we only do rotations. o these are the operations you can do, and in this model really the only thing you can do is a search. OK, maybe should write down the words to go with that argument, but o, this part is empty by next touch of x being greater than next touch of y. And this part is empty else, or its empty because any key between x and y is in the left subtree of y, which would imply if its touched then so is y. But y cannot be touched, by assumption. All right thats what we claim, one of these two has to have a point in it. By that definition, this one will be at the root, but youre going to touch these nodes, and possibly you could rotate them. o how you do know if the root is still going to be some guy in that subtree? You just take this binary search tree, and apply this transformation, and it tells you if you have a binary search tree, you can turn it into a way to satisfy a point set. OK, now one of these is the one we actually want to access. o if youre always searching for the root of the tree, in particular, thats going to be good. o what the rest of todays lecture is going to be about is a geometric view, which ve kind of been hinting at with this picture, but its, theres more to it than just plot the points of searches. And so per search, if youre careful about how you do this, now this is not totally obvious in the model that ve set up, because said search always starts at the root. f we just count how many nodes are touched, then the cost of the binary search tree is equal to the cost of the minimum satisfied superset of your point. We can so it must actually be that at this time, at time i, a is right here. But if you believe, which we dont know how to prove, if you believe that theres no it doesnt really buy you anything to go off the path now, you can always do it later, then this algorithm is optimal offline. And really we just care about their order, so m going to relabel them to be one up to n. And say we search for x 1 first, then we search for x 2, and so on, up to x m. o, these indices are time, basically. Lets say this is that time i, we access key x, and this one is at time j, we access key y. Lets suppose y is greater than x, theres the symmetric picture. You only have to look at each time and add the points at that time that are useful, in terms of the past. Just so its clear what the name of this game is, we require the data to be stored in a binary search tree as its keys. o, what this tells you is the guy that has to be accessed next, in particular, must be in your set. ADENE: OK, but then how do you but didnt we change the next touch times of all the nodes in there? o, you want to build a binary search tree that doesnt know the future, it doesnt know what accesses are to come, but it has to be within a constant factor of the offline solution that does know. ts in this case because x is to the right of its parent it remains now to the right of its, what now becomes its child. You do this for x. x is the thing you search for, now x is up here, then you repeat. should mention, again this is not obvious how to do it with a binary search tree, but it can be done. And theres a nice theorem that if you have two binary search trees on the same, keys theres a way to get there with the linear number of rotates, so thats for free. We have this sequence x1 to x n, Theres no randomness here, but we just measure what is the fraction, what is sort of the observed probability for each of these x is. And the question becomes, is this the best you can do? This is great, that means the thing youre searching for is always at the root, and its basically free to touch. o, said OK, if you have two points and that rectangle is satisfied, there is some point, possibly in the interior, in that rectangle. f keep going, in the end can conclude that not only is this rectangle nonempty, but there has to be a point on one of these two sides, because if choose any other point, get a smaller rectangle. think what want to look at is, now, which is the moment were drawing this diagram, versus the next time of y. orry, so this is x comma now. We can define the cost of a binary search tree to be how many nodes does it touch. Theres a way, not only to see what the searches are, but to see what the binary search tree algorithm is. Turns out theres a geometric view, in which its actually very easy to see what the algorithm is, but it takes a little bit of work to prove that its the right thing. ADENE: o that you drew, and all the everything in that subtree root that gets taken to Like the picture on the right is also, its the same subtree right? o were left with one more case, which is that at time i, a is here, heres a, and at time j, a is here. o, what we want to do is find a minimum superset of this point set that is satisfied. ts like the obvious thing to do, which well get to, and we can actually make it online. f youre focusing your attention on k elements for a while, you only pay log k. But in general, for any access sequence you can compute, what is the working set costs, which is you sum over all the is of log t i, that is your total cost for the access sequence. o it looks if look at a key, and its going to be accessed next, it better be at the root. The worry would be that the next touch time of x is greater than the next touch time of y. o suppose next touch of x is greater than next touch of y. This would be a problem, because then you would not be a heap. For the other direction, if were given a point set that corresponds to, that has this satisfaction property, we have to somehow build from scratch a binary search tree. f can do this in constant amortize time, its as if this node was at the root in the first place. The one thing we have to optimize for is, what if youre splitting like right here, very close to the left? f you want to search for x, you search for x. You do a binary search for x in the tree. mean if you use a red black tree, and you search for one, then search for two, then search for n, youre not going to get constant amortized, because it doesnt realize this is happening. And the rough idea is, that if you access a key that is close in space to a key that was accessed recently in time, then your access should be fast. Theres a convenient theorem, if you have one tree and you want to convert it into another, you can always do it in a linear number of rotations. And then just say, what is the, what is the entropy of that distribution? o, this is this gives us a binary search tree that is so obviously optimal, where splay trees is kind of vaguely feels good, this is really obvious, yet we cant prove it. We want to argue that there is some other point in this rectangle. What we learn, what we know is the next access to x, is sometime in the future. What we know is that this can be done, this property can be achieved by a pointer machine data structure, which is more powerful than a binary search tree. We have to somehow use rotations to maintain this heap order property, and only touch the nodes that were supposed to touch. o before get to well the central question here is, is there one best binary search tree? The name of the game in dynamic optimality is to consider every case separately, and do as well as you can all the time. ntil x is either the root, and youre done, or its one child from the root, and then you do one rotation to make it the root. OK, but if you look at in the geometric view, suddenly it looks online, because youre only looking at the points and all the points youve accessed in the past. Easy way to see how to achieve it is, this is your binary search tree. This is the over all binary search trees, mean to be precise should talk about binary search tree algorithms, meaning you specify somehow how to do a search, and it may involve rotations and walks. o then you have this binary search tree, and then you access two, and then you do a rotation. OK, if look at, dont know, this one, it has another point in it. The cost of this access is one, the cost of this access is two, two, three. o this is where things get interesting, because in terms of a tree view, with this transformation, this looks like an offline tree because it needs to know the future. Binary search tree, and if you ask for some item x, you can move x to the root. Any suggestions, if you dont have the notes in front of you, of access sequences that we can do in less than log n time per operation. o, in our spacetime diagram we have some access at time i, we want to look backwards in time to the last time that key was accessed. And in tree land, if you follow through this reduction, which we did, you know if you convert this into a treap, what this is saying is, look you search for an item, you follow a path. f you dont need to do it in a binary search tree model, an easy way to do it is with whats called a level linked tree. OK, so this is saying that point set is treelike, if it has this property. Well you have to take these two halves of the tree and interleave them to make them a binary search tree. Now the trouble is this is not a binary search tree. o your adversary could always be, could always just look at your tree and say, Oh m going to pick anybody whos a log in depth or lower, and if youre going to start at the root and walk left and right to get there, youre going to have to pay log n time every single, every single operation. o, as long as there is some key in the recent past that is close in space, you take the min over all such keys, magically a unified structure has to find what is the most recent close item and search from there, and achieve log of that. OK, heres the plan, let a be the lowest common ancestor of x and y. x is a key, y is a key, they are nodes in the tree. o, want to know when is the point set a valid BT execution, and claim it is, if and only if, in every rectangle spanned by two points not on a common horizontal or vertical line, in every rectangle, there theres another point. And in terms of the point set its in the point set of view its kind of nice because it almost looks online, right. o, for example, at this moment all three of these nodes are going to be accessed at the same time. ts a little more subtle than that because were only counting distinct keys, and lets say you find this key, and this key is good because its temporal distance, this time, is only this big. o else x must be rotated at some time k, where k is between i and j. And if we set it up right, guess because here a was still x, and here a is y, then it must be strictly between. And we all know how to do order log in balanced binary search trees. o assuming this was an edge, x is OK, it was in the local treap, y somehow is going to be bad, because its next touch time was, should be higher. You can split in order log the min of n one and n two. o if get to see the x is then get to build a binary search tree. o think of it as, when touch all these items just sort of throw them all in the root, but that root is represented by a split tree. The cost of that algorithm, on that access sequence x, and you want the total cost of your algorithm on x to be within a constant factor of the optimal algorithm on x. x is a vector. o were doing x one x two up to x m. And we say, if we measure the key distance between our current access, x i and the previous access, x i minus 1. f that equals k, then ideally we should be able to do log k amortize per operation. And the claim is, that is globally the node that will be touched next. entral question here is, is there one binary search tree thats at least as good as all other binary search trees? Any balanced binary search tree is log n competitive, because best case you could hope for is constant, the worst case you can hope for is log n. o at least within the log factor, but we can do exponentially better. This is what we need, if you think about here. Then well have to do the reverse, actually build a binary search tree out of a point set. an you find the offline optimal, is whats the minimum number of points to add in order to make your point set arborally satisfied. s that the right way to think of it? Take all the nodes on the path that you follow, rearrange them optimally for the future, build a heap based on the next access time, next touch time, whenever that happens to be. erased the model, but in general with a binary search tree, we only have one pointer into the tree. f you look at any two points, this one and this one, they span a rectangle, theres another point in there. Then the other case is the zigzag case, y, w, x. o, here the two parent pointers are in different directions, one is left, one is right. o then you have a left subtree of x, right subtree of x, and then delete x, and now youre left with things that are less than x, and things that are greater than x. Things are greater than x. And you can do all this in constant amortize time. This is a pointer machine data structure, its not a binary search tree, because binary search trees we only allowed to do walk left, walk right, walk parent, But its an easy way to do dynamic finger. ADENE: o, this is for and access sequence, so every time corresponds to just seeking like one individually thing in the tree? This one, m going to add a point here, and this one, m going to add a point here. Then you charge to the fact that log n is going down by one, and overall you get a linear cost to splitting nodes. There is a transformation that if you have an online satisfying point set, meaning you can decide what points to add based only on the past, not on the future. o thats how found that point set, and in general claim is this is a good algorithm. o, lets say were, to make things a little clean, lets assume that the key is were storing are the numbers, integers, one up to n. o in particular theres n of them. f there were some other node down here that has a smaller next access time, then but we know that it was heap ordered before, so all of these guys are heap ordered. m going to add a point here, because thats the row, this is current time. We need to rearrange the tree to still be heap ordered by that new next touch time. aybe you set one of these nodes to have a very large next touch time, so its got to be really deep down there. That means that whoever were looking at, some point between x and y, has to be in this top tree. Before we go there, want to tell you about two structures, to binary search trees that we conjecture are dynamically optimal, but we cant prove it. ts a binary search tree because it has to be. Thats very easy, you just map it to the set of points where, guess m going to switch things on you. f this is my node x, and do rotate x, its going to be my definition of rotation, x moves to the root. ould be at the corner, or this corner, but one of those two sides has to have a point on it. And you can show theres always a tree where every node ends up at depth log one over p k. Again, not hard, but we wont do it here. What wed like, is that the total cost of all your accesses, so this is like amortized cost, is within a constant factor of the optimal.",0.135911052107534
24,24,"ANNONER: The following program is brought to you by altech. YAER ABOTAFA: Welcome back. Last time, we introduced the main result in learning theory, and there were two parts. The first part is to get a handle on the growth function m_H of N, which characterizes the hypothesis set H. And the way we got a handle on it is by introducing the idea of a break point, and then bounding the growth function in terms of a formula that depends on that break point. There was a simple recursion that you recall by this figure. And then we finally found the formula that upperbounds the growth function, given that it has a break point k. And its a combinatorial formula that is fairly easy to understand. And the most important aspect about it, as far as the theory is concerned, is that this is polynomial. t is bounded above by a polynomial in N, since k is a constant. And if you look at this, this is indeed a polynomial, and the maximum power you have in this expression is N to the k minus 1. o not only is it polynomial, but also the order of the polynomial depends on the break point. We were interested in the growth function, because it was our way of characterizing the redundancy that we need to understand, in order to be able to switch from the Hoeffding inequality to the V inequality. And the V inequality will be the case that handles the learning proper. And in order to do that, we looked at the bad events that are characterized by a small area according to Hoeffding. And then we went here, and looked at the redundancy that results from the fact that the different hypotheses have, by and large, overlapping bad regions. And the way to characterize this was through the growth function. And after an argument that took the redundancy and related it to the growth function, and then got rid of a technical problem with E_out, if you recall that one, we ended up switching completely from the Hoeffding inequality, which is the top one, into the V inequality, which is the final theoretical result in machine learning the characterization of generalization. And they are very similar except for a fundamental difference, which is here, and technical differences, which are in the constants. o as you go through the proof, you will have to change 2s into 4s, and epsilon into a smaller epsilon, and whatnot. But the main thing is that instead of the number of hypotheses , we were able to replace it by the growth function. And we had a final technical finesse, because we took the growth function not on N points, in spite of the fact that we have only N points in the sample. We needed to have 2N in order to have another sample and carry the argument, not for the single sample that we have, but for the difference between two samples. And that got rid of the technicality that we alluded to, which is the role of E_out that really destroys the utility of the growth function, because the growth function depends on dichotomies, and the E_out depends on the full hypothesis itself. o this is where we stand. n todays lecture, am going to put this together in the main notion of the theory, which is the V dimension. t will not be a new notion for you. ts very much related to the break point. But it is the quantity that you are going to remember from all of this after a while. o you may forget about the recursion. You may forget about the growth function. But you will remember the V dimension. And when you are in a learning situation, you ask yourself, what is the V dimension? s it 7 or 10? And then you say, oh, this guy is using a hypothesis set with V dimension 5000. He must be crazy. And so on. o this will be the currency we use out of the theory, in order to use in a real learning situation. The topics for today, first m going to give the definition, this will be an easy definition. And m going to discuss it a little bit, to make sure that everybody understands it. And then we are going to spend some time getting the V dimension of perceptrons. We will be able to compute the V dimension exactly for perceptrons, in anydimensional space. t doesnt have to be twodimensional space, like the one we did before. You take any dimension, and we will get the value of the V dimension exactly. This is a rare case. Because usually, when we get the V dimension, we get a bound on the V dimension, just out of the practicality of the situation. But here, well be able to get it exactly. And that will help us going through the interpretation of the V dimension. Well ask ourselves. Now we understand it, and we compute it for a complete case that we are familiar with. Then, we would like to understand, what does it signify? How do we apply it in practice? And this will be the subject of the interpretation. Finally, will spend the last few minutes of the lecture transforming the theory into a form that is extremely simple, and its very easy to remember. And this is the one that will survive with us for the rest of the course, and well be able to relate it to different theories and techniques as we go. o lets start with the definition. The V dimension is a quantity that is defined for a hypothesis set. You give me hypothesis set, return a number which call the V dimension. And the notation for it will be d, as in dimension, sub V as in Vapnikhervonenkis. And it is applied to H. And every now and then, we will drop the dependency on H, where it is clear from the context. o we dont have to carry this long notation. We will just say d_V, and well understand that this is the V dimension. What is it? n words, it is the most points you can shatter. That is not a foreign notion for us. o if you can shatter 20 points, and that is the most you can do, then the V dimension is 20. n terms of the technical quantities we defined, this will be the largest value of N such that the growth function is 2 to the N. o if you go one above, the 2 to the N will be broken. You can think, if V dimension is the maximum, next one must be a break point. And that is, indeed, the case. The most important thing to realize is that we are talking about the most points you can shatter. t doesnt guarantee that every N points lets say that the V dimension is N. t doesnt say that every N points can be shattered. All you need is one set of N points that can be shattered, in order to say that you can shatter N points. That has always been the case in our analysis. Lets try to take this definition and interpret it. Lets say that computed the V dimension. told you the V dimension in this case is 15. Now, what can you say about N which is at most 15, in terms of the ability to shatter or not? You can say that if N is at most 15, the V dimension, then H is guaranteed to be able to shatter N points. Which N points havent said, but there has to be N points which the hypothesis set can shatter. Why is that? ts simply because, since the V dimension is this number, there will be that many points that can be shattered. Well, any subset of them will have to be shattered as well. Therefore, a smaller number will also be shattered. Which means that if N is smaller, you can shatter it. The other direction is also meaningful. f N is greater than the V dimension, now the statement is strong N is a break point. You cannot shatter any set of that many points. Because by definition, the V dimension was the maximum. And although called it N here, we used to call it small k. o anything above the V dimension is a break point, and anything below, you can shatter. Very simple notion. f you look at the growth function in terms of the V dimension, when we had the break points, in terms of this k, we were able to find the bound that showed in the review. o we know that the growth function is bounded above by this formula, and k appears here for the index of summation, which gives us the maximum power of this formula. Now, in terms of the V dimension, its not a big deal because the smallest break point is 1 above the V dimension. o all you need to do is substitute, and you will get this formula involving the V dimension. The V dimension is unique, because its the maximum. o you get that number. And now you can say that the growth function, for any hypothesis set that has V dimension d_V, is bounded above by this. A nicer formula than this, because it doesnt have the annoying 1. And furthermore, when you look at the maximum power in this polynomial, the maximum power happens to be N to the V dimension. o the V dimension will also serve as the order of the polynomial, that bounds the growth function of a hypothesis set that has that V dimension. All of this is very simple. Now, lets take examples in order to get the V dimension in some cases. And you have seen this before. Remember positive rays? How many points can positive rays shatter? Just one point, right? This is where you can get all possible patterns. f you get two, its a break point. Remember that argument? Therefore, V dimension here is 1. Good. How about 2D perceptrons? How many can we shatter? We remember that constellation. f we have three points in that position, then we can get all possible patterns. And 4 is a break point, so we cannot go up. Therefore, the V dimension is 3. onvex sets. What is the V dimension of convex sets in two dimensions, the example that we gave before? We had this funny construction where, if you choose your points on the perimeter of a circle, you can shatter any number of points. Right? Therefore, what would be the V dimension in this case? t will be infinite. There is no maximum. Now, we said before that this one is particularly pessimistic, which indeed it is, because this is a very specific way of getting the points. And in all of the analysis, you will find that using the V dimension always gives an upper bound worst case. You cannot violate it, but you can do better at times. Here, for example, you can do better if you choose the points, lets say, uniformly over a space. Therefore, some of them will be internal points. And therefore, the corresponding growth function, which can be defined in this case, will not be 2 to the N, and you may be able to learn. Now, lets look at the V dimension as it relates to learning. This is an important viewgraph. When we talk about learning, we have to go back to our friend, the learning diagram. And in case you forgot it, let me magnify it a little bit. There are different components. And we have studied the matter so well, that we now can relate more to this. Remember, this is the target function, gives me the examples. Learning algorithm picks the hypothesis, puts it as the final one. We hope that the final hypothesis approximates this guy. And we introduced this thing, in order to get the probabilistic analysis. We have seen this before. Now, lets look at this diagram, and see what the V dimension says. The main result is that if the V dimension is finite thats all you are asking then, now the green final hypothesis will generalize. That we have established by the theory. o you dont even need to know its value. You just need to know its finite, and then you can say the g will generalize. That we have in the bag. Now, d like to understand the rest of the diagram, in terms of the V dimension. We can understand the green part. Here, well generalize to the target function, for better or for worse. t could be doing very poorly in the insample, and that will generalize. Or could be doing great in the insample, and that will generalize. We are only talking about generalization here. Now, this statement is independent of the learning algorithm. Why is that? Because the learning algorithm here, if it picks a hypothesis, it will have to pick it from the hypothesis set. We have gone through all of this trouble, in order to guarantee that generalization will happen, uniformly, regardless of which hypothesis you pick. Therefore, you can find the craziest learning algorithm, and it can pick anything you want, and you still can make the statement about the final hypothesis. o the learning algorithm doesnt matter, as far as generalization is concerned. Lets punish it by graying it out. Now, its also independent of the input distribution. This is the box. This was technically introduced in order to get Hoeffding. And obviously, it has to survive in order to get the V inequality. The reason am talking about the independence here is because of an interesting point. We mentioned that when we define the growth function or the V dimension, give you the budget N and then you choose the points any way you want, with a view to maximizing the dichotomies, right? o now there is no probability distribution that can beat you. can pick the weirdest probability distribution that has preferences for funny points, and your choice of the points will be fine, because you chose the points that maximize. o whatever the probability distribution does, you will be doing more. And therefore, your bound will hold. Therefore, we dont have to worry about probability distributions. The learning statement, that this guy will generalize, will hold for any probability distribution. o another guy bites the dust. Now, you look at this and then there is a third guy which is an obvious one, which is the target function. All of this analysis, the target function didnt matter at all as far as generalization is concerned. We are generalizing to it, but we dont care what it is. As long as it generates the examples we learn from, and then we test on it, thats all we care about. The generalization statement will hold. o it also goes. o now as far as the V theory is concerned, we really have three blocks. The first one is the hypothesis. That is the one that we are claiming the generalization with respect to. Thats number one. The hypothesis set is where we define the V dimension. And if you remember very early on, told you that the hypothesis set is a little bit of an artificial notion to introduce as part of the learning diagram. And said that we are going to introduce it because there is no downside. There is no loss of generality, which is true. And there is an upside for the theory. Now, you can see the upside. The entire V theory deals with the hypothesis set by itself. Thats whats has a V dimension, and thats what will tell you, you are able to generalize. The rest of the guys that are more intuitive, that disappeared here, are not relevant to that theory. Now, that training examples are left, because the statement that involves the V dimension is a probabilistic statement. t says that, with high probability, you will generalize. With high probability with respect to what? ts with respect to generating the data. You may get a very unlucky data set, for which you are not going to generalize. The guarantee is that this happens with a very small probability. o this remains here, just because it is part of the statement. And this triangle is where the V inequality lives. Now we go into a fun thing of computing the V dimension for the perceptrons. There are two goals for doing this. Well do it exactly. Well get exact formula for it. One thing is to test your understanding of the definition. The definition is a little bit tricky because give you N. You choose the points any which way. You maximize this. This is a bound, so what is minimum, what is maximum, and whatnot, may be a little bit fuzzy. And trying to get the number for a particular case will seal the deal. o thats number one. Number two is that, because we understand the perceptron model so well, we will be able to get the result, which is the V dimension of perceptrons. And that will give us insight into what the V dimension signifies. And that will set the stage, when we go to interpreting the V dimension. o this is an important part, that will take a little bit of analysis. For the twodimensional perceptron, we already have done the exercise, and we got the V dimension to be 3. Right? Now, if you go for the general case and you have ddimensional space, you expect the V dimension to be more. Because even if you just go to three dimensions, the troublesome case of four points, that we had before, is very easily shattered in this case. Just pick the points not on a plane. And remember the problem with those guys is that if you want these guys to be 1 and these guys to be +1, it was a problem for the plane. Now, you can very easily separate any two points from the other two points and you can shatter four points. o the V dimension went up for sure. And we ask ourselves: how much did it go up? Well, it turns out to be a very simple formula. The V dimension of perceptrons is exactly d plus 1. Now we need to prove that. And we are going to prove it in two stages, very simple stages. One of them is that we are going to show that the V dimension is at most d plus 1. Then, we are going to show that the V dimension is at least d plus 1. And that leaves the single possibility that the V dimension is d plus 1. o lets go. Here is one direction. And by the way, pay attention because am going to give you a quiz in the middle of the argument, to make sure that you are paying attention. This is for real. And for the online audience as well. Here is the first direction. am going to construct a specific set of N points, and that N in this case is d plus 1, because thats the number of points want to shatter. And am going to construct them in the ddimensional Euclidean space, R to the d. am going to construct them with a view to being able to shatter them. o get to choose the points, which is my privilege. As long as can shatter them, we are OK. o what are these points? am going to construct them using a matrix. And you have seen this matrix before. Remember our old friend linear regression? We actually set the input points in linear regression this way, in order to get the algorithm, the pseudoinverse, and all of that. And in the case of linear regression, this was a very tall matrix where this is one data point, which means that its d plus 1 dimensional vector. The 1 dimension is the constant x_0, which is the constant +1 we add to take care of the threshold. And then the rest of the dimensions, from 1 to d, are actually the coordinates of the point. o we put these. And this is one data point. This is the second. This is the third. And usually, since we have many, many more points than dimensions, this is a tall matrix. n this case, am choosing N to be exactly d plus 1. And since we already established that this is d plus 1, this is actually a square matrix in this case. But thats all need for the purpose that am after here. need to give you the identity of these guys. What are these guys? These guys look like this. This is no mystery. ee, if you look at the first column, its all 1s. Well, it has to be. Thats dictated. That is the constant coordinate. t has to be +1. f want a legitimate point in this representation, the d plus 1 dimensional representation, the first coordinate has to be 1. The rest of the guys, chose the simplest possible form can imagine. You have basically a diagonal matrix here, and added all 0s here. o these are the guys that are my data set. Now, you can see that chose them such that X is invertible, because that is the technique m going to use in order to be able to shatter them. You will see in a moment. Do know that this is invertible? Yes, the determinant is 1, actually. And that means its invertible. an you compute the determinant? This is 1. And then every time you have this guy, you have the 0 term wiping out everything. o get a 1. o this is an invertible matrix. Why am interested in an invertible matrix? Because we can shatter the data set. This is how we are going to do it. Look at any set of labels you want. o this is a dichotomy. This is the value at the first one, +1 or 1, +1 or 1, and +1 or 1, on the x points that just showed you. All of these could be any pattern of +1 or 1s. would like to tell you that any dichotomy you pick from this +1, +1, 1, 1, +1, et cetera, can find a perceptron that realizes this dichotomy. f do that, then have showed you that can shatter the set. Let us look for the w that satisfies and what does it satisfy? t satisfies this condition. This computes the signal for all the points at once, in vector form. You take the sign of that. And you would like the sign of that to agree with the particular y you chose. o you give me y, am supposed to come up with w, such that this holds. f can do that for every choice of y you give me, then am done. have shattered your set. Or, my set, the set chose. How am going to do that? Oh, its pretty easy. What am going to do, am going to do even better than this. am going to actually have X w numerically equal y, even before taking the sign. o when you multiply the matrix X by w, you are going to get specifically a pattern of +1 or 1s. Well, if you get +1 or 1, guess what happens when you take the sign of that? Youll get the same thing, +1 or 1. o that will satisfy that. But that is easy to handle, because now have algebra working for me. Remember that X was invertible. Thats pretty easy. o all you do is just solve for it. w would be the inverse of X times y. And you have a solution that realizes any dichotomy you can think of. Thats wonderful. o we were able to shatter d plus 1 points. Now comes the quiz. We can shatter these points. Wonderful. But we are not really interested in shattering for its own sake. We were trying to establish the value of the V dimension. o lets see what we have established. showed you particular d plus 1 points. showed you that we can shatter them. What is the conclusion? s it: oh, we have established the V dimension is d plus 1? Thank you. Or, oh, we only established that its greater than or equal to d plus 1? Wait a minute. We actually established that it is less than or equal to d plus 1. Or maybe we didnt establish anything at all, as far as the value of the V dimension. ask you to think about it, and tell me which of those can we conclude? And d like the online audience to text a, b, c, or d, as if you are solving a homework. And tell me, which of these choices is a valid conclusion given what we have argued? o lets say by shouting. You just shout a or b or c or d. And hope that there is enough signal that will be able to decipher the majority. hout. ADENE: b. PROFEOR: OK. You guys are a tough crowd! Well, why is that? We were able to shatter d plus 1 points. o we are guaranteed that, for at least d plus 1 points, we are OK. t is conceivable that we can shatter a bigger set. We havent argued that yet. But if we even fail, at least we have the V dimension to be at least d plus 1. f we shatter a higher set, it will be even bigger. f we cannot, it will be equal. o that is what we have established. ince you are very good at this, lets do another quiz. o have now greater than or equal to d plus 1. Now need to show that the V dimension is less than or equal to d plus 1. wonder what need to do, in order to achieve that. We need to show that one of several choices. Oh, need to show that there are some points, a set of d plus 1 points, that we cannot shatter. No, no. need to show that there is a set of d plus 2 points that cannot shatter. Oh. No, no. aybe we need to show that we cannot shatter any set of d plus 1 points. Or, was it d plus 2? m confused now. What among those guys will establish the premise? The premise that we are trying to establish is that the V dimension is at most d plus 1. Which of these statements will establish that? Again, think about it. And similarly, for the online audience to text the result. ll give you 10 seconds. And then, well also answer by shouting. OK, shout. ADENE: d. PROFEOR: like this. How about the online audience? d, OK. o everybody gets the idea. Now, we know what we want to prove. Lets go ahead and prove it. Now its a question of any d plus 2 points. o dont get to choose the points, you get to choose them. o tell you, choose them please. And give them to me. When you give me your points, am going to make a statement about the points you chose. Well, how can make a statement? You chose them any which way. m going to make a statement. am going to say that, for those particular points that you chose, which really dont know what they are, can say that you have more points than dimensions. Why is that? Each of these guys still comes from a d plus 1 vector, right? Because its a ddimensional space plus the added coordinate. o each one is d plus 1 dimensions. And asked you to give me d plus 2. o obviously, d plus 2 is bigger than d plus 1. What do you know when you have more vectors than dimensions? Oh, know that they must be linearly dependent. Therefore, we must have that one of them will be a linear combination from the rest of the guys. o you take j, whichever it might be, and this will be equal to the sum over the rest of the guys of some coefficients that dont know, times those guys. This will apply to any set you choose. And this is the property that am actually going to use, in order to establish what want. Furthermore, can actually make something about the a_is. The a_is could be anything for this statement to hold. But m going to claim that not all of them are zeroes, in this case. At least some of the a_is are nonzero. How do know that? This is not part of the linear dependence. This is actually because of the particular form of these guys, where the first coordinate of all of these guys is always 1. o when you look at this and apply it to the first coordinate, 1 equals well, these cannot all be zeroes because it has to add up to 1. Therefore, some of the a_is will be nonzero. Thats all need. need this to hold. need some of the a_is not to be 0. Everybody buys that this is the case? Thats all need. And then we go and show the dichotomy that you cannot implement. We have that, right? onsider the following dichotomy. am going to take the x_is corresponding to nonzero a_is. ome of the a_is are nonzero for sure. aybe some of them are zeroes. am going to focus only on the nonzero guys. dont care what you do with the terms that have a_i equals 0. Do whatever you want. Give them any +1 or 1. Lets not look at them. m looking at those guys, and am now constructing a dichotomy that am going to show you that you cannot implement using a perceptron. o for the x_is with the nonzero a_i, am going to give the label, which happens to be the sign of that coefficient. That is a nonzero number. ts positive or negative. o will give it +1 or 1 according to whether its positive or negative. And will do that for every nonzero term here. Everybody sees that? And now m going to complete the dichotomy, by telling you what will happen with x_j. m going to require that x_j goes to 1. Now, all you need to realize is that this is a dichotomy. These are values of +1 or 1 on specific points. The other guys, which happened to be 0, give them any +1 or 1. You choose. And for the final guy which is sitting here, am going to it 1. This is a legitimate dichotomy. And m going to show you that you cannot implement this particular one. How is that? Because really dont know your points. o must be using just that algebraic property, in order to find this. And the idea is very simple. This is the form we have, x_j happens to be the linear sum of these guys. am going to multiply by any w. For any w the perceptrons, you multiply by w. That is what makes it a perceptron. o m going to multiply by it. And then realize that w transposed times x_j, which would actually be the signal for the last guy, is actually the sum of the signals for the different guys, with these coefficients. That has to happen. o what is the problem? The problem is that, when you take this as your perceptron, then by definition the label is the sign of this quantity. For the guys where a_i is nonzero, we force this quantity, which is the value y_i, to be the sign of a_i. Thats what we constructed. What can you conclude given that the sign of this fellow is the same as the sign of this fellow? t must be that these guys agree in sign. They are either both positive, or both negative, right? Therefore, can conclude that if you multiply them, you get something positive. That is for sure. o now have a handle on this term. This forces the sum of these guys to be greater than 0. Why is that? Because this happens for every nonzero a_i. For zero a_is, they dont contribute anything here. o if add up a bunch of positive numbers and zeroes, am going to get a positive number. What is this quantity? Do see it anywhere else on the slide? Oh, yeah, can see it here. This actually is the signal on the outstanding point. o know that the signal on the outstanding point is positive. What does this force the value of the perceptron, your perceptron, the one you had here, to be? t will have to be +1. Therefore, its impossible to get that to be 1, if you chose this. This is a choice that is legitimate. t is a dichotomy. And now if you pick those guys, pick the rest of the zerocoefficient guys any way you want, you are forbidden from having this as 1. Therefore, you cannot shatter your set, for any set you choose. Therefore, you cannot shatter any set of d plus 2 points. And have the result. o lets put it together. First, we showed that the V dimension is at most d plus 1. And then we showed that its at least d plus 1. Or actually, did we do it this way, or the other way around? Thats another quiz. No, its not another quiz! The conclusion is that the V dimension is d plus 1. And now, ddimensional perceptron the V dimension is d plus 1. Lets ask ourselves the simple question: what is exactly d plus 1 in a ddimensional perceptron? ts one above the dimensions. You can find many interpretations for it. But the interpretation of interest to us will be the fact that this is actually the number of parameters in the perceptron model. What are the parameters in the perceptron model? We used to call it the vector w. Lets spell it out, in order to count. This happens to be w. w_0, this is the one for the threshold, w_1 up to w_d. These are the parameters you are free to choose, and that we have been choosing through this argument. And how many of them there are? d plus 1. Why am attaching the V dimension to the number of parameters? The V dimension gives me the maximum number of points can shatter. o now can do any which way. The reason can do any which way, because have a bunch of parameters that can set one way or the other, in order to achieve that. o it stands to logic that when have more parameters, will have a higher V dimension. And that will be the basic part of the interpretation that we are going to go into. o lets do now the interpretation of the V dimension. Look at this we are going to prove two things. No, not prove, but show two things in terms of the interpretation. One of them, we understand the mathematical definition of the V dimension. What does it signify? Thats number one. And it will relate to the number of parameters. And well get it a little bit more elaborately. The second one, know what it signifies. s it at all useful for me? am a learning person. went through the theory, just because you asked us to do that. But when all is said and done, just care about the result and how m going to use it in practice. How can we apply the value of the V dimension in practice? Thats number two. These are the two parts of the interpretation. The main idea of understanding what the V dimension signifies, is to look at the degrees of freedom. What is that? When you have a model, the model is characterized by a set of hypotheses. You get one hypothesis or another, by setting the set of parameters one way or another. o the parameters give you degrees of freedom, in order to create one hypothesis or another hypothesis. o think of this picture. Think of the parameters as knobs. o this is w_0, w_1, w_2, et cetera. When you are actually having a hypothesis set, you are given this. And you are able to set the knobs any way you want. ncrease the volume, decrease this, et cetera. Just do this, and you get a setting that tells you what the hypothesis is. These are, obviously, degrees of freedom. And it actually has a pleasant thing. Because lets say you are buying a bigtime audio system. sually, if you are not very much into stereo and stuff, you want a couple of knobs, and youll adjust them and get it right. f give you 17 channels and this and that, and give you this panel, thats great, if you know how to use it. f you dont know how to use it ah. What? cant. o now the problem that we are facing is actually here. Because now, the specification that is needed in order to get you to pick the right hypothesis, is pretty elaborate. You need a lot of examples here. That is the relation we are going to see. Now, the parameters happen to be analog degrees of freedom. When talk about w_0, w_0 can assume a continuous value from R, and it matters if you pick a different threshold, you will get to a different perceptron. t will return different values for parts of the space. o this is, genuinely, degrees of freedom that happen to be analog. The great thing about the V dimension is that, it translated those into binary, if you will, degrees of freedom. Because all you are trying to do is get a dichotomy. o am asking myself: when am free to get any dichotomy want? For any point, can get the +1 or can get 1, independently of the second point. get +1 or 1 all the way. o you can keep adding the points, and see how far you can get away. And the maximum you can get is the V dimension. o by the time you get there, you have that number, which is the V dimension degrees of freedom. But they are binary degrees of freedom, which is what matters here, because inside the box that tells you its a perceptron or a neural network or something like that, there may be parameters playing around, and whatnot. As far as am concerned, all m interested in, how expressive is this model? How many different things can actually get. o the V dimension now abstracts whatever the mathematics that goes inside, et cetera. You go outside. And if can shatter 20 points, thats good. f someone else can shatter 30 points, they have more degrees of freedom to be able to do that, regardless of where this came from. o now, lets look at the usual suspects, and see if the correspondence between degrees of freedom and V dimension holds. Who are the usual suspects? think this is the last lecture where were going to see the positive rays and the rest of the gang. o dont despair! o, positive rays. What are positive rays? What is the V dimension? That was 1. can shatter at most one point. What were positive rays in the first place? Oh, yeah, that was the diagram. We had this thing, and present +1 here, and we take 1 here. And what determines one hypothesis versus the other within this model is the choice of a. Oh, the choice of a? One parameter, one degree of freedom corresponds to V dimension equals 1. Thats nice. Lets see if this survives. Positive intervals. OK, the positive intervals. The V dimension was 2. That is the most can shatter. What do they look like? Oh, in this case. have this guy, the small blue guy. And there is a beginning and an end. And between them return +1, and here return 1. o, depending on the choice of the beginning and the end, will get one hypothesis versus another. How many parameters, or how many degrees of freedom? 2. And what is the V dimension? 2. Then by induction, its true. No, induction is not true! This is just to illustrate the idea. Now, lets go back and contradict ourselves. ts not just parameters. ts really degrees of freedom. And d like to make the distinction. o lets take an example, where the parameters are not contributing to degrees of freedom. ll construct an artificial example just to give you the idea. n more complicated models, it may be difficult to argue what is a parameter that is contributing and what is not, but at least we are establishing the principle that a parameter may not necessarily contribute a degree of freedom. And since the V dimension is a bottom line it looks at what you were able to achieve, it will be a more reliable way of measuring what is the actual degrees of freedom you have, instead of going through the analysis of your model. Lets take onedimensional perceptron. Very simple model. You have only one variable and you are going to give it a weight, thats one parameter. And then you are going to compare it with a threshold w_0, thats the second parameter. And then you are going to give me +1 or 1. o this fellow has two parameters. ndeed, two degrees of freedom. And get the V dimension to be 2, because its d plus 1. We proved it generally. And here, d the dimensionality of the space, is 1. This is just a real number. Now, this is not my model. Actually, this is only part of the model. What m going to do, am going to take that output, and feed it into a perceptron. And then get that output, and feed it into a perceptron. And then get that output, and feed it into yet another perceptron. And that will give me the output of the model. Now, lets see how many parameters have. This guy has 2, one for the weight here and one for the threshold. Whatever the output here, gets weighted by something. Thats a third parameter. ompared to a threshold fourth. Fifth, sixth, seventh, eighth. have eight parameters in this model. Anybody would argue at all about this? have eight parameters. Theres no question about it. Do get eight degrees of freedom? No, because these guys are horribly redundant. By the time did this, am done. This doesnt add anything. Take +1 or 1, give it a weight, compare to a threshold, what are you going to get? You are either going to get +1 for +1, and 1 for 1, or the viceversa. o we are just replicating a function, and doing it again, and again, and again. This whole thing is a very, very elaborate perceptron in one dimension. Thats all. know that constructed it in a very funny way, but thats the function it does. o if you are counting the number of parameters, you will say have a bunch of parameters. But if you are resorting to the V dimension, you dont care about this box. You dont even know it. ts a black box. You look at x and y and ask yourself, how many points can shatter? And you get the answer that you will get for one of these blocks by itself. The rest of the guys dont matter. o you can think of now the V dimension as measuring the effective number of parameters, rather than the raw number of parameters. And gave you a case, where the effective number of parameters is smaller, which seems to be the case. Believe it or not, you can construct, mathematically, a case where you have one parameter, literal parameter. A number that is a real number and then you can milk out of it so many degrees of freedom, that you can get more than one degree to freedom. any more than the degree of freedom, from one parameter. But the other case is really you construct it because you want to. But the message here is that, you dont look at the number of parameters, you look at the effective number of parameters. And effective, for us, is as far as the result. And the result is captured by the V dimension, so this is our quantity for measuring the degrees of freedom. Now, lets look at the number of data points needed, which a practitioner would be interested in, and doesnt care about the rest of the story that told you. o you have a system. Lets say that you manage a learning system, and you look at the hypothesis set, and you say V dimension is 7. want a certain performance. ould you please tell me how many examples need? First, we know that the most important theoretical thing is the fact that there is a V dimension, finite one. t means that you can learn. That is the biggest achievement that we have done in theory. But now we go a little bit closer. And ask yourself, the value of the V dimension, how does it affect the number of examples you need? n order to do that, lets do the following. We notice that the V inequality, in which the V dimension arose, has two small quantities performance quantities that wed like to be small. Lets remind ourselves. One of them is this fellow. You didnt want E_in to be far from E_out. o you said that they should be tracking, within epsilon. And therefore, the probability that they are not tracking within epsilon, the small number, should be small. The probability is small. The other quantity is the other small quantity. And we are just going to call it something now. ts a small quantity, delta. ts not small because of the expression, expression is long! ts small in value. Hopefully that when you get large N, this will reduce to a small number. o were just going to call it delta for now. Try to phase out the details of this and look at: have two quantities. This is the probability. This is the approximation. And we are making a statement that is probably approximately correct, as we said before. These are our two guys. n a normal situation, what you are trying to do is that you are trying to say, want a particular epsilon and delta. want to be at most 10% away from E_out. And want that statement to be correct 95% of the time. Thats your starting point. And then after you said that, you ask yourself, how many examples do need? Fair enough. When you say want to be 10% away from E_out, what you are really saying is that epsilon is 0.1. When you say that want to be 95% sure that the statement is correct, you are picking delta to be 5%, or 0.05. o thats what you do. You want certain epsilon and delta. And then you ask yourself, how does N depend on the V dimension? You are competing with someone else. You are solving the same problem. The guy gives you the same data. And you look at it and you say, am using a V dimension and they are using a V dimension. f achieve this with this V dimension, can he also achieve it with the bigger V dimension? Because a bigger V dimension will give them more flexibility. They might be able to fit the data better. They will get a better E_in. o if they can get the same generalization bound, they are better off. o really am interested in this question. just want to know how they relate. n order to do this, we are going to look at this function, which is a polynomial. A very simple polynomial, just one term. ts not polynomial. ts monomial guess. N to the d. m just writing it as d. t will play the role of the V dimension. just want the notation to be simple. o N to a certain power times e to the minus N. What is this quantity? This quantity is a caricature of this quantity. There are constants here. have here multiple terms. m just taking the biggest of them, because its the dominant. have a negative exponential, but it has this terribly damping coefficients, et cetera. am leaving them out. am just trying to understand the behavior of functions that look like this. am taking the simplest case, because this will give me the tradeoff between d and N, which is the tradeoff want here. o lets look at it. This is our quantity. And what am going to do, am going to plot it for you. Lets plot it for the case of d, the power, equals 4. Here is what it looks like. Basically, the initial part is mostly N to the 4. f there wasnt exponential, this would be going up and up and up and up. The negative exponential is just warming up here. And then it starts becoming really, really effective here. And then it wins over, and keeps doing it. And by then, you will forget that it was N to the 4. You will just remember that there is a negative exponential. And the interesting part here is, obviously, this will play the role of the righthand side of the V inequality. o this will be a probability. And wed like the probability to be small. o obviously, the initial part of the curve is completely meaningless. You tell the probability the probability now is less than 5. Thats very nice. But its only interesting once you cross back to the interesting region, which is less than 1. And then you are actually making a statement. o this will be the interesting part. Lets look at the next one. This is N to the 4. Lets look at N to the 5, just to get a feel for these quantities. h! Well, this one it will peak at a different point. But the main point is, its huge now. The probability now is, what? Less than 20 something. Thats nice. But eventually, the exponential wins. Thats the good part. o it goes down, and then it goes here. You can see that this is the interesting region. And m going to ask myself, in order to get to this region, which is the performance you want, how many examples, which is this coordinate, do need given the different V dimensions, which supposedly is 4 here and 5 here? Again, this is a caricature, because this is not the function have. f you start adding the real constants here, this thing, instead of becoming what, 5 examples, 10 examples? t probably will be 5,000 example, 10,000 example. ts a pessimistic estimate, because its an upper bound. But the shape will remain the same. And similarly, if you add the other probabilities, the probability will not be 1. t will be 2, et cetera, but all you need to do is just shift a little bit, and you will be getting here. o if you understand this quantity, you will be able to translate it to the other one. But because its going up so fast, have to do something to make it visible. And ll do that in a second. o lets now look at it. You fix the value here at a small value. Whatever the value is. Not 1, maybe 0.5 or 0.1 or 0.01, et cetera. And you would like to see how N changes with d. What am going to now, am going to switch plots on you. And am going to have the ycoordinate here, which is the probability, drawn on a logarithmic scale. The reason m doing that is because, obviously, if know N equals 4 and N equals 5 get me that, if get to N equals 10, that will go upstairs. o really want to keep a handle on it. o am going to do this in order to keep a handle on it. And more importantly, because this is really you see this very, very thin slice? Thats all m interested in. o want to magnify it and look at it. And this happens to be less than the value of 1. o all the negative logarithms will be here, and ll be able to look at it clearly. o lets go and plot the N equals 5 on a logarithmic scale. Thats what it looks like. This is exactly can afford to have more examples because will have more curves. But this is the blue curve we had before. t peaks at a certain point and then goes down. And 10 to the 0 here, thats the probability 1. This is the interesting region. Below the line here is the interesting region that, when you tell me what delta is, you are telling me a level. And these levels are not going to be very much different. On this scale, this is 1. This is, what? This is 10 to the minus 5. Thats a very, very, very small probability. o the play here is very small, as you vary delta significantly. And the play with epsilon, which will affect the exponent, is that these guys will be spread. nstead of being 20, 40, it will be 2000, 4000, and so on. But this will be the shape. o now, lets add the other curve. o this is N to the 5. How about N to the 10. What does it look like? Now, it didnt go upstairs. Well, it went upstairs if you had it linearly, because now, look at the value of the top. This is really serious business. And you get this one. Varying the V dimension, am getting a different curve, and m getting the behavior in the interesting region. You see the point here. And keep adding. and will add up to 30. o these are the curves that get by varying the V dimension, the alleged V dimension here. 5, 10, 15, 20, 25, 30. omething very nice is observable here. These guys are extremely regular in their progression. They are very much linear. mean, they are not exactly linear, but very close to linear. And indeed, you will find that, theoretically, the bound in terms of the number of examples to achieve a certain level lets say you cut the level here, and you are increasing the V dimension. t basically is proportional to the V dimension. You go from 5 to 10 to 15 to 20, et cetera. This is the number of examples you want. And they are pretty much proportional to this. Now, this is in terms of the bound. The problem with the bound is that, if am using a system and the other guy is using a system, have a V dimension and he has his. Now, know that my performance is less than or equal to something, and his performance is less than or equal to something. And lets say that this thing is this way. He has a better bound than mine. Thats the bound. There is no guarantee that, when we get the actual quantity that is bounded, they will also follow that same monotonicity. t is conceivable that the two quantities are this way. The bounds were this way. The quantities are this way. nder those conditions, the bounds are satisfied, correct? And the bounds are monotonic in one direction, and the quantity is monotonic in the other. That is a pretty annoying feature. o what am going to make now is a statement that is not a mathematical statement, but a practical observation. That is almost as good as a mathematical statement. The practical observation is that, the actual quantity we are trying to bound follows the same monotonicity as the bound. That is the observation. You use bigger V dimension, the quantities you will get are bigger. And actually, very close to proportional. This is an observation by trying this N times, where N is very large! That is the observation got, and many other people got. o in spite of the fact that we cannot get an absolute value, because this is just the bound, the relative aspect of the V dimension holds. And therefore, if you have a bigger V dimension, you will need more examples. And in that case, you can even there are some estimates, practical estimate, that if you take the ratio of the V dimension to the number of examples, this will give you a handle on the error. And we will see provable versions of that, when we get to the biasvariance tradeoff next time, in a different theoretical line of analysis. o the first lesson is that there is a proportionality between the V dimension and the number of examples needed, in order to achieve a certain level of performance. That is a theoretical observance for the bound, and a practical observance for the actual quantity you get. Thats number one. Number two is that, give us just a guide proportional, not proportional. just want to know, if have just a reasonable epsilon and a reasonable delta, how many example does it take me to get over this hump? How many examples does it take me to get to the comfort zone of the V inequality, where m actually making a statement? o now have the probability is less than 1. Do take the V dimension? Twice the V dimension? A hundred times the V dimension? This is a practical observation. And again, its the practical observation. Obviously, this depends on your epsilon and delta. And this depends on the particular application. o the statement am making is that, for a huge range of reasonable epsilon and delta, and for a huge range of practical applications, the following rule of thumb holds. You have a V dimension and you are asking for a number of examples, in order to get reasonable generalization. The rule is, you need 10 times the V dimension. No proof. ts not a mathematical statement. And m saying greater than or equal, obviously. Because if you get more, you will get better performance. But that will get you in the middle of the interesting region. That will get you in the region where the probability statement is meaningful, rather than completely unknown what the generalization will be like. Now, ll spend just a couple of minutes to talk about the generalization bounds, which is a form of the theory we have, that will survive with us. We are not going to talk about growth functions anymore. We are not going to go through this. We are only going to remember that the V dimension is there, and it determines the number of examples. And it also corresponds to the degrees of freedom. And the form of the theoretical bound we are going to carry will be the following form. am just rearranging things. There is absolutely nothing new introduced here, except simplification. But its an important simplification, because its surviving with us. We start with the V inequality. We can bid farewell. This is the last time well see it in this form. ts complex and all, but we will now simplify it. o now we have epsilon and delta again. And give them different color. And the logic we were using is that, you specify epsilon, and then will compute what delta is, depending on the number of examples. o you tell me what your tolerance is, and ll tell you what the probability is. Another way of looking at it is the other way around. You tell me what delta is. You would like to make a statement with reliability 95%. an you tell me what tolerance can you guarantee, under the 95%? You start with the delta, and you go to the epsilon. Thats not very difficult. There is nothing mysterious about this. delta equals that. an solve for epsilon? f start with delta, can solve for epsilon? will get this part, put it on the other side. That makes this ready to take a logarithm of both sides. o this now goes down. Now need to get rid of the extra constants. They go on the other side. But now, there is a logarithm on the other side, because took a logarithm here. And finally, take a square root. o thats what you get. Very straightforward. Now, can start with this fellow and get epsilon. m going to call this formula capital Omega, which is the notation that will survive with us. ts a formula that depends on several things. And as you can see, if the growth function is bigger the V dimension is bigger, Omega is worse. Thats understood, because the bigger the V dimension, the worse the guarantee on generalization, which is this approximation thing. And if have more examples, am in good shape. Because now, the growth function is polynomial. By the time you take the natural logarithm, this guy becomes logarithmic. And logarithmic gets killed by linear, as much as linear gets killed by an exponential. o this is just a one step down, in the exponential scale, of the previous statement. ndeed, if have more examples, will get a smaller value of Omega. And obviously, if you are more finicky, if you want the guarantee to be 99% instead of 95%. o now delta is 0.01, instead of 0.05. Well, then epsilon will be looser, because you are making a statement that is true for more of the time. o you will have to accommodate bigger epsilon. Thats what we have. o the statement now is a positive one. t used to be that we are characterizing bad events, right? Now we are going to state the good event. The good event happens most of the time. t happens with probability greater than or equal to 1 minus delta. And that statement is that, E_in tracks E_out. They are within this Omega, and Omega happens to be a function of the number of examples this goes down with it, of the hypothesis set it goes up with its V dimension, and of the delta, the probability you chose to make the statement about. And this guy will go up with smaller delta. m just keeping it in this form, because we dont worry about this anymore. We just want to understand this fellow. o lets look at it. And that will be called the generalization bound. With probability greater than 1 minus delta, we have this fellow. o now m going to simplify this. Heres the first simplification. nstead of the absolute value of E_out minus E_in, am going to have just E_out minus E_in. Why is that? Well, because can. f have the absolute value, guarantee this one and its opposite. o among other things, guarantee this one, so can make the statement. The reason m making it is twofold. First, this is really the direction that matters. Because invariably, E_in will be much smaller than E_out. At least, smaller than E_out. Because E_in is the guy you minimize deliberately. n terms of a sample, this is E_out. And there is a sample, so the sample will have values here. Now, you start deliberately pulling this down. The other guy there is an elastic band, but the elastic band is getting looser and looser as you make more effort. But invariably, E_in now has a bias, which is an optimistic bias. And therefore, this will be the quantity that actually happens to be positive. Well, that doesnt say that E_out is always less than E_in. Once in a blue moon, maybe on your birthday or something, you will get E_out that is smaller than E_in. But the rule in general is the fact, E_out is bigger than E_in. o they are less than or equal to that. n spite of the fact that they have all of these dependencies, am just going to forget about these dependencies for the moment. know that Omega is an elaborate quantity. dont want to carry the details. understand its general behavior, so have this fellow. Now we rearrange this thing. By the way, this fellow is called the generalization error, because its the difference between what you did out of sample versus in sample. o this is a bound on the generalization error. And when you rearrange it, you can say with probability greater than or equal to 1 minus delta, and this is the form that will survive with us. You just take E_in and put it on the other side. Now, this is a generalization bound. And it is very interesting to look at it. t bounds E_out, on the lefthand side, with E_in plus Omega. This guy we dont know. Both of these guys we know, and have some control over. This one we are minimizing. This one is according to the choice of our hypothesis set. o it tells us something about E_out, in terms of quantities that we control. Furthermore, it shows that remember when we talked about a tradeoff. Remember when someone asked: bigger hypothesis set is good or bad? ts good for E_in, but bad for generalization. Now you can see why. This guy goes down with a bigger hypothesis set. This guy goes up with a bigger hypothesis set. Poorer generalization. Therefore, its not clear that its a good idea to pick a bigger hypothesis set, or a smaller hypothesis set. There may be a balance between them that will make the sum the smallest possible, and that affects the quantity care about. o this will translate to that. The other thing is that, now that got rid of the absolute value, well be able to take expected values in certain cases, and compare it with other stuff. o this will be a very friendly quantity to do. ts so friendly, that we are going to derive a technique, one of the most important techniques in machine learning, based on this. ts called regularization. And the idea here is that use E_in as a proxy for E_out. Now, after all of this analysis, realize that its not E_in only that affects the game. ts also the choice of this guy. o maybe, instead of using E_in as a proxy, m going to use E_in plus something else as a proxy, hoping that this will be a better reflection that will get me the E_out want. And that will be the subject of regularization. Well stop here and take questions and answers after a short break. Lets go for the QA. Are there questions? ODERATOR: Yeah. There was one confusion. Why is the V dimension exactly k minus 1? PROFEOR: OK. When we defined the break point, we defined that, can call k break point if cannot get all dichotomies on any k points. That means really, that if have a break point, then any bigger point is also a break point. And most of the discussion deals with the smallest break point. o the notion of a break point covers a lot of values. The V dimension is a unique one, which happens to be the biggest value just short of the first break point. Does this cover it? ODERATOR: Yeah. Yeah, because people were wondering if it was the break point for some set of N points, or for all sets. PROFEOR: ts always the case that when say you are able to shatter, give you the privilege of picking the points to shatter. o insist that you get all possible dichotomies, but you get to choose which points to shatter. That is always the logic in those guys. This does not affect a break point versus the V dimension, or whatever. This is always the case when we talk about shattering N points. t means that you shatter some set of N points. Now, the only distinction between a break point and a V dimension is that a break point poses it negatively, and the V dimension poses it positively. Break point is a failure to shatter, and V dimension is an ability to shatter. And obviously, if you take the maximum ability to shatter, which will give you the value of the V dimension, that will be one short of the next guy, which you failed to shatter. o that is your smallest break point and the other ones would be other break points. ODERATOR: Also, can you repeat the practical interpretation of epsilon and delta? PROFEOR: epsilon and delta. o the epsilon and delta as two quantities, they are the performance parameters of learning. There are two things that want to make sure of. want to make sure that E_in tracks E_out. The level of tracking is epsilon. Thats the approximation parameter. Now, cannot guarantee that statement absolutely. can only guarantee it in a probabilistic sense. But d like that probability to be as high as possible. o the probability that that statement doesnt hold is small. And that happens to be delta, which is the probability measure. o there are always these two quantities, and that is an integral part of this type of analysis. think we have an inhouse question. TDENT: wanted to know, what is the effect of error measure on the number of points that you have to choose? PROFEOR: OK. Obviously, as you can see from the V analysis, the error measure has always been a probability of error. o it was always a binary error. When you go to other codomains, realvalued or multivalued. Or you go to other error measures, you need to modify these things. ome variances will come in, and some other aspects. o for example, in case of the error measure, the binary error measure happens to be bounded. Therefore, you never worry about the variance, because there is an upper bound on the variance. f you talk about, lets say, mean squared error, depending on the probability distribution you put on things, this could be very big. o you need first to say that the variance is finite. And then, actually the value of the variance will come into these inequalities, to go through. However, the reason didnt venture into that is very simple. There is really nothing added, conceptually. And as you can see from the utility were using, we are not going to go back and unravel the mathematics, and apply it to a practical situation. We borrowed the following. We borrowed that: finite, can learn. The value is proportional to the number of examples. And the rest are rules of thumb. Thats where we stand. o its not worth sweating bullets over the other technicalities, when this is the message we are getting. And that message will hold intact in other situations. ODERATOR: A question about when youre mentioning the bound, you usually say the V dimension is known. s it true that for most hypotheses, this is really known? PROFEOR: To get the V dimension exactly is an exception, not the rule, as mentioned. o getting it for perceptrons is really a great achievement, because the perceptron is a real model that you use. And we know the V dimension, exactly. When you go to neural networks, we will get a V dimension estimate. Well say the V dimension cannot be above for the same reason that we had, when we talked about parameter versus effective number of parameters. Because in a neural network, the parameters will go from one layer to another, and there will be some cancellation or redundancy. And therefore, you cant really keep track of these redundancies exactly, or say it cannot be more than the number of parameters because even taking into consideration. o in many of the cases, the V dimension is estimated as a bound. But again, we are already in a bound. Even if you know it exactly, its not like we know what the generalization error will be like. We know a bound on the generalization error. o in this series of logical development, we get a bound on a bound on a bound on a bound. o by the time we are done, the bound is so loose that, in absolute value, its really not indicative at all. But the good news is that, in relative value, it maintains its conceptual meaning. We can use it as a guide to compare models, and to get a general number of examples, notwithstanding the fact that if you decide to say, m going to go for a perceptron in two dimensions. And m going to want epsilon to be 0.1 and delta to be 0.05. ould you please tell me how many examples need? f you actually go and solve the V inequality and try to get a bound, the bound will be ridiculously high. uch higher than you will actually need in practice. But you dont use it as an absolute indication. You use it only as a relative indication. ODERATOR: Have you come across any interesting examples, where N has to be much bigger than 10 times the V dimension? PROFEOR: The interesting example is when the customer is very finicky, and wants a very small epsilon and delta. Because the smaller epsilon and delta, the more number of examples you had. o the rule of thumb is not to tell you: use 10 times the V dimension. t tells you that you are in the thick of the game, when you have 10 times now we are talking. There is actually generalization. There is a certain level. There is some compromise between epsilon and delta. Now you can tighten the screws, and try to get it better. This is just a rule of thumb, for getting into the interesting region of the V inequality. And that has stood the test of time. ODERATOR: s there a relation between this material and the topic of design of experiments, and the number of experiments you require to achieve a certain confidence? PROFEOR: Yeah, there is a relationship. ome of the experimental design and whatnot there are lots of commonalities between here. You have control over certain things that you may not have here, but some of the principles definitely extend to that. As mentioned, when we talked about the premise of learning, its so general, that it would not be a surprise at all that many of the concepts go and tackle situations that are not strictly learning, but have the same theme as learning. ODERATOR: think thats it. Theres no more. PROFEOR: o we will see you on Thursday.","o this is N to the 5. This is N to the 4. o if you can shatter 20 points, and that is the most you can do, then the V dimension is 20. n terms of the technical quantities we defined, this will be the largest value of N such that the growth function is 2 to the N. o if you go one above, the 2 to the N will be broken. And in that case, you can even there are some estimates, practical estimate, that if you take the ratio of the V dimension to the number of examples, this will give you a handle on the error. One of them is that we are going to show that the V dimension is at most d plus 1. They are within this Omega, and Omega happens to be a function of the number of examples this goes down with it, of the hypothesis set it goes up with its V dimension, and of the delta, the probability you chose to make the statement about. We are only going to remember that the V dimension is there, and it determines the number of examples. And if you look at this, this is indeed a polynomial, and the maximum power you have in this expression is N to the k minus 1. o not only is it polynomial, but also the order of the polynomial depends on the break point. Number two is that, because we understand the perceptron model so well, we will be able to get the result, which is the V dimension of perceptrons. And the maximum you can get is the V dimension. am going to construct a specific set of N points, and that N in this case is d plus 1, because thats the number of points want to shatter. And this is the one that will survive with us for the rest of the course, and well be able to relate it to different theories and techniques as we go. o by the time you get there, you have that number, which is the V dimension degrees of freedom. And obviously, if you take the maximum ability to shatter, which will give you the value of the V dimension, that will be one short of the next guy, which you failed to shatter. And what is the V dimension? But the interpretation of interest to us will be the fact that this is actually the number of parameters in the perceptron model. f you look at the growth function in terms of the V dimension, when we had the break points, in terms of this k, we were able to find the bound that showed in the review. And since the V dimension is a bottom line it looks at what you were able to achieve, it will be a more reliable way of measuring what is the actual degrees of freedom you have, instead of going through the analysis of your model. This is actually because of the particular form of these guys, where the first coordinate of all of these guys is always 1. o when you look at this and apply it to the first coordinate, 1 equals well, these cannot all be zeroes because it has to add up to 1. This is how we are going to do it. You take any dimension, and we will get the value of the V dimension exactly. All you need is one set of N points that can be shattered, in order to say that you can shatter N points. And that will be the basic part of the interpretation that we are going to go into. This is the value at the first one, +1 or 1, +1 or 1, and +1 or 1, on the x points that just showed you. And m going to ask myself, in order to get to this region, which is the performance you want, how many examples, which is this coordinate, do need given the different V dimensions, which supposedly is 4 here and 5 here? The premise that we are trying to establish is that the V dimension is at most d plus 1. When you say that want to be 95% sure that the statement is correct, you are picking delta to be 5%, or 0.05. o thats what you do. We mentioned that when we define the growth function or the V dimension, give you the budget N and then you choose the points any way you want, with a view to maximizing the dichotomies, right? The first part is to get a handle on the growth function m_H of N, which characterizes the hypothesis set H. And the way we got a handle on it is by introducing the idea of a break point, and then bounding the growth function in terms of a formula that depends on that break point. Now, you can see that chose them such that X is invertible, because that is the technique m going to use in order to be able to shatter them. This is the number of examples you want. And after an argument that took the redundancy and related it to the growth function, and then got rid of a technical problem with E_out, if you recall that one, we ended up switching completely from the Hoeffding inequality, which is the top one, into the V inequality, which is the final theoretical result in machine learning the characterization of generalization. And indeed, you will find that, theoretically, the bound in terms of the number of examples to achieve a certain level lets say you cut the level here, and you are increasing the V dimension. You can say that if N is at most 15, the V dimension, then H is guaranteed to be able to shatter N points. The problem is that, when you take this as your perceptron, then by definition the label is the sign of this quantity. But it is the quantity that you are going to remember from all of this after a while. What is the V dimension? o in spite of the fact that we cannot get an absolute value, because this is just the bound, the relative aspect of the V dimension holds. And the logic we were using is that, you specify epsilon, and then will compute what delta is, depending on the number of examples. Just do this, and you get a setting that tells you what the hypothesis is. And when you rearrange it, you can say with probability greater than or equal to 1 minus delta, and this is the form that will survive with us. o the V dimension will also serve as the order of the polynomial, that bounds the growth function of a hypothesis set that has that V dimension. PROFEOR: ts always the case that when say you are able to shatter, give you the privilege of picking the points to shatter. The conclusion is that the V dimension is d plus 1. And therefore, the corresponding growth function, which can be defined in this case, will not be 2 to the N, and you may be able to learn. Then, we are going to show that the V dimension is at least d plus 1. o if you understand this quantity, you will be able to translate it to the other one. And get the V dimension to be 2, because its d plus 1. And similarly, if you add the other probabilities, the probability will not be 1. t will be 2, et cetera, but all you need to do is just shift a little bit, and you will be getting here. We actually set the input points in linear regression this way, in order to get the algorithm, the pseudoinverse, and all of that. o all you need to do is substitute, and you will get this formula involving the V dimension. And this is the property that am actually going to use, in order to establish what want. And this happens to be less than the value of 1. o all the negative logarithms will be here, and ll be able to look at it clearly. That is the one that we are claiming the generalization with respect to. We were interested in the growth function, because it was our way of characterizing the redundancy that we need to understand, in order to be able to switch from the Hoeffding inequality to the V inequality. We will just say d_V, and well understand that this is the V dimension. n order to do this, we are going to look at this function, which is a polynomial. And now you can say that the growth function, for any hypothesis set that has V dimension d_V, is bounded above by this. This is 1. The main result is that if the V dimension is finite thats all you are asking then, now the green final hypothesis will generalize. And by then, you will forget that it was N to the 4. Now, if you go for the general case and you have ddimensional space, you expect the V dimension to be more. And then the rest of the dimensions, from 1 to d, are actually the coordinates of the point. Now need to show that the V dimension is less than or equal to d plus 1. wonder what need to do, in order to achieve that. This is the probability. Now, we said before that this one is particularly pessimistic, which indeed it is, because this is a very specific way of getting the points. We can use it as a guide to compare models, and to get a general number of examples, notwithstanding the fact that if you decide to say, m going to go for a perceptron in two dimensions. But the message here is that, you dont look at the number of parameters, you look at the effective number of parameters. You start with the delta, and you go to the epsilon. And ask yourself, the value of the V dimension, how does it affect the number of examples you need? o you take j, whichever it might be, and this will be equal to the sum over the rest of the guys of some coefficients that dont know, times those guys. And that got rid of the technicality that we alluded to, which is the role of E_out that really destroys the utility of the growth function, because the growth function depends on dichotomies, and the E_out depends on the full hypothesis itself. Now, what can you say about N which is at most 15, in terms of the ability to shatter or not? ts simply because, since the V dimension is this number, there will be that many points that can be shattered. A number that is a real number and then you can milk out of it so many degrees of freedom, that you can get more than one degree to freedom. Because usually, when we get the V dimension, we get a bound on the V dimension, just out of the practicality of the situation. Now, all you need to realize is that this is a dichotomy. Now, this is in terms of the bound. TDENT: wanted to know, what is the effect of error measure on the number of points that you have to choose? And 10 to the 0 here, thats the probability 1. Because now, the specification that is needed in order to get you to pick the right hypothesis, is pretty elaborate. And you get this one. The reason can do any which way, because have a bunch of parameters that can set one way or the other, in order to achieve that. And therefore, this will be the quantity that actually happens to be positive. But the main thing is that instead of the number of hypotheses , we were able to replace it by the growth function. t tells you that you are in the thick of the game, when you have 10 times now we are talking. am going to say that, for those particular points that you chose, which really dont know what they are, can say that you have more points than dimensions. That is the relation we are going to see. o the first lesson is that there is a proportionality between the V dimension and the number of examples needed, in order to achieve a certain level of performance. And the result is captured by the V dimension, so this is our quantity for measuring the degrees of freedom. You have a V dimension and you are asking for a number of examples, in order to get reasonable generalization. This is the form we have, x_j happens to be the linear sum of these guys. What does this force the value of the perceptron, your perceptron, the one you had here, to be? And remember the problem with those guys is that if you want these guys to be 1 and these guys to be +1, it was a problem for the plane. But the other case is really you construct it because you want to. You can see that this is the interesting region. Now, d like to understand the rest of the diagram, in terms of the V dimension. And as you can see from the utility were using, we are not going to go back and unravel the mathematics, and apply it to a practical situation. And this will be the subject of the interpretation. Now, you look at this and then there is a third guy which is an obvious one, which is the target function. That will get you in the region where the probability statement is meaningful, rather than completely unknown what the generalization will be like. And it is applied to H. And every now and then, we will drop the dependency on H, where it is clear from the context. think this is the last lecture where were going to see the positive rays and the rest of the gang. And it will relate to the number of parameters. o for the x_is with the nonzero a_i, am going to give the label, which happens to be the sign of that coefficient. Now, lets look at the number of data points needed, which a practitioner would be interested in, and doesnt care about the rest of the story that told you. f give you 17 channels and this and that, and give you this panel, thats great, if you know how to use it. And the interesting part here is, obviously, this will play the role of the righthand side of the V inequality. And that will set the stage, when we go to interpreting the V dimension. And when you are in a learning situation, you ask yourself, what is the V dimension? But that will get you in the middle of the interesting region. The main idea of understanding what the V dimension signifies, is to look at the degrees of freedom. What can you conclude given that the sign of this fellow is the same as the sign of this fellow? The value is proportional to the number of examples. f do that, then have showed you that can shatter the set. And if you remember very early on, told you that the hypothesis set is a little bit of an artificial notion to introduce as part of the learning diagram. And the most important aspect about it, as far as the theory is concerned, is that this is polynomial. The most important thing to realize is that we are talking about the most points you can shatter. Lets say that you manage a learning system, and you look at the hypothesis set, and you say V dimension is 7. want a certain performance. Well, then epsilon will be looser, because you are making a statement that is true for more of the time. And as you can see, if the growth function is bigger the V dimension is bigger, Omega is worse. And you would like the sign of that to agree with the particular y you chose. This is, what? You just need to know its finite, and then you can say the g will generalize. What is that?",0.1532488114104596
25,25,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer highquality educational resources for free. To make a donation or to view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. RYAN ALEXANDER: All right, so as this XKD comic points out, in , it can be very difficult to figure out when something is just really hard or something is virtually impossible. And until a couple years ago, people thought this idea of image classification would be something that was closer to the impossible side. But with the advent of deep learning typology, weve made significant strides in image classification, and now the problems actually quite practical. o today well be going through how the process of image classification with deep learning works. o were first going to talk about what deep learning is, and then well move into some of the image processing techniques that researchers use, followed by the architecture of the convolutional neural networks, which will be the main focus in our presentation. Well also talk about the training process, and then go through some results and limitations of NNs and image classification. o what is deep learning? Well, the term is particularly vague, and its purposely so for a couple of reasons. The first is mystery is always good for marketing. But the second reason is that deep learning refers to a pretty wide range of machine learning algorithms. They do have some commonalities. They all seek to solve problems of a complexity that previously, people thought only people could solve. o these are more sophisticated classification problems than traditional conventional machine learning algorithms can do. o how do they go about doing this? Well, all of these deep learning programs tend to take all the processes that need to happen, and split them up. Theyve got different parts of their program working on different things, all while performing calculations, and then at the end, it all comes together, and we get a result. Of course, this isnt unique to deep learning, and lots of distributed systems decentralize their calculations. But the key thing about deep learning is that every part is performing these calculations. The calculations are not simple calculations. Theyre not well do this one simple operation over, and over again on a lot of data, and then well get a result at the end. Each part is performing some particularly complicated process on all the little parts before they come together. o why is this architecture a good idea? Why did engineers come up with this sort of decentralized, multilayered complex process? Well, we take the example of image classification. t turns out that the human brain does a pretty similar process. o heres the human visual system, and its pretty much a hierarchical process. o you begin by moving from the retina into the first areas of the brain, and as the information gets processed, it moves from one region of the brain to the other, and each spatial element of your brain is performing an entirely different calculation. For example, the v1 area over here is picking out edges and corners, and then over here, a couple steps later in v4, youre starting to group those figures together. And so the brain kind of operates in a way that is very similar to the way these networks operate. o lets talk about to classify a face. f asked you guys how would you classify a face, what is the first thing you might do? Well, as mentioned before, the first thing out brain does is it finds these edges. The first thing to do is identify where the face is versus everything else. Now, does anyone have any idea as to what we could with the next step? Julian, you have an idea? ADENE: aybe you could group these edges together. RYAN ALEXANDER: Right. We could maybe identify some of these features that were working with. o these are things like noses, and lips, and eyes. And then what do we do after we have these individual features? teve. ADENE: Well, maybe we can group some of those together. RYAN ALEXANDER: Exactly. Yeah, we can organize them into what we know the pattern to be. We know that a face has to have two eyes, above a nose, and then above the mouth. o that is precisely what a neural network actually ends up doing, and well walk through the process of how it does this later on in the talk. But as you can see, the intuitive way that we classify a face, and the way our brains are wired to do it, is pretty similar to the way got these neural networks to operate. o like said, were talking a lot about these convolutional neural networks. There are other types of architectures involved. Like we mentioned before, deep learning is a pretty wide variety of algorithms, but were going to focus on these NNs. To give you a precursor of how good these NN are, this results from mageNet competition. o the mageNet competition is basically exactly what it sounds like, a bunch of computer scientists get together, and see how many images they can correctly classify. And their error rate was pretty high. Almost a third error rate over here in 2010, 2011, and then in 2011, the NNs were introduced to the topic, and the error rate plummeted. As you can see over here in 2015, weve got a significant improvement in these mageNet competitions. o clearly, the NNs have been very effective, and its definitely been something that is exciting in the field and happening right now. All right, so now were going to move into image processing. HWARYA ANANTHABHOTLA: OK, so Ryan gave us a nice overview of where we get this concept of neural networks, but lets take a time travel, and go into a quick history lesson. o suppose had a chair, and wanted the computer to classify this chair. have some a priori knowledge about what sort of things make up a chair. o might be interested in looking at arms, and corners of the chair, legs, things like that. o would go ahead, and feature engineer my discovery scheme to be looking for specific things. o m going to talk about some techniques that are traditionally used. For example, chairs, doors, these things have corners. o might use an image processing technique called a Harris orner Detector, where we basically look at large changes in intensity as groups of pixels move from an image to an image that indicate the presence of corners, and you can use common corners to say that OK, all of these images are chairs, or doors, or whatever. imilarly, want to say have a bunch of pictures of chairs of different sizes, but that they all must have so many corners or something. o typically, we use a sift algorithm to scale invariant feature transforms. t basically says that across different sizes, still should be able to extract information about the placement of corners. Another common technique thats used in image processing is what we call HOG, Histogram of Gradients. o basically, for example, in this image, if want to say want to find all the images that have faces in them, or consist of faces, lets say, might come up with a template of a face that basically assigns gradients to groups of pixels that form an outline of what looks like a face, and then scan it across my sample images, and say OK, a face is present in this image. Obviously, there are some errors. A mead cap, and a logo back here have become a face, but this is the traditional approach. But heres the problem, what if dont actually necessarily know what features are the most critical depending on the dataset that get? want the system itself to figure out what techniques to apply without having any a priori knowledge about the dataset. o this is exactly the idea of NNs, the convolutional neural networks. We want the techniques to be learned automatically by the process, by the system. o if m trying to classify faces, want the system to figure out that eyes, and ears, and nose, these are the most important things. Or if m trying to classify elephants, the ears and trunks are the critical features, without me having to say OK, were going to do corner detection, so on, and so forth. o this is the idea. o to be able to understand this process in greater detail, m first going to go into a little bit of math, and the idea is to present the most fundamental operation here, which is the convolution. o this is the formal definition of the twodimensional convolution, and since were working with images, were only considering the twodimensional case. o in a more graphical presentation, which is a little bit easier to understand than just seeing the formula, the idea is that we have a kernel, or a convolutional filter that we seek to apply on another image, and that extracts some information about that image that we can use to help us classify the convolution. o assume that this is our kernel, or this is our filter, and suppose this oh, there it is. o suppose were applying the kernel here to the image thats in green. o the idea is we want to slide this filter across the image, and what were basically doing this is a succession of dot products. o at each placement on the image, we multiply the overlayed numbers, and the sum becomes the output image on the convolt output. o this is basically the way the process works. You probably notice that theres a reduction in dimension, and Henry will talk a little bit more about why this is. Let me get to it, and then o lets see some examples of what information we get by applying the convolution. o you see the image of a tiger on the top left. When we apply a filter thats a low pass filter, basically its a Gaussian then we get low spatial frequency information about this image. o basically, we blurred it, and this tells us something specific that we might want to learn. o the kernel actually looks like a twodimensional Gaussian function thats been distributed across this threebythree kernel. imilarly, we might be interested in high spatial frequency information. o in this case, were looking at sharp features. o horizontal edges or vertical edges. o a question for you is if have this kernel, which of these outputs when this kernel was applied to the original image, which of these outputs do you think it produced? ADENE: The third one on the right. HWARYA ANANTHABHOTLA: Yeah, thats exactly right, and its probably pretty easy to see why thats the case, given that the numbers are all horizontal bands here. Lastly, we also may be interested in extracting information at a particular frequency. o we can take the difference of a high pass filter and a low pass filter, and add it to your frequency you can extract information about that as well. OK, one last helpful piece of information is that theres another way that you can think of the information thats learned at each stage because a convolution can also be thought of as a Fourier transform in the frequency domain. You can think of the image transformation that way. o from an image perspective, what a Fourier transform is is basically a sum of a set of sinusoidal gratings that differ, say, in frequency, in orientation, in amplitude, and in phase. o you can think about the zebra image here thats actually a composite of different gradients that might look like this, and the Fourier coefficients would be how much of each of these pieces come together to make that final image. o just to get a sense of what kind of information this could convey, we typically take a Fourier transformation, and break it apart into the magnitude and phase representation. o you see magnitude, and you see phase. o those images werent particularly clear, but this is a really good example for this. o if we take the Fourier transform of all the horizontal text here, you see how the magnitude reflects this, and you can go back to the math to understand why its reflected in a vertical marking. And similarly, if were to take that same image, and rotate it, and then ask for the Fourier transform, you see how that information is contained very clearly in the magnitude spectrum. o these might be things that a network would learn at each stage to try to identify this as a text, or as as body of text thats tilted one way or the other, so on and so forth. o with that, we can now go into what the actual architecture of a convolutional neural is. HENRY NAF: All right, so as it was said earlier, in order to classify or detect objects, you actually need certain features. You need to be able to identify these features. And the way you can identify these features is using certain convolutions or certain filters. n many cases, we dont know what these features are, and as a result of that, we dont actually know what the filters are to extract these features. And what convolution neural networks allow us to do is actually determine what these features are, and also determine what the filters are in order to extract these features. Now, the idea for convolutional neural networks, or the idea for replicating how the brain works started in about 1960s or 1950s after some experiments by Hubel and Wesel. And what happened in these experiments, as can be seen here, is a cat was actually shown a light band at different angles, and the neural activity of the cat was measured using an electrode. And the outcome from this experiment show that based on the angle at which the light was shown, the neural response of the cat was different. As you can see here, the number of neurons, as well as the neurons that were firing were very different based on the angle. o what you can see also here is really a plot of the response versus the orientation of the light. And what this has led Hubel and Wesel to is the idea that neurons in the brain are organized in a certain topographical order, and at each filter, it fills a specific role, and the only fires when its specific input is shown, or when the angle is show, or the angle is specified. Now, the first step to actually replicating how the brain works in code is really understanding how the building block, the neuron, works. Thats a quick reminder of 7012 here. o a neuron is actually a cell with dendrites, nucleus, axon, and a terminal. And what the neuron actually does is aggregate the action potentials or the inputs it gets from all the neighboring neurons that are connected to it through the timelines, and it sums these action potentials, and then compares them to a certain threshold that it has internally, and that would determine whether or not this neuron would fire an action potential. And that very simple idea can actually be replicated in code. An artificial neuron looks very much like a natural one. o what you would have is a set of inputs. Here we have three inputs that are summed inside of a cell, or a neuron. The sum here is not just a regular sum, its a weighted sum. o the neuron specifies some weight, which you can think of as how much it values the input coming from a specific neuron, and then the input is multiplied by its weight, and then the total sum that the neuron computes is then fed into an activation function that produces the output that the neuron then basically produces. Now, what we just saw here is really a simple neuron, a single neuron. You cant really do much with just one neuron, so what you would do is combined these neurons in a certain topography, or in that case, we have a network with seven neurons organized in three different layers. And what you can think of that is really as one big neuron with 12 inputs, and one output. o for example, in the case of the chair that was previously mentioned, if youre trying to identify whether a specific image has a chair in it or not, these 12 inputs here could be some sub images, or some small areas of the initial image that you feed into the network, and the output here could be a yes or no. Whether the image has a chair, or doesnt have a chair. And that is really the concept behind convolutional neural networks, which well go into details in a bit. o what each neuron would be doing in that case, is really just performing a dot product, which if you aggregate that with the dot products computed by each of the other neurons, you would obtain a convolution. o what we have here is three inputs. f the input, in that case, is an image or a sub image, then the inputs would be pixels. The weights that you would be using here would be the filter weights, which is the filter that you use in the convolution. And then the sum here would be the dot product of the weights and the inputs, and that sum would be computed by a specific neuron in your network. Then, that would be the convolution step, and then that convolution step would happen at the first layer in the network. o you would be applying this to the input, but you also would be applying this at the second layer, and third layer. n that case, were only showing what happens in the first layer. The next step after the convolution would be the activation step. o the dot product computed here would be theres a function that would be applied to the sum, and then that function would produce the output of the neuron. And this is where the activation layer is. You also have another activation layer here, and then a final one here. What we just went through now are convolutions and activations, but this is not the only thing that actually happens in a neural net. What we also have is a step called subsampling, which we will be talking about next. For now, we will dig deeper into the activation, and specifically, what activation functions to use. n that case, we can see that thats a neuron, and what the neuron is doing here is the weighted sum that we talked about, or the dot product. And then the output from this would be fed into a certain activation function. ommon activation functions are sigmoid, tanh, or rectify linear unit, and we will go through each one independently. o here, we can see the sigmoid activation function. o what this function essentially does is map any input to an output in the range of zero to one, and its defined as one divided by one plus e to the minus x. The other common activation function is tanh, and thats any input to an output between minus one and one. And then finally, would be the rectified linear unit, which maps an input to itself if its positive, or to zero if its negative. Now, in theory, you could use any function as an activation function in your network, but thats not what you want to do in practice. You want your activation functions to be nonlinear for one main reason, that the goal of the activation function is actually to introduce nonlinearity in your system. And if all your activation functions are linear, then you would essentially be having a linear system, which prevents you from achieving the level of complexity that you would ideally want to achieve with a neural network. And theres a formal proof for as to why you need nonlinear activation functions. They dont all need to be nonlinear, but you need to have a least a few nonlinear activation functions in your network. And the proof is available in the appendix, or the link to the paper that has the proof. o after weve discussed what happens at the activation layer, now we want to talk about convolution. o as said earlier, an image is obviously a twodimensional image, but were using RGB images. o actually need three channels. o what this means is that an image is actually threedimensional, and each 2D matrix represents one channel. One of them corresponding to R, one of them corresponding to G, one of them corresponding to B. o a 32 by 32 image would essentially be represented by a 32 by 32 by three matrix, as can be seen here. o what happens at the convolution layer? o here we have a nice animation that shows what is happening at each convolutional layer. o assume we have a five by five by three filter. o what this is, essentially, would be doing is covering a certain patch in the original image, which is 32 by 32 by three. o what can see here is that for that five by five by three patch in the original image, we have a neuron that is actually performing the dot product on all the pixels in that specific patch. o what is happening here is the pixel values, which in that case, we have five by five by three pixels, are being multiplied by the filter values, and this operation is being performed here. Then, after that dot product is performed, its fed into an activation function, as can be seen here, and this produces the output of this neuron. Now, this is what this single neuron is actually doing. ts just covering that area of the original image. What you would have in a neural net is many neurons, each covering a certain area of the original image. And if you aggregate the output of all of these neurons, what you would be doing or performing is, essentially, a convolution on the original image. And to formalize what happens here, or whats the output thats being produced from that operation, we can look at that from a more mathematical perspective. o if you have an input of size H1, W1, D1, and youre performing a convolution with a filter, then the output, W2, would be related to W1 with the following formula. o W2 plus W1 minus filter width plus one, and the same formula applies for the height, and the depth would actually be the same because in that case, were using a filter that has the same depth, or three, as the original image. o what this would produce in aggregate is if you have 28 by 28 by one neurons, each one performing a dot product on some pixels in the original image, the output would be an activation map of size 28 by 28 by one, and the output of each neuron would be one pixel in the activation map. Now, if we go back to the points that we made earlier, one thing we said was that the reason you use a neural network is because you dont know exactly what features you want to extract, and you dont actually have specific filters that you want to apply to the image. o ideally, what you want to do is have multiple filters being applied to the first image, and perform multiple convolutions, and this is what you can do with multiple neuron layers. o what we described before was just for one neuron layer. n that case, we can assume we have five different neuron layers, each one performing a different convolution on the original image. o in that case, we would have 28 by 28 by one neuron per layer, and then if we aggregate all these neurons together, we need to multiply it by five, and that would be the total number of neurons we have in that specific number. o this actually leaves us with a pretty complicated system. t would have many parameters. The neurons have weights, the number of neurons is also a parameter o how do we actually formalize that? f we have an input volume of 32 by 32 by three, which is our original image, and a filter size of five by five by three, then the size of the activation map that would be reduced would be 28 by 28 by one. Then in that case, we also said we have five different neuron layers that perform five different convolutions. Then the total number of neurons would be 28 by 28 by five, and then the weights per neuron are five by five by three, which is 75. n that case, were assuming that the neurons independently keep track of their own weights. This could be simplified to each layer having their own weight, which would tremendously reduce the number of parameters. But in that case, just to get an upper bound, this leaves us with a total number of parameters of 294,000. And this is just using a 32 by 32 by three image. You can think of this as a pretty small image. o if you have a bigger image, you will have many more parameters. Great. o what we just saw now, and described, were convolutions, activations, and these steps happen sequentially in a convolutional neural network, specifically as can be see here. One step that also happens occasionally is subsampling, and well discuss that step in detail here. o there are two main reasons why you would actually subsample your input. One is to obviously reduce the size of your input, and your feature space, but also because you want to keep track of the most important information, and get rid of everything else that you dont think is going to be relevant to your classification. And the common methods used in subsampling are either max pooling or average pooling. We will describe max pooling here. o what happens in max pooling is, essentially, you are dividing the image into different sub images, nonoverlapping sub images, and you perform an at max operation. o in that case, if we consider two by two filters, we would split the image, which in that case is four by four. Wed split it into four sub images, and for each two by two square, we would take the maximum. n that case, for the first square it would be six, then eight, then three, then four. And the reason that actually works is because what you want to do is really keep track of the response of the neurons that or the highest response produced by your neurons. n that case, for example, the first highest response in the first square is six, and that means that if you get that high of a response, it means that something has been detected in the image, or has been detected. And this is something you want to keep track of as you move forward in your network. And although this moves around the location of pixels, because you can think of that as subsampling an image, it does keep track of the information you care about because you only care about the fact that something has been detected in the image. At this point, you dont really care about where its located in the image, and you want to keep track of all the features that your neurons have detected in order to be able to eventually classify the input correctly. o if you have multiple feature maps so in that case, if you have 224 by 224 by 64, what your subsampling operation would be doing is reducing the height and the width so the depth would remain unchanged. o in that case, you would go from 224 by 224 by 64 to 112 by 112 by 64, and that would be reducing your output size by a factor of four. And formally, what this would look like is if you have an input of size H1, W1, D1, the size of your output would be related to your input in the following ways. W2 would be W1 minus the pool width plus one. The same applies for H2, and the depth would remain unchanged. o these are, essentially these are the steps that happen in a convolutional neural network. What you could be doing is repeating these steps on a certain number of times in your network. But eventually, you have to make a classification, and decide in our case, whether our image has a chair or doesnt have a chair. o how does that happen? o after you perform all these steps, theres a step that happens here that would allow you to make that prediction, and that step is usually called a fully connected layer, or a multilayer perception. And what this essentially is is layers that are very similar, or exactly the same as what you had before, except that every neuron in the layer is connected to all the previous neurons. o what its allowed you to do is really consider everything you currently have about your input, or everything thats left about your input, and compute a dot product on that, rather than focusing on a subset subsample of your input like previous layers do. n that case, if youre actually trying to classify your input into four classes, you would ideally have four different neurons in your output layer, each one corresponding to one of the classes that you have, and then you would perform the same operation as you would in a previous layer, compute the dot product, and then once you obtain the values at every neuron, you would perform a normalization operation on all the output. This organization operation is called softmax, or normalized exponential, and what it does is really, put more weight on the highest value. And by computing the softmax at the output, youre able to compute the posterior probabilities, and allows you to make a more informed or basically make a classification decision on your input. Great. o thats everything. And now, the next step will be talking about back propagation. HWARYA ANANTHABHOTLA: OK. o now that Henry has given us an overview of the entire architecture of a NN, m going to quickly spend some time, and talk about standard preprocessing tricks and tips that people might use on the image dataset before they actually feet it through a neural net to classify the images. o lets suppose we have a dataset x, and there are n number of data points in the dataset, and each point has a dimension, D. o they have D features per point. o in this example, we use these graphs as an example. Basically, our original data here has just two dimensions, and it spans this range of values. o for example, if we want to center this data, what we would do is a mean subtraction. o we basically subtract the mean across all the features of all the points, and we basically center it, and you can see that transformation here. And then we might, again, go for normalizing the dimension so that you have it. The data points span the same range of values in both dimensions. o you can see that transformation, and how its taken place here. And we just divide by the standard deviation to do this. omething thats very commonly done is called PA, or Principal omponent Analysis. And the idea here is sometimes we have a dataset that has a very, very high dimensionality, and we would like to reduce that dimensionality. o basically, what our goal is is to project the higher dimensional space onto a lower dimensionality space by taking the subset of those features. And if youve seen a little bit of 1806 from linear algebra, the way we do this is by generating a covariance matrix, then doing the single variable decomposition. And ll gloss over the math now, but thats the idea. And you can see here how the original data spanned two dimensions. would decorrelate it so that it spans a single dimension. And even with this data, you might want to ensure that its widened, which is the same deal. You want the values to span the same range in both dimensions. o then you would just divide by your Eigenvalues to get the widened data. This last bit is something thats very commonly done as a preprocessing trick, though people arent entirely sure why it works very well, or that it really does help, but its something that people do, and its called data augmentation. o basically, if have a dataset that contains a bunch of images of chairs, a bunch of images of tables, and then a bunch of images of say, trees, might want to intentionally augment that dataset further by introducing a few variations on these same images. o might take the chair image, rotate some, reflect it a few more, scale, crop, remap the color space, or just kind of have a process that does this randomly to create more variation on the same dataset. And this is a good illustration of why this makes a difference. ve taken an image here of what looks like a waterfall or some spot of nature, and simply just inverted the colors. And what see, if were to just see this image alone, it maybe looks like a curtain, or a bit of texture, or something. And the idea is even to a human perception, these images have two very different meanings, and so its interesting to see what effect they would have on a neural network. And with that, well go over to image classification results. AL OYLEEZOGL: o, so far weve seen how convolutional neural networks are built, and certain image processing techniques we can use on the input images to get them into formats that are there for the classification process, but so far, it seems a bit abstract. ts good to know how NNs work, why NNs work, but why dont we take a look at some of the practical results from NNs, and what theyre used for so that when youre done watching this lecture, you can go home, and try classifying images on your own time? With that, lets first revisit the mageNet competition. hope you remember the graph at the bottom from the beginning of the lecture, where we used this to motivate the use of NNs. NNs came onto the picture in 2012, but the winning NN from 2012 was used on the 2010 mageNet competition as well, and it managed to bring down the top five error rate to 0.17, which is pretty much on the same level as how performed in the 2012 competition when it was first used, which was at 0.16. o this just goes to show that these convolutional neural networks are the state of the art when it comes to image classification, and thats why were currently focusing on that. But you might be wondering what the mageNet competition exactly looks like, what the images looks like. o why dont we take a look at that. As you can see here, these are images from the mageNet competition. nderneath each image is a bold caption, which is considered to be the ground truth, or what the competition believes to be the correct classification of the image. nderneath that ground truth, you see a list of five different labels. Now, these five labels are produced by a convolutional neural network, and the different bars the different lengthened bars, some pink and others blue, represent how confident the NN is that what it sees in that image is that specific label. As you can see in certain examples, the NN is pretty confident in that it has a correct answer. For example, when we look at the container ship, its pretty confident that whats in that image is exactly a container ship. There are certain cases when it doesnt get the correct label on its first try, but it does have in its top five labels. For example, you can see grill and mushroom. Now, the funny thing about the mushroom image is that what it thinks the image should be classified as is agaric. And if you dont know, agaric is actually a type of mushroom, and in fact, its a mushroom that image. And it make sense that their confidence levels are pretty much the same. Agaric is slightly its slightly more complex that what it sees in the image is agaric. But there are certain cases when the NN fails to classify the image correctly in its top five levels. This will be registered as a top five error, as you just saw in the previous slide about the top error rate. One example here on this slide is cherry. Now, the mageNet competition believed that this should be classified correctly as cherry, even though theres also a Dalmatian in the background. The NN, on the other hand, is pretty confident that what it sees in this image is the Dalmatian. But if you look at some of the other results within the top five, although it doesnt guess cherry at all, it does guess certain fruits that it may think look sort of like cherries like grape or elderberry. o the NN does actually pick up on two different distinct objects within the image, but as a result of how its built, or its training set, it ends up classifying it as a Dalmatian. But it goes to show you that NNs could also be used not just as an image classification, but also as object detection, which we do not touch up on in this lecture at all. o m not going to go further into that. Now, this is all fun and all, but what about some real world applications? o this is a study that they did at Google with Google treet View house numbers, where they used the NN to classify photographic images of house numbers, as you can see here, of certain examples of these house numbers what they look like. o what the NN was tasked with doing was that it was supposed to recognize the individual digits within the image, and then understand that its not just one digit that its looking at, but its actually a string of digits connected, and successfully classified as the correct house number. This can be quite challenging, even for humans sometimes when the image is quite blurry. You might not exactly know what the house number exactly is, but they managed to get the convolutional neural network to operate around human operator levels. o that corresponds to around 96% to 97% accuracy, and what that enables Google to do is that they can deploy the NN such that the NN automatically extracts the house numbers from the images online, and uses that to geocode these addresses. And its gotten to a point were the NN is successfully able to do this process in less than an hour for all of the street view house numbers in all of France. Now, you might asking where this could be useful for. f you dont have access a lot of resources to actually do this geocoding process where you match latitude and longitude to street addresses, then your only resource might be actually photographic images. o you actually need something, hopefully not human, but some sort of software that can do this successfully. And so this is, for example, a place in outh Africa, a birds eye view. Not sure if you can exactly see, but there are these small numbers on top of each of the houses. All of these numbers were extracted and correctly classified using this previously seen NN. Another example from robotics is recognizing hand gestures. o obviously, robots come equipped with a lot of different hardware. They can sense sounds, they can also capture images of their surroundings. And if youre able to classify what you see if the robots is able to classify what it sees, then it can actually act upon it, and take certain actions. Thats why it becomes really helpful to successfully classify the images. o this is what they did using hand gestures, where there were five different classes. Each class corresponds to the number of extended fingers. o a, b, c, d, the top row, corresponds to the same class. They all have two fingers sticking out. The bottom row has three fingers sticking out. o thats another class. And they get the error rate down all the way to 3%. o 97% of the time, the convolutional neural net correctly classified the hand gesture. And you can use these hand gestures then to give certain commands to a robot, and it can train the NN to act upon something else besides hand gestures. For example, if its in some sort of terranean and you train it on certain images that you might find in nature, then it can take those classifications, and act upon it once it sees, for example, a tree, or some sort of body of water. ts all thanks to image classification. Now, obviously, gestures are not necessarily static. You could be waving your hand, and so that would require a temporal component. o its not just an image youre looking at, but a video. And so if follows that we can probably extend image classification into video classification. After all, videos are just images with an added component, specifically time. Obviously, the added temporal component comes with a lot of additional complexity. o were not going to dive into any of that, but in the end, it comes down to the same thing. You extract features from the videos, and you attempt to classify them using convolutional neural nets. o why dont we look at a study done, again, at Google, where they extracted one million videos from YouTube, sports videos, with somewhere between 400 to 500 different classes, and they used NNs to attempt to classify these videos. Now, they used different approaches. They used different approaches, different tests, different types of NNs that m not going to go into. But as you can see here, these are certain stills from these videos where the caption highlighted in blue is what the correct answer should be, and underneath it, the top five labels that the convolutional neural network producers. The one highlighted in green is supposed to be the correct answer. o you can see on all of these, it gets it within the top five, and for the most part, within the top two, and its pretty confident when it does get it correctly. Now, when said that they use different types of classifieds, some of them were more stacked classifieds, where they were just trained on stills within these images, while others were what they called fusion ones, where they sort of add the temporal component by fusing in different stills from these photo images together. Now, the current accuracy rate the best one theyve achieved so far has been around 80% accuracy within the top five label. Now, 80% accuracy is nowhere near what we saw with the mageNet classification, where in 2015, they had managed to get it up to 98% or 99% accuracy. But obviously, theres way more complexity involved in this. o it makes sense that its not quite there yet. But it does provide a good benchmark, and something to improve upon in the future as well. Now, that being said, convolutional neural networks do come with certain limitations. Theyre not perfect. And so Julian will now talk about the limitations. JLAN BROWN: Thanks, Ali. o Ali talked about the mageNet competition, and talked about how the recent winners have been convolutional neural nets. o before, the best was about 26% top five error rate, but now theyve actually gotten it down to a 4.9% top five error rate, and that was the winner of that competition, the 2015 one, was actually icrosoft. Theyve got the current state of the art implementation, and so because its the mageNet competition, that means they can identify exactly 1,000 different categories of images. o there are few problems, actually, with the implementation, or just in general with convolutional neural nets. o one of them is that 1,000 categories, well, it may seem like a lot mageNet is actually one of the largest competitions thats not actually that many categories. o it doesnt contain things like hot air balloons, for instance. o these things that children would be able to classify, the neural nets actually arent able to, even in the biggest competition. And each of these categories also requires thousands of training images, whereas you could show a child a couple examples of a dog or a cat, and theyd be able to, generally, get a feel for what a dog or a cat looks like. t takes thousands of images per category for the neural nets to learn, which means that the total number of images you need to train for the mageNet competition is over a million. And so this leads to very long training times, even with all of the heavy optimizations that shwari was telling us about like how efficient convolution is, it still takes weeks to train on multiple parallel GPs working together to train the net. Theres actually a more fundamental problem with neural nets as well. o here on the left, we have a school bus, some kind of bird, and an ndian temple. And all of these images on the left side are actually correctly identified by convolutional neural nets. But when we add this small distortion here in the middle, that doesnt change any of the images perceptively to the human, this actually causes the neural network to misclassify these images, and now all three of them are ostriches. o thats a little weird. How does this work? How did we find those distortions. o here on the left side, we see how a neural network typically works. You start with some images, you put it through the different layers of the neural network, and then it tells you a certain probability that it is a guitar, or a penguin. o it classifies it, and so we can use a modification of that method by applying an evolutionary algorithm, or a hillclimbing or gradient ascent algorithm. We take a couple of images, and we put them through, it classifies it, and we see what the classification is. And then we can do some crossover between the images. o we take the ones that look a lot like what were training for in the guitars or penguins, in this case, and we take the features of those that identify very strongly as a guitar, and we combine those together in the crossover phase. This is for the evolutionary algorithm. Then we mutate the images, which is making small changes to each one, and then we reevaluate by plugging it back in through the neural network, and only the best images, the ones that looked the most like a guitar or penguin, are then selected for the next iteration. And this continues until you get to a very high identification rates, even higher than actual images of the objects. o using gradient ascent, these are some of the images that you could produce if you start with just a flat grey image, and then you run it through this algorithm. o here on the side, we have a backpack, and we can actually see the outline of what looks like a backpack in there. And over here, we have what looks like a Windsor tie right here, but all of these objects and perhaps, there are things in these other images, but they seem to be lost in the LD trip of colors here. o thats kind of strange. Thats definitely not how humans do it. o lets try a different method. What if instead of directly encoding, which is where we change individual pixels, what if we change patterns in the images, like different shapes? Then this is the kind of output that we get. o in the upper left, we have a starfish. o you can see that it has the orange and blue of the orange in the starfish, and also the blue of the ocean of the environment that typical images of starfish are taken in. And you can also see that it has the points, the jagged lines, the triangles that we associate with the arms of a starfish. But the strange thing here is that theyre not arranged in a circular pattern. Theyre not pointing outwards like this, like we would expect of an actual starfish. o clearly, its not latching onto the same large scale features that humans do. ts actually just looking at the low down features. Even though its a deep neural network, it doesnt grab onto these abstract concepts like a human would. o the reason for this problem, or at least why we think neural networks arent as good as humans at things like this is the type of model. o a human would have more of whats called a generative model, which means if we have examples here, these dark blue dots, say, of lines, a few examples of images of lines, then we could construct a probability distribution, and say that images that fall somewhere in this region are lines. And over here, we have a few examples of giraffes, say, and so anything that falls in this region would be a giraffe. And so if you had a red triangle in here, that would be a lion. But if the red triangle is instead over here, it actually wouldnt classify at all. We wouldnt know what that is. We would say thats something other than a lion or a giraffe, but neural networks dont work the same way. They just draw a decision boundary. They just draw lines between the different categories. o they dont say that something really far away from the lion class is necessary not a lion. t just depends how far away it is from the decision boundary. o if we have the red triangle way over there, its very far away from giraffes, and its just generally closer lions, even though it isnt explicitly very close to it at all, and that will still be identified as a lion. o thats why we think were able to fool these neural networks in such a simplistic way or in such a really abstract way. o the main takeaways from our presentation, and the salient points are that deep learning is a very powerful tool for image classification, and it relies on multiple layers of a network. o multiple processing layers. Also NNs outperform basically every other method for classifying images, and thats their primary use right now. Were currently exploring other uses, but thats generally where its at, and this is because convolutional filters are just so incredibly powerful. Theyre very fast and very efficient. Also back propagation is the way that we train neural networks. Normally, if you were to train a neural network that has a lot of layers, theres actually an exponential growth in the time it takes to train because of the branching when you go backwards because each neuron is connected to a large number of neurons in the previous layer. You get this exponential growth in the number of dependencies from a given neuron. By using back propagation, it actually reduces it to linear time to train the networks. o this allows for efficient training. And even with back propagation and convolution being so efficient, it still takes a very large number of images, and a long time with a lot of processing power to train neural networks. Also, if youd like to get started working with neural networks, there are a couple of really nice open source programming platforms for neural networks. o one of them that we used for our pset was actually TensorFlow, which is Googles open source neural network platform, and another one would be afe, which is Berkleys neural network platform. And they actually have an online demo where you can plug in images, and immediately get identifications. o you can get started very quickly with that one. Thank you.","Now, if we go back to the points that we made earlier, one thing we said was that the reason you use a neural network is because you dont know exactly what features you want to extract, and you dont actually have specific filters that you want to apply to the image. And if you aggregate the output of all of these neurons, what you would be doing or performing is, essentially, a convolution on the original image. n that case, we can see that thats a neuron, and what the neuron is doing here is the weighted sum that we talked about, or the dot product. o what can see here is that for that five by five by three patch in the original image, we have a neuron that is actually performing the dot product on all the pixels in that specific patch. And what this essentially is is layers that are very similar, or exactly the same as what you had before, except that every neuron in the layer is connected to all the previous neurons. o in a more graphical presentation, which is a little bit easier to understand than just seeing the formula, the idea is that we have a kernel, or a convolutional filter that we seek to apply on another image, and that extracts some information about that image that we can use to help us classify the convolution. n that case, if youre actually trying to classify your input into four classes, you would ideally have four different neurons in your output layer, each one corresponding to one of the classes that you have, and then you would perform the same operation as you would in a previous layer, compute the dot product, and then once you obtain the values at every neuron, you would perform a normalization operation on all the output. At this point, you dont really care about where its located in the image, and you want to keep track of all the features that your neurons have detected in order to be able to eventually classify the input correctly. NNs came onto the picture in 2012, but the winning NN from 2012 was used on the 2010 mageNet competition as well, and it managed to bring down the top five error rate to 0.17, which is pretty much on the same level as how performed in the 2012 competition when it was first used, which was at 0.16. o this just goes to show that these convolutional neural networks are the state of the art when it comes to image classification, and thats why were currently focusing on that. o for example, in the case of the chair that was previously mentioned, if youre trying to identify whether a specific image has a chair in it or not, these 12 inputs here could be some sub images, or some small areas of the initial image that you feed into the network, and the output here could be a yes or no. o in that case, we would have 28 by 28 by one neuron per layer, and then if we aggregate all these neurons together, we need to multiply it by five, and that would be the total number of neurons we have in that specific number. o were first going to talk about what deep learning is, and then well move into some of the image processing techniques that researchers use, followed by the architecture of the convolutional neural networks, which will be the main focus in our presentation. o you can think about the zebra image here thats actually a composite of different gradients that might look like this, and the Fourier coefficients would be how much of each of these pieces come together to make that final image. But when we add this small distortion here in the middle, that doesnt change any of the images perceptively to the human, this actually causes the neural network to misclassify these images, and now all three of them are ostriches. And then the sum here would be the dot product of the weights and the inputs, and that sum would be computed by a specific neuron in your network. And the reason that actually works is because what you want to do is really keep track of the response of the neurons that or the highest response produced by your neurons. But as you can see here, these are certain stills from these videos where the caption highlighted in blue is what the correct answer should be, and underneath it, the top five labels that the convolutional neural network producers. o the neuron specifies some weight, which you can think of as how much it values the input coming from a specific neuron, and then the input is multiplied by its weight, and then the total sum that the neuron computes is then fed into an activation function that produces the output that the neuron then basically produces. Then we mutate the images, which is making small changes to each one, and then we reevaluate by plugging it back in through the neural network, and only the best images, the ones that looked the most like a guitar or penguin, are then selected for the next iteration. But as you can see, the intuitive way that we classify a face, and the way our brains are wired to do it, is pretty similar to the way got these neural networks to operate. o we take the ones that look a lot like what were training for in the guitars or penguins, in this case, and we take the features of those that identify very strongly as a guitar, and we combine those together in the crossover phase. o if we take the Fourier transform of all the horizontal text here, you see how the magnitude reflects this, and you can go back to the math to understand why its reflected in a vertical marking. The weights that you would be using here would be the filter weights, which is the filter that you use in the convolution. One is to obviously reduce the size of your input, and your feature space, but also because you want to keep track of the most important information, and get rid of everything else that you dont think is going to be relevant to your classification. The NN, on the other hand, is pretty confident that what it sees in this image is the Dalmatian. Normally, if you were to train a neural network that has a lot of layers, theres actually an exponential growth in the time it takes to train because of the branching when you go backwards because each neuron is connected to a large number of neurons in the previous layer. o basically, for example, in this image, if want to say want to find all the images that have faces in them, or consist of faces, lets say, might come up with a template of a face that basically assigns gradients to groups of pixels that form an outline of what looks like a face, and then scan it across my sample images, and say OK, a face is present in this image. And what this has led Hubel and Wesel to is the idea that neurons in the brain are organized in a certain topographical order, and at each filter, it fills a specific role, and the only fires when its specific input is shown, or when the angle is show, or the angle is specified. Then this is the kind of output that we get. And what the neuron actually does is aggregate the action potentials or the inputs it gets from all the neighboring neurons that are connected to it through the timelines, and it sums these action potentials, and then compares them to a certain threshold that it has internally, and that would determine whether or not this neuron would fire an action potential. o you can see that it has the orange and blue of the orange in the starfish, and also the blue of the ocean of the environment that typical images of starfish are taken in. o the idea is we want to slide this filter across the image, and what were basically doing this is a succession of dot products. You start with some images, you put it through the different layers of the neural network, and then it tells you a certain probability that it is a guitar, or a penguin. And although this moves around the location of pixels, because you can think of that as subsampling an image, it does keep track of the information you care about because you only care about the fact that something has been detected in the image. o what each neuron would be doing in that case, is really just performing a dot product, which if you aggregate that with the dot products computed by each of the other neurons, you would obtain a convolution. o to be able to understand this process in greater detail, m first going to go into a little bit of math, and the idea is to present the most fundamental operation here, which is the convolution. o we basically subtract the mean across all the features of all the points, and we basically center it, and you can see that transformation here. o the dot product computed here would be theres a function that would be applied to the sum, and then that function would produce the output of the neuron. And the idea is even to a human perception, these images have two very different meanings, and so its interesting to see what effect they would have on a neural network. f the input, in that case, is an image or a sub image, then the inputs would be pixels. And similarly, if were to take that same image, and rotate it, and then ask for the Fourier transform, you see how that information is contained very clearly in the magnitude spectrum. o what this would produce in aggregate is if you have 28 by 28 by one neurons, each one performing a dot product on some pixels in the original image, the output would be an activation map of size 28 by 28 by one, and the output of each neuron would be one pixel in the activation map. And you can also see that it has the points, the jagged lines, the triangles that we associate with the arms of a starfish. Now, these five labels are produced by a convolutional neural network, and the different bars the different lengthened bars, some pink and others blue, represent how confident the NN is that what it sees in that image is that specific label. And over here, we have what looks like a Windsor tie right here, but all of these objects and perhaps, there are things in these other images, but they seem to be lost in the LD trip of colors here. We take a couple of images, and we put them through, it classifies it, and we see what the classification is. You cant really do much with just one neuron, so what you would do is combined these neurons in a certain topography, or in that case, we have a network with seven neurons organized in three different layers. o that is precisely what a neural network actually ends up doing, and well walk through the process of how it does this later on in the talk. o with that, we can now go into what the actual architecture of a convolutional neural is. o that corresponds to around 96% to 97% accuracy, and what that enables Google to do is that they can deploy the NN such that the NN automatically extracts the house numbers from the images online, and uses that to geocode these addresses. o now that Henry has given us an overview of the entire architecture of a NN, m going to quickly spend some time, and talk about standard preprocessing tricks and tips that people might use on the image dataset before they actually feet it through a neural net to classify the images. o you can see on all of these, it gets it within the top five, and for the most part, within the top two, and its pretty confident when it does get it correctly. nderneath each image is a bold caption, which is considered to be the ground truth, or what the competition believes to be the correct classification of the image. Then, that would be the convolution step, and then that convolution step would happen at the first layer in the network. o you begin by moving from the retina into the first areas of the brain, and as the information gets processed, it moves from one region of the brain to the other, and each spatial element of your brain is performing an entirely different calculation. Then, after that dot product is performed, its fed into an activation function, as can be seen here, and this produces the output of this neuron. o what this function essentially does is map any input to an output in the range of zero to one, and its defined as one divided by one plus e to the minus x. The other common activation function is tanh, and thats any input to an output between minus one and one. o what the NN was tasked with doing was that it was supposed to recognize the individual digits within the image, and then understand that its not just one digit that its looking at, but its actually a string of digits connected, and successfully classified as the correct house number. o might use an image processing technique called a Harris orner Detector, where we basically look at large changes in intensity as groups of pixels move from an image to an image that indicate the presence of corners, and you can use common corners to say that OK, all of these images are chairs, or doors, or whatever. What we just went through now are convolutions and activations, but this is not the only thing that actually happens in a neural net. o this is exactly the idea of NNs, the convolutional neural networks. o ideally, what you want to do is have multiple filters being applied to the first image, and perform multiple convolutions, and this is what you can do with multiple neuron layers. o W2 plus W1 minus filter width plus one, and the same formula applies for the height, and the depth would actually be the same because in that case, were using a filter that has the same depth, or three, as the original image. o what is happening here is the pixel values, which in that case, we have five by five by three pixels, are being multiplied by the filter values, and this operation is being performed here. And formally, what this would look like is if you have an input of size H1, W1, D1, the size of your output would be related to your input in the following ways. But it goes to show you that NNs could also be used not just as an image classification, but also as object detection, which we do not touch up on in this lecture at all. Now, the funny thing about the mushroom image is that what it thinks the image should be classified as is agaric. n that case, for example, the first highest response in the first square is six, and that means that if you get that high of a response, it means that something has been detected in the image, or has been detected. And if youre able to classify what you see if the robots is able to classify what it sees, then it can actually act upon it, and take certain actions. And this is where the activation layer is. t takes thousands of images per category for the neural nets to learn, which means that the total number of images you need to train for the mageNet competition is over a million. OK, one last helpful piece of information is that theres another way that you can think of the information thats learned at each stage because a convolution can also be thought of as a Fourier transform in the frequency domain. As you can see here, the number of neurons, as well as the neurons that were firing were very different based on the angle. o what you can see also here is really a plot of the response versus the orientation of the light. For example, if its in some sort of terranean and you train it on certain images that you might find in nature, then it can take those classifications, and act upon it once it sees, for example, a tree, or some sort of body of water. As you can see here, these are images from the mageNet competition. And its gotten to a point were the NN is successfully able to do this process in less than an hour for all of the street view house numbers in all of France. o using gradient ascent, these are some of the images that you could produce if you start with just a flat grey image, and then you run it through this algorithm.",0.1080757726819541
26,26,"The following content is provided under a creative commons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: One of the things that you should probably have noticed is as were moving in the terms, the problem sets are getting less well defined. line And ve seen a lot of email traffic of the nature of what should we do with this, what should we do with that? For example, suppose the computer player runs out of time. Or the person runs out of time playing the game. hould it stop right away? hould it just give them zero score? Thats left out of the problem set. n part because one of the things were trying to accomplish is to have you folks start noticing ambiguities in problem statements. Because thats life in computing. And so this is not like a math problem set or a physics problem set. Or, like a high school physics lab, where we all know what the answer should be, and you could fake your lab results anyway. These are things where youre going to have to kind of figure it out. And for most of these things, all we ask is that you do something reasonable, and that you describe what it is youre doing. o dont much care, for example, whether you give the human players zero points for playing after the time runs out. Or you say youre done when the time runs out. Any of that thank you, heila is ok with me. Whatever. What dont want your program to do is crash when that happens. Or run forever. Figure out something reasonable and do it. And again, well see this as an increasing trend as we work our way through the term. The exception will be the next problem set, which will come out Friday. Because thats not a programming problem. ts a problem, as youll see, designed to give you some practice at dealing with some of the, dare say, more theoretical concepts weve covered in class. Like algorithmic complexity. That are not readily dealt with in a prograing problem. t also deals with issues of some of the subtleties of things like aliasing. o theres no programming. And in fact, were not even going to ask you to hand it in. ts a problem set where weve worked pretty hard to write some problems that we think will provide you with a good learning experience. And you should just do it to learn the material. Well help you, if you need to see the TAs because you cant do them, by all means make sure you get some help. o m not suggesting that its an optional problem set that you shouldnt do. Because you will come to regret it if you dont do it. But were not going to grade it. And since were not going to grade it, it seems kind of unfair to ask you to hand it in. o its a short problem set, but make sure you know how to do those problems. OK. Today, for the rest of the lecture, were going to take a break from the topic of algorithms, and computation, and things of the sort. And do something pretty pragmatic. And were going to talk briefly about testing. And at considerable length about debugging. have tried to give this lecture at the beginning of the term, at the end of the term. Now m trying it kind of a third of the way, or middle of the term. never know the right time to give it. These are sort of pragmatic hints that are useful. suspect all of you have found that debugging can be a frustrating activity. y hope is that at this point, youve experienced enough frustration that the kind of pragmatic hints m going to talk about will not be, ""yeah sure, of course."" But theyll actually make sense to you. Well see. OK. n a perfect world, the weather would always be like its been this week. The in T would stand for aui, instead of assachusetts. Quantum physics would be easier to understand. All the supreme court justices would share our social values. And most importantly, our programs would work the first time we typed them. By now you may have noticed that we do not live in an ideal world. At least one of those things mentioned is not true. m only going to address the last one. Why our programs dont work. And will leave the supreme court up to the rest of you. There is an election coming up. Alright, First a few definitions. Things want to make sure we all understand what they mean. Validation is a process. And want to emphasize the word process. Designed to uncover problems and increase confidence that our program does what we think its intended to do. want to emphasize that it will increase our confidence, but we can never really be sure weve got it nailed. And so its a process that goes on and on. And also want to emphasize that a big piece of it is to uncover problems. o we need to have a method not designed to give us unwarranted confidence. But in fact warranted confidence in our programs. ts typically a combination of two things. Testing and reasoning. Testing, we run our program on some set of inputs. And check the answers, and say yeah, thats what we expected. But it also involves reasoning. About why thats an appropriate set of inputs to test it on it. Have we tested it on enough inputs? aybe just reading the code and studying it and convincing ourselves that works. o we do both of those as part of the validation process. And well talk about all of this as we go along. Debugging is a different process. And thats basically the process of ascertaining why the program is not working. Why its failing to work properly. o validation says whoops, its not working. And now we try and figure out why not. And then of course, once we figure out why not, we try and fix it. but today m going to emphasize not how do you fix it, but how do you find out whats wrong. sually when you know why its not working, its obvious what you have to do to make it work. There are two aspects of it. Thus far, the problem sets have mostly focused on function. Does it exhibit the functional behavior? Does it give you the answer that you expected it to give? Often, in practical problems, youll spend just as much time doing performance debugging. Why is it slow? Why is it not getting the answer as fast as want it to? And in fact, in a lot of industry for example, if youre working on building a computer game, youll discover that in fact the people working the game will spend more time on performance debugging than on getting it to do the right thing. Trying to make it do it fast enough. Or get to run on the right processor. ome other terms weve talked about is defensive programming. And weve been weaving that pretty consistently throughout the term. And thats basically writing your programs in such a way that it will facilitate both validation and debugging. And weve talked about a lot of ways we do that. One of the most important things we do is we use assert statements so that we catch problems early. We write specifications of our functions. We modularize things. And well come back to this. As every time we introduce a new programming concept, well relate it back, as we have been doing consistently, to defensive programming. o one of the things want you to notice here is that testing and debugging are not the same thing. When we test, we compare an input output pair to a specification. When we debug, we study the events that led to an error. ll return to testing later in the term. But do want to make a couple of quick remarks with very broad strokes. There are basically two classes of testing. Theres unit testing, where we validate each piece of the program independently. Thus far, for us its been testing individual functions. Later in the term, well talk about unit testing of classes. The other kind of testing is integration testing. Where we put our whole program together, and we say does the whole thing work? People tend to want to rush in and do this right away. Thats usually a big mistake. Because usually it doesnt work. And so one of the things that think is very important is to always begin by testing each unit. o before try and run my program, test each part of it independently. And thats because its easier to test small things than big things. And its easier to debug small things than big things. Eventually, its a big program, run it. t never works the first time if its a big program. And end up going back and doing unit testing anyway, to try and figure out why it doesnt work. o over the years, ve just convinced myself might as well start where m going to end up. Whats so hard about testing? Why is testing always a challenge? Well, you could just try it and see if it works, right? Thats what testing is all about. o we could look at something small. Just write a program to find the max of x and y. Where x and y are floats. However many quotes need. Well, just see if it works. Lets test it in all possible combinations of x and y and see if we get the right answer. Well, as arl agan would have said, there are billions and billions of tests we would have to do. Or maybe its billions and billions and billions. Pretty impractical. And its hard to imagine a simpler program than this. o we very quickly realize that exhaustive testing is just never feasible for an interesting program. o as we look at testing, what we have to find is whats called a test suite. A test suite is small enough so that we can test it in a reasonable amount of time. But also large enough to give us some confidence. Later in the term, well spend part of a lecture talking about, how do we find such a test suite? A test suite that will make us feel good about things. For now, just want you to be aware that youre always doing this balancing act. o lets assume weve run our test suite. And, sad to say, at least one of our tests produced an output that we were unhappy with. t took it too long to generate the output. Or more likely, it was just the wrong output. That gets us to debugging. o a word about debugging. Where did the name come from? Well heres a fun story, at least. This was one of the very first recorded bugs in the history of computation. Recorded eptember 9th, 1947, in case youre interested. This was the lab book of Grace urray Hopper. Later Admiral Grace urray Hopper. The first female admiral in the .. navy. Who was also one of the words first programmers. o she was trying to write this program, and it didnt work. t was a complicated program. t was computing the arctan. o you can imagine, right? You had a whole team of people trying to figure out how to do arctans. Times were different in those days. And they tried to run it, and it ran a long time. Then it basically stopped. Then they started the cosine tape. That didnt work. Well they couldnt figure out what was wrong. And they spent a long time trying to debug the program. They didnt apparently call it debugging. And then they found the problem. n relay number 70, a moth had been trapped. And the relay had closed on the poor creature, crushing it to death. The defense department didnt care about the loss of a moth. But they did care about the fact that the relay was now stuck. t didnt work. They removed the moth, and the program worked. And youll see at the bottom, it says the first actual case of a bug being found. And they were very proud of themselves. Now its a wonderful story, and it is true. After all, Grace wouldnt have lied. But its not the first use of the term ""bug."" And as youll see by your handout, ve attempted tend to trace it. And the first one could find was in 1896. n a handbook on electricity. Alright. Now debugging is a learned skill. Nobody does it well instinctively. And a large part of being a good programmer, or learning to be a good programmer, is learning how to debug. And its one of these things where its harder. ts slow, slow, and you suddenly have an epiphany. And you now get the hang of it. And m hoping that todays lecture will help you learn faster. The nice thing, is once you learn to debug programs, you will discover its a transferable skill. And you can use it to debug other complex systems. o for example, a laboratory experience. Why isnt this experiment working? Theres a lecture ve given several times at hospitals, to doctors, on doing diagnosis of complex multi illnesses. And go through it, and almost the same kind of stuff m going to talk to you about, about debugging. Explaining that its really a process of engineering. o want to start by disabusing you of a couple of myths about bugs. o myth one is that bugs crawl into programs. Well it may have been true in the old days, when bugs flew or crawled into relays. ts not true now. f there is a bug in the program, its there for only one reason. You put it there. i.e. you made a mistake. o we like to call them bugs, because it doesnt make us feel stupid. But in fact, a better word would be mistake. Another myth is that the bugs breed. They do not. f there are multiple bugs in the program, its because you made multiple mistakes. Not because you made one or two and they mated and produced many more bugs. t doesnt work that way. Thats a good thing. Typically, even though they dont breed, there are many bugs. And keep in mind that the goal of debugging is not to eliminate one bug. The goal is to move towards a bug free program. emphasize this because it often leads to a different debugging strategy. People can get hung up on sort of hunting these things down, and stamping them out, one at a time. And its a little bit like playing Whackaole. Right? They keep jumping up at you. o the goal is to figure out a way to stamp them all out. Now, should you be proud when you find a bug? ve had graduate students come to me and say found a bug in my program. And theyre really proud of themselves. And depending on the mood m in, either congratulate them, or say ah, you screwed up, huh? Then you had to fix it. f you find a bug, it probably means there are more of them. o you ought to be a little bit careful. The story ve heard told is youre at somebodys house for dinner, and youre sitting at the dining room table, then you hear a . And then your hostess walks in with the turkey in a tray, and says, "" killed the last cockroach."" Well it wouldnt increase my appetite, at least. o be worried about it. For at least four decades, people have been building tools called debuggers. Things to help you find bugs. And there are some built into dol. y personal view is most of them are not worth the trouble. The two best debugging tools are the same now that they have almost always been. And they are the print statement, and reading. There is no substitute for reading your code. Getting good at this is probably the single most important skill for debugging. And people are often resistant to that. Theyd rather single step it through using dol or something, than just read it and try and figure things out. The most important thing to remember when youre doing all of this is to be systematic. Thats what distinguishes good debuggers from bad debuggers. Good debuggers have evolved a way of systematically hunting for the bugs. And what theyre doing as they hunt, is theyre reducing the search space. And they do that to localize the source of the problem. Weve already spent a fair amount of time this semester talking about searches. Algorithms for searching. Debugging is simply a search process. When you are searching a list to see whether it has an element, you dont randomly probe the list, hoping to find whether or not its there. You find some way of systematically going through the list. Yet, often see people, when theyre debugging, proceeding at what, to me, looks almost like a random fashion of looking for the bug. That is a problem that may not terminate. o you need to be careful. o lets talk about how we go about being systematic, as we do this. o debugging starts when we find out that there exists a problem. o the first thing to do is to study the program text, and ask how could it have produced this result? o theres something subtle about the way ve worded this. didnt ask, why didnt it produce the result wanted it to produce? Which is sort of the question wed immediately like to ask. nstead, asked why did it produce the result it did. o m not asking myself whats wrong? Or how could make it right? m asking how could have done this? didnt expect it to do this. f you understand why it did what it did, youre half way there. The next big question you ask, is it part of a family? This gets back to the question of trying to get the program to be bug free. o for example, oh, it did this because it was aliasing, where hadnt expected it. Or some side effect of some mutation with lists. And then say, oh you know ve used lists all over this program. ll bet this isnt the only place where ve made this mistake. o you say well, rather than rushing off and fixing this one bug, let me pull back and ask, is this a systematic mistake that ve made throughout the program? And if so, lets fix them all at once, rather than one at a time. And that gets me to the final question. How to fix it. When think about debugging, think about it in terms of what you learned in high school as the scientific method. Actually, should ask the question. aybe m dating myself. Do they still teach the scientific method in high school? Yes, alright good. All is not lost with the American educational system. o what does the scientific method tell us to do? Well it says you first start by studying the available data. n this case, the available data are the test results. And by the way, mean all the test results. Not just the one where it didnt work, but also the ones where it did. Because maybe the program worked on some inputs and not on others. And maybe by understanding why it worked on a and not on b, youll get a lot of insight that you wont if you just focus on the bug. Youll also feel a little bit better knowing your program works on at least something. The other big piece of available data we have is, of course, the program text. As you the study the program text, keep in mind that you dont understand it. Because if you really did, you wouldnt have the bug. o read it with sort of a skeptical eye. You then form a hypothesis consistent with all the data. Not just some of the data, but all of the data. And then you design and run a repeatable experiment. Now what is the thing we learned in high school about how to design these experiments? What must this experiment have the potential to do, to be a valid scientific experiment? omebody? Whats the key thing? t must have the potential to refute the hypothesis. ts not a valid experiment if it has no chance of showing that my hypothesis is flawed. Otherwise why bother running it? o it has to have that. Typically its nice if it can have useful intermediate results. Not just one answer at the end. o we can sort of check the progress of the code. And we must know what the result is supposed to be. Typically when you run an experiment, you say, and think the answer will be x. f its not x, youve refuted the hypothesis. This is the place where people typically slip up in debugging. They dont think in advance what they expect the result to be. And therefore, they are not systematic about interpreting the results. o when someone comes to me, and theyre about to do a test, ask them, what do you expect your program to do? And if they cant answer that question, say well, before you even run it, have an answer to that. Why might repeatability be an issue? Well as well see later in the term, were going to use a lot of randomness in a lot of our programs. Where we essentially do the equivalent of flipping coins or rolling dice. And so the program may do different things on different runs. Well see a lot of that, because its used a lot in modern computing. And so you have to figure out how to take that randomness out of the experiment. And yet get a valid test. ometimes it can be timing. f youre running multiple processes. Thats why your operating systems and your personal computers often crash for no apparent reason. Just because two things happen to, once in a while, occur at the same time. And often theres human input. And people have to type things out of it. o you want to get rid of that. And well talk more about this later. Particularly when we get to using randomness. About how to debug programs where random choices are being made. Now lets think about designing the experiment itself. The goal here, there are two goals. Or more than two. One is to find the simplest input that will provoke the bug. o its often the case that a program will run a long time, and then suddenly a bug will show up. But you dont want to have to run it a long time, every time you have a hypothesis. o you try and find a smaller input that will produce the problem. o if your word game doesnt work when the words are 12 letters long, instead of continuing to debug 12 letter hands, see if you can make it fail on a three letter hand. f you can figure out why fails on three letters instead of 12, youll be more than half way to solving the problem. What typically do is start with the input that provoked the problem, and keep making it smaller and smaller. And see if cant get it to show up. The other thing you want to do is find the part of the program that is most likely at fault. n both of these cases, strongly recommend binary search. Weve talked about this binary search a lot already. Again, the trick is, if you can get rid of half of the data at each shot, or half of the code at each shot., youll quickly converge on where the problem is. o now want to work through an example where we can see this happening. o this is the example on the handout. ve got a little program called illy. And its called illy because its really a rather ugly program. ts certainly not the right way to write a program to do this. But it will let us illustrate a few points. o the trick, what were going to go through here, is this whole scientific process. And see whats going on. o lets try running illy. o this is to test whether a list is a palindrome. o well put one as the first element, maybe a is the second element. And one is the third element. And just return, its done. t is a palindrome. That make sense. The list one a one reads the same from the front or from the back. o thats good. aking some progress. Lets try it again. And now lets do one, a, two. Whoops. t tells me it is a palindrome. Well, it isnt really. have a bug. Alright. Now what do do? Well m going to use binary search to see if cant find this bug. As go through, m going to try and eliminate half of the code at each step. And the way m going to do that is by printing intermediate values, as go part way through the code. m going to try and predict what the value is going to be. And then see if, indeed, get what predicted. Now, as do this, m going to use binary search. m going to start somewhere near the middle of the code. Again, a lot of times, people dont do that. And theyll test an intermediate value near the end or near the beginning. Kind of in the hope of getting there in one shot. And thats like kind of hoping that the element youre searching for is the first in the list and the last in the list. aybe. But part of the process of being systematic is not assuming that m going to get a lucky guess. But not even thinking really hard at this point. But just pruning the search space. Getting rid of half at each step. Alright. o lets start with the bisection. o were going to choose a point about in the middle of my program. Thats close to the middle. t might even be the middle. And were going to see, well all right. The only thing ve done in this part of the program, now m going to go and read the code, is ve gotten the user to input a bunch of data. And built up the list corresponding to the three items that the user entered. o the only intermediate value have here is really res. o m going to, just so when m finished know what it is that think ve printed. But in fact maybe ll do even more than that here. Let me say what think it should be. And then well see if it is. o think put in one a two, right? Or one a two? o it should be something like one, a, two. o predicted what answer m expecting to get. And ve put it in my debugging code. And now ll run it and see what we get. Well save it. Well all right, a syntax error. This happens. And theres a syntax error. see. Because ve got a quote in a quote. Alright m just going to do that. What expected. o what have learned? ve learned that with high probability, the error is not in the first part of the program. o can now ignore that. o now have these six lines. o well try and go in the middle of that. ee if we can find it here. And notice, by the way, that commented out the previous debugging line, rather than got rid of it. ince m not sure wont need to go back to it. o what should look at here? Well there are a couple of interesting intermediate values here, right? Theres tmp. And theres res. Never type kneeling. Right? find something to tmp. And need to make sure maybe havent messed up res. Now it would be easy to assume, dont bother looking at . Because the code doesnt change res. Well remember, that started this with a bug. That means it was something didnt understand. o m going to be cautious and systematic. And say lets just print them both. And see whether theyre okay. Now, lets do this. o it says tmp is two a one, and res is two a one. Well lets think it. s this what we wanted, here? Whats the basic idea behind this program? How is it attempting to work? Well what its attempting to do, and now is when have to stand back and form a hypothesis and think about whats going on, is it gets in the list, it reverses the list, and then sees whether the list and the reverse were identical. f so it was a palindrome, otherwise it wasnt. o ve now done this, and what do you think? s this good or bad? s this what should be getting? No. Whats wrong? omebody? yeah. TDENT: PROFEOR: Yeah. omehow wanted to Got to work on those hands. didnt want to change res. o, now know that the bug has got to be between these two print statements. m narrowing it down. ts getting a little silly, but you know m going to really be persistent and just follow the rules here of binary search, rather than jumping to conclusions. Well clearly what probably want to do here is what? Print these same two things. ee what get. Whoops. have to, of course, do that. Otherwise it just tells me that illy happens to be a function. Alright. How do feel about this result? feel pretty good here. Right? The idea was to make a copy of res and temp. And sure enough, theyre both the same. What expected them to be. o know the bug is not above. Now m really honing in. now know its got to be between these two statements. o lets put it there. Aha. ts gone wrong. o now ve narrowed the bug down to one place. know exactly which statement its in. o something has happened there that wasnt what expected. Who wants to tell me what that bug is? Yeah? TDENT: . PROFEOR: Right. Bad throw, good catch. o this is a classic error. ve not made a copy of the list. ve got an alias of the list. This was the thing that tripped up many of you on the quiz. And really what should have done is this. Now well try it. Ha. ts not a palindrome. o small silly little exercise, but m hoping that youve sort of seen how by being patient. Patience is an important part of the debugging process. have not rushed. ve calmly and slowly narrowed the search. Found where the statement is, and then fixed it. And now m going to go hunt through the rest of my code to look for places where used assignment, when should have use cloning as part of the assignment. The bug, the family here, is failure to clone when should have cloned. Thursday well talk a little bit more about what to do once weve found the bug, and then back to algorithms.","Well what its attempting to do, and now is when have to stand back and form a hypothesis and think about whats going on, is it gets in the list, it reverses the list, and then sees whether the list and the reverse were identical. o one of the things want you to notice here is that testing and debugging are not the same thing. And for most of these things, all we ask is that you do something reasonable, and that you describe what it is youre doing. The only thing ve done in this part of the program, now m going to go and read the code, is ve gotten the user to input a bunch of data. The other thing you want to do is find the part of the program that is most likely at fault. o the first thing to do is to study the program text, and ask how could it have produced this result? And thats like kind of hoping that the element youre searching for is the first in the list and the last in the list. And go through it, and almost the same kind of stuff m going to talk to you about, about debugging. Lets test it in all possible combinations of x and y and see if we get the right answer. And they do that to localize the source of the problem. This gets back to the question of trying to get the program to be bug free. And you now get the hang of it. And keep in mind that the goal of debugging is not to eliminate one bug. And since were not going to grade it, it seems kind of unfair to ask you to hand it in. ve learned that with high probability, the error is not in the first part of the program. sually when you know why its not working, its obvious what you have to do to make it work. And so you have to figure out how to take that randomness out of the experiment. o well try and go in the middle of that. And in fact, were not even going to ask you to hand it in. Today, for the rest of the lecture, were going to take a break from the topic of algorithms, and computation, and things of the sort. PROFEOR: One of the things that you should probably have noticed is as were moving in the terms, the problem sets are getting less well defined. And you should just do it to learn the material. Well, you could just try it and see if it works, right? These are things where youre going to have to kind of figure it out. And thats basically the process of ascertaining why the program is not working. And in fact, in a lot of industry for example, if youre working on building a computer game, youll discover that in fact the people working the game will spend more time on performance debugging than on getting it to do the right thing. And now ll run it and see what we get. And then well see if it is. m going to try and predict what the value is going to be. And people have to type things out of it. ts certainly not the right way to write a program to do this. y hope is that at this point, youve experienced enough frustration that the kind of pragmatic hints m going to talk about will not be, ""yeah sure, of course."" have tried to give this lecture at the beginning of the term, at the end of the term. line And ve seen a lot of email traffic of the nature of what should we do with this, what should we do with that? And maybe by understanding why it worked on a and not on b, youll get a lot of insight that you wont if you just focus on the bug. o when someone comes to me, and theyre about to do a test, ask them, what do you expect your program to do? Now m trying it kind of a third of the way, or middle of the term. Alright m just going to do that. And then of course, once we figure out why not, we try and fix it. And so one of the things that think is very important is to always begin by testing each unit. Why is it not getting the answer as fast as want it to? And also want to emphasize that a big piece of it is to uncover problems. And were going to see, well all right. o ve now done this, and what do you think? Again, the trick is, if you can get rid of half of the data at each shot, or half of the code at each shot., youll quickly converge on where the problem is. But its not the first use of the term ""bug."" Just write a program to find the max of x and y. Where x and y are floats. The most important thing to remember when youre doing all of this is to be systematic. have to, of course, do that. And the way m going to do that is by printing intermediate values, as go part way through the code. Not just the one where it didnt work, but also the ones where it did. Does it give you the answer that you expected it to give? n part because one of the things were trying to accomplish is to have you folks start noticing ambiguities in problem statements. And if they cant answer that question, say well, before you even run it, have an answer to that. And we must know what the result is supposed to be. Typically when you run an experiment, you say, and think the answer will be x. f its not x, youve refuted the hypothesis. o you say well, rather than rushing off and fixing this one bug, let me pull back and ask, is this a systematic mistake that ve made throughout the program? Well as well see later in the term, were going to use a lot of randomness in a lot of our programs. As you the study the program text, keep in mind that you dont understand it. Because you will come to regret it if you dont do it. And now we try and figure out why not. Thursday well talk a little bit more about what to do once weve found the bug, and then back to algorithms. but today m going to emphasize not how do you fix it, but how do you find out whats wrong. And end up going back and doing unit testing anyway, to try and figure out why it doesnt work. o it has to have that. But part of the process of being systematic is not assuming that m going to get a lucky guess. About why thats an appropriate set of inputs to test it on it. What typically do is start with the input that provoked the problem, and keep making it smaller and smaller. One is to find the simplest input that will provoke the bug. When think about debugging, think about it in terms of what you learned in high school as the scientific method. When you are searching a list to see whether it has an element, you dont randomly probe the list, hoping to find whether or not its there. This was the thing that tripped up many of you on the quiz. And now m going to go hunt through the rest of my code to look for places where used assignment, when should have use cloning as part of the assignment.",0.15460642428521
27,27,"Well, nows as good a time as any to go over some interesting and very useful properties of the Laplace transform. And the first is to show that it is a linear operator. And what does that mean? Well, lets say wanted to take the Laplace transform of the sum of the we call it the weighted sum of two functions. o say some constant, c1, times my first function, f of t, plus some constant, c2, times my second function, g of t. Well, by the definition of the Laplace transform, this would be equal to the improper integral from 0 to infinity of e to the minus st, times whatever our function that were taking the Laplace transform of, so times c1, f of t, plus c2, g of t think you know where this is going all of that dt. And then that is equal to the integral from 0 to infinity. Lets just distribute the e the minus st. That is equal to what? That is equal to c1e to the minus st, f of t, plus c2e to the minus st, g of t, and all of that times dt. And just by the definition of how the properties of integrals work, we know that we can split this up into two integrals, right? f the integral of the sum of two functions is equal to the sum of their integrals. And these are just constant. o this is going to be equal to c1 times the integral from 0 to infinity of e to the minus st, times f of t, d of t, plus c2 times the integral from 0 to infinity of e to the minus st, g of t, dt. And this was just a very longwinded way of saying, what is this? This is the Laplace transform of f of t. This is the Laplace transform of g of t. o this is equal to c1 times the Laplace transform of f of t, plus c2 times this is the Laplace transform the Laplace transform of g of t. And so, we have just shown that the Laplace transform is a linear operator, right? The Laplace transform of this is equal to this. o essentially, you can kind of break up the sum and take out the constants, and just take the Laplace transform. Thats something useful to know, and you might have guessed that was the case anyway. But now you know for sure. Now well do something which consider even more interesting. And this is actually going to be a big clue as to why Laplace transforms are extremely useful for solving differential equations. o lets say want to find the Laplace transform of f prime of t. o have some f of t, take its derivative, and then want the Laplace transform of that. Lets see if we can find a relationship between the Laplace transform of the derivative of a function, and the Laplace transform of the function. o were going to use some integration by parts here. Let me just say what this is, first of all. This is equal to the integral from 0 to infinity of e to the minus st, times f prime of t, dt. And to solve this, were going to use integration by parts. Let me write it in the corner, just so you remember what it is. o think memorized it, because recorded that last video not too long ago. m just going to write this shorthand. The integral of u well, lets say uv prime, because that will match what we have up here better is equal to both functions without the derivitives, uv minus the integral of the opposite. o the opposite is u prime v. o here, the substitution is pretty clear, right? Because we want to end up with f of x, right? o lets make v prime is f prime, and lets make u e to the minus st. o lets do that. u is going to be e to the minus st, and v is going to equal what? v is going to equal f prime of t. And then u prime would be minus se to the minus st. And then, v prime oh, sorry, this is v prime, right? v prime is f prime of t, so v is just going to be equal to f of t. hope didnt say that wrong the first time. But you see what m saying. This is u, thats u, and this is v prime. And if this is v prime, then if you were to take the antiderivative of both sides, then v is equal to f of t. o lets apply integration by parts. o this Laplace transform, which is this, is equal to uv, which is equal to e to the minus st, times v, f of t, minus the integral and, of course, were going to have to evaluate this from 0 to infinity. ll keep the improper integral with us the whole time. wont switch back and forth between the definite and indefinite integral. o minus this part. o the integral from 0 to infinity of u prime. u prime is minus se to the minus st times v v is f of t dt. Now, lets see. We have a minus and a minus, lets make both of these pluses. This s is just a constant, so we can bring it out. o that is equal to e to the minus st, f of t, evaluated from 0 to infinity, or as we approach infinity, plus s times the integral from 0 to infinity of e to the minus st, f of t, dt. And here, we see, what is this? This is the Laplace transform of f of t, right? Lets evaluate this part. o when we evaluated in infinity, as we approach infinity, e to the minus infinity approaches 0. f of infinity now this is an interesting question. f of infinity dont know. That could be large, that could be small, that approaches some value, right? This approach 0, so were not sure. f this increases faster than this approaches 0, then this will diverge. wont go into the mathematics of whether this converges or diverges, but lets just say, in very rough terms, that this will converge to 0 if f of t grows slower than e to the minus st shrinks. And maybe later on well do some more rigorous definitions of under what conditions will this expression actually converge. But lets assume that f of t grows slower than e to the st, or it diverges slower than this converges, is another way to view it. Or this grows slower than this shrinks. o if this grows slower than this shrinks, then this whole expression will approach 0. And then you want to subtract this whole expression evaluated at 0. o e to the 0 is 1 times f of 0 so thats just f of 0 plus s times we said, this is the Laplace transform of f of t, thats our definition so the Laplace transform of f of t. And now we have an interesting property. What was the lefthand side of everything we were doing? The Laplace transform of f prime of t. o let me just write all over again. And ll switch colors. The Laplace transform of f prime of t is equal to s times the Laplace transform of f of t minus f of 0. And now, lets just extend this further. What is the Laplace transform and this is a really useful thing to know what is the Laplace transform of f prime prime of t? Well, we can do a little pattern matching here, right? Thats going to be s times the Laplace transform of its antiderivative, times the Laplace transform of f prime of t, right? This goes to this, thats an antiderivative. This goes to this, thats one antiderivative. inus f prime of 0, right? But then whats the Laplace transform of this? This is going to be equal to s times the Laplace transform of f prime of t, but whats that? Thats this, right? Thats s times the Laplace transform of f of t, minus f of 0, right? just substituted this with this. inus f prime of 0. And we get the Laplace transform of the second derivative is equal to s squared times the Laplace transform of our function, f of t, minus s times f of 0, minus f prime of 0. And think youre starting to see a pattern here. This is the Laplace transform of f prime prime of t. And think youre starting to see why the Laplace transform is useful. t turns derivatives into multiplications by f. And actually, as youll see later, it turns integration to divisions by s. And you can take arbitrary derivatives and just keep multiplying by s. And you see this pattern. And m running out of time. But ll leave it up to you to figure out what the Laplace transform of the third derivative of f is. ee you in the next video.","This is the Laplace transform of f of t. This is the Laplace transform of g of t. o this is equal to c1 times the Laplace transform of f of t, plus c2 times this is the Laplace transform the Laplace transform of g of t. And so, we have just shown that the Laplace transform is a linear operator, right? The Laplace transform of this is equal to this. This is equal to the integral from 0 to infinity of e to the minus st, times f prime of t, dt. The Laplace transform of f prime of t is equal to s times the Laplace transform of f of t minus f of 0. This is going to be equal to s times the Laplace transform of f prime of t, but whats that? o this Laplace transform, which is this, is equal to uv, which is equal to e to the minus st, times v, f of t, minus the integral and, of course, were going to have to evaluate this from 0 to infinity. This is the Laplace transform of f of t, right? And then you want to subtract this whole expression evaluated at 0. o e to the 0 is 1 times f of 0 so thats just f of 0 plus s times we said, this is the Laplace transform of f of t, thats our definition so the Laplace transform of f of t. And now we have an interesting property. o this is going to be equal to c1 times the integral from 0 to infinity of e to the minus st, times f of t, d of t, plus c2 times the integral from 0 to infinity of e to the minus st, g of t, dt. What is the Laplace transform and this is a really useful thing to know what is the Laplace transform of f prime prime of t? And we get the Laplace transform of the second derivative is equal to s squared times the Laplace transform of our function, f of t, minus s times f of 0, minus f prime of 0. This is the Laplace transform of f prime prime of t. And think youre starting to see why the Laplace transform is useful. o say some constant, c1, times my first function, f of t, plus some constant, c2, times my second function, g of t. Well, by the definition of the Laplace transform, this would be equal to the improper integral from 0 to infinity of e to the minus st, times whatever our function that were taking the Laplace transform of, so times c1, f of t, plus c2, g of t think you know where this is going all of that dt. o that is equal to e to the minus st, f of t, evaluated from 0 to infinity, or as we approach infinity, plus s times the integral from 0 to infinity of e to the minus st, f of t, dt. Thats going to be s times the Laplace transform of its antiderivative, times the Laplace transform of f prime of t, right? Thats s times the Laplace transform of f of t, minus f of 0, right? That is equal to c1e to the minus st, f of t, plus c2e to the minus st, g of t, and all of that times dt. u is going to be e to the minus st, and v is going to equal what? v is going to equal f prime of t. And then u prime would be minus se to the minus st. o lets say want to find the Laplace transform of f prime of t. o have some f of t, take its derivative, and then want the Laplace transform of that. And then that is equal to the integral from 0 to infinity. This is u, thats u, and this is v prime. v prime is f prime of t, so v is just going to be equal to f of t. hope didnt say that wrong the first time. u prime is minus se to the minus st times v v is f of t dt.",0.2748299319727891
28,28,"Lets now get some practice with separable differential equations, so lets say have the differential equation, the derivative of Y with respect to X is equal to two Ysquared, and lets say that the graph of a particular solution to this, the graph of a particular solution, passes through the point one comma negative one, so my question to you is, what is Y, what is Y when X is equal to three for this particular solution, so the particular solution to the differential equation that passes through the point one comma negative one, what is Y when X is equal to three, and encourage you to pause the video, and try to work through it on your own. o m assuming you had a go at it, and the key with a separable differential equation, and thats a big clue that m even calling it a separable differential equation, is that you separate the Xs from the Ys. Or all the Xs and the DXs from the Ys and DYs. o how do you do that here? Well, what could do, let me just rewrite it. o its gonna be DY DX is equal to two Ysquared, is equal to two Y, equal to two Ysquared. o lets see, we can multiply both sides by DX, and lets see, so then were gonna have, that cancels with that if we treat it as just a value, or as a variable. Were gonna have DY is equal to two Y squared DX. Well, were not quite done yet. We gotta get this two Y squared on the left hand side. o we can divide both sides by two Ysquared. o if we divide both sides by two Ysquared, two Ysquared, the left hand side, we could rewrite this as 1/2 Y to the negative two power, is going to be equal to DY, DY is equal to DX, and now, we can integrate both sides. o we can integrate both sides. Let me give myself a little bit more space. And so, what is, what is this left hand side going to be? Well, we increment the exponent, and then divide by that value, so Y to the negative two, if your increment is Y to the negative one, and then divide by negative one, so this is going to be 1/2 Y to the negative one power, and we could do a plus like we did in the previous video, but were gonna have a plus on both sides, and you could subtract, or you know, you have different arbitrary constants on both sides and you could subtract them from each other, so m just gonna write the constant only on one side. o you have that is equal to, well if integrate just DX, thats just going to give me X, thats just gonna give me X. o this right over here is X, and of course can have a plus over there, and f want can, can solve for Y if multiply, lets see, can multiply both sides by negative two, and then m gonna have, the left hand side youre just gonna have Y to the negative one, or 1/Y is equal to, if multiply the right hand side times negative two, m gonna have negative two times X plus, well its some arbitrary constant, its still going to, its gonna be negative two times this arbitrary constant but could still just call it some arbitrary constant, and then if we want we can take the reciprocal of both sides, and so we will get Y is equal to, is equal to 1/2X+. And now we can use, we can use the information they gave us right over here, the fact that our particular solution needs to go through this point to solve for . o, when X is negative one, so when X is negative one. Oh sorry, when X is one, when X is one, Y is negative one, so we get negative one is equal to 1/2+, or we could say minus two, we could multiply both sides times minus two, if then we will get, actually let me just scroll down a little bit, so if you multiply both sides times minus two, negative one times minus two is going to be negative plus two or two minus is equal to one. All did is multiplied minus two times both sides, and then, lets see, can subtract two from both sides, so negative is equal to negative one, and then if multiply both sides by negative one, we get is equal to one. o our particular solution is Y is equal to 1/2X+1. And we are almost done, they didnt just ask for, we didnt just ask for the particular solution, we asked, what is Y when X is equal to three. o Y is going to be equal to one over, three times negative two is negative six plus one, which is equal to negative, is going to be equal to 1/5, or 1/5. And we are done.","o if we divide both sides by two Ysquared, two Ysquared, the left hand side, we could rewrite this as 1/2 Y to the negative two power, is going to be equal to DY, DY is equal to DX, and now, we can integrate both sides. o you have that is equal to, well if integrate just DX, thats just going to give me X, thats just gonna give me X. o this right over here is X, and of course can have a plus over there, and f want can, can solve for Y if multiply, lets see, can multiply both sides by negative two, and then m gonna have, the left hand side youre just gonna have Y to the negative one, or 1/Y is equal to, if multiply the right hand side times negative two, m gonna have negative two times X plus, well its some arbitrary constant, its still going to, its gonna be negative two times this arbitrary constant but could still just call it some arbitrary constant, and then if we want we can take the reciprocal of both sides, and so we will get Y is equal to, is equal to 1/2X+. Well, we increment the exponent, and then divide by that value, so Y to the negative two, if your increment is Y to the negative one, and then divide by negative one, so this is going to be 1/2 Y to the negative one power, and we could do a plus like we did in the previous video, but were gonna have a plus on both sides, and you could subtract, or you know, you have different arbitrary constants on both sides and you could subtract them from each other, so m just gonna write the constant only on one side. All did is multiplied minus two times both sides, and then, lets see, can subtract two from both sides, so negative is equal to negative one, and then if multiply both sides by negative one, we get is equal to one. Oh sorry, when X is one, when X is one, Y is negative one, so we get negative one is equal to 1/2+, or we could say minus two, we could multiply both sides times minus two, if then we will get, actually let me just scroll down a little bit, so if you multiply both sides times minus two, negative one times minus two is going to be negative plus two or two minus is equal to one. Lets now get some practice with separable differential equations, so lets say have the differential equation, the derivative of Y with respect to X is equal to two Ysquared, and lets say that the graph of a particular solution to this, the graph of a particular solution, passes through the point one comma negative one, so my question to you is, what is Y, what is Y when X is equal to three for this particular solution, so the particular solution to the differential equation that passes through the point one comma negative one, what is Y when X is equal to three, and encourage you to pause the video, and try to work through it on your own. o Y is going to be equal to one over, three times negative two is negative six plus one, which is equal to negative, is going to be equal to 1/5, or 1/5.",0.29375
29,29,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: Just a reminder, drop day is tomorrow. o if you were thinking about dropping the course or in danger of a bad grade or something, tomorrows the last chance to bail out. Last time we began our discussion on probability with the onty Hall game the onty Hall problem. And as part of the analysis, we made assumptions of the form that given that arol placed the prize in box 1, the probability that the contestant chooses box 1 is 1/3. Now, this is an example of something thats called a conditional probability. And thats what were going to study today. Now, in general, you have something like the conditional probability that an event, A, happens given that some other event, B, has already taken place. And you write that down as a probability of A given B. And both A and B are events. Now, the example from onty Hall and actually, we had several but you might have B being the event that arol places the prize in box 1. And A might be the event that the contestant chooses box 1. And we assumed for the onty Hall game that the probability of A given B in this case was 1/3 third because the contestant didnt know where the prize was. Now in general, theres a very simple formula to compute the probability of A given B. n fact, well treat it as a definition. Assuming the probability of B is nonzero than the probability of A given B is just the probability of A and B happening, both happening, divided by the probability of B happening. And you can see why this makes sense when the picture say this is our sample space. And let this be the event, A, and this be the event, B. Now were conditioning on the fact that B happened. Now once weve conditioned on that, all this stuff outside of B is no longer possible. All those outcomes are no longer in the space of consideration. The only outcomes left are in B. o in some sense weve shrunk the sample space to be B. And all we care about is the probability that A happens inside this new sample space. And that is, were asking the probability 1 of these outcomes happens given that this is the sample space. Well, this is just A intersect B because you still have to have A happen, but now youre inside of B. And then we divide by probability of B. o we normalize this to be probability one. OK. Because were saying B happened were conditioning on that. Therefore, the probability of these outcomes must be 1. o we divide by the probability of B. o we normalize. This now becomes the probability of A given B is this share of B weighted by the outcomes. OK. All right. For example then, whats the probability of B given B? whats that equal? 1. OK. Because we said it happened so it happens with probability 1. Or, using the formula, thats just probability of B and B divided by probability of B. Well, that equals the probability of B divided by the probability of B, which is 1. All right. Any questions about the definition of the conditional probability? Very simple. And its easy to work with using the formulas. Now, theres a nice rule called the product rule, which follows from the definition very simply. The product rule says that the probability of A and B for two events is equal to the probability of B times the probability of A given B. And thats just follow straightforwardly from this definition. Just multiply by probability of B on both sides. All right. o now you have a rule of computing a probability of two events simultaneously happening. o for example, in the onty Hall problem, whats the probability that arol places the prize in box one and thats the box the contestant chooses? All right? o if we took A and B as defined up there, thats the probability that arol places it in box one and the contestant chose it. Well, thats the probability that the contestant chooses it is 1/3 times the probability that arol put it there, given the contestant chose it, or actually, vice versa, s 1/9. OK? And this extends to more events. t is called the general product rule. o if you want to compute the probability of A1 and A2 and all the way up to An, thats simply the probability of a 1 happening all by itself times the probability of A2 given A1 times well, ll do the next one times the probability of A3 given A1 and A2, dot, dot dot, times, finally, the probability of An given all the others. o that starts to look a little more complicated. But it gives you a handy way of computing the probability that an intersection of events takes place. do This is proved by induction on n, just taking that rule and using induction on n. ts not hard. But we wont go through it. All right. Lets do some examples. Well start with an easy one. ay youre playing a playoff series and youre going to play best 2 out of 3. All right. o you have a best 2 out of 3 series. o whoever wins the first two games, best two out of three wins. And say youre told that the probability of winning the first game is 1/2. o the teams are matched 5050 for the first game. But then youre told that the probability of winning a game after a victory is higher. ts 2/3. o the probability of winning immediately after a game following a win is two thirds. And similarly, the probability of winning after a loss is 1/3. All right. And the idea here is that you win a game, youre sort of psyched, youve got momentum, and going into the next day youre more likely to win. imilarly, if you lost youre sort of down and the other guy has a better chance of beating you. Now, what were going to try to figure out is the probability of winning the series given you won the first game. All right? Now, conditional probability comes up in two places in this problem. Anybody tell me places where its come up? o got the problem statement and the thats the goal is to figure out the probability you win the series given you won the first game. o whats one place conditional probability is entering into this problem? Yeah? ADENE: The probability changes depending on the result of the previous game. PROFEOR: Thats true. The probability of winning any particular game is influenced by the previous game. o youre using conditional probability there. All right. And where else? Yeah. ADENE: you have to take into account . PROFEOR: Thats interesting. That will be another question were going to look at. Whats the probability of playing three games? Yep. Thats one. OK. Well, the question were after, whats the probability of winning the series given that you won the first game. Were going to compute a conditional probability there. o its coming up in a couple of places here. All right. Lets figure this out. ts easy to do given the tree method. o lets make the tree for this. o we have possibly three games theres game one, game two, and game three. Game one, you can win or lose. Theres two branches. Game two you can win or lose. And now, game three well, it doesnt even take place here. But it does here. You can win or lose here. And you could win or lose here. And here the series is over. o there is no game three in that case. The probabilities are next we put a probability of every branch here. Game one is 5050. Whats the probability you take this branch? 2/3, because youre on the path where you won the first game. You win the second game with 2/3. You lose with 1/3. Now here youre on the path where you lost the first game. o this has 1/3 and this has 2/3. All right? And then lastly, whats the probability have the win on the third game here? 1/3, because just lost the last game. Thats all m conditioning on. o that becomes 1/3. And this is 2/3 now. And then here just won a game. o ve got 2/3 and 1/3. All right. o got all the probabilities. And now need to figure out for the sample points whats their probability. o this sample point well call winwin. This sample point is winlosewin. This ones winloselose. Then we have losewinwin, losewinlose, and then loselose. o got six sample points. And lets figure out the probability for each one. Now remember the rule we had for the tree method. just multiply these things. Well, in fact, the reason we have that rule is because that is the same as the product rule. Because what m asking here to compute the probability of this guy is so the product rule gives the probability of a winwin scenario win the first game, win the second game. By the product rule is the probability that win the first game times the probability that win the second game given that won the first game. Thats what the product rule says. Probability win the first game is 1/2 times the probability win the second given that won the first is 2/3. o that equals 1/3. o what were doing here now is giving you the formal justification for that rule that we had last time and that youll always use is the probability of a sample point is the product of the probabilities on the edges leading to it. ts just the product rule. Now the next example is this one. And here were going to use the general product rule to get it. The probability of winlosewin by the general product rule is the probability that you win the first game times the probability you lose the second game given the that you win the first times the probability you win the third given what? What am given on the product rule? Won the first, lost the second. All right. Well, now we can fill in the numbers. The probability win the first is a 1/2. The probability that lose the second given that won the first, thats 1/3. And then this one here, the probability that win the third given that won the first and lost the second, that simplifies the probability win the third given that lost the second. Doesnt matter what happened on the first. And thats 1/3. o this is 1/2 times 1/3 times 1/3 is 118. And thats 1/18. And its just the product because the product rule saying product of the first probability times this one, which is the conditional probability of being here times this one, which is a conditional probability if these events happened before. Any questions about that? Very simple to do, which is good. Yeah. s there a question? OK. All right. o lets fill in the other probabilities here. got 1/2, 1/3, and 2/3. Thats 1/9. ame thing here is 1/9. This is 1/18 and 1/3. OK. o those are the probabilities in the sample points. Now, to compute the probability of winning the series given that we won the first game, lets define the events here. o A be the event that we win the series. B will be the event that we win the first game. And want to compute the probability of A given B. And we use our formula. Wheres the formula for that? ts way back over there. The probability of A given B is the probability of both happening, the probability of A and B divided by the probability of B. o now just have to compute these probabilities. o to do that got to figure out which sample points are in A and B here. o lets write that down. Theres A, B, A and B. All right. o A is the event that we win the series. Now this sample point qualifies, that one does, and this one. B is the event we won the first game. And thats these three sample points. And then A and B intersect B is these two. All right. o for each event that care about figure out which sample points are in that event. And now just add the probabilities up. o whats the probability of A and B? 7/18. 1/3 plus 1/18. Whats the probability of B? Yeah. 1/2, 9/18. got these three points. o thisll be 1/3 third plus 1/18 plus the extra one, 1/9. o ve got 7/18 over 9/18. 7/9 is the answer. o the probability we win the eries given we won the first game is 7/9. Any questions? Were going to do this same thing about 10 different times. OK? And it will look a little different each time maybe. But its the same idea. And the beauty here is its really easy to do. m going to give you a lot of confusing examples. But really, if you just do this is its going to be very easy. All right. omebody talked about the series lasting three games. Whats the probability the series lasts three games? an anybody look at that and tell me? 1/3 because what you would do is add up these three sample points. And its the opposite of these two. o its 2/3 chance of two games, a 1/3 chance of three games. o its not likely to go three games. All right. o to this point, weve seen examples of a conditional probability where its A given B where A follows B, like, were told B happened. Now whats the chance of A. And A is coming later. The probability of winning todays game given that you won yesterdays game, the probability of winning the series given you already won the first game. Next, were going to look at the opposite scenario where the events are reversed in order. The probability that you won the first game given that you won the series. All right. Now, this is inherently confusing because if youre trying to figure if you know you the series, well, you already know what happened in the first game because its been played. o how could there be any probability there? t happened. Well, so what the meaning is is over all the times where the series was played, sort of what fraction of the time did the team that won the series win the first game is one way you could think about it. Or, maybe you just dont know. The game was played. You know you won the series. But you dont know who won the first game. And so you could think of a probability still being there. Now when you think about it, it gets me confused still. But just think about it like the math. ts the same formula. OK. t doesnt matter which happened first in time. You use the same mathematics. n fact, they give a special name these kinds of things. Theyre called a postieri conditional probabilities. ts a fancy name for just saying that things are out of order in time. All right? o its a probability of B given A where B precedes A in time. All right? o its the same math. ts just theyre out of order. o lets figure out the probability that you won the first game given that you want the series. Lets figure it out. o want probability of B given A now for this example. Well, its just the probability of B and A over the probability of A. We already computed the probability of A and B. Thats 1/3 plus 1/18. whats the probability of A, the probability of winning the first game? 1/2. ts those three sample points and they better add up to 1/2 because we sort of said, the probability of the first games 1/2. o thats over 1/2, which is 9/18. Well this was 7/18 over 9/18. ts 7/9. o the probability of winning the first game given that you won series is 7/9. Anybody notice anything unusual about that answer here? ts the same as the answer over there. s that a theorem? No. The probability of A given B is not always the probability of B given A. t was in this case. t is not always true. n fact, we could make a simple example to see why thats not always the case. All right. o say heres your sample space. And say that this is B and this is A. Whats the probability of A given B in this case? 1. f youre in B wait. No. ts not 1. Whats the probability of A given B f got some probably less than 1. ight be ve drawn it as 1/3 third if it was uniform. But in this case, the probability of A given B is less than 1. Whats the probability of B given A? 1, because if m in A m definitely in B. All right. o thats an example where they would be different. And thats the generic case is theyre different. All right? When are they equal because they were equal in this case? What makes them equal? Lets see. When does the probability of A given B equal a probability of B given A? Lets see. Well, f plugin the formula, this equals the probability of A and B over the probability of B. That equals the probability of B and A over a probability of A. o when are those equal? Yeah. When probability A equals probability B. All right. o thats one case. Whats the other case? Yeah when its 0. Probability theres no intersection. Probability of A intersect B is 0. Thats the other case. All right. But usually these conditions wont apply just happened to in this example by coincidence. Any questions about that? All right. Yeah. o the math is the same with a postieri probabilities. ts really, really easy. All right. o lets do another simple example thatll start to maybe be a little more confusing. ay weve got two coins. One of them is a fair coin. And by that, mean the probability comes up heads is the same as the probability comes up tails is 1/2. The other one is an unfair coin. And in this case, that means its always heads. The probability of heads is 1. The probability of tails is 0. All right? ve got two such coins here. All right. Here is the unfair coin heads and heads. Actually, they make these things look like quarters sometimes. Heres the fair coin heads and tails. All right. Now suppose pick one of these at random, 5050, pick one of these things, and flip it, which m doing behind my back, and lo and behold, it comes out and, you see a heads. Whats the probability m holding the fair coin? picked the coin, 5050, behind my back. o one answer is, picked the fair coin with 50% probability. But then flipped it behind my back and showed you the result. And you see heads. Of course, if d have shown you tails, You would have known for sure it was the fair coin because thats the only one with the tails. But you dont know for sure now. You see a heads. Whats the probability this is the fair coin given that you saw a heads after the flip? How many people think 1/2? After all, picked it with probability 1/2. How many people think its less than 1/2? Good. OK. omebody even said 1/3. Does that sound right? A couple people like 1/3. OK. All right. Now, part of what makes this tricky is told you picked the coin with 50% probability. But then gave you information. o ve conditioned the problem. And so this is one of those things you could have an ask arilyn about. s it 1/2 or is it 1/3? Because picked it with 50% chance, what does the information do for you? Now, ll give you a clue. Bobo might have written in and said its 1/2. And his proof is that three other mathematicians agreed with him. All right? OK. o lets figure it out. And really its very simple. ts just drawing out the tree and computing the conditional probability. o were going to do the same thing over and over again because it just works for every problem. Of course, you could imagine debating this for awhile, arguing with somebody. s it 1/2 or 1/3? uch simpler just to do it. o the first thing is we have, which coin is picked? o it could be fair and told you that happens with probability 1/2 or unfair, which is also 1/2. Then we have the flip. The fair coin is equally likely to be heads or tails, each with 1/2. The unfair coin, guaranteed to be heads, probability 1. All right. Now we get the sample point outcomes. ts fair in heads with the probability 1/4, fair in tails, probability 1/4, unfair in heads, probability 1/2. Now we define the events of interest. A is going to be that we chose the fair coin. And B is at the result, is heads. And of course what want to know is the probability that chose the fair coin given that saw a heads. o to do that we plug in our formula. Thats just the probability of A and B over the probability of B. And to compute that got to figure out the probability of A and B and the probability of B. o ll make my diagram. A here, B here, A and B. A is the event chose the fair coin. Thats these guys. B is the event the result is heads. Thats this one and this one. And A intersect B, Thats the only point. o this is really easy to compute now. Whats the probability of A and B? 1/4. ts just that sample point. Whats the probability of B? 3/4, 1/4 plus 1/2. o the probability of A given B is 1/3. Really simple to answer this question. Just dont even think about it. Just write down the tree when you get these things. o much easier just to write the tree down. All right. Now the key here is we knew the probability of picking the fair coin in the first place. aybe its worth writing down what happens if thats a variable sum variable P. Lets do that. For example, what if hadnt told you the probability that picked the fair coin? just picked one and flipped it. Think thatll change the answer? t should because you got to plug something in there for the 1/2 for this to work. o lets see what happens. ay picked the fair coin with probability P and the unfair coin with 1 minus P. And this is the same heads and tails, 1/2, 1/2. Heads, the probability 1. Well now, instead of 1/4 get P over 2 up here. And this is now 1 minus P instead of 1/2. o the probability of A given B is the probability of A and B is p over 2. And the probability of B is P over 2 plus 1 minus P. Thats P over 2 up top, one minus P over 2, and that is all multiplied by what am going to multiply 2 here. ll get P over 2 minus P. o the probability with which picked the coin to start with impacts the answer here. For example, what if picked the unfair coin for sure? That would be P being 0. Well, the probability that picked the fair coin is 0 over 2, which is 0. All right though even know showed you the heads, theres no chance it was the fair coin because picked the unfair coin for sure. ame thing if picked the fair coin for sure, better be the case this is 1. o get 1 over 2 minus 1. ts 1. Any questions? o its important you know the probability picked the fair coin to start with. Otherwise, you cant go anywhere. All right. What if do the same game? Pick a coin with probability p. But now flip it K times. ay flip it 100 times. And every time it comes up heads. mean youre pretty sure you got the unfair coin because you never saw a tails. Right? o lets do that. Lets compute that scenario. o instead of a single heads get K straight heads and no tails. This would happen with 1 over 2 to the K. This would happen with 1 minus 1 over 2 to the K. o this is now p over 2 to the K. This is now P1 minus 2 to the minus K. Lets recompute the probabilities. m going somewhere where this. Wait a minute. o now were looking at the event that B is K straight heads. ome up. And want to know the probability that picked the fair coin given that it just never comes up tails. The math is the same. The probability now that picked the fair coin and got k straight heads is just p times 2 to the minus K. The probability that got K straight heads is P times 2 to the minus K plus the chance picked the unfair coin, which is 1 minus P. And if multiply top and bottom by 2 to the K, get P over P plus to the K 1 minus B. All right. o it gets very unlikely that ve got the fair coin here as K gets big. Like if K is 100 got a big number down here. And basically its 0 chance close to 0 chance of the fair coin. But now say do the following experiment. dont tell you P. But pull a coin out and 100 flips in a row its heads. Which coin do you think have? flipped it 100 straight times and its heads every time. Yeah. Theres not enough information. You dont know. What do you want to say? You want to say its the unfair coin but you have no idea because might have picked the fair coin with probability 1, in which case it is the fair coin and it just was unlucky that it came up heads 100 times in a row. But it could be. o you could say nothing if you dont know the probability P. Because sure enough, if plug in P being 1 here, that wipes out the 2 to the K and just get probability 1. OK? All right. Now when this comes up in practice is with things like polling. Like, we just had an election. And people do poles ahead of time. And they sample thousands of voters from 1% of the population. And they say, OK, that 60% of the people are going to vote Republican. And they might have a margin of error, three points, whatever that means. And well figure that out next week. What does that tell you about the electorate as a whole the population if they sample 1% at random, 60% are Republican. Yeah? ADENE: The options you have, is it all heads or is it all tails? t should be one option all heads and another option at least one tails. PROFEOR: Youre right. Oops. All right. At least one tail for this one. Yeah. Good. That is true. OK. Any questions about that example? OK. Now were back to the election and theres a pole that says they sampled 1% of the population at random and 60% said theyre going to vote Republican. And the margin of error is 3% or something. What does that tell you about the population of the country? Nothing. Thats right. t is what it is. All you can conclude is that either the population is close to 60% Republican or you were unlucky in the 1% you sample. Thats what you can conclude because the population really is fixed in this case. t is what it is. Theres no randomness in the population. All right? o you have next week for recitation. Youre going to design a pole and work through how to calculate the margin of error and work through what that really means in terms of what the population is like. Now of course, if it comes out 100 straight times heads, youve got to be really unlucky to have the fair coin. And the same thing with designing the poll if youre way off. Any questions about that? OK. The next example comes up all the time in practice. And thats with medical testing. aybe ll leave no. ll take that down. We know that now. Now in this case in fact, this is a question we had on the final exam a few years ago. And theres a good chance this kind of questions going to be on the final this year. Theres a disease out there. And you can have a test for it. But like most medical tests, theyre not perfect. ometimes when it says youve got the disease you really dont. And if it ways you dont have it, you really do. o in this case, were going to assume that 10% of the population has the disease, whatever it is. You dont get symptoms right away. o you have this test. But if you have the disease there is a 10% chance that the test is negative. And this is called a false negative, because the test comes back negative but its wrong, because you have the disease. And similarly, if you have the disease or sorry if you dont have the disease, theres a 30% chance that the test comes back positive. And its called a false positive because it came back positive, but you dont have it. o the test is pretty good. Right? ts 10% false negative right, 30% false positive right. Now say you select a random person and they test positive. What you want to know is the probability they have the disease given that its a random person. o actually, this came up in my personal life. any years ago when my wife was pregnant with Alex, she was exposed to somebody with TB here at T. And she took the test. And it came back positive. Now the bad thing TBs a bad thing. You dont want to get it. But the medicine for it you take for six months. And she was worried about taking medicine for six months when shes pregnant because who knows what the TB medicine does kind of thing if you have a baby. o she asked the doc, whats the probability really have the disease? The doc doesnt know. The doc maybe could give you some of these steps, 10% false negative, 30% false positive. But it tested positive. o they just normally give you the medicine. o say this was the story. What would you say? What do you think? How many people think that its a least a 70% chance you got the disease? he tested positive and its only got a 30% false positive rate. Anybody? o you dont think shes likely to have it. How many people think its better than 5050 you have the disease? A few. How many people think less than 50%. A bunch. Yeah. Youre right, in fact. Lets figure out the answer. ts easy to do. o A is the event the person has the disease. And B is the event that the person tests positive. And of course what we want to know is the probability you have the disease given that you tested positive. And thats just the probability of both events divided by the probability of testing positive. o lets figure that out by drawing the tree. o first, do you have the disease? And its yes or no. And lets see. The probability of having the disease, what is that for a random person? 10%. that the stat. o its actually, well call it 0.1. And 9.9 you dont have it. And then theres the test. Well, you can be positive or negative. Now if you have the disease, there is a the chance you test negative is 10%, 0.1. Therefore theres a 90% chance you test positive. Now if, you dont have the disease, you could test either way. f you dont have the disease theres a 30% chance you test positive. 30 here and 70% percent chance youre negative. Now we can compute each sample point probability. This one is 0.1 times 0.9 is 0.09. 0.1 times 1 is 0.01. 0.9 and 0.3 is 0.27. 0.9 and 0.7 is 0.63. o all sample points are figured out. Now we figure out which sample points are in which sets. o we have event A, event B, and A intersect B. Lets see. A is the event you have the disease. Thats these guys. B is the event you test positive. Thats this one and this one. A intersect B is just this one. All right. Were almost done. Lets just figure out the probability you have the disease. Whats the probability of A intersect B? 0.09. ts just that one sample point. Whats the probability that you tested positive? 0.36. Yeah. 0.09 plus 0.27, which is 0.36. o got 0.09 over 0.36 is 1/4. Wow. That seems bizarre. Right? Youve got a test, 10% percent false negative, 30% false positive. Yet, when you test positive theres only a 25% chance you have the disease. o maybe you dont take the medicine. o if theres risk both ways, probably dont have the disease. Yeah? ADENE: disease change because youve already been exposed to somebody that has it? PROFEOR: Thats a great point, great point, because theres additional information conditioning this in the personal example cited. You were exposed to somebody. o we need to condition on that as well, which raises the chance you have the disease. Thats a great point. Yeah. Just like in the well, we havent got to that example. Do another example with that exact kind of thing is very important. All right. o this is sort of paradoxical that it looks like a pretty good test low false positive, full false negatives, but likely be wrong, at least if it tells you have the disease. n fact, lets figure out. Whats the probability that the test is correct? Whats the probability the test is right in general? 72%. Lets see. o it would be 0.09 plus 0.63. 72%. o its likely to be right. But if it tells you you have the disease its likely to be wrong. ts hard. Why is this happening? Why does it come out that way? Yeah? ADENE: Then there is only a 1 in 64 chance that you have the disease. o if it comes back negative, then its a pretty good indication that youre OK. PROFEOR: Yeah. f it comes back negative than it really is doing very well. Thats right. But why is it when it comes back positive that youre unlikely to have the disease if its a good test. Yeah. ADENE: The disease is so rare. PROFEOR: The disease is so rare. Absolutely. This number here is so small. And thats whats doing it. Because if you look at how many people have the disease and test positive, its 0.09. o many people dont have the disease that even with a small false positive rate, this number swamps out that number. n fact, imagine nobody had the disease. Youd have a 0 here. All right? And then you would always be wrong if you said you had it. OK? Thats good. OK. This comes up in weather prediction, the same paradox. For example, say youre trying to predict the weather for eattle. ometimes it seems like this in Boston. And you just say, its going to rain. Forget all the fancy weather forecasting stuff, the radar, and all the rest. Just say its going to rain tomorrow. Youre going to be right almost all the time. All right? And in fact, if you try to do fancy stuff, youre probably going to be wrong more of the time. All right. For example, in this case, if you just say the person does not have the disease, forget the lab test. Just come back with negative. How often are you right? 90% of the time youre right. uch better than the test you paid a lot of money for. see. Youve got to be careful what youre looking for, how you measure the value of a test or a prediction. Because presumably the one you paid for is better, even though accurate less of the time. Any questions about that? OK. o For the rest of today were going to do three more paradoxes. And in each case theyre going to expose a flaw in our intuition about probability. But the good news is in each case its easy to get the right answer. Just stick with the math and try not to think about it. Now the first example is a game involving dice thats called carnival dice that you can find in carnivals and you can also find in casinos. ts a pretty popular game, actually. o the way it works is as follows. The player picks a number from 1 to 6 well call it N and then rolls three dice. And lets say theyre fair and mutually independent. We havent talked about independent. o theyre fair dice. For now, normal dice nothing fishy. And the player wins if and only if the number he picked comes up on at least one of the dice. o you either win or you lose the game depending on if your lucky number came up at least once. Now youve got three dice, each of which has a 1 in 6 chance of coming up a winner for you. o how many people think this is a fair game you got a 5050 chance of winning three dice, each 1/6 chance of winning? Anybody think its not a fair game? A bunch of you. How many people think it is a fair game 5050? A few. All right. Well, lets figure it out. And instead of doing the tree method, which we know were supposed to do, were just going to wing it, which is always seems easier to do. f youre in the asino you want to just wing it instead of taking your napkin out and drawing a tree. o the claim, question mark, is the probability you win in 1/2. And the proof, question mark, is you let Ai be the event that the ith die comes up N. And i is 1 to 3 here. o then you say, OK. The probability win is the probability of A1 could win that way or A2, or A3. All need is one of the die to come up my way. And that is the probability of A1 plus the probability of A2 plus the probability of A3. And each die wins for me with probability 1/6. And that is then 1/2. o thats a proof that we win with probability of 1/2. What do you think? Any problems with that proof? ADENE: PROFEOR: Well thats a great point. Yeah. o if extended this nice proof technique couldnt have probability of 7/6 of winning with seven die. Yeah? ADENE: PROFEOR: Yeah. Youre very close. didnt technically assume that. ADENE: PROFEOR: They could double up. Yeah. Theres no intersection in the events. n fact, there is intersection because theres a chance rolled all six all Ns. ay N is 6. could roll all sixes and then each of these would be a winner. But dont get to count them separately. Then only win once in that case. n other words, all of these could turned on at the same time. Theres an intersection here. o this rule does not hold. need the Ai to be disjoined for this to be true the events to be disjoined. And theyre not disjoined because theres a sample point were two or more of the die could come up the same being a winner, which means the same sample point, namely all die are N, comes up in each of these three. o theyre not disjoined. Now whats the principal you used two weeks ago when you did cardinality of a set cardinality of a union of sets? nclusion, exclusion. And the same thing needs to be done here. o lets do that. And then well figure out the actual probability. o this is a fact based on the inclusion, exclusion principle. The probability of A1, union A2, union A3, is just what you think it would be from inclusion, exclusion. ts a probability of A1 plus a probability of A2 plus the probability of A3 minus the pairwise intersections. A1 intersect A3 minus probability of A2 intersect A3. And is there anything else? Plus, the probably of all of them matching. OK. o the proof is really the same proof you use for inclusion, exclusion with sets. The only difference is that in a probability space, we have weights on the elements. And the weight corresponds to the probability. o in fact, if you were drawing the sample space, say heres A1 and heres A2, and heres A3. Well, you need to add the probabilities here, here, and here. Then you subtract off the double counting from here, from here, and from here. And then you add back again what you subtracted off too much there. ame proof, its just your have weights on the elements of probabilities. All right. o lets figure out the right probability. Thats 1/6, 1/6, 1/6. Whats the probability of the first two die matching both of them? 1/36. Well talk more about why that is next time. But theres a 6 for A1 then given that 1/6 for the second die matching. o its 1/6 times 1/6 minus the 1/36. 1/36, the chance that all three match is 1/216 or 6 cubed. o when you add all that up you get the 0.421 and some more. o the chance of winning this game is 41% which makes it a worst game in the casino. t is hard to find a worse game than this. Roulette, much better. Well study Roulette in the last lecture much better game. And even thats a terrible game to play. o it looks like an easy game. Theres a quick proof that its 5050. But its horrible odds against the house. Now, this is a nice example because it shows how a rule you had for computing the cardinality of a set gives you the probability. All right. n fact, all the set laws you learned a couple weeks ago work for probability spaces the same way. And there were several of those in homework that you just had the last problem set. Any questions about that? OK. Now in addition, all those set laws you did also work for conditional probabilities. For example, this is true. The probability of A union B given whoops given , is the probability of A given plus the probability of B given minus the intersection, A intersect B given . n other words, take any probability rule you have and condition everything on an event, , and it still works. And the proof is not hard. You can go through each individual law but it all comes out to be fine. All right. You have to be a little careful though because you got to remember which side youre doing, which what youre putting on either side of the bar here. For example, what about this one? s this true? laim. Lets take say and D are disjoined. s this true? Then the probability of A conditioned on union D. o given that either or D is true, does that equal the probability of A given plus probability of A given D? We know that if swapped all these, its true. The probability of union D when and D are disjoined is the probability that given A plus the probability of D given A. That just claimed. And what about this way? an swap things around? Yeah? ADENE: would union D be 0? PROFEOR: f and D are disjoined, union D would just be union D. But youre not a good point. What if and D are disjoined? Thats a good example. Lets draw that. Lets look at that case. o weve got a sample space here. And youve got here and D here. And just for fun, lets make A be here include all of them. Whats the probability is this going to do what want? Yeah. Whats the probability of A given ? 1. f m in m in A. A is everything here. o the probability of A given is one. Whats the probability of A given D? 1. All right. Well, this is a problem because cant have the probability ot whats the probably of A given union D? Well, it cant be 2. Right? ts 1. They are not equal. o you cannot do those set rules on the right side of the conditioning bar. You can do them on the left, not on the right. All right. o this is not true. Now nobody would do this. Right? mean, the probability of not that its see this example? This you just would never make this mistake again seeing that example. Everybody understand the example, how its clearly not always the case that probability of A given union D is a probability of A given plus probability of A given D? Because now m going to show you an example where youre going to swear its true. All right? And this is a real life example. any years ago now there was a sex discrimination suit at Berkeley. There was a female professor in the math department. And she was denied tenure. And she filed a lawsuit against Berkeley alleging sex discrimination. aid she wasnt tenured because shes a woman. Now, unfortunately sex discrimination is a problem in math departments. ts historically been a difficult area. But its always hard to prove. ts a nebulous kind of thing. They dont say, hey, you cant have tenure because youre a woman. Theyd get sued and get killed for that. o she had to get some mat to back her up. o what she did is she looked into Berkeleys practices and she found that in all 22 departments, every single department, the percentage of male PhD applicants that were accepted was higher than the percentage of female PhD applicants that were accepted. Now you could understand some of the departments accepting more male PhDs than female PhDs. But all 22? What are the odds of that? mean, so the immediate conclusion is, well, thats clearly theres sex discrimination going on at Berkeley. OK? Well Berkeley took a look at that and said, nothing good. That doesnt look good for them. But they did their own study of PhD applicants. And they said that if the university as a whole look at the niversity as a whole, actually, the women, the females have a higher acceptance rate for the PhD Program than the men. o look. Berkeley said, were accepting more women than men percentagewise. o how could we be discriminating against women? And this is where the same argument the female faculty members making, But theyre saying as a university as a whole, when you add up all 22 departments. Well, that sounds pretty good. How could they be discriminating? OK. o the question for you guys, is it possible that both sides were telling the truth, that in every single department the women have a lower acceptance rate than men, but on the university as a whole the women are higher percentage? t sounds like its and just to avoid any confusion here, people only apply to one department and theyre only one sex. o you cant arroll didnt apply. How many people think that one of the sides, actually, when they look at the studies was wrong, that theyre contradictory? Nobody? Youve been in 6 over 2 too long. How many people think its possible that both sides were right? Yeah. All right. o lets see how this works. And to make it simple m going to get down to just two departments rather than try to do data for all 22. And m going to do not the actual data but something thats represents whats going on. OK. o were going to look at the following events. A is the event that the applicant is admitted. F is the event that the applicant is female and applying to . FEE is the event that the applicant is female and applying to EE. is the event the applicant is a male and . And then finally we have EE is the event the applicant is male and in EE. o were just going to look at two departments here and try to figure out if it can happen that in both departments the women are worse off but if you take the union theyre better off. o the female professors argument effectively is, the probability of being admitted given that youre a female in is less than the probability of being admitted given that youre a male at . And same thing in EE. Probability of being admitted in EE if youre a female is less than if youre a male. OK? Now Berkeley is saying its sort of the reverse. The probability that youre admitted given that youre a female in either department is bigger than the probability of being admitted if youre a male in either department. OK. o weve now expressed their arguments as conditional probabilities Any questions? an you sort of see why this seems contradictory? Not plus, union. Because this is sort of like these are just joined. This is the sum of those. And this is sort of the sum of those. And yet the inequality changed. All right. n fact, this is the logic that weve just debunked over there exactly that claim. n fact, these are not equal as the sum. o lets do an example. ay that lets do it over here. ll put the real values in over here. ay that for women in computer science, 0 out of 1 were admitted compared to the men, were 50 out of 100 were admitted. And then in EE, 70 out of 100 women were admitted compared to the men, which had 1 out of 1. All right? o as ratios, 70% is less than 100%. 0% is less than 50. Now if look at the two departments is a whole, get 70 over 101 is in fact bigger than 51 over 101. All right? And so as a whole women are a lot more likely to be admitted even though in each department theyre less likely to be admitted. OK? o what went wrong with the intuition, which you didnt fall victim to, but people often do, that it shouldnt have been possible given that? Whats going on here that make it so that its not a less than when you look at the union of the departments? Yeah? ADENE: theyre weighted differently? PROFEOR: Yeah. Theyre weighted very differently. You got huge waves here. Right? o if look at the average of the percentages here, well its 35% for the women versus 75% for the men. o the average of the percentage is just what youd think. 35 is less than 75. But ve got huge weightings on these guys, which changes the numbers quite dramatically. o it all depends how you count it. Actually, who do you think had a better Yeah. Go ahead. ADENE: PROFEOR: Who won the lawsuit? Actually, the woman won the lawsuit. And which argument would you buy now? Youve got two arguments. Which one would you believe if either? Which one? mean, now if look at exactly this data might side might side with Berkeley looking at these numbers. Then again, when you think about all 22 departments and the fact they werent this lopsided, not so good. o in the end Berkeley lost. m going to see another example in a minute where its even more clear which side to believe in. But it really depends on the numbers as to which one you might, if you had to vote, which way youd vote. Heres another example. This is from a newspaper article on which airlines are best to fly because they have the best ontime rates. And in this case they were comparing American Airlines and America West, looking at ontime rates. And heres the data they showed for the two airlines. Heres American Airlines. Heres America West. And they took five cities, LA, Phoenix, an Diego, an Francisco, and eattle. And then you looked at the number on time, the number of flights, and then the rate, percentage on time. And then same thing here. Number on time, number of flights, and the rate. o m just going to give you the numbers here. o they had 500 out of 560 for a rate of 89%, 220 over 230 for 95, 210 over 230 for 92%, 500 over 600 for 83%, and then eattle. They had a lot of flights. Thats where theyre we have a hub of 2,200 for 86%. And if you added them all up, they got 3,300 out of 3,820 for 87% on time. Now the data for American West looks something like the following. n LA its 700 out of 800 for 87%. theyre based in Phoenix. They got a zillion flights there. 4,900 out of 5,300 for 92%. And 400 over 450 for 89%, 320, over 450, 71%, 200 over 260 for 77%. And then you add all them up. And youve got 6,520 over 7,260 for 90%. o the newspaper concluded and literally said that American West is the better airline to fly because theyre ontime rate is much better. ts 90% versus 87%. What do you think? Which airline would you fly looking at that data? ADENE: PROFEOR: know which one d fly. t looks like America West is better. Every single city, American Airlines is better. 92 versus 89. Everywhere its better by a bunch. 83 versus 71. 86 versus 77. Every single city, American Airlines is better. Yet, America West is better overall. And thats what the newspaper said. They went on this. But of course, no matter where youre going youre better off with American Airlines. All right? Now what happened here? The waiting. n fact, America West flies out of Phoenix where the weathers great. o you get a higher ontime rate when in a goodweather city. And they got most of their flights there. American Airlines got a lot of flights in eattle where the weather sucks and youre always delayed. All right? And so they look worse on average because so many of their flights are in a bad city and so many of America West are in a good city. All right? o it makes America West look better when in fact, in this case, its absolutely clear whose better. American Airlines is better, every single city. All right. Thats why ark Twain said, ""Theres three kinds of lies lies, damned lies, and statistics."" Well see more examples next time.","o the probability of A given B is the probability of A and B is p over 2. Thats just the probability of A and B over the probability of B. And to compute that got to figure out the probability of A and B and the probability of B. o ll make my diagram. The probability of winlosewin by the general product rule is the probability that you win the first game times the probability you lose the second game given the that you win the first times the probability you win the third given what? And say that this is B and this is A. Whats the probability of A given B in this case? o got the problem statement and the thats the goal is to figure out the probability you win the series given you won the first game. The product rule says that the probability of A and B for two events is equal to the probability of B times the probability of A given B. And thats just follow straightforwardly from this definition. The probability of A given B is the probability of both happening, the probability of A and B divided by the probability of B. o now just have to compute these probabilities. The probability now that picked the fair coin and got k straight heads is just p times 2 to the minus K. The probability that got K straight heads is P times 2 to the minus K plus the chance picked the unfair coin, which is 1 minus P. And if multiply top and bottom by 2 to the K, get P over P plus to the K 1 minus B. All right. Well, so what the meaning is is over all the times where the series was played, sort of what fraction of the time did the team that won the series win the first game is one way you could think about it. o what were doing here now is giving you the formal justification for that rule that we had last time and that youll always use is the probability of a sample point is the product of the probabilities on the edges leading to it. And of course what we want to know is the probability you have the disease given that you tested positive. Now, what were going to try to figure out is the probability of winning the series given you won the first game. Well, its just the probability of B and A over the probability of A. We already computed the probability of A and B. Thats 1/3 plus 1/18. o the probability of A given B is 1/3. The probability of union D when and D are disjoined is the probability that given A plus the probability of D given A. That just claimed. o lets figure out the probability that you won the first game given that you want the series. o the probability of winning the first game given that you won series is 7/9. Now, to compute the probability of winning the series given that we won the first game, lets define the events here. whats the probability of A, the probability of winning the first game? o whats the probability of A and B? Whats the probability of A and B? You want to say its the unfair coin but you have no idea because might have picked the fair coin with probability 1, in which case it is the fair coin and it just was unlucky that it came up heads 100 times in a row. o the probability of A given is one. Well, the question were after, whats the probability of winning the series given that you won the first game. By the product rule is the probability that win the first game times the probability that win the second game given that won the first game. The probability of A given B is not always the probability of B given A. t was in this case. And say youre told that the probability of winning the first game is 1/2. And then this one here, the probability that win the third given that won the first and lost the second, that simplifies the probability win the third given that lost the second. And that is the probability of A1 plus the probability of A2 plus the probability of A3. And that is, were asking the probability 1 of these outcomes happens given that this is the sample space. Well, f plugin the formula, this equals the probability of A and B over the probability of B. That equals the probability of B and A over a probability of A. o when are those equal? The probability of heads is 1. o if you want to compute the probability of A1 and A2 and all the way up to An, thats simply the probability of a 1 happening all by itself times the probability of A2 given A1 times well, ll do the next one times the probability of A3 given A1 and A2, dot, dot dot, times, finally, the probability of An given all the others. Probability win the first game is 1/2 times the probability win the second given that won the first is 2/3. The probability that you won the first game given that you won the series. And its just the product because the product rule saying product of the first probability times this one, which is the conditional probability of being here times this one, which is a conditional probability if these events happened before. Whats the probability this is the fair coin given that you saw a heads after the flip? Well, the probability that picked the fair coin is 0 over 2, which is 0. What you want to know is the probability they have the disease given that its a random person. And of course what want to know is the probability that chose the fair coin given that saw a heads. Whats the probability the test is right in general? o you could say nothing if you dont know the probability P. Because sure enough, if plug in P being 1 here, that wipes out the 2 to the K and just get probability 1. Because what m asking here to compute the probability of this guy is so the product rule gives the probability of a winwin scenario win the first game, win the second game. Lets just figure out the probability you have the disease. Or, using the formula, thats just probability of B and B divided by probability of B. Well, that equals the probability of B divided by the probability of B, which is 1. A is the event you have the disease. Well, this is just A intersect B because you still have to have A happen, but now youre inside of B. And then we divide by probability of B. o we normalize this to be probability one. The probability win the first is a 1/2. Then the probability of A conditioned on union D. o given that either or D is true, does that equal the probability of A given plus probability of A given D? But if you have the disease there is a 10% chance that the test is negative. The probability that lose the second given that won the first, thats 1/3. ay picked the fair coin with probability P and the unfair coin with 1 minus P. And this is the same heads and tails, 1/2, 1/2. Now if you have the disease, there is a the chance you test negative is 10%, 0.1. But in this case, the probability of A given B is less than 1. And want to know the probability that picked the fair coin given that it just never comes up tails. Whats the probability is this going to do what want? Whats going on here that make it so that its not a less than when you look at the union of the departments? Whats the probability of B? Whats the probability of B? Now the key here is we knew the probability of picking the fair coin in the first place. The probability of winning todays game given that you won yesterdays game, the probability of winning the series given you already won the first game. And the proof, question mark, is you let Ai be the event that the ith die comes up N. And i is 1 to 3 here. The probability of tails is 0. And we assumed for the onty Hall game that the probability of A given B in this case was 1/3 third because the contestant didnt know where the prize was. And by that, mean the probability comes up heads is the same as the probability comes up tails is 1/2. Well, in fact, the reason we have that rule is because that is the same as the product rule. Everybody understand the example, how its clearly not always the case that probability of A given union D is a probability of A given plus probability of A given D? Assuming the probability of B is nonzero than the probability of A given B is just the probability of A and B happening, both happening, divided by the probability of B happening. And let this be the event, A, and this be the event, B. Now were conditioning on the fact that B happened. ts those three sample points and they better add up to 1/2 because we sort of said, the probability of the first games 1/2. Whats the probability of B given A? Whats the probability of A given D? Whats the probability of A given ? And then lastly, whats the probability have the win on the third game here? o in this case, were going to assume that 10% of the population has the disease, whatever it is. ADENE: The options you have, is it all heads or is it all tails? And the probability of B is P over 2 plus 1 minus P. Thats P over 2 up top, one minus P over 2, and that is all multiplied by what am going to multiply 2 here. o A is the event that we win the series. All you can conclude is that either the population is close to 60% Republican or you were unlucky in the 1% you sample. o the probability we win the eries given we won the first game is 7/9. mean, the probability of not that its see this example? And this is 2/3 now. The probability of A union B given whoops given , is the probability of A given plus the probability of B given minus the intersection, A intersect B given . Well, thats the probability that the contestant chooses it is 1/3 times the probability that arol put it there, given the contestant chose it, or actually, vice versa, s 1/9. All right though even know showed you the heads, theres no chance it was the fair coin because picked the unfair coin for sure. F is the event that the applicant is female and applying to . Now, this is inherently confusing because if youre trying to figure if you know you the series, well, you already know what happened in the first game because its been played. Whats the probability that the test is correct? And as part of the analysis, we made assumptions of the form that given that arol placed the prize in box 1, the probability that the contestant chooses box 1 is 1/3. The probability of having the disease, what is that for a random person? For example, in this case, if you just say the person does not have the disease, forget the lab test. o lets figure out the right probability. For example then, whats the probability of B given B? And thats just the probability of both events divided by the probability of testing positive. o for example, in the onty Hall problem, whats the probability that arol places the prize in box one and thats the box the contestant chooses? Of course, if d have shown you tails, You would have known for sure it was the fair coin because thats the only one with the tails. o if we took A and B as defined up there, thats the probability that arol places it in box one and the contestant chose it. Now, this is a nice example because it shows how a rule you had for computing the cardinality of a set gives you the probability. This would happen with 1 over 2 to the K. This would happen with 1 minus 1 over 2 to the K. o this is now p over 2 to the K. This is now P1 minus 2 to the minus K. Lets recompute the probabilities. And similarly, if you have the disease or sorry if you dont have the disease, theres a 30% chance that the test comes back positive. This now becomes the probability of A given B is this share of B weighted by the outcomes. For example, what if hadnt told you the probability that picked the fair coin? B is the event we won the first game. And the idea here is that you win a game, youre sort of psyched, youve got momentum, and going into the next day youre more likely to win. But the good news is in each case its easy to get the right answer. And lets figure out the probability for each one. But why is it when it comes back positive that youre unlikely to have the disease if its a good test. Well, this is a problem because cant have the probability ot whats the probably of A given union D? The only outcomes left are in B. o in some sense weve shrunk the sample space to be B. And all we care about is the probability that A happens inside this new sample space. o were just going to look at two departments here and try to figure out if it can happen that in both departments the women are worse off but if you take the union theyre better off. And here the series is over. And that is then 1/2. o the female professors argument effectively is, the probability of being admitted given that youre a female in is less than the probability of being admitted given that youre a male at . is the event the applicant is a male and . o to do that got to figure out which sample points are in A and B here. A here, B here, A and B. A is the event chose the fair coin. But really, if you just do this is its going to be very easy. A is the event that the applicant is admitted. Theres A, B, A and B. All right. And theyre not disjoined because theres a sample point were two or more of the die could come up the same being a winner, which means the same sample point, namely all die are N, comes up in each of these three. And this is sort of the sum of those.",0.1620732208967503
30,30,"PROFEOR PATRK WNTON: was in Washington for most of the week prospecting for gold. Another byproduct of that was that forgot to arrange a substitute Bob Berwick for the Thursday recitations. shall probably go to hell for this. n any event, we have many explanations, none of them good. But today well try to get back on track and youll learn something fun. n particular you will learn how a graduate student of mine ark , together with a summer ROP student, Brett van Zuiden, one of you managed to pull off a tour de force and recognize in these two descriptions the pattern that we humans commonly call ""revenge."" t was discovered. The system didnt have a name for it, of course. t just knew that there was a pattern there and sat waiting for us to give a name to it. Thats where were going to end up. But itll be a bit of a journey before we get there, because weve got to go through all that stuff on the outline. And in particular, we want to start off by a little tiny bit of review. Because some of the stuff we did last time went by pretty fast. n particular, you may remember they had this wonderful joint probability table, which tells us all we want to know, all we want to know. We can decide what the probability of the police being called is given the this and the that, and all that sort of stuff, by clicking the appropriate boxes. The trouble is, gee, there are only three variables there. And when there are lots of variables it gets pretty hard to make up those numbers or to even collect them. o were driven to an alternative. And we got to that alternative just at the end of the show a week ago. And we got to the point where we were defining these inference nets, sometimes called ""Bayes nets."" And the one we worked with looked like this. Theres a burglar, a raccoon, the possibility of a dog barking, the police being called, and a trash can being overturned. o more variables than that. That only has three. This has got five. But were able to do some magic with this because we, as humans, when we define when we draw this graph were making an assertion about how things depend or dont depend on one another. n particular, theres something to break down and memorize to the point where it rolls off your tongue. And that is that any variable on this graph is said by me to be independent of any other nondescendant given its parents. ndependent of any nondescendant given its parents. o that means that the probability of the dog barking, given its parents, doesnt depend on T, the trash can being overturned. Because the intuition is all of the causality is flowing through the parents and cant get to this variable D without going through the parents. o that is property of the nets that we draw. And we tend to draw them in a way that reflects causality. o it tends to make sense. o somehow this thing is going to be were going to use this thing instead of that thing. But wait. We may need that thing in order to do all the computations we want to perform. o we need to be able to show that we can get to that thing by doing calculations on this thing. o what to do? Well, were going to use the chain rule. And remember that the chain rule came to us by way of the basic Axioms of Probability plus the definition plus a little colored chalk. o we got to the point last time where we sort of believed this. ts a really magical thing. t says that the probability of all this stuff happening together is given as the product of a bunch of conditional probabilities. And the conditional probabilities in this product are arranged such that this first guy depends on everybody else. The second guy doesnt depend on the first guy but depends on everything else. o that list of dependencies gets smaller and smaller as you go down here until it depends only one thing. Theres no conditional at all. o thats going to come to our rescue because it enables us to go from calculations in here to that whole table. But first have to show you a little bit more slowly how that comes to be. One thing m going to do before think about probability is m going to make a linear list of all these variables. And the way m going to make it is m going to chew away at those variables from the bottom. ve taken advantage of a very important property of these nets. And that is there no loops. You can follow the arrows in any way so as you get back to yourself. o theres always going to be a bottom. o what m going to do is m going to say, well, there are two bottoms here, theres and T. o have a choice. m going to choose . o m going to take that off and pretend its not there anymore. Then m going to take this guy. Thats now a bottom because theres nothing below it. ve already taken out. o well take that out next. And now ve got this guy, this guy, and this guy. This guy no longer has anything below it. o can list it next. Now over here ve got raccoon and trashcan. But trashcan is at the bottom. o ve got to take it next because m working from the bottom up. want to ensure that there are no descendants before me in this list. o finally get to raccoon. o the way constructed this list like so ensures that this list arranges the elements so that for any particular element, none of its descendants appear to its left. And now thats the magical order for which want to use the chain rule. o now can write can pick to be my variable n. And can say that the chain rule says that the joint probability of all these variables P of , D, B, T, and R the probability of any particular combination of those things is equal to the probability of given everybody else. Next in line is D given everybody else. Next in line is T next in line is B given everybody else. And next in line is T given everybody else. And finally, just R. o this combination of things has a probability that is given by this chain rule expression. Ah. But first of all, none of those expressions condition any of the variables on anything other than nondescendants, all right? Thats just because of the way ve arranged the variables. And can always do that because are no loops. can always chew away at the bottom. That ensures that whenever write a variable, its going to be conditioned on stuff other than its descendants. o all of these variables in any of these conditional probabilities are nondescendants. Oh wait. When drew this diagram, asserted that no variable depends on any nondescendant given its parents. o if know the parents of a variable know that the variable is independent of all other nondescendants. All right? Now can start scratching stuff out. Well, lets see. know that , from my diagram, has only one parent, D. o given its parent, its independent of all other nondescendants. o can scratch them out. D he has two parents, B and R. But given that, can scratch out any other nondescendant. B is conditional on T and R. Ah, but B has no parent. o it actually is independent of those two guys. The trashcan, yeah, thats dependent on R. And R over here, the final thing in the chain, thats just a probability. o now have a way of calculating any entry in that table because any entry in that table is going to be some combination of values for all those variables. Voila. o anything can do with a table, can do in principle with this little network. OK? But now the question is, ve got some probabilities m going to have to figure out here. o let me draw a slightly different version of it. o up here weve got the a priori probability of B. Well, thats just probability of B. Down here with the dog, ve got a bigger table because ve got probabilities that depend on the values of its parents. The probability of dog barking depends on the condition of the parents, nothing else. o lets see. ve got to have a column for B. ve got to have a column for the burglar and the raccoon. And there are a bunch of possibilities for those guys. But once get those then ll be able to calculate the probability of the dog barking. o there are two of these variables. o there are four combinations. Theres T T. Theres T R, R T, and whoa, what am doing? Wake up! T false. False true. And false false. o what really want to do is want to calculate all of these probabilities that give the probability of the dog condition of the burglar and the raccoon. imilarly, want to calculate the probability of B happening doesnt depend on anything else. o dont know what to do. Well, what m going to actually do is m going to do the same thing had to do up there. m going to keep track of m going to try a bunch of m going to get myself together a bunch of data. aybe do a bunch of experiments. aybe somebody hands it to me. But m going to use that data to construct a bunch of tallies which are going to end up giving me the probabilities for all of those things. o dont know, lets see. How should we start? tep one, find colored chalk. tep two, m going to extend these tables a little bit so can keep track of the tallies. o this is going to be all the ones that end up in a particular row. And these are going to be the ones for which dog is true. imilarly, m going to extend this guy up here in order to keep track of some tallies. This is going to be the ones for which B is true. And this one will be all. o thats my set up. And now suppose that my first experiment comes roaring in. And its all Ts. o have T T T. Thats my first experimental result, my first data item. o lets see. The arrangement here is burglar, raccoon, dog. o burglar as a true. And theres one tally count in there. Likewise, the T T, thats the burglar and the raccoon, that brings me down to this first row. o that gives me one tally in there and dog is true so that gives me a tick mark in that one. All right? Are you with me so far? And now lets suppose that the next thing happens be all false. Well, burglar is false. But there is one experiment. Everybodys false. o we come down here to false false. And thats the row were going to work on. We get a tally in there. Do we put one in here? No, because thats false. Dog is false. Thats what our data element says. o thats cool. aybe one more. Lets suppose we have T T F. Well in that case, we have a tick mark here and a tick mark here because the burglar element is true. Then we have T T. That brings us to the first row again. o we get a tick mark there. But dog as false, so no tick mark there. Thats how it works. suppose youd like to see a demonstration, right? Always like to see a demonstration. o heres what it actually looks like. o on the left you see the network as weve constructed it, with a bunch probabilities there. And what m going to do now is m going to start simulating away so as to accumulate tick marks, tally marks, and see what kinds of probabilities that they indicate for the table. happen to be using a process for which the model on the left is a correct reflection. o theres one simulation. o the dog barking lets see, the burglar is false. The raccoon is true. get one tick mark. o the probability there is one. Of course, m not going to just go with one. want to put a whole bunch of stuff in there. o ll just run a bunch more simulations. No dont even have an entry at all yet for T F here. Thats because havent run enough data. o let me clear it instead of doing it one at a time. Let me run 100 simulations. ee, its still not too good. Because it says this T T probability true. This just because m feeding it data, right? And m keeping track of what the data elements tell me about how frequently a particular combination appears. Yes, TDENT: o when youre doing one simulation, is that variables? PROFEOR PATRK WNTON: When m doing one simulation, m just keeping track of that combination in each of these tables. Because its going to tell me something about the probabilities that want reflected in those tables. o its pretty easy to see when go up here to burglar. f have a lot of data elements, theyre all going to tell me something about the burglar as well as the other variables. o if just look at that burglar thing, the fraction of time that it turns out true over all the data elements is going to be its probability. o now when go down to the joint tables, can still get these probability numbers. But now theyre conditioned on reticular condition of its parents. o thats how get these probabilities. o didnt do too well here because that T T combination gave me an excessively high probability. o maybe 100 simulations isnt enough. Lets run 10,000. o with that much data running through, the probabilities get lets see, ve got 893 here, instead of 0.9, 807 instead of 0.8, 607 instead of 0.6. And that ones deadon at 0.01. o if run enough of these simulations, get a pretty good idea what the probabilities ought to be given that ve got a correct model. OK, so that takes care of that one. And of course, didnt draw the other things in here. But by extension, you can see how those would work. Oh. But you know what? think will put a little probability of raccoon table in here. Because the next thing want to do is want to go the other way. This is recoding tallies from some process so can develop a model. But once ve got these probabilities, of course, then can start to simulate what the model would do. All right? How would do that? Well, do want to use the same table? think just to keep things sanitary, what ll do is ll go over here and do it again. Heres B. ts got a probability of B. Heres R. Heres a table probability of R. That comes down into a joint table for dog. And its got four elements. Depending on the burglar condition and the raccoon condition, we get a probability of dog. And now, imagine these have all been filled in. o what do want to do if want to simulate this system generating some combination of values for all the variables? Well, do the opposite of what did when was working around with this chain rule showing that could go from the table to those probabilities. Now ve got the probabilities. m going to go the other direction. nstead of chewing away from the bottom, m going to chew away from the top. Because when go into the top and chew way, everything need to know to do a coin flip is there. o in particular, when go up in here, ve got the probability of burglar now. o m going to use that probability to flip a coin. ay it produces a T. o that takes care of this guy. And can now scratch it off since its no longer in consideration. ts no longer a top variable. o now go over into raccoon and do the same thing. take this probability. do a flip. And say it produces an F. Whatever its probability is, flip a biased coin and thats what happen to get. But now, having dealt with these two guys, that uncovers this dog thing. And ve got enough information, because ve done everything above, to make the calculation for whether to dog is going to be barking or not. But wait. have to know that ve got a T and a T and a T and an F and an F and a T and an F and an F. Because have to select the right row. o know that B is T. And know that R is F. o that takes me into the table into the second row. o now get this probability. flip that coin and get some result, say, T. Voila. can do that with the other two variables. And ve got myself an experimental trial that is produced in accordance with the probabilities of the table. OK? Of course yeah, in fact, how did get those numbers? Actually what did is used the model on the left to generate the samples that were used to compute the probabilities on the right. o youve seen that a demonstration of this already. Now of course dont know, all of this sort of depends on having everything right. ve written a thing to write it one more time. Burglar, raccoon, dog, call the police, trashcan. But somebody else may say, oh, youve got it all wrong. This is what it really looks like. The dog doesnt care about the raccoon at all. o thats a correct model. Now when do a simulation, could fill in the tables in either model, right? m sure youd like to see a demonstration. o let me show you a demonstration of that. o there are the two tables. And can run 10,000 simulations on those guys, too. Now, look. The guy on the left is a pretty good reflection of the probabilities in a model used to produce the data. But the guy on the right doesnt know any better. it just fills in its own tables, too. o what to do? say this ones the right model. And you say that ones the right model. Whos right? aybe well never know. And the guy on the left will get rich in the stock market and the guy on the right will go broke. would be nice if we could actually figure out whos right. o would you to see how to figure out whos right? Yeah, so would . What were going to do is were going to look at naive Bayesian inference. And thats our next chore. o heres how it works. We know, from the definition of conditional probability, we know that the probability of A given B is equal to the probability of A and B divided by the probability of B, right? Equal to by definition. o that means that the probability of A given B times the probability of B m just multiplying it out it equal to that joint probability. Oh, but by symmetry, theres no harm in saying can turn that around and say that the probability of B given A times the probability of B is also equal to that joint probability, right? ve just expanded it a different and symmetric way. f ve got to write a, b on B, b, a on A. Thank you. Who was complaining? Good work. That would have been a majorleague disaster. But now, having written that, can forget about the middle. Because all m really interested in is how ve turned the probabilities around in that conditional. Why would care about doing that? By the way, were now talking about the work of the Reverend Bayes. Because we can rewrite this yet again as the probability of A given B is equal to the probability of B given A times the probability of A divided by the probability of B. Thats just elementary algebra. But now m going to do something magical. m going to say ve got a classification problem. want to know which disease you have. Thats a classification problem. aybe youve got the swine flu. aybe youve got indigestion. Who knows. But get all these symptoms. get all these pieces of evidence. Youve got a fever. Youre throwing oh, well, lets not go into too much detail, there. But what m going to do is m going to say, well, lets suppose that A is equal to a class that m interested in, the disease youve got. And B is equal to the evidence, the symptoms observe. Voila. may have a pretty hard time figuring out what the probability of the class is given the evidence. But figuring out the probability of the evidence given the class might not be so hard. Let me get another board in play and show you what mean. By plugging class and evidence into Bayes rule, what get is the probability of some class given the evidence is equal to the probability of the evidence given the class times the probability of the class divided by the probability of the evidence. Now youve got to let that sing to you a little bit. uppose ve got several classes that m trying to decide between. m trying to select the best out of that batch of classes. Well, ve got the evidence. And if know the probability of the evidence given each of those classes, and if know, a priori, the initial probability the class, then m done. Because ve got the two elements in the numerator. Why am done? Because the denominator is the same for all the classes. ts just the probability of the evidence. And then could just sum everything up. know it adds to 1 anyway. o thats cool. But sometimes theres evidence actually theres more than one piece of evidence. Lets say that theres some class. some i, and were trying to figure out if thats the correct class. o weve got c sub i there and c sub i there. And suppose that that evidence is actually a bunch of pieces of evidence. o it could be e sub 1, e sub n, oops, premature right bracket. All that evidence, given the class i times the probability of the class i over some denominator that we dont care about because its going to be the same for everybody. o well just write that as d. Now what if these pieces of evidence are all independent given the class? o if you have the swine flu, the probability you have a fever is independent of the probability youre going to throw up, say. Then can we write this another way? An easier way? ure. Because when things are independent, the joint probability is equal to the product of the individual probabilities. o that is to say its easier to see it if you write it down than if you just say it this probability here from these two elements here is equal to the probability of e sub 1 conditioned on c sub i times the probability of e sub 2 conditioned on c sub i, all the way down to the probability of e sub n conditioned on c sub i divided by some denominator we dont care about. ee, what m going to try to do is m going to go through this for all the ci and see which ones the biggest. TDENT: Thats the ci, right? PROFEOR PATRK WNTON: This is the probability of TDENT: . PROFEOR PATRK WNTON: Right here? Oh yes, youre quite right. Oh yeah, thanks. cant write and think at the same time. Thanks. OK. o ve just figure out which one of these is the biggest. And ve identified the class. Now you say to me, well, would like to see an example. o dont know, does anyone have any spare change? A nickel, a quarter. This is not because of infinitesimally low raises here at T. just need it for a demonstration. need two coins. Dont forget to get these back, tend to be Now suppose these two coins are not exactly the same. One of these points is a legitimate, highlyprized American quarter. The other one is a fake. And with this one, the probability of heads, let us say, is 0.8 instead of 0.5. o mix these all up. And pick one. And start flipping it. And get a head. Then flip it again. And get a tail. Which coin did pick? Well, were going to use this stuff to figure it out. Heres what happens. Before forget. Thank you very much. o what weve done is weve selected these things from my hands. And cant draw hands. o ll draw a little cup here. And there are two coins in here. And were going to pick one. And one has a probability of heads equal to 0.8. And this one has a probability of a head of 0.5. o heres the draw. pick one. Each has a probability of 0.5. This one is the one with the 0.8 as the probability of head. And this one is the one with the probability of 0.5 as a head. OK? o now suppose the first flips as it was is T. Well, thats a piece of evidence. Thats here. Probably of evidence given the class. Well in the case of having drawn this biased coin, the probability of coming up with a tail ah, lets say a head, just to make my numbers a little easier. Probability of coming out there with a head is equal 0.8 given that its up here in this choice. The probability given that you have a fair coin is 0.5. o now if we take the next coin and take it to be a tail then the probability of this guy given that evidence is 0.2. And the probability of this guy given that evidence its a fair coin, so it doesnt care. ts still 0.5. o now whats the probability of this class given this evidence? ts the product 0.5 times 0.8 times 0.2. And whats the probability of this guy? ts 05 times 0.5 times 0.5, divided by a denominator which is the same in both cases. o lets forget about this early 0.5 here. Because its the same in both cases. And we just multiply those numbers together. That gives us 0.8 times 0.2. Whats that? 0.16? And this guy, 0.5 times 0.5, thats 0.25. o it looks an awful lot like with this combination that ve picked the coin thats fair. One more flip? o lets flip it again, and suppose we come up with a head. o that puts a 0.8 in here. And 0.5 in here. When you multiply those out thats 0.125. And this is 0.128. o its about equal. o you see how that works? All right. o were using the coin flips as evidence to figure out which class is involved. OK so dont know, youd probably like to see a demonstration of this, too, right? You say to me, gosh, just two kinds of coins. Thats not very interesting. Lets try five kinds of coins. o what want to show you is how the probabilities for all these coins there are five of them, colorcoded how the probabilities vary with a series of flips. Lets suppose ve got a head the grey line, by the way, is the fraction of heads so thats going to be one. Because m just doing heads. You see that black line rising? hould look like a rocket. Thats the probability that the thats the coin which only shows heads, the probability of head is 1. And m flipping a whole bunch of heads here. snt that cool? Now what happens if suddenly put in a tail? By the way, youll no doubt, here one the extreme left the initial probability of the P=0 coin was 0.1. As soon as flipped a head that went to 0. And it will never get off 0, right? That makes sense. Because if the probability that youll get a head is 1 you should never see a tail. f you ever do, that isnt your coin. What happens now if interrupt a series of heads and produce a tail? TDENT: . PROFEOR PATRK WNTON: Whats that? TDENT: . PROFEOR PATRK WNTON: The black one will go to 0. What else happens? By the way, the blue one is the one with the highest probability of being a head. Boom! That blue one shot up. Not going up slowly. t shot up. Because now the preponderance of evidence with all those heads is that ve flipped the coin with a bias of 0.75 towards heads. o lets clear this. Pick any probability you want. 0.25, 0.5, and so on. dont know, lets pick 0.25 since weve been at the upper end. o orange is 0.25. And sure enough, the probability that ve selected the 0.5 coin is going up and up and up and up after the original irregularity. The Law of Large Numbers is setting in. And a probability that ve got that 0.25 coin in play is pretty close to 1. All right. o thats cool. Now you say to me, thats awfully nice but stop. Awfully nice, but not very realworldish. o let me give you another problem. ts wellknown that you are, with high probability, of the same political persuasion as your parents. o if wanted to figure out which party a parent belongs to, could look at the party that their children belong to, right? o its just like flipping coins. The particular coin have chosen corresponds to the parent. ndividual flips correspond to the political party that the child belongs to. o lets get up a little bit by the way, wrote all this stuff over the weekend. o who knows if any of it will work. But lets see. A parent party classifier. There it is, Democrats and Republicans. And now the prior for being a Republican given here is 0.5. But dont know, this is a little bit Democratic state. o lets adjust that down a little bit. omewhere in there might be about right But lets just, for the sake of a classroom illustration, go down here. o now the meter is showing the prior probability because thats the only thing in the formula so far. ve got no evidence. o now lets suppose that child number one is a Republican. Back to neutral. o ve got a low probability that the parent a priori probability that the parent is a Republican and a child whos a Republican. notice that 0.2 and 0.8, the conditional is 0.8. And the prior is 0.2. Thats why it comes out to balance each other, right? o now if we get another Republican in there it goes way up. f have a Democratic child it goes back down. f have an equal balance between children then it goes way back down because of that prior probability being low. o if make that high, even though the children are balanced, m still going to have a high probability of being a Republican. Now lets see. f take that slider there, the conditional probability, and drive it to the left here let me make that equally in. And lets make that one thing. dont know. What am doing now? f make the probability less than 0.5, whats that mean? That means youre sore at your parents and you want to belong to a different party. All right, so now, whats next? Oh gosh. Whats next? This is whats next. Whats next to somewhere? Yeah, this is whats next. This here. Weve got two models. Remember when said we wanted to decide between them? an we use that Bayesian hack to do that, too? ure. Because weve got these two models. Weve got the probabilities in them. o now can take my data and calculate the probability of a left model given the data and the probability of the right model given the data, multiply that times their a priori probabilities, which ll assume are equal. Then can do a model selection deal much in defiance to what was hinting at before. so lets try that. Whoa. There are my two models. Yes, there they are. Weve already trained them up. And theyve got their probabilities. Now what were going to do is were going to use the original model to simulate the data. o what were going to do is were going to simulate draws, simulate events, similarly combinations of all variables using a model that looks like the one on the left, that is the one on the left except for the slight differences in probabilities, OK? Then were going to do this Bayesian thing and see where the meter goes. o well run one data point. Oops, went the wrong way. akes me nervous. just finished this at 9:15. aybe theres a bug. Oops, two data points, swings to the left. Three data points, back to the right. Of course thats not much data. o lets put some more data in. Yeah. Boom, there it goes. Lets try that again. That was cool. o lets run 1,000 simulations and one data point. t bobbles around a little bit and goes flat over to the left. Because that is the model that reflects the one that the data is generated from. o now we got Bayesian classification, except now the classification has gone one step more and it becomes structure discovery. Weve got two choices of structure. And we can use this Bayesian thing to decide which of the two structures is best. snt that cool? Well, its only cool if you could do what? o if you had two choices you can select between them and pick the best one but there are gosh, for this number of variables, there are a whole lot of different networks that satisfy the no looping criteria and dont have very many parents. Theres an awful lot of them. n fact, if you strict this network to two parents there are probably thousands and thousands of possible structures. o do try them all? Probably not. ts too much work when you get 30 variables or something like that. o what do you do? We know what to do, right? Were almost veterans a 6034. We have to search! o what we do is we take the loser and we modified it. And then we modify it again. And we keep modifying it until we drop dead or we get something that were happy with. o lets see what happens if we change this problem a little bit and do structure discover. Were starting out with nothing linked. And were going to just start running this guy. o whats going to happen is that the good guy will prevail. And the bad guy will be a copy of the good guy perturbed in some way. o its a random search. Youll notice that score its too small for you to read. All these things are too small to read. Let me make it a little bigger. Too small to read, but that number on the right there is not the product of the probabilities, actually. ts the sum of the logarithms of the probabilities. They go together, right? And the reason you use this instead of the probabilities is because these numbers get so small that was a 32bit machine, you eventually lose. o use the log of the probabilities rather than the product of the probabilities. You use the sum of the logs instead of the product of the probabilities. And eventually, you hope that this thing converges on the correct interpretation. But you know what? This thing is so flat as a space and so a large and so telephone polelike that its full of local maxima. o what this program is doing is every once in awhile think with probability 1 and 10; forgot what parameters used every once in awhile, itll do a total radical rearrangement of the structures. n other words, its a random restart. t keeps track of the best guy so far. And every once in awhile it does a totally random restart in its effort to search the space. o thats how you go from probabilistic inference to structure discovery. Now when is this stuff actually useful? Well, hinted at a medical diagnosis, right? Thats a situation where youve got some symptoms. And you want to know what the disease is. o as soon as you use the keyword ""diagnosis,"" youve got a problem for which this stuff is a candidate. o what other kinds of diagnosis problems are there? Well, you might be lying to me. o can put a lie detector on you. And each of those variables that are measured by the lie detector are an independent indication whether youre telling the truth or not. o its this kind of Bayesian discovery thing. Naive Bayesian lassification. What other kinds of problems speak to the issue of diagnosis? Well, we like to know how well you know the material! o we can use quizzes as pieces of evidence. Thank god we dont use exactly a naive Bayesian classifier, because then we wouldnt be able to do that combination. We have to use a slightly more complex what you can think of as a slightly more complex Bayesian net to do that particular kind of diagnosis. You might have a spacecraft or an airplane or other piece of equipment with all sorts of symptoms. Youre trying to figure out what to do next, what the cause is. o using the evidence to go backward to the cause. o maybe youve got some program that doesnt work. Happens to me a lot. o use the evidence from the symptoms of the misbehavior to figure out what the most probable cause is. But now to conclude the day last time there werent any powerful ideas. But if you take the combination of the last lecture and this lecture to be a candidate for gold star ideas, these are the ones d like to leave you with. We got here is this Bayesian stuff, all these probabilistic calculations are the right thing to do. Theyre the right way to work when you dont know anything, which would make it sound like youre not very useful, because you think you always well, in fact, there are a lot of situations where you either cant know everything, dont have time to know everything, or dont want to take the effort to know everything. o in medical diagnosis all youve got is the symptoms. You cant go in there and figure out in a more precise way exactly whats wrong. o you use the symptoms to determine what the cause is. And then all those other kinds of cases that mentioned. But now, what other kinds of structure discovery are there? Well, the kind of structure discovery that hinted at in the beginning will be the subject that well begin with during our next and sadly final conversation here in on Wednesday. t will feature not only a discussion of how this stuff can be used to discover patterns and stories, but well also talk about whats on the final, what kind of thing you could do next, that sort of thing to finish off the subject. And thats the end of the story for today.","o now can write can pick to be my variable n. And can say that the chain rule says that the joint probability of all these variables P of , D, B, T, and R the probability of any particular combination of those things is equal to the probability of given everybody else. o what really want to do is want to calculate all of these probabilities that give the probability of the dog condition of the burglar and the raccoon. The probability given that you have a fair coin is 0.5. o now if we take the next coin and take it to be a tail then the probability of this guy given that evidence is 0.2. We know, from the definition of conditional probability, we know that the probability of A given B is equal to the probability of A and B divided by the probability of B, right? But what m going to do is m going to say, well, lets suppose that A is equal to a class that m interested in, the disease youve got. We can decide what the probability of the police being called is given the this and the that, and all that sort of stuff, by clicking the appropriate boxes. And this one is the one with the probability of 0.5 as a head. This one is the one with the 0.8 as the probability of head. o up here weve got the a priori probability of B. Well, thats just probability of B. Down here with the dog, ve got a bigger table because ve got probabilities that depend on the values of its parents. All that evidence, given the class i times the probability of the class i over some denominator that we dont care about because its going to be the same for everybody. o what were going to do is were going to simulate draws, simulate events, similarly combinations of all variables using a model that looks like the one on the left, that is the one on the left except for the slight differences in probabilities, OK? Because we can rewrite this yet again as the probability of A given B is equal to the probability of B given A times the probability of A divided by the probability of B. Thats just elementary algebra. By plugging class and evidence into Bayes rule, what get is the probability of some class given the evidence is equal to the probability of the evidence given the class times the probability of the class divided by the probability of the evidence. Oh, but by symmetry, theres no harm in saying can turn that around and say that the probability of B given A times the probability of B is also equal to that joint probability, right? o if just look at that burglar thing, the fraction of time that it turns out true over all the data elements is going to be its probability. o that means that the probability of A given B times the probability of B m just multiplying it out it equal to that joint probability. ee, what m going to try to do is m going to go through this for all the ci and see which ones the biggest. Lets suppose ve got a head the grey line, by the way, is the fraction of heads so thats going to be one. Thats the probability that the thats the coin which only shows heads, the probability of head is 1. o now can take my data and calculate the probability of a left model given the data and the probability of the right model given the data, multiply that times their a priori probabilities, which ll assume are equal. Too small to read, but that number on the right there is not the product of the probabilities, actually. But m going to use that data to construct a bunch of tallies which are going to end up giving me the probabilities for all of those things. o that is to say its easier to see it if you write it down than if you just say it this probability here from these two elements here is equal to the probability of e sub 1 conditioned on c sub i times the probability of e sub 2 conditioned on c sub i, all the way down to the probability of e sub n conditioned on c sub i divided by some denominator we dont care about. Now what were going to do is were going to use the original model to simulate the data. But now the question is, ve got some probabilities m going to have to figure out here. And whats the probability of this guy? o the probability there is one. Well, what m going to actually do is m going to do the same thing had to do up there. t says that the probability of all this stuff happening together is given as the product of a bunch of conditional probabilities. Because that is the model that reflects the one that the data is generated from. We got here is this Bayesian stuff, all these probabilistic calculations are the right thing to do. And if know the probability of the evidence given each of those classes, and if know, a priori, the initial probability the class, then m done. Because the next thing want to do is want to go the other way. And with this one, the probability of heads, let us say, is 0.8 instead of 0.5. o mix these all up. And you want to know what the disease is. o this is going to be all the ones that end up in a particular row. And the way m going to make it is m going to chew away at those variables from the bottom. f take that slider there, the conditional probability, and drive it to the left here let me make that equally in. t will feature not only a discussion of how this stuff can be used to discover patterns and stories, but well also talk about whats on the final, what kind of thing you could do next, that sort of thing to finish off the subject. o what m going to do is m going to say, well, there are two bottoms here, theres and T. o have a choice. And the probability of this guy given that evidence its a fair coin, so it doesnt care. By the way, the blue one is the one with the highest probability of being a head. o in particular, when go up in here, ve got the probability of burglar now. The guy on the left is a pretty good reflection of the probabilities in a model used to produce the data. And what m going to do now is m going to start simulating away so as to accumulate tick marks, tally marks, and see what kinds of probabilities that they indicate for the table. Likewise, the T T, thats the burglar and the raccoon, that brings me down to this first row. o if you have the swine flu, the probability you have a fever is independent of the probability youre going to throw up, say. Actually what did is used the model on the left to generate the samples that were used to compute the probabilities on the right. And this one has a probability of a head of 0.5. o heres the draw. o if you had two choices you can select between them and pick the best one but there are gosh, for this number of variables, there are a whole lot of different networks that satisfy the no looping criteria and dont have very many parents. ts just the probability of the evidence. o what want to show you is how the probabilities for all these coins there are five of them, colorcoded how the probabilities vary with a series of flips. Because when things are independent, the joint probability is equal to the product of the individual probabilities. Theyre the right way to work when you dont know anything, which would make it sound like youre not very useful, because you think you always well, in fact, there are a lot of situations where you either cant know everything, dont have time to know everything, or dont want to take the effort to know everything. And now lets suppose that the next thing happens be all false. You use the sum of the logs instead of the product of the probabilities. And these are going to be the ones for which dog is true. o well just write that as d. Now what if these pieces of evidence are all independent given the class? This is going to be the ones for which B is true. Probability of coming out there with a head is equal 0.8 given that its up here in this choice. The trashcan, yeah, thats dependent on R. And R over here, the final thing in the chain, thats just a probability. Because the intuition is all of the causality is flowing through the parents and cant get to this variable D without going through the parents. And we can use this Bayesian thing to decide which of the two structures is best. One thing m going to do before think about probability is m going to make a linear list of all these variables. have to know that ve got a T and a T and a T and an F and an F and a T and an F and an F. Because have to select the right row. And that is that any variable on this graph is said by me to be independent of any other nondescendant given its parents. But itll be a bit of a journey before we get there, because weve got to go through all that stuff on the outline. o now the meter is showing the prior probability because thats the only thing in the formula so far. o use the evidence from the symptoms of the misbehavior to figure out what the most probable cause is. Well in the case of having drawn this biased coin, the probability of coming up with a tail ah, lets say a head, just to make my numbers a little easier. And we got to that alternative just at the end of the show a week ago. But once ve got these probabilities, of course, then can start to simulate what the model would do. And thats the row were going to work on. o we need to be able to show that we can get to that thing by doing calculations on this thing. may have a pretty hard time figuring out what the probability of the class is given the evidence. We know what to do, right? Because when go into the top and chew way, everything need to know to do a coin flip is there. But figuring out the probability of the evidence given the class might not be so hard. But if you take the combination of the last lecture and this lecture to be a candidate for gold star ideas, these are the ones d like to leave you with. And the reason you use this instead of the probabilities is because these numbers get so small that was a 32bit machine, you eventually lose. And that ones deadon at 0.01. o if run enough of these simulations, get a pretty good idea what the probabilities ought to be given that ve got a correct model. And the guy on the left will get rich in the stock market and the guy on the right will go broke. And sure enough, the probability that ve selected the 0.5 coin is going up and up and up and up after the original irregularity. o that means that the probability of the dog barking, given its parents, doesnt depend on T, the trash can being overturned. Thats just because of the way ve arranged the variables. Well, do the opposite of what did when was working around with this chain rule showing that could go from the table to those probabilities. By the way, youll no doubt, here one the extreme left the initial probability of the P=0 coin was 0.1. And you say that ones the right model. And B is equal to the evidence, the symptoms observe. o now have a way of calculating any entry in that table because any entry in that table is going to be some combination of values for all those variables. o somehow this thing is going to be were going to use this thing instead of that thing. ts still 0.5. o now whats the probability of this class given this evidence? And a probability that ve got that 0.25 coin in play is pretty close to 1. Well, were going to use this stuff to figure it out. o m going to use that probability to flip a coin. o if know the parents of a variable know that the variable is independent of all other nondescendants. Then were going to do this Bayesian thing and see where the meter goes. Youre trying to figure out what to do next, what the cause is. f have a lot of data elements, theyre all going to tell me something about the burglar as well as the other variables. o know that B is T. And know that R is F. o that takes me into the table into the second row. And ve got myself an experimental trial that is produced in accordance with the probabilities of the table. And one has a probability of heads equal to 0.8. o what do want to do if want to simulate this system generating some combination of values for all the variables? Because the denominator is the same for all the classes. And say it produces an F. Whatever its probability is, flip a biased coin and thats what happen to get. The probability of dog barking depends on the condition of the parents, nothing else. o the dog barking lets see, the burglar is false.",0.1576340326340326
31,31,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: o for this last class exercise we have this following code. o we have 46 for A gold, 27 for K gold, 1 for Romanian gold. We make this variable, here, Total Gold, to be the sum of those three which believe is 74. Then were going to print total gold. OK? Then were going to increase the value for Romanian gold and were going to print it again. o as always, you can just copy this, pop it into Python, and run it to test yourself, but you should do it just in your mind first. o notice it print 74 in 74, and thats because we never told the program to calculate the new total. o we only calculated the total, way up here on line 5. f we calculated the total again down here, then it would be 74.75 o lets look at the answers, hopefully you guys got that. Perfect, majority are good to go. f you didnt get that please try to review it, review why it didnt work.","Then were going to increase the value for Romanian gold and were going to print it again. Then were going to print total gold. We make this variable, here, Total Gold, to be the sum of those three which believe is 74. o notice it print 74 in 74, and thats because we never told the program to calculate the new total.",0.256198347107438
32,32,"n the last video, started with this matrix right here, and right from the get go, we said the span of this matrix is just the span of the column vectors of it, and just wrote it right there. But we werent clear whether this was linearly independent, and if its not linearly independent, it wont be a sufficient basis. And then we go off and we figure out the null space of A. We find out that the null space of A contains more than just the zero vector. ts just the span of these two vectors here, which means that these columns are not linearly independent. We saw that several videos ago. And we used that information that theyre not linearly independent to try to make them linearly independent by getting rid of the redundant ones. o we were able to get rid of this guy and this guy, because these were essentially the columns associated with the free variables. And we were able to do it using this little technique down here, where we set one of these equal to 0, the other one equal to negative 1 and then solved for the pivot variables. And then we set the other one equal to 0 and the other one equal to negative 1 and solved for the pivot variables. And you could imagine that this is a generalizable process. f you have a bunch of free variables, you can set all of them but one to equal 0, and then that one that you didnt set to 0, you set it to equal negative 1. And you can express it as a sum of the pivot variables, where the pivot variables are a function of the free variables. n general, this would be a quick way of doing it. We had to do it more slowly over here. f have some matrix A, and want to figure out the basis for the column space, the column space is just the span of these, but if wanted a linearly independent basis, need to figure out some set of these that are linearly independent. What can do is put this guy into reduced row echelon form. When put them in reduced row echelon form, and did that over here, this is the reduced row echelon form of A, can look at the variables that are associated with the pivot entries. o this is x1. Let me scroll up a little bit. This is associated with x1, right? When you multiply this times x1, you get this column times x1, this column times x2, this column times x3, this column times x4 like that. When you look at just regular A, when you look at just your matrix A, its the same thing. f you were to write Ax equal to 0, this column would be associated with x1, this column would be associated with x2, x3 and x4 like that. What you can do is you put it in reduced row echelon form. You say which columns have my pivot entries or are associated with pivot variables? You say, OK, x1 and x2 are associated with pivot variables, or they are the pivot variables, and theyre associated with these first two columns, and so those first two columns would be a basis for the column space. How did get this? Am just making up things on the fly? Well, no! t all comes from the reality that you can always construct a situation where the vectors associated with the free variables you can write them as linear combinations of the vectors associated with the pivot variables, and we did a special case of that last time. But a very quick and dirty way of doing it, and dont know if its actually dirty, is just take your matrix, put it in it reduced row echelon form, and you say, look, this column and this column are associated with my free variables. Therefore, this column and this column must be associated with my free variables. The solution sets are the same to Ax equal to 0 or the reduced row echelon form of Ax is equal to 0. o theyre the same. o if this column and this column are associated with free variables, so are this column and this column, which means that they can be expressed just by judiciously selecting your values of your free variables as linear combinations of the columns associated with the pivot variables, with the pivot entries, which are that column and that column. o this column and this column would be a basis for A, and we see that. We see that all the way down here. 1, 2, 3 and 1, 1, 4, We did a lot of work and we got all the way there, and we said this is a basis for the column span of A. Now, doing all of that work, lets see if we can actually visualize what the column space of A looks like. have a strange feeling might have said column span a couple of times, but the column space, what does it look like? Well, theres a couple of ways to think about what it looks like. One way is we can say, look, this span this is 2 thats a member of R3. Thats a vector in R3 and this is a vector in R3. Let me draw my x, z and well, normally its drawn this is normally y, x, and zaxes in R3, if m want to represent them as threedimensional space. Then the vector 1, 2, 3 might look like this where its one, one, two, one, two, three, so we go out one down here, then up three. o the vector will look like that in its standard form. Thats that one right there. And the vector 1, 1, 4 would be one, one and go up four, so it might look something like this. ts hard to actually draw them very well in three dimensions, but you get the idea. But the column space is the span of these, so all of the linear combinations of these two guys. o all of the linear combinations of these two guys are going to create a plane that contains these two vectors. f you just sum these guys up in multiple combinations, you can get any vector up there. f you just add them up, youll get that vector right there. f you add this guy plus 2 times that, youll get some vector up here. o if you view them as position vectors, theyll essentially form a plane in R3. But lets see if we can get the equation for that plane. Well, how can we do that? Well, we know that we can figure out the equation of a plane based on the fact that a normal vector dotted with any let me write my normal vector like this. The normal vector dotted with any position vector specifying a position on the plane. o let me call that x minus any point on the plane or any vector position on the plane. o could do that minus the vector 1, 2, 3 has to be equal to 0. And we can use this information to figure out the equation for this plane. But what is a normal vector? How can we find a normal vector to this plane? o this would be a vector. Let me see if can draw this in a way that doesnt confuse the issue. f the plane is like that, the normal vector would come out like that. o how can create a normal vector? Well, we learned that you take the cross product of any two vectors in R3, and the cross product ve only defined so far in R3, and will get a vector thats normal to both of those vectors. o lets take the cross product. This is a nice way of thinking about it, because its really integrating everything that weve covered so far. o let me define my normal vector to be equal to 1, 2, 3 cross 1, 1, 4. And what does this equal? o my first term, ignore that. get 2 times 4 minus 3 times 1. 2 times 4 is 8. 2 times 4 minus 3 times 1. 8 minus 3. Then for my second row, have 1 times 4, and my temptation is do 1 times 4 minus 3 times 1. But you reverse it. You do 3 times 1, so its 3, minus 1 times 4. Weve done that multiple times. You might want to review the cross product video if that seems strange. You ignore the middle row, and you normally do 1 times 4 minus 3 times 1, but the middle row you switch. Were only defining it for R3, so instead, we do 3 times 1 minus 1 times 4. And then finally for the last row, we ignore it, and we say 1 times 1, which is 1, minus 2 times 1, which is minus 2. And this is equal to the vector 5, minus 1, minus 1, which by definition of the cross product, and ve shown this to you multiple times, is normal to both of these vectors, and so itll be normal to any linear combination of these two vectors. o now that we have our normal vector, we can define the traditional equation for the plane. o we now know that our normal vector 5, minus 1, minus 1, that got by taking the cross product of our basis vectors dot any vector in our plane. o let me just write any vector. Let me just write it x, y, z. o x, y, z since thats how defined my axes up here. This was the xaxis. x, y and z. x, y, z minus just picked one of these. could have picked either of them minus 1, 2, 3 has got to be equal to 0. o whats this? This is going to be equal to let me write it a little smaller, a little neater 5, minus 1, minus 1 dot whats this guy going to be? x minus 1, y minus 2, and z minus 3 has got to be equal to 0. And whats the dot product? ts 5 times x minus 1 plus or maybe should say minus 1, so its plus minus 1 times y minus 2 plus minus 1 times z minus 3 is equal to 0. Thats just the definition of our dot product. f simplify this, get 5x minus 5 minus y plus 2 minus z plus 3 is equal to 0. You have 2 plus 3 is 5 minus 5, so those all cancel out. Those will equal 0. And we get 5x minus y minus z is equal to 0. This plane in R3 is the column space of A. o weve now shown you that its truly a plane in A. And actually, it makes sense that this plane intersects the origin. f you set x, y, and z equal to 0, it satisfies this equation. And that makes sense, because we said that a column space of a matrix has to be a valid subspace, and a valid subspace has to contain the zero vector. And in R3 thats the coordinate 0, 0, 0. Now, what want to do now is see if we can get at the same answer going a completely different way, or approaching it in a completely different way. Let me get my original A, which ve forgotten. ve marked it up a good bit, but let me just copy and paste it. o thats my original A right there. Let me copy it. Let me paste it. Nope. Thats not what wanted to do. Lets see, so my original A copied and pasted the wrong thing. Let me do it a little dont want to waste your time. Edit, copy, edit, paste. There you go and let me scroll this down and get to a point thats relatively clean. Bring my A down. ve used a lot of space. o here you go. This is my original A right there. And what want to do is see if can get this result completely different. got this result by figuring out the basis of the column span, finding a normal vector by taking the cross product of our two basis vectors, and then using the dot product of the normal vector with the difference this vector right here, where you take any vector on our plane minus one of our basis vectors, thats to find some vector in the plane. This is some vector in the plane. o any vector in the plane dotted with my normal vector is going to be equal to 0. And actually, should probably make a side note here, that the only reason was able to say that the normal vector is a cross product of my two basis vectors, is because knew that these two basis vectors, not only do they specify some point on the plane so lets say that this guy right here is this blue vector. Not only does he specify some point on the plane right there, but the vector lies completely on the plane. How did know that? Because knew from the get go that the 0, 0 vector is in my span, right? knew that if draw this guy in just standard position, the point 0, 0, 0, is in my span, and know its end point is in the span, so this whole vector has to be in my plane, and likewise, this whole vector has to be in my plane. o if take the cross product, anything normal to these guys or any combination of these guys is going be normal to the plane, and we got this result right here. But let me take this right here and use our other definition of column span. Our other definition, or its really an equivalent definition, is all of the valid solutions to Ax where x is a member of Rn. Or another way we could think of it is, we could view it as all of the valid bs where Ax is equal to b, and x is a member of Rn. These are equivalent statements. m just defining b here to be Ax, so these are equivalent statements. But lets run with this a little bit. o lets say that define my b, so b is going to be a vector in R3, right? We already have an intuition like that. When take Ax, get a b thats equal to x, y, z. And want to figure out what x, ys and zs can get valid solutions for? o if take my vector A and then let me multiply it times well, actually, the best way to do it, think were used to it right now. f m solving the equation Ax is equal to b, can essentially just create the augmented matrix, where have the matrix A and can augment it with b, and put this in reduced row echelon form, and thatll essentially represent the solution set. o let me do that. o if just augment this matrix right here with b, so write x, y, z. o this is A augmented with b. This is A, this is b. let me put this in reduced row echelon form and find the solution set. And these are x, y, and zs that define a valid b. o what do get? The first thing want to do, and weve done this exercise before, lets keep my first row the same. 1, 1, 1, 1, and get an x. And lets replace our second row with the second row minus the first row. Actually let me do it this way. Let me replace the second row with 2 times the first row minus the second row. o 2 times the first row minus the second row, were going to get a 2x minus y up there. And then 2 times 1 minus 2 is 0. 2 times 1 minus 1 is 1. 2 times 1 minus 4 is minus 2. 2 times 1 minus 3 is minus 1. Fair enough. And now let me replace my third row with the third row minus 3 times the first row. o were going to do the third row minus no, let me do it this way. ts the third row minus 3 times the first row. o m just doing the b column first, because can remember what did. The third row minus 3 times the first row. 3 minus 3 times 1 is 0. 4 minus 3 times 1 is 1. 1 minus 3 times 1 is minus 2. And then 2 minus 3 times 1 is minus 1. Now, could go all the way to reduced row echelon form, but something interesting is already happening. o let me from the get go try to zero out this third row. And the best way to zero out this third row is to just replace the third row. o the first row well, wont even write the first row. The second row is 0, 1, minus 2, minus 1, and 2x minus y. m not even going to worry about the first row right now. But lets replace the third row, just in our attempt to go into reduced row echelon form. Lets replace it with the second row minus the third row. o you get 2x minus y minus z plus 3x. just took this minus this. o minus z plus 3x. o 0 minus 0 is 0. 1 minus 1 is 0. inus 2 minus minus 2 is 0, and thats also 0. o were only going to have a valid solution to Ax equals b if this guy right here is equal to 0. What happens if hes not equal to zero? Then were going to have a bunch of zeroes equaling some number, which tells us that theres no solution. o if pick a b where this guy does not equal zero, then ll have no solution. f this guy equals 5, if pick x, y, and zs such as that this expression is equal to 5, then Ax equal to b will have no solution, because it will have 0 is equal to 5. o this has to equal 0. o 2x minus y minus z plus 3x must be equal to 0 in order for b to be valid, in order for b to be a member of the column space of A, in order for it to be a valid vector that Ax can become, or the product A times x can become for some x. o what does this equal to? f we add the 2x plus the 3x, get 5x minus y minus z is equal to 0, which is the exact same outcome we got when we figured out the basis vectors. We said oh, you know what? The basis vectors, they have to be in the column space themselves by definition. o let me find a normal vector to them both by taking the cross product. did that, and said the cross product times any valid vector in our space minus one of the basis vectors has to be equal to zero, and then got this equation. Or we could have done it the other way. We couldve actually literally solved this equation setting our b equal to this. We said what bs will give us a valid solution? And our only valid solution will be obtained when this guy has to be equal to zero, because the rest of his row became zero. And when we set that equal to zero, we got the exact same equation. o, hopefully, you found this to be mildly satisfying, because we were able to tackle the same problem from two different directions and get the same result, and it kind of shows you the beauty of linear algebra, how it all starts to fit together.","And this is equal to the vector 5, minus 1, minus 1, which by definition of the cross product, and ve shown this to you multiple times, is normal to both of these vectors, and so itll be normal to any linear combination of these two vectors. did that, and said the cross product times any valid vector in our space minus one of the basis vectors has to be equal to zero, and then got this equation. f this guy equals 5, if pick x, y, and zs such as that this expression is equal to 5, then Ax equal to b will have no solution, because it will have 0 is equal to 5. o this has to equal 0. o 2x minus y minus z plus 3x must be equal to 0 in order for b to be valid, in order for b to be a member of the column space of A, in order for it to be a valid vector that Ax can become, or the product A times x can become for some x. o what does this equal to? got this result by figuring out the basis of the column span, finding a normal vector by taking the cross product of our two basis vectors, and then using the dot product of the normal vector with the difference this vector right here, where you take any vector on our plane minus one of our basis vectors, thats to find some vector in the plane. And then 2 minus 3 times 1 is minus 1. And we get 5x minus y minus z is equal to 0. o if just augment this matrix right here with b, so write x, y, z. o this is A augmented with b. This is A, this is b. let me put this in reduced row echelon form and find the solution set. o could do that minus the vector 1, 2, 3 has to be equal to 0. n the last video, started with this matrix right here, and right from the get go, we said the span of this matrix is just the span of the column vectors of it, and just wrote it right there. 1 minus 1 is 0. inus 2 minus minus 2 is 0, and thats also 0. o were only going to have a valid solution to Ax equals b if this guy right here is equal to 0. And then finally for the last row, we ignore it, and we say 1 times 1, which is 1, minus 2 times 1, which is minus 2. o 2 times the first row minus the second row, were going to get a 2x minus y up there. o were going to do the third row minus no, let me do it this way. And actually, should probably make a side note here, that the only reason was able to say that the normal vector is a cross product of my two basis vectors, is because knew that these two basis vectors, not only do they specify some point on the plane so lets say that this guy right here is this blue vector. 4 minus 3 times 1 is 1. 3 minus 3 times 1 is 0. 2 times 1 minus 1 is 1. 2 times 1 minus 4 is minus 2. 2 times 1 minus 3 is minus 1. 1 minus 3 times 1 is minus 2. Well, we know that we can figure out the equation of a plane based on the fact that a normal vector dotted with any let me write my normal vector like this. 1, 2, 3 and 1, 1, 4, We did a lot of work and we got all the way there, and we said this is a basis for the column span of A. Now, doing all of that work, lets see if we can actually visualize what the column space of A looks like. o any vector in the plane dotted with my normal vector is going to be equal to 0. And then 2 times 1 minus 2 is 0. And we were able to do it using this little technique down here, where we set one of these equal to 0, the other one equal to negative 1 and then solved for the pivot variables. f we add the 2x plus the 3x, get 5x minus y minus z is equal to 0, which is the exact same outcome we got when we figured out the basis vectors. And now let me replace my third row with the third row minus 3 times the first row. The second row is 0, 1, minus 2, minus 1, and 2x minus y. m not even going to worry about the first row right now. x minus 1, y minus 2, and z minus 3 has got to be equal to 0. o if take my vector A and then let me multiply it times well, actually, the best way to do it, think were used to it right now. knew that if draw this guy in just standard position, the point 0, 0, 0, is in my span, and know its end point is in the span, so this whole vector has to be in my plane, and likewise, this whole vector has to be in my plane. But a very quick and dirty way of doing it, and dont know if its actually dirty, is just take your matrix, put it in it reduced row echelon form, and you say, look, this column and this column are associated with my free variables. The third row minus 3 times the first row. When put them in reduced row echelon form, and did that over here, this is the reduced row echelon form of A, can look at the variables that are associated with the pivot entries. Well, we learned that you take the cross product of any two vectors in R3, and the cross product ve only defined so far in R3, and will get a vector thats normal to both of those vectors. o if take the cross product, anything normal to these guys or any combination of these guys is going be normal to the plane, and we got this result right here. Then for my second row, have 1 times 4, and my temptation is do 1 times 4 minus 3 times 1. f m solving the equation Ax is equal to b, can essentially just create the augmented matrix, where have the matrix A and can augment it with b, and put this in reduced row echelon form, and thatll essentially represent the solution set. And the best way to zero out this third row is to just replace the third row.",0.1997471554993679
33,33,"OPERATOR: The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation, or view additional material from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: All right, so today were returning to simulations. And m going to do at first, a little bit more abstractly, and then come back to some details. o theyre different ways to classify simulation models. The first is whether its stochastic or deterministic. And the difference here is in a deterministic simulation, you should get the same result every time you run it. And theres a lot of uses well see for deterministic simulations. And then theres stochastic simulations, where the answer will differ from run to run because theres an element of randomness in it. o here if you run it again and again you get the same outcome every time, here you may not. o, for example, the problem set thats due today is that a stochastic or deterministic simulation? omebody? tochastic, exactly. And thats what were going to focus on in this class, because one of the interesting questions well see about stochastic simulations is, how often do have to run them before you believe the answer? And that turns out to be a very important issue. You run it once, you get an answer, you cant take it to the bank. Because the next time you run it, you may get a completely different answer. o that will get us a little bit into the whole issue of statistical analysis. Another interesting dichotomy is static vs dynamic. Well look at both, but will spend more time on dynamic models. o the issue its not my phone. f its your mother, you could feel free to take it, otherwise OK, no problem. nevitable. n a dynamic situation, time plays a role. And you look at how things evolve over time. n a static simulation, there is no issue with time. Well be looking at both, but most of the time well be focusing on dynamic ones. o an example of this kind of thing would be a queuing network model. This is one of the most popular and important kinds of dynamic simulations. Where you try and look at how queues, a fancy word for lines, evolve over time. o for example, people who are trying to decide how many lanes should be in a highway, or how far apart the exits should be, or what should the ratio of Fast Lane tolls to manually staffed tolls should be. All use queuing networks to try and answer that question. And well look at some examples of these later because they are very important. Particularly for things related to scheduling and planning. A third dichotomy is discrete vs continuous. magine, for example, trying to analyze the flow of traffic along the highway. One way to do it, is to try and have a simulation which models each vehicle. That would be a discrete simulation, because youve got different parts. Alternatively, you might decide to treat traffic as a flow, kind of like water flowing through things, where changes in the flow can be described by differential equations. That would lead to a continuous model. Another example is, a lot of effort has gone into analyzing the way blood flows through the human body. You can try and model it discretely, where you take each red blood cell, each white blood cell, and look at how they move, or simulate how they move. Or you could treat it continuously and say, well, were just going to treat blood as a fluid, not made up of discrete components, and write some equations to model how that fluid goes through and then simulate that. n this course, were going to be focusing mostly on discrete simulations. Now if we think about the random walk we looked at, indeed it was stochastic, dynamic, and discrete. The random walk was an example of whats called a onte arlo simulation. This term was coined there. Anyone know what that is? Anyone want to guess? ts the casino, in onaco, in onte arlo. t was at one time, before there was even a Las Vegas, the most famous casino in the world certainly. till one of the more opulent ones as you can see. And unlike Las Vegas, its real opulence, as opposed to faux opulence. And this term onte arlo simulation, was coined by lam and etropolis, two mathematicians, back in 1949, in reference to the fact that at onte arlos, people bet on roulette wheels, and cards on a table, games of chance, where there was randomness, and things are discrete, in some sense. And they decided, well, this is just like gambling, and so they called them onte arlos simulations. What s it that makes this approach work? And, in some sense, wont go into a lot of the math, but would like to get some concepts across. This is an application of what are called inferential statistics. You have some sample size, some number of points, and from that you try to infer something more general. We always depend upon one property when we do this. And that property is that, a randomly chosen sample tends to exhibit the same properties as the population from which it is drawn. o you take a population of anything, red balls and black balls, or students, or steps, and at random draw some sample, and you assume that that sample has properties similar to the entire population. o if were to go around this room and choose some random sample of you guys and write down your hair color, we would be assuming that the fraction of you with blonde hair in that sample would be the same as the fraction of you with blonde hair in the whole class. Thats kind of what this means. And the same would be true of black hair, auburn hair, etc. o consider, for example, flipping a coin. And if were to flip it some number of times, say 100 times, you might be able to, from the proportion of heads and tails, be able to infer whether or not the coin was fair. That is to say, half the times it would be heads, in half the times it would be that tails, or whether it was unfair, that it was somehow weighted, so that heads would come up more than tails. And you might say if we did this 100 times and looked at the results, then we could make a decision about what would happen in general when we looked at the coin. o lets look in an example of doing that. o wrote a little program, its on your handout, to flip a coin. o this looks like the simulations we looked at before. ve got flip trials, which says that the number of heads and tails is 0 for i in x range. What is x range? o normally you would have written, for i in range zero to num flips. What range does, is it creates a list, in this case from 0 to 99 and goes through the list of one at a time. Thats fine, but supposed num flips were a billion. Well, range would create a list with a billion numbers in it. Which would take a lot of space in the computer. And its kind of wasted. What x range says is, dont bother creating the list just go through the, in this case, the numbers one at a time. o its much more efficient than range. t will behave the same way as far as the answers you get, but it doesnt use as much space. And since some of the simulations well be doing will have lots of trials, or lots of flips, its worth the trouble to use x range instead of range. Yeah? Pardon? TDENT: Like, why would we ever use range instead of x range? PROFEOR: No good reason, when dealing with numbers, unless you wanted to do something different with the list. But, theres no good reason. The right answer for the purposes of today is, no good reason. typically use x range all the time if m thinking about it. t was just something that didnt seem worth introducing earlier in this semester. But good question. ertainly deserving of a piece of candy. All right, so for i in x range, coin is equal some random integer 0 or 1. f coin is equal to 0 then heads, else tails. Well, thats pretty easy. And then all m going to do here is go through and flip it a bunch of times, and well get some answer, and do some plots. o lets look at an example. Well try well flip 100 coins, well do 100 trials and see what we get. Error in multiline statement. All right, what have done wrong here? Obviously did something by accident, edited something did not intend edit. Anyone spot what did wrong? Pardon? The parentheses. typed where didnt intend. Which line? Down at the bottom? Obviously my, here, yes, deleted that. Thank you. All right, so we have a couple of figures here. Figure one, m showing a histogram. The number of trials on the yaxis and the difference between heads and tails , do have more of one than the other on the xaxis. And so we what we could see out of my 100 trials, somewhere around 22 of them came out the same, close to the same. But way over here weve got some funny ones. 100 and there was a difference of 25. Pretty big difference. Another way to look at the same data, and m doing this just to show that there are different ways of looking at data, is here, what ve plotted is each trial, the percent difference. o out of 100 flips. And this is normalizing it, because if flip a million coins, might expect the difference to be pretty big in absolute terms, but maybe very small as a percentage of a million. And so here, we can again see that as these stochastic kinds of things, theres a pretty big difference, right? Weve got one where it was over 25 percent, and several where its zero. o the point here, we can see from this graph, that if d done only one trial and just assumed that was the answer as to whether my coin was weighted or not, could really have fooled myself. o the the main point is that you need to be careful when youre doing these kinds of things. And this green line here is the mean. o it says on average, the difference was seven precent. Well suppose, maybe, instead of, flipping 100, were to flip 1,000. Well, doesnt seem to want to notice it. One more try and then ll just restart it, which is always the safest thing as weve discussed before. Well, we wont panic. ometimes this helps. f not, here we go. o lets say we wanted to flip 1,000 coins. o now what do we think? s the difference going to be bigger or smaller than when we flipped 100? s the average difference between heads and tails bigger or smaller with 1,000 flips than with 100 flips? Well, the percentage will be smaller, but in absolute terms, its probably going to be bigger, right? Because ve got more chances to stray. But well find out. o here we see that the mean difference is somewhere in the twenties, which was much higher than the mean difference for 100 flips. On the other hand, if we look at the percentage, we see its much lower. nstead of seven percent, its around two and a half percent in the main. Theres something else interesting to observe in figure two, relative to when we looked at with 100 flips. What else is pretty interesting about the difference between these two figures, if you can remember the other one? Yeah? TDENT: There are no zeros? PROFEOR: There are no zeros. Thats right, as it happens there were no zeros. Not so surprising that it didnt ever come out exactly 500, 500. What else? What was the biggest difference, percentagewise, we saw last time? Over 25. o notice how much narrower the range is here. nstead of ranging from 2 to 25 or something like that, it ranges from 0 to 7, or maybe 7 and a little. o, by flipping more coins, the experiment becomes more reproduceable. m looks like the same because of scaling, but in fact the range is much narrower. Each experiment tends to give an answer closer to all the other experiments. Thats a good thing. t should give you confidence that the answers are getting pretty close to right. That theyre not bouncing all over the place. And if were to flip a million coins, we would find the range would get very tight. o notice that even though theres similar information in the histogram and the plot, different things leap out at you, as you look at it. All right, we could ask a lot of other interesting questions about coins here. But, well come back to this in a minute and look at some other questions. want to talk again a little bit more generally. ts kind of easy to think about running a simulation to predict the future. o in some sense, we look at this, and this predicts what might happen if flipped 1,000 coins. That the most likely event would be that d have something under 10 in the difference between heads and tails, but that its not terribly unlikely that might have close to 70 as a difference. And if ran more than 100 trials d see more, but this helps me predict what might happen. Now we dont always use simulations to predict what might happen. We sometimes actually use simulations to understand the current state of the world. o for example, if told you that we are going to flip three coins, and wanted you to predict the probability that all three would be either heads, or all three would be tails. Well, if youd studied any probability, you could know how to do that. f you hadnt studied probability, you would say, well, thats OK, we have a simulation right here. Lets just do it. Here we go again. And so lets try it. Lets flip three coins, and lets do it 4,000 times here. Well, thats kind of hard to read. ts pretty dense. But we can see that the mean here is 50. And, this is a little easier to read. This tells us that, how many times will the difference, right, be zero 3,000 out of 4,000. s that right? What do you think? Do you believe this? Have done the right thing? three coins, 4,000 flips, how often should they all be heads, or how often should they all be tails? What does this tell us? t tells us onefourth of the time theyll all be the difference between, wait a minute, how can the difference between somethings wrong with my code, right? Because only have two possible values. hadnt expected this. obviously messed something up. Pardon? TDENT: ts right. PROFEOR: ts right, because? TDENT: Because you had an odd number of flips, and when you split them PROFEOR: Pardon? TDENT: When you split an odd number PROFEOR: Exactly. o it is correct. And it gives us what we want. But now, lets think about a different situation. Anybody got a coin here? Anyone give me three coins? can trust somebody, hope. What a cheap anybody got silver dollars, would be preferable? All right, look at this. hes very carefully given me three pennies. he had big, big money in that purse, too, but she didnt want me to have it. All right, so m going to take these three pennies, jiggle them up, and now ask you, whats the probability that all three of them are heads? Anyone want to tell me? ts either 0 or 1, right? And can actually look at you and tell you exactly which it is. And you cant see which it is. o, how should you think about what the probability it? Well, you might as well assume that its whatever this graph tells you it is. Because the fact that you dont have access to the information, means that you really might as well treat the present as if its the future. That its unknown. And so in fact we frequently, when theres data out there that we dont have access to, we use simulations and probabilities to estimate, make our best guess, about the current state of the world. And so, in fact, guessing the value of the current state, is really no different from predicting the value of a future state when you dont have the information. n general, all right, now, just to show that your precautions were unnecessary. Where was ? Right. n general, when were trying to predict the future, or in this case, guess the present, we have to use information we already have to make our prediction or our best guess. o to do that, we have to always ask the question, is past behavior a good prediction of future behavior? o if flip a coin 1,000 times and count up the heads and tails, is that a good prediction what will happen the next time? This is a step people often omit, in doing these predictions. ee the recent meltdown of the financial system. Where people had lots of stochastic simulations predicting what the market would do, and they were all wrong, because they were all based upon assuming samples drawn from the past would predict the future. o, as we build these models, thats the question you always have to ask yourself. s, in some sense, this true? Because usually what were doing is, were choosing a random sample from the past and hoping it predicts the future. And that is to say, is the population we have available the same has the one in the future. o its easy to see how one might use these kinds of simulations to figure out things that are inherently stochastic. o for example, to predict a poker hand. Whats the probability of my getting a full house when draw this card from the deck? To predict the probability of coming up with a particular kind of poker hand. s a full house more probable than a straight? Or not? Well, you can deal out lots of cards, and count them up, just as lam suggested for olitaire. And thats often what we do. nterestingly enough though, we can use randomized techniques to get solutions to problems that are not inherently stochastic. And thats what want to do now. o, consider for example, pi. any of you have probably heard of this. For thousands of years, literally, people have known that there is a constant, pi, associated with circles such that pi times the radius squared equals the area. And theyve known that pi times the diameter is equal to the circumference. o, back in the days of the Egyptian pharaohs, it was known that such a constant existed. n fact, it didnt acquire the name pi until the 18th century. And so they called it other things, but they knew it existed. And for thousands of years, people have speculated on what its value was. ometime around 1650 B, the Egyptians said that pi was 3.16, something called the Rhind Papyrus, something they found. any years later, about 1,000 years later, the Bible said pi was three. And quote, this is describing the specifications for the Great Temple of olomon. ""He made a molten sea of 10 cubits from brim to brim, round in compass, and 5 cubit the height thereof, and a line of 30 cubits did compass it round about."" o, all right, so what weve got here is, weve got everything we need to plug into these equations and solve for pi, and it comes out three. And it does this in more than one place in the Bible. will not comment on the theological implications of this assertion. arah Palin might. And ike Huckabee certainly would. The first theoretical calculation of pi was carried out by Archimedes, a great Greek mathematician from yracuse, that was about somewhere around 250 B. And he said that pi was somewhere between 223 divided by 71, and 22 divided by 7. This was amazingly profound. He knew he didnt know what the answer was, but he had a way to give an upper and a lower bound, and say it was somewhere between these two values. And in fact if you calculate it, the average of those two values is 3.1418. Not bad for the time. This was not by measurement, he actually had a very interesting way of calculating it. All right, so this is where it stood, for years and years, because of course people forgot the Rhind Papyrus, and they forgot Archimedes, and they believed the Bible, and so three was used for a long time. People sort of knew it wasnt right, but still. Then quite interestingly, Buffon and Laplace, two great French mathematicians, actually people had better estimates using Archimedes methods long before they came along, proposed a way to do it using a simulation. Now, since Laplace lived between 1749 and 1827, it was not a computer simulation. o m going to show you, basically, the way that he proposed to do it. the basic idea was you take a square, assume thats a square, and you inscribe in it a quarter of a circle. o here, you have the radius of the square r. And then you get some person to, he used needles, but m going to use darts, to throw darts at the shape. And some number of the darts will land in the circle part, and some number of the darts will land out here, in the part of the square thats not inscribed by the circle. And then we can look at the ratio of the darts in the shaded area divided by the total number of darts in the square. And thats equal to the shaded area divided by the area of the square. The notion being, if theyre landing at random in these places, the proportion here and not here will depend upon the relative areas. And that certainly makes sense. f this were half the area of the square, then youd expect half the darts to land in here. And then as you can see in your handout, a little simple algebra can take this, plus pi r squared equals the area, you can solve for pi, and you can get that pi is equal to 4, and ll write it, h, where h is the hit ratio, the number falling in here. o people sort of see why that should work intuitively? And that its a very clever idea to use randomness to find a value that theres nothing random about. o we can now do the experiment. need volunteers to throw darts. ome on. ome on up. need more volunteers. have a lot of darts. Anybody else? Anybody? All right, then since youre all in the front row, you get stuck. o now well try it. And you guys, well see how many of you hit in the circle, and how many of you hit there. Go ahead, on the count of 3, everybody throw. 1, 2, 3. Ohh! He did that on purpose. Youll notice Professor Grimson isnt here today, and thats because told him he was going to have to hold the dart board. Well, what we see here is, we ignore the ones that missed all together. And well see that, truly, m assuming these are random throws. We have one here and two here. Well, your eyes will tell you thats the wrong ratio. Which suggests that having students throw darts is not the best way to solve this problem. And so you will see in your handout a computer simulation of it. o lets look at that. o this is find pi. o at the beginning of this code, by the way, its not on your handout, is some magic. got tired of looking at big numbers without commas separating the thousands places. Youve see me in other lectures counting the number of zeros. What we have here is, that just tells it have to have two versions, one for the ac, and one for the P. To set some variables that had to write integers, things in general, and m saying here, do it the way you would do it in the nited tates in English. And TF8 is just an extended character code. Anyway, you dont need to learn anything about this magic, but its just a handy little way to make the numbers easier to read. All right, so lets lets try and look at it. Theres not much interesting to see here. ve done this little thing, format ints, that uses this magic to say grouping equals true, that means put a comma in the thousand places. But again, you can ignore all that. The interesting part, is that from Pylab import star, import random in math. As some of you observed, the order these imports matters. think sent out an email yesterday explaining what was going on here. This was one of the things that knew, and probably shouldve mentioned. But since knew it, thought everyone knew. illy me. t was of course a dumb thing to think. And then m going to throw a bunch of darts. The other thing youll notice is, throw darts has a parameter called should plot. And thats because when throw a billion darts, really dont want to try and take the time to plot a billion points. o lets first look at a little example. Well try throwing 10,000 darts. And it gives me an estimated value of pi of 3.16. And what well see here, is that the number of darts thrown, the estimate changes, right? When threw one dart, the estimate of pi was 4. threw my second dart, it dropped all the way to 3. And then it bounced around a while, and then at the end, it starts to really stabilize around the true value. Youll notice, by the way, that what ve got here is a logarithmic xaxis. f you look at the code, youll see ve told it to be semi log x. And thats because wanted you to be able to see what was happening early on, where it was fluctuating. But out here its kind of boring, because the fluctuations are so small. o that was a good way to do it. All right now. Do think have enough samples here? Well, dont want you to cheat and look at the estimate and say no, you dont, because you know thats not the right answer. And, its not even as good as Archimedes did. But how could you sort of look at the data, and get a sense that maybe this is not the right answer? Well, even at the end, if we look at it, its still wiggling around a fair amount. We can zoom in. And its bouncing up and down here. m in a region, but its sort of makes us think that maybe it hasnt stabilized, right? Youd like it to not be moving very much. Now, by the way, the other thing we couldve looked at, when we ran it, lets run it again, probably get a different answer by the way. Yeah, notice the different answer here. Turns out to be a better answer, but its just an accident, right? Notice in the beginning it fluctuates wildly, and it fluctuates less wildly at the end. Why is that? And dont just say because its close to right and it knows it. Why do the mathematics of this, in some sense, tell us it has to fluctuate less wildly at the end? Yes? TDENT: PROFEOR: Exactly, exactly right. f ve only thrown two darts, the third dart can have a big difference in the average value. But if ve thrown a million darts, the million and first cant matter very much. And what this tells me is, as want ever more digits of precision, have to run a lot more trials to get there. And thats often true, that simulations can get you in the neighborhood quickly, but the more precision you want, the number of steps grows quite quickly. Now, the fact that got such different answers the two times ran this suggests strongly that shouldnt believe either answer. Right? o we need to do something else. o lets try something else. Lets try throwing a lot more darts here, and see what we get. Now if you look at my code, youll see m printing intermediate values. Every million darts, m printing the value. And did that because the first time ran this on a big number, was afraid had an infinite loop and my program was not working. o just said, all right, lets put a print statement in the loop, so could see that its making progress. And then decided it was just kind of nice to look at it, to see what was going on here. o now you see that if throw 10 million darts, m starting to get a much better estimate. Youll also see, as predicted, that as get further out, the value of the estimate changes less and less with each million new darts, because its a smaller fraction of the total darts. But its getting a lot better. till not quite there. Lets just see what happens, can throw in another one. This is going to take a little while. o can talk while its running. Oops, what did do here? o, its going to keep on going and going and going. And then if we were to run it with this number of darts several times over, we would discover that we got answers that were very, very similar. From that we can take comfort, statistically, that were really getting close to the same answer every time, so weve probably thrown enough darts to feel comfortable that were doing whats statistically the right thing. And that there maybe isnt a lot of point in throwing more darts. Does that mean that we have the right answer? No, not necessarily, and thats what were going to look at next week.","What we have here is, that just tells it have to have two versions, one for the ac, and one for the P. To set some variables that had to write integers, things in general, and m saying here, do it the way you would do it in the nited tates in English. And thats what were going to focus on in this class, because one of the interesting questions well see about stochastic simulations is, how often do have to run them before you believe the answer? But how could you sort of look at the data, and get a sense that maybe this is not the right answer? And what well see here, is that the number of darts thrown, the estimate changes, right? And that is to say, is the population we have available the same has the one in the future. o here, you have the radius of the square r. And then you get some person to, he used needles, but m going to use darts, to throw darts at the shape. And then decided it was just kind of nice to look at it, to see what was going on here. The number of trials on the yaxis and the difference between heads and tails , do have more of one than the other on the xaxis. And then as you can see in your handout, a little simple algebra can take this, plus pi r squared equals the area, you can solve for pi, and you can get that pi is equal to 4, and ll write it, h, where h is the hit ratio, the number falling in here. Well, dont want you to cheat and look at the estimate and say no, you dont, because you know thats not the right answer. And then we can look at the ratio of the darts in the shaded area divided by the total number of darts in the square. And then all m going to do here is go through and flip it a bunch of times, and well get some answer, and do some plots. And if were to flip it some number of times, say 100 times, you might be able to, from the proportion of heads and tails, be able to infer whether or not the coin was fair. Another way to look at the same data, and m doing this just to show that there are different ways of looking at data, is here, what ve plotted is each trial, the percent difference. Because the fact that you dont have access to the information, means that you really might as well treat the present as if its the future. f you look at the code, youll see ve told it to be semi log x. And thats because wanted you to be able to see what was happening early on, where it was fluctuating. o the point here, we can see from this graph, that if d done only one trial and just assumed that was the answer as to whether my coin was weighted or not, could really have fooled myself. o for example, if told you that we are going to flip three coins, and wanted you to predict the probability that all three would be either heads, or all three would be tails. And the difference here is in a deterministic simulation, you should get the same result every time you run it. And you might say if we did this 100 times and looked at the results, then we could make a decision about what would happen in general when we looked at the coin. o if were to go around this room and choose some random sample of you guys and write down your hair color, we would be assuming that the fraction of you with blonde hair in that sample would be the same as the fraction of you with blonde hair in the whole class. And some number of the darts will land in the circle part, and some number of the darts will land out here, in the part of the square thats not inscribed by the circle. That is to say, half the times it would be heads, in half the times it would be that tails, or whether it was unfair, that it was somehow weighted, so that heads would come up more than tails. And so in fact we frequently, when theres data out there that we dont have access to, we use simulations and probabilities to estimate, make our best guess, about the current state of the world. That the most likely event would be that d have something under 10 in the difference between heads and tails, but that its not terribly unlikely that might have close to 70 as a difference. And can actually look at you and tell you exactly which it is. And then if we were to run it with this number of darts several times over, we would discover that we got answers that were very, very similar. And so here, we can again see that as these stochastic kinds of things, theres a pretty big difference, right? And since some of the simulations well be doing will have lots of trials, or lots of flips, its worth the trouble to use x range instead of range. o notice that even though theres similar information in the histogram and the plot, different things leap out at you, as you look at it. o here we see that the mean difference is somewhere in the twenties, which was much higher than the mean difference for 100 flips. And what this tells me is, as want ever more digits of precision, have to run a lot more trials to get there. Now, by the way, the other thing we couldve looked at, when we ran it, lets run it again, probably get a different answer by the way. o at the beginning of this code, by the way, its not on your handout, is some magic. And so we what we could see out of my 100 trials, somewhere around 22 of them came out the same, close to the same. o if flip a coin 1,000 times and count up the heads and tails, is that a good prediction what will happen the next time? ve got flip trials, which says that the number of heads and tails is 0 for i in x range. And so, in fact, guessing the value of the current state, is really no different from predicting the value of a future state when you dont have the information. And this is normalizing it, because if flip a million coins, might expect the difference to be pretty big in absolute terms, but maybe very small as a percentage of a million. o that was a good way to do it. But we can see that the mean here is 50. And thats often true, that simulations can get you in the neighborhood quickly, but the more precision you want, the number of steps grows quite quickly. All right, so this is where it stood, for years and years, because of course people forgot the Rhind Papyrus, and they forgot Archimedes, and they believed the Bible, and so three was used for a long time. What range does, is it creates a list, in this case from 0 to 99 and goes through the list of one at a time. And this term onte arlo simulation, was coined by lam and etropolis, two mathematicians, back in 1949, in reference to the fact that at onte arlos, people bet on roulette wheels, and cards on a table, games of chance, where there was randomness, and things are discrete, in some sense. Well, what we see here is, we ignore the ones that missed all together. Lets try throwing a lot more darts here, and see what we get. One way to do it, is to try and have a simulation which models each vehicle. Does that mean that we have the right answer? o m going to show you, basically, the way that he proposed to do it.",0.1134202183867559
34,34,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: All right, today we continue our exciting adventure into dynamic programming. Are you excited? m excited, super excited. Dynamic programming, as you recall way back before Thanksgiving, is a super exciting powerful technique to design algorithms, especially to solve optimization problems where you want to maximize or minimize something. Last time, we saw how two algorithms we already knew namely, how to compute the nth Fibonacci number and how to compute shortest paths via BellmanFord are really dynamic programs in disguise. And indeed for, at least for BellmanFord, thats how they were invented, was to apply a general technique which were going to see today in full generality, more or less most of this is generality in five easy steps. And were going to see that technique applied to two new problems which are much more interesting than the ones weve already solved namely, how to make your text look nice in a paragraph, where to break the lines. Thats text justification. And how to win and make loads of money at blackjack. o lots of practical stuff here, and were going to see one new technique for general dynamic programming. These are some things wrote last time. Actually, one of them didnt write last time. n general, you can think of dynamic programming as a carefully executed brute force search. o in some sense, your algorithm is going to be trying all the possibilities, but somehow avoiding the fact that there are exponentially many of them. By thinking of it in a clever way, you can reduce the exponential search space down to a polynomial one, even though youre still not being very intelligent youre still blindly trying all possibilities. o thats the brute force part. n more detail, the three main techniques in dynamic programming are the idea of guessing, the idea that, oh, want to find the best way to solve a problem. Lets pick out some feature of the solution that want to know. dont know it, so ll guess the answer meaning ll try all the possibilities for that choice and take the best one. o guessing is really central to dynamic programming. Then we also use a recursion, some way to express the solution to our problem in terms of solutions to subproblems. o its usually very easy to get a recursion for a lot of problems as long as they have some kind of substructure. Like shortest paths, we had that some paths of shortest paths were also shortest paths, so that was handy. sually the recursion by itself is exponential time, like even with Fibonacci numbers. But we add in this technique of memoization, which is just once we compute an answer weve stored in a lookup table, if we ever need that answer again we reuse it instead of recomputing it. o we store it. We write down in our memo pad anything that we compute. Those techniques, all these techniques together give you, typically, a polynomial time dynamic program when they work, of course. emoization makes the recursion polynomial time. The guessing is what is doing a brute force search. And magically, it all works if youre careful. Another perspective kind of an orthogonal perspective or another way of thinking about it, which think should be comfortable for you because we spent a lot of time doing shortest paths and expressing problems that we care about in terms of shortest paths even if they dont look like it at first glance dynamic programming in some sense is always computing shortest paths in a DAG. o you have some problem you want to solve, like you have text you want to split up into lines so it looks nice in a paragraph, you express that problem somehow as a directed acyclic graph. And then we know how to compute shortest path in directed acyclic graphs in linear time. And thats basically what dynamic programming is doing. didnt realize this until last week, so this is a new perspective. ts an experimental perspective. But think its helpful. ts actually dynamic programming is not that new. ts all about how to be clever in setting up that DAG. But in the end, the algorithm is very simple. And then we had this other perspective back to this perspective, guess. n general, we have the real problem we want to solve, we generalize it in some sense by considering lots of different subproblems that we might care about. Like with Fibonacci, we had the nth Fibonacci number. We really just wanted the nth Fibonacci number. But along the way, were going to compute all f1 up to fn. o those are our subproblems. And if we compute the amount of time we need to solve each subproblem and multiply that by the number of subproblems we get, the total time required by the algorithm. This is a general true fact. And the fun part here is we get to treat any recursive calls in this recursion as free, as constant time, because we really only pay for it first time. Thats counted out here. The second time we call it, its already memoized, so we dont have to pay for it. o this is, in some sense, an amortization, if you remember amortization from table doubling. Were just changing around when we count the cost of each subproblem, and then this is the total running time. OK, so thats the spirit we saw already. m going to give you the five general steps, and then were going to apply them to two new problems. o five easy steps to dynamic programming. nfortunately, these are not necessarily sequential steps. Theyre a little bit interdependent, and so ""easy"" should be in quotes. This is how you would express a dynamic program, and in some sense how youd invent one, but in particular how you would explain one. OK, let me get to the main steps first. First step is to figure out what your subproblems are going to be. econd part is to guess something. Third step is to relate subproblem solutions, usually with a recurrence. guess always with a recurrence. Fourth step is to actually build an algorithm. And we saw two ways to do that last time. One is to use recursion and memoization, which is the way like to think about it. But if you prefer, you can follow the bottom up approach. And usually thats called building a table. And that ones basically to turn our recursion and memoization, which is kind of fancy, into a bunch of for loops, which is pretty simple. And this is going to be more practical, faster, and so on. And depending on your preference, one of them is more intuitive than the other. t doesnt matter. They have the same running time, more or less, in the worst case. Then the fifth step is to solve the original problem. All right, so weve sort of seen this before. n fact have, over here, a convenient table. ts called cheating. The two problems we saw last time, Fibonacci numbers and shortest paths. And ve got steps one, two, three, four ran out of room, so didnt write five yet. But well get there. o what are our subproblems? Well, for Fibonacci, they were f1 through fn. o there were n different subproblems. And in general because of this formula, we want to count how many subproblems are there. o number of subproblems is this is what we need to do algorithmically. And then for analysis, we want to counter number of subproblems for step one. And so for Fibonacci there were n of them. For shortest paths, we defined this delta sub k of sv. This was the shortest path from s to v they uses at most k edges. That was sort of what BellmanFord was doing. And the number of different subproblems here was v squared, because we had to do this for every vertex v and we had to do it for every value of k between 0 and v minus 1. v minus was is the number of rounds we need in BellmanFord. o its v times v, different subproblems, b squared of them. OK, second thing was we wanted to solve our problem. And we do that by guessing some feature of the solution. n Fibonacci, there was no guessing. o the number of different choices for your guess is one. Theres nothing. Theres only one choice, which is to do nothing. And for shortest paths, what we guessed was we know were looking for some path from s v. B Lets guess what the last edge is. Theres some last edge from u to v, assuming the path has more than one edge or more than zero edges. When could the edge possibly be? Well, its some incoming edge to v. o theres going to be indegree of v different choices for that. And to account for the case that thats zero, we do a plus 1. But thats not a big deal. o that was the number of different choices. n general if were going to guess something, we need to write down the number of choices. For the guess, how many different possibilities are there? Thats our analysis. OK, the next thing is the recurrence. Thats step three. We want to relate all the subproblem solutions to each other. For Fibonacci, thats the definition of Fibonacci numbers. o its really easy. For shortest paths, we wrote this min. n general, typically its a min or a max, whatever youre trying to solve here. Were doing shortest paths. You could do longest paths in the same way. o you compute them in of delta sub sk minus 1 of su. The idea is we want to compute this part of the path, the s to u part. And we know that has one fewer edge, because we just guessed what the last edge was. Except we dont really know what the last edge was, so we have to try them all. We try all the incoming edges into v thats this part and for each of them we compute forgot something here. This is the cost of the first part of the path. Then also need to do plus the weight of the uv edge. That will be the total cost of that path. You add those up, you do it for every incoming edge. That is, in some sense, considering all possible paths. Assuming you find the shortest path from s to u, thats going to be the best way to get there. And then use some edge from u to v for some choice of u. This will try all of them. o its really trying all the possibilities. o its pretty clear this is correct if there are no negative weight cycles. You have to prove some things. Weve already proved them. ts just slow, but once you add memoization, its fast. Now, how long does it take to evaluate this recurrence, constant time, if you dont count the recursive calls or count them as constant? Over here, were taking a min over n degree of v things. o we have to pay n degree of v time, again the recursions as free. But for each one of them, we have to do an addition. o its constant work per guess. And this is quite common. Often, the number of guesses and the running time per subproblem are the same, the constant factors. ometimes theyre different. Well see some examples today. OK, step four. Lets see. o here we evaluate the time per subproblem. Once you have the recurrence, that becomes clear. You want to make sure thats polynomial. Often these are the same. And then we add the recursive memorize or build a DP table. m not going to write those. We did it for Fibonacci last time, shortest paths. Pretty easy. And in general, what we need to check here is that the sub problem recurrence is acyclic. n other words, that it has a topological order so we can use topological sort. We dont actually use topological algorithm usually. You can just think about it. n the case of Fibonacci numbers, its clear you want to start with the smallest one and end up with the biggest one. You cant do the reverse, because then when youre trying to computer the nth you dont have the ones you need, the n minus 1 and n minus 2. But if you do it in this order, you always have the one you need by the time you get there. n general, theres a DAG there and for Fibonacci, it was like this. Every node depends on the previous and the second previous. But you just choose a topological order, which is here left to right, and youre golden. And these are actually the for loops you get in the bottom of DP. For shortest paths, you have to think a little bit. You have to do the for loop over k on the outside, the for loop over V on the inside. The reverse does not work. wont go through that, but we drew the DAG last time. And thats the main thing you need to do here. And then, of course, you use this formula to compute the overall running time, which is just multiplying this quantity with this quantity. Total time. Then theres just one last step that usually isnt that big a deal, but you have think about it. You need to make sure that the problem you actually cared about solving gets solved. n the case of Fibonacci and shortest paths, this is pretty clear. didnt write it. We can do it on here. olve the original problem. Fibonaci, it is Fn. And this is one of our subproblems, so if we solve all of them, were done. For shortest paths, its basically delta sub v minus 1 of sv for all v. Thats single source shortest paths. And by our BellmanFord analysis, that gives us the right shortest paths. There are no negative weight cycles. And sometimes this requires extra time to combine your solutions to get the real thing. Here of course, we just have the answers, so writing them down does not take very long. o thats the dominant running time which didnt write, should have written in under for here this ends up being n, this ends up being VE. OK, dont want to spend more time on those examples. Lets go to new things. o first problem were going to look at today is text justification. And the informal statement of this problem is youre given some text which means a string, a whole bunch of characters. And we want to split them into good lines. The rules of the game here are were going to, like in the early lectures of document distance where you have some definition of splitting a document into words separated by spaces. And what we want to do is cut. We can only cut between word boundaries. And we want to write some text, its going to have some spaces in it. Then theres a new line, something like that. And we want to justify our text on the right here. And so wed like to avoid big gaps like this because they look ugly, theyre hard to read. Now, if you use icrosoft Word at least before the latest versions they follow a greedy strategy, which is very simple. You pack as many words as you can on the first line, then you go to the next line, pack as many words as you can on the second line. Keep going like that. And that strategy is not optimal. f you use LaTeX as some of you have been doing on problem sets, and think also new versions of Word but m not sure then it uses dynamic programming to solve this problem. And thats what were going to do here. o let me specify a little bit more about what we mean here. o the text were going to think of as a list of words. And were going to define a quantity badness. And this is an anesthetic quantity, if you will. m going to tell you what LaTeX uses. But this is sort of how bad it is to use or lets say, yeah, words i through j as a line. o this is python notation. o it starts at i and ends at J minus 1. Thatll be convenient. o have this list of words. And if look at words i through j minus 1 and think of what happens if pack them in a line, well, they may fit or they may not fit. o there are going to be two cases. f they dont fit, m going to write infinity. o thats really bad. o have some notion of how wide my line can be. And if the sum of the lengths of those words plus the sum of the lengths of the spaces as small as possible is bigger than the width of my screen or page, guess then say they dont fit, and then define badness to be infinity meaning, never want to do that. This is actually LaTeX sloppy mode, if you want to be technical. Otherwise, its going to be page width minus total width cubed. Why cubed? Who knows. This is the LaTeX rule. And squared would probably also be fine. o this is the width of the page minus the total width of those words, which you also have to include the spaces here. You take the difference. You cube it. And so when this is small mean, when these are very close then this is going to be close to zero. Thats good. That means you use most of the line. When the total width is much smaller than the page width, then this will be a large value. You cube it, it will be even larger. o this will highly discourage big gaps like this. And it will very much discourage not fitting. o theres a tradeoff, of course. And the idea is you might in the greedy algorithm, you make the first line as good as you can. But it might actually be better to leave out some of the words that would fit here in order to make the next line better. n general, its hard to tell, where should cut the lines in order to get the best overall strategy? What d like to minimize is the sum of the badnesses of the lines. o its a sum of cubes, and thats really hard to think about. But thats what dynamic programming is for. You dont have to think. ts great because its brute force. OK, so the first thing we need to do is define subproblems. This is, in some sense, the hard part. The rest will follow easily. o think actually it might be easier to think about, for this problem, what would be the brute force strategy? How would you try all possibilities, exponential time? uggestions? Yeah? ADENE: Try all partitions of the words that dont fit? PROFEOR: Try all partitions of the word, so of the string of words. o mean, it could be it all fits in on one line. t could be its split into two lines. try all possible splits there. n general, m guessing for every word, does this start a line or not? That would be all ways. And so there are 2 to the n. f have n words, theres 2 to the n different splits. For every word say yes or no, does this is begin a line? o what d like to figure out is where those lines begin. That was the point of that exercise. o any suggestions? aybe its actually easier to jump ahead and think, what would guess in my solution if have this big string of words? Whats the natural first thing to guess? Yeah? ADENE: Guess how long the first line is? PROFEOR: Guess how long the first line is, yeah. We know that the first word begins a line. But where does the second line begin? o d like to guess where the second line begins. Thats so you know, have the beginning of a line here and then have a beginning of a line here at the fourth word. Where does the second line begin? dont know. Guess. o m going to try all the possible words after the first word. And say, well, what if started my second line here? At some point m going to be packing too much into the first line, and so abort. But ll try them all. Why not? OK, thats good. The issue is that once ve chosen where the second line is, of course the next thing want to guess is where the third line begins. And then want guess where the fourth line begins, and so on. n general, need to set up my subproblems so that after do the first guess have the problem of the original type. o originally have all the words. But after guess where the second line begins, have the remaining words. Whats a good word for the remaining words? f give you a list of words and want from here on, its called what? A subproblem, yes. Thats what we want to define. ts called a suffix of the array. Thats the word was looking for. ts tough when only have one word answers. o my subproblems are going to be suffixes. Which is, in python notation, i colon. They call it splices. And how many subproblems are there if have n words? Two? orry? ADENE: 2 to the n. PROFEOR: 2 the n? That would be a problem if its 2 to the n. hope its only n. Originally, we said, OK, for every word, were going to say, is this in our out? s this the beginning or not? Thats 2 to the n. But here, the idea is were only thinking about, well, what are the words that remain? And it could be youve dealt with the first 100 words and then youve got n minus 100 left, or it could be youve dealt with the first thousand words and youve got n minus 1,000. Theres only n choices for that. Were only remembering one line, this is the key. Even though we may have already guessed several lines, were just going to remember, well, OK. This is what we have left to do. o lets forget about the past. This is what makes dynamic programming efficient. And were just going to solve it, solve these subproblems, forgetting about the past. o the subproblem m not going to write it here is if give you these words, never mind the other words, how do pack them optimally into a paragraph? dont care about the other words, just these words. o this is a different version of the same problem. nitially, we have n words to do. Now have n minus i words to do. But its again text justification. want to solve this problem on those words. Thats just how m going to define it. This will work if can specify a recurrence relation. As we said, what we guess is where to break the first line, where to start the second line for those words. OK, so this is it could be the i plus first line. t could be the i plus second line or sorry, word. ome word after i is where we guess the second word. The number of choices for the guess is at most n minus i. m just going to think of that as order n. t wont matter. The third part is we need a recurrence relation. claim this is very easy. m going to didnt give this problem a name, so m just going to write it as DP of i. o this is going to be the solution to that suffix, words from i onward. And d like to what want to do is consider all possible guesses. o mean this is going to be pretty formulaic at this point. After ve set up these ideas theres pretty much only one thing can write here, which is want to do a for loop. That would be the for loop of where the second line can start. cant start at i, because thats where the first line starts. But it could start at i plus 1. And this special value of n will mean that there is no second line. OK, so DP of i now want to do this for loop in order to try all the possible guesses. j will be the word where the next thing starts. o then what do write up here? f make this guess all right, so have word i is the first word of the first line. And then word j is the first word of the second line. And then theres more stuff down below. dont know what that is. But how can use recursion to specify this? DP of j, exactly. guess if m doing recursion, should use parentheses instead of brackets. But if youre doing it bottom up, it would be square brackets. o thats just DP of j. Thats the cost of the rest of the problem. And can assume that thats free to compute. This is the magic of dynamic programming. But then also have to think about, well, what about the first line? How much does that cost? Well, thats just badness of ij. And weve already defined that. We can compute it in constant time. Dynamic programming doesnt really care what this is. t could be anything. As long as youre trying to minimize the sum of the badnesses, whatever function is in here, we just compute it here. Thats the power of dynamic programming. t works for all variations of this problem, however you define badness. o you might say, oh, thats a weird definition. want to use something else instead. Thats fine, as long as you can compute it in terms of just i and j and looking at those words. OK, now need to do a min over the whole thing. o want to minimize the sum of the badnesses. o compute for every guess of j, compute the cost of the rest of the problem plus the cost of that first line. And this, is in some sense, checking all possible solutions magically. OK. Thats the recurrence. We need to check some things. guess right now we just want to compute how much time does this cost, time per subproblem. To do this for loop, basically do constant work all of this is constant work for each choice. o theres order n choices, so this is order n. Now we have to check that theres a topological order for this problem or for these subproblems. And this is easy, but a little different from what weve done before because we have to actually work from the end backwards, because were expressing DP of i in terms of DP of larger values of i. j is always bigger than i. And so we have to do it from the right end back to the beginning. And n minus 1 down to 0. didnt actually define DP of n. Theres a base case here which is DP of n equals 0. Because the meaning of DP of n is have zero words, the nth word onward. There is no nth word. ts 0 to n minus 1 in this notation. o dont pay anything for a blank line. OK, so thats our top logical order. This one, of course, is instantaneous. And then we work backwards. And always whenever we need to compute something, we already have the value. The total time we get is going to be the number of sub problems which is n times the running time per subproblem. which is order n, which is order n squared. And in the worst case, it is indeed theta n squared. Although in practice its going to work better, because lines cant be too long. o thats the running time. Then finally we have to check that the original problem actually gets solved. And in this case, the original problem we need to solve is DP of 0 because DP of 0 means take words from 0 onwards. Thats everybody. o thats the actual problem want to solve. o we work backwards. We solve all these subproblems that we dont directly care about, but then the first one is the one we want. And were done. o in quadratic time, we can find the best way to pack words into lines. Question? ADENE: PROFEOR: DP of j is returning. ts like this. o DP of this is a recursive definition. magine this is a recursive function. wrote equals, which is Haskell notation, if you will. But normally, you think of this as like def DP of i is return min of this. This is python. o its returning the cost. What was the best way to pack those lines from j onwards? Thats what DP of j returns. o its a number. ts going to be a sum of badness values. Then we add on one new badness value. ts still a sum of badness values. We return the best one that we find. Now, this does not actually pack the words. Thats a good maybe your implicit question. ts not telling you how to pack the words. ts telling you how much it costs to pack the words. This is a lot like shortest paths where we didnt it was annoying to actually figure out what the shortest path was. Not that annoying, but thats not what we were usually aiming to do. We were just trying to figure out the shortest path weight. And then once we knew the shortest path weight, it was pretty easy to reconstruct the paths. o maybe ll take a little diversion to that and talk about parent pointers. The idea with parent pointers is just remember which guess was best. its a very simple idea, but it applies to all dynamic programs and lets you find the actual solution, not just the cost of the solution. We did the same thing with shortest paths. We even called them parent. o when we compute this min, were trying all choices of j. One of them or maybe more than one, but at least one of them actually gave you the min. Thats usually called the arg min in mathematics. ts what was the value of j that gave you the minimum value of this thing. o mean, when you compute the min, youre iterating over every single one. Just keep track of which one was the best. Thats it. all that the parent pointer. Do need to write that? Here, parent parent of i is going to be arg min of that same thing. o its a j value. ts the best j value for i. And so we store that for each i. t cost no more work, just a constant factor more work than computing the min. We also write down the arg min. o were already storing the min in the DP table. DP of i would get sorted to be that. We also store parent of i. And then once were done, we start with our original problem and we follow parent pointers to figure out what the best choices were. o we start at 0 because we know word zero begins a line. And then 0 will be the first line. Then we go to parent of 0. That will be where the second line begins. Then we go to parent of parent of 0. That will be where the third line begins. OK, because these were the best choices for where the second line begins, this is the best place where the second line begins. Given that this is the first line, this is the best line where the second line begins given that this was the first line. o thats really the third line given this was the second line. Little confusing, but you just a simple for loop. You start with 0 because thats our original problem. You keep calling parent of the thing you currently have. n linear time, you will reconstruct where the lines break. o you can use this technique in any DP. ts very simple. ts totally automatic. Just like memoization is a technique that you can apply without thinking, you could even write a program, given a recursive algorithm, would turn into a memorized recursive algorithm. ts totally automated. ame thing with the bottom up DP table. As long as you know what the topological order is, just make those for loops and then put exactly the recursive call but turn it into an array call. Boom, youve got a bottom up algorithm. Totally automatic, no thinking required. Parent pointers also, no thinking required. As long as youre following the structure of trial guesses compute some value just remember what the guess was you reconstruct your solution. Thats the great thing about dynamic programming is how much of it is automatic. The hard part is figuring out what to guess and then what your subproblems are, or the other order. Whatever works. Any other questions about text? would like to move on to blackjack. OK, now brought some cards, because some of you may not know the rules to blackjack. How many people know blackjack? OK. How many people do not and are willing to admit it? A few, all right. o this is for you and for fun, entertainment. o m going to bring Victor up to help demonstrate the rules of blackjack. Were going to play standard asino blackjack as in the movie 21, or whatever. o m going to just do a random cut here so cant sheet. You have a tablet, thats scary. Youre going to look at strategy. VTOR: Nothing special. PROFEOR: All right. Hopefully you do not have xray vision. o the way it works is theres a dealer player and one or more players. Were just going to do it with one player to keep it simple. m going to be the dealer. o my strategy is actually totally deterministic, theres nothing interesting. Victor has the hard part of winning. o to start out, believe we deal to you first, then to me, then to you, then to me. o lets hold up these cards, Victor, so that people can see them. You dont get to see one of my cards. Thats some peculiarity of the rule. And if the sum of our cards goes over 21, we lose the game. Victor first. cannot have a value more than 21 in these hands, because only have two cards. You have a value of ha, ace. Great. An ace can be a 1 or an 11. Thats the fun rule. o this is either an 8 or an 18. And so Victor has a choice of whether to take another card or not. What would you like to do? VTOR: tandard strategy says stand. PROFEOR: He stands. o hes going to stick to that. At this point, my cards flip over. have 17, which is same you, which believe means forget about tie rules. VTOR: have 18. PROFEOR: You have 18. All right. VTOR: ee? The strategy works. PROFEOR: o thats good. m going to hit in the hope that have a small card that will push me right above you. But do not. lose. m sad. VTOR: t says always stand on a 17. PROFEOR: Oh, always stand on 17? Huh. All right, never mind. Thanks. Yeah, still lose. The game is over. y strategy is always stand on a value VTOR: tand on 17. PROFEOR: 17 or higher. And if have a value less than 17, always take another card. o lets do it one more time to get it right. o m going to deal to you, deal to me, deal to you, deal to me. o hold up your cards. You have 18 again. Are you cheating? VTOR: still have to stand. PROFEOR: You still stand, according to tablet. o , in this case, have a 20. And so this win. o you get the idea. Lets say in each case were betting $1. o at this point, wed be even. He won $1, won $1. But in general, slight think its balanced. VTOR: For these rules, theres a 1% advantage for the house. PROFEOR: 1% advantage for the house. nteresting. All right, well, thats beyond this class. What were going to see is how to cheat in blackjack. o this is going to be encourage you to try this out at casinos. Just kidding. This is a little bit difficult to actually do in a casino unless you have an inside man. o if you have an inside man, go for it. ts guaranteed to win you lots of money because its going to play optimally. n perfect information blackjack, suppose that already know the entire deck. uppose somehow either get to put the deck there, or have some xray vision. get to see the entire deck ahead of time. And then somebodys going to play through a game over and over with me or not over and over, but until the deck is depleted and want to know in each case, should hit, or should stand? And claim with dynamic programming you can figure that out using exactly the same strategy as text, actually. ts really for each word, should start a new line or not? ame problem here. ts slightly more complicated to write down. o lets say the deck is a sequence of cards. And m going to call it c0, c1 up to cn minus 1, n cards. And you are one player. First is the dealer. dont know how to solve this for two players, interesting open problem. But for one player can do it. Lets say $1 bet per hand, think theyre called. m not sure. Per play? Per box? Whatever. Youre not allowed to double. Youre not allowed to split. All these fancy rules are harder to think about, although you might be able to solve them as well. o the idea is have some cards. hould hit or should stand? dont know. ll guess. o our guessing lets jump ahead to the guessing part is whether we want to hit or stand given a card. Actually, it would be easier to think about an entire play, an entire hand. Were going to guess, how many times should hit in the first play? o initially, four cards are dealt. look at my hands. Actually, dont really look at my hand. m just going to guess ahead of time. think ll hit five times this time. think ll hit zero times this time. mean, m just going to try them all. o dont really have to be intelligent here, OK? ts kind of crazy but it works. Our subproblems, can anyone tell me what our subproblems would be, n one word or less? Less would be impressive. Yeah? ADENE: Where you start the new hand. PROFEOR: Where do you start the new hand? Yeah. o its going to be suffixes of the cards. o at some point we do a play, and then we get to ith card. And then the rest of the game will be from the ith card on. o its going to be suffix ci colon, guess would be the notation here. ts a bit awkward. These are the cards that remain. And so the subproblem is, what is the best play? Whats the best outcome given $1 bets? How much money can make maximize my winning, say given these cards onward? Who knows what happened to their earlier cards, but just these are the cards. m left with. Number of subproblems is hmm? n. How many choices of i are there? n choices. This really important. ts really useful that were thinking about suffixes. ts not that some subset of the cards have been played. That would be really hard, because theres exponentially many different subsets that could be left. ts always a prefix that gets played, and therefore suffix is left. And theres only n suffixes, remember that. Were going to use it over and over in dynamic programming. o now we need to solve the subproblem. tarting from ci, whats the best way to play? Well, the first four cards are fixed, and then we guess how many hits are left. o its going to be something like n minus i minus four different possibilities for mean, that would be the maximum number of hits could take all the remaining cards. That would be the most. And lets see, so the number of choices ll just say its, at most, n. dont have to be fancy here. OK, now we go to the recurrence. o m going to call this blackjack of i. ts going to be the solution. want to solve this subproblem from i onwards. Whats the best play? And guess its going to be a max if m measuring winnings. And whats the winnings if decide to hit this many times? ts a little bit hard to write down the exact formula. m going to write a rough version which is the outcome of that first play. ts going to be either lose $1, we tie, or win $1. o if we end up with the same value, you actually in most versions you get your money back, nothing changes. The bet is nullified. o thats a zero outcome. But if were only betting $1, these are the three possible outcomes. You can compute this, right? f told you how many times you hit, then you just execute through those cards and you compute the values of my hand, of your hand versus the dealers hand. You see, did anyone bust? f so, they lose. Otherwise you compare the values and you see which is bigger or smaller. This is easy to do in linear time. No biggie. Whats useful here is that the dealer strategy is deterministic. o after you know how many cards you take, what the dealer does is force, because he just looks. Do have 17 or greater? f not, take another card and keep repeating that. o its a deterministic strategy. n linear time, you can figure out what the outcome is. Then you also have to add the outcome of all the remaining cards, which is just BG of j. This is recursion, super easy. We do this for all choices of j. ts like a range of i plus 4 up to n, think. ure, thatll work. should probably put an if here, which is if its a valid play. There are some constraints here. f ve already busted, cant hit again. o in fact what you have to do in this for loop is say, well, maybe take another hit. aybe take another hit. At some point go over 21, and then you have to stop the for loop. o m writing that as an if. You can also do it with a break, however you want. But thats youre considering all possible options, all valid options of play. For each of them, you see what the outcome was after the dealer takes some more cards. This is actually a little bit funny. orry, this should really be the number of hits in range from, lets say, 0 to n. aybe you dont hit at all. And then j is a little bit tricky, because this is actually i plus 4 plus the number of hits plus the number of dealer hits. OK, so you have to run this algorithm to compute what happened, which computes how many times a dealer took a card. Thats how many cards got consumed. And so thats if you do i plus 4 plus that plus that, thats how many cards are left, or where the cards resume. And then you call BG on that. o were, in general, from BG of i if you think of the DAG theres some position, maybe i plus 4 happens. aybe it doesnt happen. t depends on what the dealer does. Were going to depend on i plus 6, i plus 5 maybe. ts going to be all of these possibilities. These are all different plays. And then on each of these edges, were going to have plus 1, 0, or minus 1. Those are the outcomes, whether won or lost or tied. And then were just computing a shortest path in this DAG. ts actually really easy if you think about it that way. This is just how many cards are left. From that position, you just see what are all the possibilities? What are all the edges that could go to? What states could to go to next? How many cards are remaining? How much did it cost me or win me? And then take longest paths in that DAG. That will give you the exact same answer. Thats what this dynamic programming is doing. n the lecture notes, theres more details where actually tried to write out this function, this recurrence as an algorithm. You could do it, assuming ve got everything right. ts not that hard. The order here is just the same as the order we did before. The running time is going to be cubic in the worst case, because we have its a little nonobvious, but we have n subproblems. For each of them, we have n choices. And for each choice we have to run the dealer strategy. And so that conceivably could take linear time. Here m assuming a general value of 21. f 21 is actually constant, it only be constant time to play out a single hand, and then its quadratic time. o it depends on your model of generalized blackjack. But thats it. And get some flavor of the power of dynamic programming, were going to see its even more powerful than this in the next two lectures.","And this is easy, but a little different from what weve done before because we have to actually work from the end backwards, because were expressing DP of i in terms of DP of larger values of i. j is always bigger than i. And so we have to do it from the right end back to the beginning. m going to didnt give this problem a name, so m just going to write it as DP of i. o this is going to be the solution to that suffix, words from i onward. And if the sum of the lengths of those words plus the sum of the lengths of the spaces as small as possible is bigger than the width of my screen or page, guess then say they dont fit, and then define badness to be infinity meaning, never want to do that. But if you do it in this order, you always have the one you need by the time you get there. And the number of different subproblems here was v squared, because we had to do this for every vertex v and we had to do it for every value of k between 0 and v minus 1. v minus was is the number of rounds we need in BellmanFord. Thats 2 to the n. But here, the idea is were only thinking about, well, what are the words that remain? Then you also have to add the outcome of all the remaining cards, which is just BG of j. This is recursion, super easy. The idea is we want to compute this part of the path, the s to u part. That would be a problem if its 2 to the n. hope its only n. Originally, we said, OK, for every word, were going to say, is this in our out? This is the cost of the first part of the path. And if we compute the amount of time we need to solve each subproblem and multiply that by the number of subproblems we get, the total time required by the algorithm. The total time we get is going to be the number of sub problems which is n times the running time per subproblem. The number of choices for the guess is at most n minus i. m just going to think of that as order n. t wont matter. As we said, what we guess is where to break the first line, where to start the second line for those words. And so there are 2 to the n. f have n words, theres 2 to the n different splits. This is what we have left to do. o thats just DP of j. Thats the cost of the rest of the problem. And this is one of our subproblems, so if we solve all of them, were done. Assuming you find the shortest path from s to u, thats going to be the best way to get there. f make this guess all right, so have word i is the first word of the first line. The running time is going to be cubic in the worst case, because we have its a little nonobvious, but we have n subproblems. And thats what were going to do here. We solve all these subproblems that we dont directly care about, but then the first one is the one we want. o compute for every guess of j, compute the cost of the rest of the problem plus the cost of that first line. Given that this is the first line, this is the best line where the second line begins given that this was the first line. o this is the width of the page minus the total width of those words, which you also have to include the spaces here. o m going to call this blackjack of i. ts going to be the solution. OK, so DP of i now want to do this for loop in order to try all the possible guesses. What d like to minimize is the sum of the badnesses of the lines. The issue is that once ve chosen where the second line is, of course the next thing want to guess is where the third line begins. And then word j is the first word of the second line. We do this for all choices of j. ts like a range of i plus 4 up to n, think. One is to use recursion and memoization, which is the way like to think about it. And we want to write some text, its going to have some spaces in it. And so the subproblem is, what is the best play? Another perspective kind of an orthogonal perspective or another way of thinking about it, which think should be comfortable for you because we spent a lot of time doing shortest paths and expressing problems that we care about in terms of shortest paths even if they dont look like it at first glance dynamic programming in some sense is always computing shortest paths in a DAG. OK, so this is it could be the i plus first line. o number of subproblems is this is what we need to do algorithmically. n general if were going to guess something, we need to write down the number of choices. Thats so you know, have the beginning of a line here and then have a beginning of a line here at the fourth word. o in some sense, your algorithm is going to be trying all the possibilities, but somehow avoiding the fact that there are exponentially many of them. n general, need to set up my subproblems so that after do the first guess have the problem of the original type. And in general, what we need to check here is that the sub problem recurrence is acyclic. And in this case, the original problem we need to solve is DP of 0 because DP of 0 means take words from 0 onwards. f you use LaTeX as some of you have been doing on problem sets, and think also new versions of Word but m not sure then it uses dynamic programming to solve this problem. That would be the for loop of where the second line can start. And these are actually the for loops you get in the bottom of DP. And the idea is you might in the greedy algorithm, you make the first line as good as you can. But this is sort of how bad it is to use or lets say, yeah, words i through j as a line. And lets see, so the number of choices ll just say its, at most, n. dont have to be fancy here. You cant do the reverse, because then when youre trying to computer the nth you dont have the ones you need, the n minus 1 and n minus 2. o its going to be suffixes of the cards. And get some flavor of the power of dynamic programming, were going to see its even more powerful than this in the next two lectures. And thats the main thing you need to do here. The hard part is figuring out what to guess and then what your subproblems are, or the other order. And the fun part here is we get to treat any recursive calls in this recursion as free, as constant time, because we really only pay for it first time. And for shortest paths, what we guessed was we know were looking for some path from s v. B Lets guess what the last edge is. OK, so the first thing we need to do is define subproblems. The second time we call it, its already memoized, so we dont have to pay for it. its a very simple idea, but it applies to all dynamic programs and lets you find the actual solution, not just the cost of the solution. o the subproblem m not going to write it here is if give you these words, never mind the other words, how do pack them optimally into a paragraph? And to account for the case that thats zero, we do a plus 1. o its going to be something like n minus i minus four different possibilities for mean, that would be the maximum number of hits could take all the remaining cards. orry, this should really be the number of hits in range from, lets say, 0 to n. aybe you dont hit at all. And then use some edge from u to v for some choice of u. This will try all of them. And what we want to do is cut. ts what was the value of j that gave you the minimum value of this thing. But it might actually be better to leave out some of the words that would fit here in order to make the next line better. And in general because of this formula, we want to count how many subproblems are there. As long as youre trying to minimize the sum of the badnesses, whatever function is in here, we just compute it here. n general, we have the real problem we want to solve, we generalize it in some sense by considering lots of different subproblems that we might care about. Thats the great thing about dynamic programming is how much of it is automatic. o the text were going to think of as a list of words. n more detail, the three main techniques in dynamic programming are the idea of guessing, the idea that, oh, want to find the best way to solve a problem. We try all the incoming edges into v thats this part and for each of them we compute forgot something here. o when we compute this min, were trying all choices of j. One of them or maybe more than one, but at least one of them actually gave you the min. Except we dont really know what the last edge was, so we have to try them all. Were just changing around when we count the cost of each subproblem, and then this is the total running time. m going to give you the five general steps, and then were going to apply them to two new problems. And were going to see that technique applied to two new problems which are much more interesting than the ones weve already solved namely, how to make your text look nice in a paragraph, where to break the lines. Then we also use a recursion, some way to express the solution to our problem in terms of solutions to subproblems. OK, because these were the best choices for where the second line begins, this is the best place where the second line begins. You pack as many words as you can on the first line, then you go to the next line, pack as many words as you can on the second line. o this is going to be encourage you to try this out at casinos. dont know it, so ll guess the answer meaning ll try all the possibilities for that choice and take the best one. We also store parent of i. And then once were done, we start with our original problem and we follow parent pointers to figure out what the best choices were. First step is to figure out what your subproblems are going to be. o in fact what you have to do in this for loop is say, well, maybe take another hit. And so thats if you do i plus 4 plus that plus that, thats how many cards are left, or where the cards resume. ts going to be all of these possibilities. And then, of course, you use this formula to compute the overall running time, which is just multiplying this quantity with this quantity. PROFEOR: Try all partitions of the word, so of the string of words. Well, its some incoming edge to v. o theres going to be indegree of v different choices for that. m going to be the dealer. o m going to try all the possible words after the first word. The rules of the game here are were going to, like in the early lectures of document distance where you have some definition of splitting a document into words separated by spaces. And then on each of these edges, were going to have plus 1, 0, or minus 1. But for each one of them, we have to do an addition. And then 0 will be the first line. o the number of different choices for your guess is one. o you have some problem you want to solve, like you have text you want to split up into lines so it looks nice in a paragraph, you express that problem somehow as a directed acyclic graph. n the case of Fibonacci numbers, its clear you want to start with the smallest one and end up with the biggest one. You have to do the for loop over k on the outside, the for loop over V on the inside. o think actually it might be easier to think about, for this problem, what would be the brute force strategy? And this is going to be more practical, faster, and so on. But then also have to think about, well, what about the first line? And then the rest of the game will be from the ith card on. o theres order n choices, so this is order n. Now we have to check that theres a topological order for this problem or for these subproblems. And indeed for, at least for BellmanFord, thats how they were invented, was to apply a general technique which were going to see today in full generality, more or less most of this is generality in five easy steps. Often, the number of guesses and the running time per subproblem are the same, the constant factors. For each of them, you see what the outcome was after the dealer takes some more cards. PROFEOR: Guess how long the first line is, yeah. Because the meaning of DP of n is have zero words, the nth word onward. What were going to see is how to cheat in blackjack. o to start out, believe we deal to you first, then to me, then to you, then to me. But normally, you think of this as like def DP of i is return min of this. What are all the edges that could go to? This is easy to do in linear time. Were going to guess, how many times should hit in the first play? m going to write a rough version which is the outcome of that first play. This is a lot like shortest paths where we didnt it was annoying to actually figure out what the shortest path was. And this special value of n will mean that there is no second line. Thats fine, as long as you can compute it in terms of just i and j and looking at those words. Here, parent parent of i is going to be arg min of that same thing. aybe its actually easier to jump ahead and think, what would guess in my solution if have this big string of words? And d like to what want to do is consider all possible guesses. At some point go over 21, and then you have to stop the for loop. n the case of Fibonacci and shortest paths, this is pretty clear. Thats just how m going to define it. And then once we knew the shortest path weight, it was pretty easy to reconstruct the paths. But we add in this technique of memoization, which is just once we compute an answer weve stored in a lookup table, if we ever need that answer again we reuse it instead of recomputing it. And then somebodys going to play through a game over and over with me or not over and over, but until the deck is depleted and want to know in each case, should hit, or should stand? And then for analysis, we want to counter number of subproblems for step one. And we want to justify our text on the right here. ts not telling you how to pack the words. guess right now we just want to compute how much time does this cost, time per subproblem.",0.1394478240524099
35,35,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer highquality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: OK, good morning. o today, were going to have a fairly packed lecture. We are going to conclude with chapter two, discrete random variables. And we will be talking mostly about multiple random variables. And this is also the last lecture as far as quiz one is concerned. o its going to cover the material until today, and of course the next recitation and tutorial as well. OK, so were going to review quickly what we introduced at the end of last lecture, where we talked about the joint PF of two random variables. Were going to talk about the case of more than two random variables as well. Were going to talk about the familiar concepts of conditioning and independence, but applied to random variables instead of events. Were going to look at the expectations once more, talk about a few properties that they have, and then solve a couple of problems and calculate a few things in somewhat clever ways. o the first point want to make is that, to a large extent, whatever is happening in our chapter on discrete random variables is just an exercise in notation. There is stuff and concepts that you are already familiar with probabilities, probabilities of two things happening, conditional probabilities. And all that were doing, to some extent, is rewriting those familiar concepts in new notation. o for example, this is the joint PF of two random variable. t gives us, for any pair or possible values of those random variables, the probability that that pair occurs simultaneously. o its the probability that simultaneously x takes that value, and y takes that other value. And similarly, we have the notion of the conditional PF, which is just a list of the condition of the various conditional probabilities of interest, conditional probability that one random variable takes this value given that the other random variable takes that value. Now, a remark about conditional probabilities. onditional probabilities generally are like ordinary probabilities. You condition on something particular. o here we condition on a particular y. o think of little y as a fixed quantity. And then look at this as a function of x. o given that y, which we condition on, given our new universe, were considering the various possibilities for x and the probabilities that they have. Now, the probabilities over all xs, of course, needs to add to 1. o we should have a relation of this kind. o theyre just like ordinary probabilities over the different xs in a universe where we are told the value of the random variable y. Now, how are these related? o we call these the marginal, these the joint, these the conditional. And there are some relations between these. For example, to find the marginal from the joint, its pretty straightforward. The probability that x takes a particular value is the sum of the probabilities of all of the different ways that this particular value may occur. What are the different ways? Well, it may occur together with a certain y, or together with some other y, or together with some other y. o you look at all the possible ys that can go together with this x, and add the probabilities of all of those pairs for which we get this particular value of x. And then theres a relation between that connects these two probabilities with the conditional probability. And its this relation. ts nothing new. ts just new notation for writing what we already know, that the probability of two things happening is the probability that the first thing happens, and then given that the first thing happens, the probability that the second one happened. o how do we go from one to the other? Think of A as being the event that X takes the value, little x, and B being the event that Y takes the value, little y. o the joint probability is the probability that these two things happen simultaneously. ts the probability that X takes this value times the conditional probability that Y takes this value, given that X took that first value. o its the familiar multiplication rule, but just transcribed in our new notation. o nothing new so far. OK, why did we go through this exercise and this notation? ts because in the experiments where were interested in the real world, typically theres going to be lots of uncertain quantities. Theres going to be multiple random variables. And we want to be able to talk about them simultaneously. Okay. Why two and not more than two? How about three random variables? Well, if you understand whats going on in this slide, you should be able to kind of automatically generalize this to the case of multiple random variables. o for example, if we have three random variables, X, Y, and Z, and you see an expression like this, it should be clear what it means. ts the probability that X takes this value and simultaneously Y takes that value and simultaneously Z takes that value. guess thats an uppercase Z here, thats a lowercase z. And if ask you to find the marginal of X, if tell you the joint PF of the three random variables and ask you for this value, how would you find it? Well, you will try to generalize this relation here. The probability that x occurs is the sum of the probabilities of all events that make X to take that particular value. o what are all the events? Well, this particular x can happen together with some y and some z. We dont care which y and z. Any y and z will do. o when we consider all possibilities, we need to add here over all possible values of ys and zs. o consider all triples, x, y, z. Fix x and consider all the possibilities for the remaining variables, y and z, add these up, and that gives you the marginal PF of X. And then theres other things that you can do. This is the multiplication rule for two events. We saw back in chapter one that theres a multiplication rule when you talk about more than two events. And you can write a chain of conditional probabilities. We can certainly do the same in our new notation. o lets look at this rule up here. ultiplication rule for three random variables, what does it say? The probability of three things happening simultaneously, X, Y, Z taking specific values, little x, little y, little z, that probability is the probability that the first thing happens, that X takes that value. Given that X takes that value, we multiply it with the probability that Y takes also a certain value. And now, given that X and Y have taken those particular values, we multiply with a conditional probability that the third thing happens, given that the first two things happen. o this is just the multiplication rule for three events, which would be probability of A intersection B intersection equals you know the rest of the formula. You just rewrite this formula in PF notation. Probability of A intersection B intersection is the probability of A, which corresponds to this term, times the probability of B given A, times the probability of given A and B. o what else is there thats left from chapter one that we can or should generalize to random variables? Well, theres the notion of independence. o lets define what independence means. nstead of talking about just two random variables, lets go directly to the case of multiple random variables. When we talked about events, things were a little complicated. We had a simple definition for independence of two events. Two events are independent if the probability of both is equal to the product of the probabilities. But for three events, it was kind of messy. We needed to write down lots of conditions. For random variables, things in some sense are a little simpler. We only need to write down one formula and take this as the definition of independence. Three random variables are independent if and only if, by definition, their joint probability mass function factors out into individual probability mass functions. o the probability that all three things happen is the product of the individual probabilities that each one of these three things is happening. o independence means mathematically that you can just multiply probabilities to get to the probability of several things happening simultaneously. o with three events, we have to write a huge number of equations, of equalities that have to hold. How can it be that with random variables we can only manage with one equality? Well, the catch is that this is not really just one equality. We require this to be true for every little x, y, and z. o in some sense, this is a bunch of conditions that are being put on the joint PF, a bunch of conditions that we need to check. o this is the mathematical definition. What is the intuitive content of this definition? The intuitive content is the same as for events. Random variables are independent if knowing something about the realized values of some of these random variables does not change our beliefs about the likelihood of various values for the remaining random variables. o independence would translate, for example, to a condition such as the conditional PF of X , given y, should be equal to the marginal PF of X. What is this saying? That you have some original beliefs about how likely it is for X to take this value. Now, someone comes and tells you that Y took on a certain value. This causes you, in principle, to revise your beliefs. And your new beliefs will be captured by the conditional PF, or the conditional probabilities. ndependence means that your revised beliefs actually will be the same as your original beliefs. Telling you information about the value of Y doesnt change what you expect for the random variable X. Why didnt we use this definition for independence? Well, because this definition only makes sense when this conditional is welldefined. And this conditional is only welldefined if the events that Y takes on that particular value has positive probability. We cannot condition on events that have zero probability, so conditional probabilities are only defined for ys that are likely to occur, that have a positive probability. Now, similarly, with multiple random variables, if theyre independent, you would have relations such as the conditional of X, given y and z, should be the same as the marginal of X. What is this saying? Again, that if tell you the values, the realized values of random variables Y and Z, this is not going to change your beliefs about how likely x is to occur. Whatever you believed in the beginning, youre going to believe the same thing afterwards. o its important to keep that intuition in mind, because sometimes this way you can tell whether random variables are independent without having to do calculations and to check this formula. OK, so lets check our concepts with a simple example. Lets look at two random variables that are discrete, take values between one and for each. And this is a table that gives us the joint PF. o it tells us the probability that X equals to 2 and Y equals to 1 happening simultaneously. ts an event that has probability 1/20. Are these two random variables independent? You can try to check a condition like this. But can we tell directly from the table? f tell you a value of Y, could that give you useful information about X? ertainly. f tell you that Y is equal to 1, this tells you that X must be equal to 2. But if tell you that Y was equal to 3, this tells you that, still, X could be anything. o telling you the value of Y kind of changes what you expect or what you consider possible for the values of the other random variable. o by just inspecting here, we can tell that the random variables are not independent. Okay. Whats the other concept we introduced in chapter one? We introduced the concept of conditional independence. And conditional independence is like ordinary independence but applied to a conditional universe where were given some information. o suppose someone tells you that the outcome of the experiment is such that X is less than or equal to 2 and Y is larger than or equal to 3. o we are given the information that we now live inside this universe. o what happens inside this universe? nside this universe, our random variables are going to have a new joint PF which is conditioned on the event that we were told that it has occurred. o let A correspond to this sort of event here. And now were dealing with conditional probabilities. What are those conditional probabilities? We can put them in a table. o its a two by two table, since we only have two possible values. What are they going to be? Well, these probabilities show up in the ratios 1, 2, 2, and 4. Those ratios have to stay the same. The probabilities need to add up to one. o what should the denominators be since these numbers add up to nine? These are the conditional probabilities. o this is the conditional PF in this example. Now, in this conditional universe, is x independent from y? f tell you that y takes this value, so we live in this universe, what do you know about x? What you know about x is at this value is twice as likely as that value. f condition on y taking this value, so were living here, what do you know about x? What you know about x is that this value is twice as likely as that value. o its the same. Whether we live here or we live there, this x is twice as likely as that x. o the conditional PF in this new universe, the conditional PF of X given y, in the new universe is the same as the marginal PF of X, but of course in the new universe. o no matter what y is, the conditional PF of X is the same. And that conditional PF is 1/3 and 2/3. This is the conditional PF of X in the new universe no matter what y occurs. o Y does not give us any information about X, doesnt cause us to change our beliefs inside this little universe. And therefore the two random variables are independent. Now, the other way that you can verify that we have independence is to find the marginal PFs of the two random variables. The marginal PF of X, you find it by adding those two terms. You get 1/3. Adding those two terms, you get 2/3. arginal PF of Y, you find it, you add these two terms, and you get 1/3. And the marginal PF of Y here is going to be 2/3. And then you ask the question, is the joint the product of the marginals? And indeed it is. This times this gives you 1/9. This times this gives you 2/9. o the values in the table with the joint PFs is the product of the marginal PFs of X and Y in this universe, so the two random variables are independent inside this universe. o we say that theyre conditionally independent. All right. Now lets move to the new topic, to the new concept that we introduce in this chapter, which is the concept of expectations. o what are the things to know here? One is the general idea. The way to think about expectations is that its something like the average value for random variable if you do an experiment over and over, and if you interpret probabilities as frequencies. o you get xs over and over with a certain frequency P(x) a particular value, little x, gets realized. And each time that this happens, you get x dollars. How many dollars do you get on the average? Well, this formula gives you that particular average. o first thing we do is to write down a definition for this sort of concept. But then the other things you need to know is how to calculate expectations using shortcuts sometimes, and what properties they have. The most important shortcut there is is that, if you want to calculate the expected value, the average value for a random variable, you do not need to find the PF of that random variable. But you can work directly with the xs and the ys. o you do the experiment over and over. The outcome of the experiment is a pair (x,y). And each time that a certain (x,y) happens, you get so many dollars. o this fraction of the time, a certain (x,y) happens. And that fraction of the time, you get so many dollars, so this is the average number of dollars that you get. o what you end up, since it is the average, then that means that it corresponds to the expected value. Now, this is something that, of course, needs a little bit of mathematical proof. But this is just a different way of accounting. And it turns out we give you the right answer. And its a very useful shortcut. Now, when were talking about functions of random variables, in general, we cannot speak just about averages. That is, the expected value of a function of a random variable is not the same as the function of the expected values. A function of averages is not the same as the average of a function. o in general, this is not true. But what its important to know is to know the exceptions to this rule. And the important exceptions are mainly two. One is the case of linear functions of a random variable. We discussed this last time. o the expected value of temperature in elsius is, you first find the expected value of temperature in Fahrenheit, and then you do the conversion to elsius. o whether you first average and then do the conversion to the new units or not, it shouldnt matter when you get the result. The other property that turns out to be true when you talk about multiple random variables is that expectation still behaves linearly. o let X, Y, and Z be the score of a random student at each one of the three sections of the AT. o the overall AT score is X plus Y plus Z. This is the average score, the average total AT score. Another way to calculate that average is to look at the first section of the AT and see what was the average. Look at the second section, look at what was the average, and so the third, and add the averages. o you can do the averages for each section separately, add the averages, or you can find total scores for each student and average them. o guess you probably believe that this is correct if you talk just about averaging scores. ince expectations are just the variation of averages, it turns out that this is also true in general. And the derivation of this is very simple, based on the expected value rule. And you can look at it in the notes. o this is one exception, which is linearity. The second important exception is the case of independent random variables, that the product of two random variables has an expectation which is the product of the expectations. n general, this is not true. But for the case where we have independence, the expectation works out as follows. sing the expected value rule, this is how you calculate the expected value of a function of a random variable. o think of this as being your g(X, Y) and this being your g(little x, y). o this is something thats generally true. Now, if we have independence, then the PFs factor out, and then you can separate this sum by bringing together the x terms, bring them outside the y summation. And you find that this is the same as expected value of X times the expected value of Y. o independence is used in this step here. OK, now what if X and Y are independent, but instead of taking the expectation of X times Y, we take the expectation of the product of two functions of X and Y? claim that the expected value of the product is still going to be the product of the expected values. How do we show that? We could show it by just redoing this derivation here. nstead of X and Y, we would have g(X) and h(Y), so the algebra goes through. But theres a better way to think about it which is more conceptual. And heres the idea. f X and Y are independent, what does it mean? X does not convey any information about Y. f X conveys no information about Y, does X convey information about h(Y)? No. f X tells me nothing about Y, nothing new, it shouldnt tell me anything about h(Y). Now, if X tells me nothing about h of h(Y), could g(X) tell me something about h(Y)? No. o the idea is that, if X is unrelated to Y, doesnt have any useful information, then g(X) could not have any useful information for h(Y). o if X and Y are independent, then g(X) and h(Y) are also independent. o this is something that one can try to prove mathematically, but its more important to understand conceptually why this is so. ts in terms of conveying information. o if X tells me nothing about Y, X cannot tell me anything about Y cubed, or X cannot tell me anything by Y squared, and so on. Thats the idea. And once we are convinced that g(X) and h(Y) are independent, then we can apply our previous rule, that for independent random variables, expectations multiply the right way. Apply the previous rule, but apply it now to these two independent random variables. And we get the conclusion that we wanted. Now, besides expectations, we also introduced the concept of the variance. And if you remember the definition of the variance, let me write down the formula for the variance of aX. ts the expected value of the random variable that were looking at minus the expected value of the random variable that were looking at. o this is the difference of the random variable from its mean. And we take that difference and square it, so its the squared distance from the mean, and then take expectations of the whole thing. o when you look at that expression, you realize that a can be pulled out of those expressions. And because there is a squared, when you pull out the a, its going to come out as an asquared. o that gives us the rule for finding the variance of a scale or product of a random variable. The variance captures the idea of how wide, how spread out a certain distribution is. Bigger variance means its more spread out. Now, if you take a random variable and the constants to it, what does it do to its distribution? t just shifts it, but it doesnt change its width. o intuitively it means that the variance should not change. You can check that mathematically, but it should also make sense intuitively. o the variance, when you add the constant, does not change. Now, can you add variances is the way we added expectations? Does variance behave linearly? t turns out that not always. Here, we need a condition. ts only in special cases for example, when the two random variables are independent that you can add variances. The variance of the sum is the sum of the variances if X and Y are independent. The derivation of this is, again, very short and simple. Well skip it, but its an important fact to remember. Now, to appreciate why this equality is not true always, we can think of some extreme examples. uppose that X is the same as Y. Whats going to be the variance of X plus Y? Well, X plus Y, in this case, is the same as 2X, so were going to get 4 times the variance of X, which is different than the variance of X plus the variance of X. o that expression would give us twice the variance of X. But actually now its 4 times the variance of X. The other extreme would be if X is equal to Y. Then the variance is the variance of the random variable, which is always equal to 0. Now, a random variable which is always equal to 0 has no uncertainty. t is always equal to its mean value, so the variance, in this case, turns out to be 0. o in both of these cases, of course we have random variables that are extremely dependent. Why are they dependent? Because if tell you something about Y, it tells you an awful lot about the value of X. Theres a lot of information about X if tell you Y, in this case or in that case. And finally, a short drill. f tell you that the random variables are independent and you want to calculate the variance of a linear combination of this kind, then how do you argue? You argue that, since X and Y are independent, this means that X and 3Y are also independent. X has no information about Y, so X has no information about Y. X has no information about Y, so X should not have any information about 3Y. o X and 3Y are independent. o the variance of Z should be the variance of X plus the variance of 3Y, which is the variance of X plus 9 times the variance of Y. The important thing to note here is that no matter what happens, you end up getting a plus here, not a minus. o thats the sort of important thing to remember in this type of calculation. o this has been all concepts, reviews, new concepts and all that. ts the usual fire hose. Now lets use them to do something useful finally. o lets revisit our old example, the binomial distribution, which counts the number of successes in independent trials of a coin. ts a biased coin that has a probability of heads, or probability of success, equal to p at each trial. Finally, we can go through the exercise of calculating the expected value of this random variable. And theres the way of calculating that expectation that would be the favorite of those people who enjoy algebra, which is to write down the definition of the expected value. We add over all possible values of the random variable, over all the possible ks, and weigh them according to the probabilities that this particular k occurs. The probability that X takes on a particular value k is, of course, the binomial PF, which is this familiar formula. learly, that would be a messy and challenging calculation. an we find a shortcut? Theres a very clever trick. Theres lots of problems in probability that you can approach really nicely by breaking up the random variable of interest into a sum of simpler and more manageable random variables. And if you can make it to be a sum of random variables that are just 0s or 1s, so much the better. Life is easier. Random variables that take values 0 or 1, we call them indicator variables. They indicate whether an event has occurred or not. n this case, we look at each coin flip one at a time. For the ith flip, if it resulted in heads or a success, we record it 1. f not, we record it 0. And then we look at the random variable. f we take the sum of the Xis, what is it going to be? We add one each time that we get a success, so the sum is going to be the total number of successes. o we break up the random variable of interest as a sum of really nice and simple random variables. And now we can use the linearity of expectations. Were going to find the expectation of X by finding the expectation of the Xis and then adding the expectations. Whats the expected value of Xi? Well, Xi takes the value 1 with probability p, and takes the value 0 with probability 1p. o the expected value of Xi is just p. o the expected value of X is going to be just n times p. Because X is the sum of n terms, each one of which has expectation p, the expected value of the sum is the sum of the expected values. o guess thats a pretty good shortcut for doing this horrendous calculation up there. o in case you didnt realize it, thats what we just established without doing any algebra. Good. How about the variance of X, of Xi? Two ways to calculate it. One is by using directly the formula for the variance, which would be lets see what it would be. With probability p, you get a 1. And in this case, you are so far from the mean. Thats your squared distance from the mean. With probability 1p, you get a 0, which is so far away from the mean. And then you can simplify that formula and get an answer. How about a slightly easier way of doing it. nstead of doing the algebra here, let me indicate the slightly easier way. We have a formula for the variance that tells us that we can find the variance by proceeding this way. Thats a formula thats generally true for variances. Why is this easier? Whats the expected value of Xi squared? Backtrack. What is Xi squared, after all? ts the same thing as Xi. ince Xi takes value 0 and 1, Xi squared also takes the same values, 0 and 1. o the expected value of Xi squared is the same as the expected value of Xi, which is equal to p. And the expected value of Xi squared is p squared, so we get the final answer, p times (1p). f you were to work through and do the cancellations in this messy expression here, after one line you would also get to the same formula. But this sort of illustrates that working with this formula for the variance, sometimes things work out a little faster. Finally, are we in business? an we calculate the variance of the random variable X as well? Well, we have the rule that for independent random variables, the variance of the sum is the sum of the variances. o to find the variance of X, we just need to add the variances of the Xis. We have n Xis, and each one of them has variance p_n times (1p). And we are done. o this way, we have calculated both the mean and the variance of the binomial random variable. ts interesting to look at this particular formula and see what it tells us. f you are to plot the variance of X as a function of p, it has this shape. And the maximum is here at 1/2. p times (1p) is 0 when p is equal to 0. And when p equals to 1, its a quadratic, so it must have this particular shape. o what does it tell us? f you think about variance as a measure of uncertainty, it tells you that coin flips are most uncertain when your coin is fair. When p is equal to 1/2, thats when you have the most randomness. And this is kind of intuitive. if on the other hand tell you that the coin is extremely biased, p very close to 1, which means it almost always gives you heads, then that would be a case of low variance. Theres low variability in the results. Theres little uncertainty about whats going to happen. ts going to be mostly heads with some occasional tails. o p equals 1/2. Fair coin, thats the coin which is the most uncertain of all coins, in some sense. And it corresponds to the biggest variance. t corresponds to an X that has the widest distribution. Now that were on a roll and we can calculate such hugely complicated sums in simple ways, let us try to push our luck and do a problem with this flavor, but a little harder than that. o you go to one of those oldfashioned cocktail parties. All males at least will have those standard big hats which look identical. They check them in when they walk in. And when they walk out, since they look pretty identical, they just pick a random hat and go home. o n people, they pick their hats completely at random, quote, unquote, and then leave. And the question is, to say something about the number of people who end up, by accident or by luck, to get back their own hat, the exact same hat that they checked in. OK, first what do we mean completely at random? ompletely at random, we basically mean that any permutation of the hats is equally likely. Any way of distributing those n hats to the n people, any particular way is as likely as any other way. o theres complete symmetry between hats and people. o what we want to do is to calculate the expected value and the variance of this random variable X. Lets start with the expected value. Lets reuse the trick from the binomial case. o total number of hats picked, were going to think of total number of hats picked as a sum of (0, 1) random variables. X1 tells us whether person 1 got their own hat back. f they did, we record a 1. X2, the same thing. By adding all Xs is how many 1s did we get, which counts how many people selected their own hats. o we broke down the random variable of interest, the number of people who get their own hats back, as a sum of random variables. And these random variables, again, are easy to handle, because theyre binary. The only take two values. Whats the probability that Xi is equal to 1, the ith person has a probability that they get their own hat? Theres n hats by symmetry. The chance is that they end up getting their own hat, as opposed to any one of the other n 1 hats, is going to be 1/n. o whats the expected value of Xi? ts one times 1/n. With probability 1/n, you get your own hat, or you get a value of 0 with probability 11/n, which is 1/n. All right, so we got the expected value of the Xis. And remember, we want to do is to calculate the expected value of X by using this decomposition? Are the random variables Xi independent of each other? You can try to answer that question by writing down a joint PF for the Xs, but m sure that you will not succeed. But can you think intuitively? f tell you information about some of the Xis, does it give you information about the remaining ones? Yeah. f tell you that out of 10 people, 9 of them got their own hat back, does that tell you something about the 10th person? Yes. f 9 got their own hat, then the 10th must also have gotten their own hat back. o the first 9 random variables tell you something about the 10th one. And conveying information of this sort, thats the case of dependence. All right, so the random variables are not independent. Are we stuck? an we still calculate the expected value of X? Yes, we can. And the reason we can is that expectations are linear. Expectation of a sum of random variables is the sum of the expectations. And thats always true. Theres no independence assumption thats being used to apply that rule. o we have that the expected value of X is the sum of the expected value of the Xis. And this is a property thats always true. You dont need independence. You dont care. o were adding n terms, each one of which has expected value 1/n. And the final answer is 1. o out of the 100 people who selected hats at random, on the average, you expect only one of them to end up getting their own hat back. Very good. o since we are succeeding so far, lets try to see if we can succeed in calculating the variance as well. And of course, we will. But its going to be a little more complicated. The reason its going to be a little more complicated is because the Xis are not independent, so the variance of the sum is not the same as the sum of the variances. o its not enough to find the variances of the Xis. Well have to do more work. And heres whats involved. Lets start with the general formula for the variance, which, as mentioned before, its usually the simpler way to go about calculating variances. o we need to calculate the expected value for Xsquared, and subtract from it the expectation squared. Well, we already found the expected value of X. ts equal to 1. o 1squared gives us just 1. o were left with the task of calculating the expected value of Xsquared, the random variable Xsquared. Lets try to follow the same idea. Write this messy random variable, Xsquared, as a sum of hopefully simpler random variables. o X is the sum of the Xis, so you square both sides of this. And then you expand the righthand side. When you expand the righthand side, you get the squares of the terms that appear here. And then you get all the crossterms. For every pair of (i,j) that are different, i different than j, youre going to have a crossterm in the sum. o now, in order to calculate the expected value of Xsquared, what does our task reduce to? t reduces to calculating the expected value of this term and calculating the expected value of that term. o lets do them one at a time. Expected value of Xi squared, what is it going to be? ame trick as before. Xi takes value 0 or 1, so Xi squared takes just the same values, 0 or 1. o thats the easy one. Thats the same as expected value of Xi, which we already know to be 1/n. o this gives us a first contribution down here. The expected value of this term is going to be what? We have n terms in the summation. And each one of these terms has an expectation of 1/n. o we did a piece of the puzzle. o now lets deal with the second piece of the puzzle. Lets find the expected value of Xi times Xj. Now by symmetry, the expected value of Xi times Xj is going to be the same no matter what i and j you see. o lets just think about X1 and X2 and try to find the expected value of X1 and X2. X1 times X2 is a random variable. What values does it take? Only 0 or 1? ince X1 and X2 are 0 or 1, their product can only take the values of 0 or 1. o to find the probability distribution of this random variable, its just sufficient to find the probability that it takes the value of 1. Now, what does X1 times X2 equal to 1 mean? t means that X1 was 1 and X2 was 1. The only way that you can get a product of 1 is if both of them turned out to be 1s. o thats the same as saying, persons 1 and 2 both picked their own hats. The probability that person 1 and person 2 both pick their own hats is the probability of two things happening, which is the product of the first thing happening times the conditional probability of the second, given that the first happened. And in words, this is the probability that the first person picked their own hat times the probability that the second person picks their own hat, given that the first person already picked their own. o whats the probability that the first person picks their own hat? We know that its 1/n. Now, how about the second person? f tell you that one person has their own hat, and that person takes their hat and goes away, from the point of view of the second person, theres n 1 people left looking at n 1 hats. And theyre getting just hats at random. Whats the chance that will get my own? ts 1/n 1. o think of them as person 1 goes, picks a hat at random, it happens to be their own, and it leaves. Youre left with n 1 people, and there are n 1 hats out there. Person 2 goes and picks a hat at random, with probability 1/n 1, is going to pick his own hat. o the expected value now of this random variable is, again, that same number, because this is a 0, 1 random variable. o this is the same as expected value of Xi times Xj when i different than j. o here, all thats left to do is to add the expectations of these terms. Each one of these terms has an expected value thats 1/n times (1/n 1). And how many terms do we have? How many of these are we adding up? ts nsquared n. When you expand the quadratic, theres a total of nsquared terms. ome are selfterms, n of them. And the remaining number of terms is nsquared n. o here we got nsquared n terms. And so we need to multiply here with nsquared n. And after you realize that this number here is 1, and you realize that this is the same as the denominator, you get the answer that the expected value of X squared equals 2. And then, finally going up to the top formula, we get the expected value of X squared, which is 2 1, and the variance is just equal to 1. o the variance of this random variable, number of people who get their own hats back, is also equal to 1, equal to the mean. Looks like magic. Why is this the case? Well, theres a deeper explanation why these two numbers should come out to be the same. But this is something that would probably have to wait a couple of chapters before we could actually explain it. And so ll stop here.","And then, finally going up to the top formula, we get the expected value of X squared, which is 2 1, and the variance is just equal to 1. o the variance of this random variable, number of people who get their own hats back, is also equal to 1, equal to the mean. o we have that the expected value of X is the sum of the expected value of the Xis. o the expected value of Xi is just p. o the expected value of X is going to be just n times p. Because X is the sum of n terms, each one of which has expectation p, the expected value of the sum is the sum of the expected values. Well, X plus Y, in this case, is the same as 2X, so were going to get 4 times the variance of X, which is different than the variance of X plus the variance of X. o that expression would give us twice the variance of X. But actually now its 4 times the variance of X. The other extreme would be if X is equal to Y. Then the variance is the variance of the random variable, which is always equal to 0. And so we need to multiply here with nsquared n. And after you realize that this number here is 1, and you realize that this is the same as the denominator, you get the answer that the expected value of X squared equals 2. o what we want to do is to calculate the expected value and the variance of this random variable X. Lets start with the expected value. The most important shortcut there is is that, if you want to calculate the expected value, the average value for a random variable, you do not need to find the PF of that random variable. Well, we have the rule that for independent random variables, the variance of the sum is the sum of the variances. And similarly, we have the notion of the conditional PF, which is just a list of the condition of the various conditional probabilities of interest, conditional probability that one random variable takes this value given that the other random variable takes that value. And you find that this is the same as expected value of X times the expected value of Y. o independence is used in this step here. The reason its going to be a little more complicated is because the Xis are not independent, so the variance of the sum is not the same as the sum of the variances. ince X1 and X2 are 0 or 1, their product can only take the values of 0 or 1. o to find the probability distribution of this random variable, its just sufficient to find the probability that it takes the value of 1. That is, the expected value of a function of a random variable is not the same as the function of the expected values. Now, the other way that you can verify that we have independence is to find the marginal PFs of the two random variables. The probability that x occurs is the sum of the probabilities of all events that make X to take that particular value. The variance of the sum is the sum of the variances if X and Y are independent. The second important exception is the case of independent random variables, that the product of two random variables has an expectation which is the product of the expectations. o the values in the table with the joint PFs is the product of the marginal PFs of X and Y in this universe, so the two random variables are independent inside this universe. f tell you that the random variables are independent and you want to calculate the variance of a linear combination of this kind, then how do you argue? Two events are independent if the probability of both is equal to the product of the probabilities. f we take the sum of the Xis, what is it going to be? o this is the same as expected value of Xi times Xj when i different than j. o here, all thats left to do is to add the expectations of these terms. The probability that x takes a particular value is the sum of the probabilities of all of the different ways that this particular value may occur. t is always equal to its mean value, so the variance, in this case, turns out to be 0. o in both of these cases, of course we have random variables that are extremely dependent. The probability that person 1 and person 2 both pick their own hats is the probability of two things happening, which is the product of the first thing happening times the conditional probability of the second, given that the first happened. Again, that if tell you the values, the realized values of random variables Y and Z, this is not going to change your beliefs about how likely x is to occur. Now, similarly, with multiple random variables, if theyre independent, you would have relations such as the conditional of X, given y and z, should be the same as the marginal of X. What is this saying? And theres the way of calculating that expectation that would be the favorite of those people who enjoy algebra, which is to write down the definition of the expected value. ts just new notation for writing what we already know, that the probability of two things happening is the probability that the first thing happens, and then given that the first thing happens, the probability that the second one happened. OK, now what if X and Y are independent, but instead of taking the expectation of X times Y, we take the expectation of the product of two functions of X and Y? Probability of A intersection B intersection is the probability of A, which corresponds to this term, times the probability of B given A, times the probability of given A and B. o what else is there thats left from chapter one that we can or should generalize to random variables? ince Xi takes value 0 and 1, Xi squared also takes the same values, 0 and 1. o the expected value of Xi squared is the same as the expected value of Xi, which is equal to p. And the expected value of Xi squared is p squared, so we get the final answer, p times (1p). o to find the variance of X, we just need to add the variances of the Xis. The expected value of this term is going to be what? Whether we live here or we live there, this x is twice as likely as that x. o the conditional PF in this new universe, the conditional PF of X given y, in the new universe is the same as the marginal PF of X, but of course in the new universe. guess thats an uppercase Z here, thats a lowercase z. And if ask you to find the marginal of X, if tell you the joint PF of the three random variables and ask you for this value, how would you find it? Now by symmetry, the expected value of Xi times Xj is going to be the same no matter what i and j you see. sing the expected value rule, this is how you calculate the expected value of a function of a random variable. claim that the expected value of the product is still going to be the product of the expected values. o the expected value now of this random variable is, again, that same number, because this is a 0, 1 random variable. o suppose someone tells you that the outcome of the experiment is such that X is less than or equal to 2 and Y is larger than or equal to 3. o we are given the information that we now live inside this universe. Think of A as being the event that X takes the value, little x, and B being the event that Y takes the value, little y. o the joint probability is the probability that these two things happen simultaneously. o the variance of Z should be the variance of X plus the variance of 3Y, which is the variance of X plus 9 times the variance of Y. The important thing to note here is that no matter what happens, you end up getting a plus here, not a minus. The way to think about expectations is that its something like the average value for random variable if you do an experiment over and over, and if you interpret probabilities as frequencies. o what you end up, since it is the average, then that means that it corresponds to the expected value. And the marginal PF of Y here is going to be 2/3. o telling you the value of Y kind of changes what you expect or what you consider possible for the values of the other random variable. o this way, we have calculated both the mean and the variance of the binomial random variable. o consider all triples, x, y, z. Fix x and consider all the possibilities for the remaining variables, y and z, add these up, and that gives you the marginal PF of X. And then theres other things that you can do. an we calculate the variance of the random variable X as well? We add one each time that we get a success, so the sum is going to be the total number of successes. The probability that X takes on a particular value k is, of course, the binomial PF, which is this familiar formula. o no matter what y is, the conditional PF of X is the same. uppose that X is the same as Y. Whats going to be the variance of X plus Y? Well, it may occur together with a certain y, or together with some other y, or together with some other y. o you look at all the possible ys that can go together with this x, and add the probabilities of all of those pairs for which we get this particular value of x. And then theres a relation between that connects these two probabilities with the conditional probability. ts the expected value of the random variable that were looking at minus the expected value of the random variable that were looking at. Expectation of a sum of random variables is the sum of the expectations. Well, we already found the expected value of X. ts equal to 1. o 1squared gives us just 1. o were left with the task of calculating the expected value of Xsquared, the random variable Xsquared. f tell you that y takes this value, so we live in this universe, what do you know about x? And then look at this as a function of x. o given that y, which we condition on, given our new universe, were considering the various possibilities for x and the probabilities that they have. And that fraction of the time, you get so many dollars, so this is the average number of dollars that you get. Expected value of Xi squared, what is it going to be? Finally, we can go through the exercise of calculating the expected value of this random variable. ts the probability that X takes this value times the conditional probability that Y takes this value, given that X took that first value. We have a formula for the variance that tells us that we can find the variance by proceeding this way. And remember, we want to do is to calculate the expected value of X by using this decomposition? o independence would translate, for example, to a condition such as the conditional PF of X , given y, should be equal to the marginal PF of X. What is this saying? o this is the difference of the random variable from its mean. Random variables are independent if knowing something about the realized values of some of these random variables does not change our beliefs about the likelihood of various values for the remaining random variables. o the probability that all three things happen is the product of the individual probabilities that each one of these three things is happening. Thats the same as expected value of Xi, which we already know to be 1/n. And if you can make it to be a sum of random variables that are just 0s or 1s, so much the better. Now, if you take a random variable and the constants to it, what does it do to its distribution? f you are to plot the variance of X as a function of p, it has this shape. The chance is that they end up getting their own hat, as opposed to any one of the other n 1 hats, is going to be 1/n. And if you remember the definition of the variance, let me write down the formula for the variance of aX. Were going to find the expectation of X by finding the expectation of the Xis and then adding the expectations. What you know about x is that this value is twice as likely as that value. And then you ask the question, is the joint the product of the marginals? o for example, this is the joint PF of two random variable. Another way to calculate that average is to look at the first section of the AT and see what was the average. Well, if you understand whats going on in this slide, you should be able to kind of automatically generalize this to the case of multiple random variables. Telling you information about the value of Y doesnt change what you expect for the random variable X. Why didnt we use this definition for independence? This is the conditional PF of X in the new universe no matter what y occurs. And the final answer is 1. o out of the 100 people who selected hats at random, on the average, you expect only one of them to end up getting their own hat back. nside this universe, our random variables are going to have a new joint PF which is conditioned on the event that we were told that it has occurred. The only way that you can get a product of 1 is if both of them turned out to be 1s. And we take that difference and square it, so its the squared distance from the mean, and then take expectations of the whole thing. o let X, Y, and Z be the score of a random student at each one of the three sections of the AT.",0.1580381471389645
36,36,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer highquality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: o ll briefly review what we were doing last time. We are gradually developing the structure that we need for thermodynamics. And the first thing that we did was to state that it wholly depends on the existence of equilibrim is the first and foremost thing that we need to know, is that the objects that we are studying in thermodynamics are in equilibrium. And once theyre in equilibrium, we can characterize them by a set of properties. o theres a list of coordinates that would have the list here. And we saw that since we will also need to think in terms of mechanical work, a good place to start and since we are familiar with mechanical coordinates is to enumerate here the mechanical coordinates that we would describe the system with. And we said that we could, in principal, characterize or divide those coordinates in the form of some generalized displacements and the corresponding conjugate forces. And typically, the displacements are extensive proportional to the size of your system. The forces are intensive. And the work is related to the product of those two in some fashion. One of the things that will do throughout is to emphasize that one of the main materials that we are going to look at, and illustrate the concept with, is the gas. And in particular, the version of the gas that is the ideal gas. But forgetting the ideal part for the time being, the coordinates that we use to describe the gas with is V and pressure. And actually, we saw that in order to ultimately be consistent with how we will define work, it is good to think of minus pressure as the appropriate force. And so basically, then if want to characterize the equilibrium state of a gas, need, for example, to specify where lie in this PV plane. OK so thats the first thing to know. But then we also realize that mechanical coordinates are insufficient to describe the properties of the systems that we encounter. For example, if we are thinking in terms of a spring and we pull on the spring so we are looking at the relationship between displacement and the force the result that we get how easy it is to pull on this depends on something else depends on temperature. o somehow, we need to include these properties that have to do with heat and heat transfer in order to have a complete description of systems that are in equilibrium. And so going along that direction, we started with the Zeroth Law. And the statement of the Zeroth Law was the transitivity of equilibrium if two objects are in equilibrium with each other they are two objects are in equilibrium with a third object, theyre also in equilibrium with each other. And we saw that what that means, is that once am at some point in this coordinate space, know that there exists some function. dont know its functional form. This empirical temperature and being in equilibrium means that there is this important function of the coordinates of the first system equal to some other functional or form potentially, depending on the coordinates of the second system. And this empirical temperature has to be the same. We said that this is kind of like being on a scale, and having a balance between different things, and then they are in balance with each other. o we know that theres something like mass. Now, when we describe this in the context of the gas, we noted that for all gases in the limit that they are dilute, the isotherms the places that would correspond in this case to always being in contact with the object is at a fixed temperature, form these hyperboles. o the isotherms in the limit of dilute are of the form PV is proportional to time span. We can call that theta. We can use that, even, to define the ideal gas temperature scale, provided that we choose some proportionality coefficient which was selected so that the temperature of water, ice, steam, coexistent at a particular value. OK, so thats the specific property of the ideal gas as far as its temperature is concerned. The next thing that we did was to look at how changes are carrying a system, a particular system. And we found that if you take the system from one location in its coordinate space to another location, do it in a manner that does not involve the exchange of heat. o basically, you idealize this thing and isolate it from any other source that the amount of work and we can then calculate mechanical work the amount of mechanical work that you do is only a function of the initial and final states, and does not depend on how you supply the work that produces this, et cetera. And so that immediately reminded us of conservation of energy, and a suggestion that there exists this function that depends on where you are in this coordinate representation that gives the total energy content of the system. What was important was that, of course, we want to relax the condition of making changes over which there is no heat exchange to the system. And more generally then, we said that the changes that we obtain in energy are the sum of the two components the amount of work that you do on it, and that we know how to measure from our various mechanical prescriptions. But if there is a shortcoming from the internal energy function that we had constructed previously, and the work that we compute for a particular process, we say that in that process, there was some heat that went into the system. And it is, again, important to state that this dE really only depends on the initial and final state. o this depends on state. While these things thats why we put a bar on them depend on the particular path that we take. Now again, if you ask, well, what can we say in this context for the ideal gas? And there is the very nice experiment by Joule where he said, lets isolate our gas, lets say, into two chambers. These are kind of rigid balls, very nicely isolated from the environment. nitially, the gas is confined completely onto one side. o lets say that the initial state is over here, with this volume. And then we release this. And so the gas goes and expands. And we wait sufficiently. We wait sufficiently. ltimately, were settled to a place, lets say over here the final state. Now, its important to say that dont know the intermediate state of this transformation. o cant say that somewhere between t equals to zero when open the valve, and t goes to infinity where the whole thing is settled. Zero and infinity points know. n between, cant really have an idea of where to put my system. t is at nonequilibrium by construction. But this is a process that by definition follow the path along which there was no input either in the form of work or heat into the system. t was isolated. o all know for sure is that however amount of energy had initially is what have finally. The observation of Joule was that if we do this for the case of a dilute gas, start with some initial temperature Ti. go to some final temperature. But that final temperature is the same as the initial temperature. And that is actually why put both of these points on the same isotherm that had drawn before. o although dont know anything about the in between points, know that this is the property of the system. And essentially, we know, therefore, that although in principle can write E as a function of P and V, it must be only a function of the product PV, because it only P changes in the process. V changes in the process. But PV remains constant. Or if you like, E is really a function of T, which is related to the product. And sorry about being kind of inconsistent with using T and theta for temperature. OK, any questions? o this is kind of recap of what we were doing. All right, now, it would be good if we can construct this function somehow at least theoretically, even. And you would say, well, if had this spring, and didnt know whether it was a Hookean or a nonlinear spring, what could do is could pull on it, calculate what the force is, and the displacement, and integrate f dx and use the formula that quite generally, the mechanical work is sum over i, lets say Ji dxi. Now, of course you know that can only do this if pull it on this spring sufficiently slowly, so that the work that do goes into changing the internal energy of the spring. And then that would be a contribution to dE. f do that rapidly, then it will send this spring into oscillation. have no idea, again, where the system is in the intermediate stages. o can use this kind of formula if, in this type of diagram that indicates the state of the system, proceed sufficiently slowly so that can put points that corresponding to all intermediate states. And so thats for things that are sufficiently slow and close to equilibrium that we call quasistatic. And so in principal, guess rather than opening this immediately, could have put a slow piston here, change its position slowly, so that at each stage can calculate where am on the PV diagram. And could have calculated what the work is, and used this formula to calculate the contribution of the work to the change in internal energy. OK, now said that it would be ideal and what we would like to do, ultimately is to have a similar formula for dQ. And if we look by analogy, we see that for W, you have Js times the Xs. The forces are the things that tell us whether systems are in equilibrium with each other. o two if have, lets say, a piston separating two parts of the gas, the piston will not move one direction or the other if the pressure from one direction is the same as the pressure from the other direction. o mechanical equilibrium tells us that Js are the same. And it suggests that if want to write a similar type of formula for heat, that the thing that should put here is temperature or empirical temperature, which is, again, a measure of thermal equilibrium. And then the question that would immediately jump onto you is, what is the conjugate that have put for the displacement? And you know ultimately that it is going to end up to be entropy. And so the next part of the story is to build that. And were going to do that through the econd Law of Thermodynamics. Any questions? Yes. ADENE: mean, this is kind of a semantics question, but what always confuses me in thermodynamics when we talk about quasistatic is, mean, if it isnt quasistatic, introductory formation of doesnt really give a means to address it. o we have to rely on the fact that its quasistatic. And the only way we could check it is to assume that what were already saying is true will be true. o guess my question is, in the context of a spring, like, theres obviously there exists a physical threshold at which thats not infinitely slow, and which you can get a reasonable measurement if youre doing an experiment. PROFEOR: o in all of these things, would sort of resort to some kind of a limiting procedure. o you could, for example, pull the spring at some velocity, and figure out what the amount of work is, and gradually reduce that velocity, and hope that the formula the values that you get are converging to something as you go to low velocity. dont think theres actually a threshold velocity, kind of implied that if you are slower than something, then it would work. think it really only works in the limit of zero velocity. And so if you like, thats kind of procedure you would use to define derivatives, right? o you cant say whats the meaning of velocity in physics. o, but it is an idealization. said at the beginning that the very actually, would say that more fundamentally, the thing that is really an idealization is this adiabatic walls. Even the concept of equilibrium what do we see around us that we are absolutely 100% sure in equilibrium? Nothing is. The fate of the universe is that all of our atoms and molecules are going to separate out and go to infinity, if it expands forever. Right? OK, econd Law all right, so again, we want to sort of set our minds back to where these laws were developed, which is in the 18th century. And the thing that was of importance at that time was sort of ndustrial Revolution get things moving. You need energy, and you would get energy from coal, and so you need to convert heat to work. And this conversion of heat to work is important to you. o typical thing that you have for your coal engine is you have something that is a source of heat. o theres, for example, a fire that youre burning. You have some kind of a machine that has components, such as a piston, that is being heated. And during this process, you extract a certain amount of heat from your fire. And then typically, you realize that you will be releasing a certain amount of energy back to the atmosphere, causing pollution. o there is a coal sink. But in the process, you will be extracting a certain amount of work. And what you want to do is to make the best use of this set up. o youre concerned with efficiency, which is defined to be the amount of work that youre going to extract out of a certain amount of heat that you expend in your coal, or whatever. And again, based on conservation of energy, we expect that this W to be less than QH by an amount that you are setting to the exhaust. And because of that, this eta has to be less than or equal to 1, in principal. Now, another device that is using the same rough principles is the same thing going in reverse, which is a refrigerator. o in order to cool this room, you use electricity or some other means of doing work on a machine whose job it is to extract heat out of the room. o lets say this is our room. We want to extract the heat. But of course, that heat has to go somewhere. o this has to have some kind of exhaust putting this again to the atmosphere or somewhere. o how good is this thing performing? o there is a measure of the performance figure of merit, if you like, for this which we will label by omega, which is how much heat you were able to remove from the room given some amount of work or energy that was put in. And again, because of conservation, this W is going to be QH minus Q. Now, this particular number, which is a useful measure of how well your refrigerator works, has no particular constraint. t can be less than 1. t can be larger than 1. o again, you want it to be as large as possible, and in principle, it can be 200 whatever. o clearly, already these are very much idealizations of some complicated process that is going on. And the thing that is interesting is that you can take a look at how well you are able to do these processes, and then make this equation that we wrote up here to be an exact differential form. o how you do that is an interesting thing that think is worth repeating and exploring. And essentially, you do that by formulating things that are possible or not possible through the econd Law, and then doing some mathematical manipulations. And we will use two formulations of the econd Law due to Kevin and lausius. o will indicate them either by K for Kelvin so Kelvins statement of the econd Law is the following no process is possible whose sole result is complete conversion of heat to work. o what that is, is a statement about how good an engine you can make. He says that you cannot make an engine that takes a certain amount of heat and completely makes it to work without having any waste Q. o basically, it says that there is no ideal engine, and that your eta has to be less than 1. There is a second variant of this, which is due to lausius says that no process is possible whose sole result is transfer of heat from cold to hot. o if you want to make an ideal refrigerator, what you want to do is to extract more and more Q for less and less W. o the ideal version would be where essentially, the limit of zero work, you would still be able to transfer heat from the room to the outside and get an ideal refrigerator. And lausius says that its not possible. o these are two statements of the econd Law. There are other versions and statements of the econd Law, but these are kind of most close to this historical perspective. And then the question is, well, which one of them do you want to use? And since will be kind of switching between using one or the other, it would be good if can show that they really are the same statement, and m really not making use of two different statements when use one or the other alternative. o what really want to do is to show that these are really, logically, the same statement. And essentially, what that means is that the two statements are equivalent. And two statements are equivalent to each other if one of them being incorrect which ve indicated by K with a bar on it implies that the other one is correct, and simultaneously, the other way around. And since this does not take more than a few minutes, and is a nice exercise, think its worth showing this. o lets say that somebody came to you and said that have constructed a machine that violates lausius. Well call that K bar. o what does that machine do? What that machine does is it converts heat, Q, completely to work, without needing to exhaust anything. o this W is this Q, and theres no exhaust machine. He says, OK have this machine. say, OK, what will do is will use that work to run a refrigerator. o connect that to a refrigerator. And that work will be used to take heat, Q, out of some room and exhaust it to the atmosphere. And will choose the exhaust location to be the same as the source of heat that had for my antiKelvin machine. OK? o then if you look at how the sum of these two machines is operating together, from the perspective of looking at the sum of the two machines, the work is just an internal operation. You dont really care. o as far as the net thing is concerned, what youre seeing is that there is no work involved externally. ts all internal. But what is happening is that youre pulling Q out of the room, and exhausting, presumably, what you have here QH minus Q to the other side. This, again, by conservation of energy must be the same as Q, right? o if you have a machine that violates Kelvin, you connect it to a refrigerator, and what you have is a machine that transfers heat from a coldair to a hotair body, and violates lausius. o thats the first part we have established. And to establish the second part, lets say that we have a machine that violates lausius. And this machine takes a certain amount of heat, Q, from the room, and deposits it to the hotter outside. What we will do immediately is to take some of this heat that has been generated by this machine and use it in a regular engine to create some amount of work. And then this has to take an amount of heat QH, deposit an amount of heat, Q, which we will select to be, essentially, the temperature or the room from which the anti lausius machine is operating. And then we will run this engine several times, or fractions of times, et cetera, and the other machine several times, ensuring that Q is the same as Q. That is, the engine puts out as much dumped heat as this antilausius machine is extracting from the room. Then if you look at the combined system, what we see is that the combined system is doing the following this was equivalent to this. This is equivalent to a combined system that does a certain amount of work. o there is a W that is coming out. There is no heat exchange with the room, because we ensured that these two heats are completely balanced. o you can sort of think of them as something that is internal to this bigger machine. And so what is happening is that theres a certain amount of heat, QH minus Q that is taken here that has to be equal to W, by conservation of energy. And we have converted heat entirely to work, violating Kelvins statement. o basically, we have proved this. Any questions? OK, one more step there is actually a couple more steps left before we get to construct our entropy function. And the next step is the arnot Engine. o this is yet another one of these idealizations that again, theoretical construct, which you can try to approach as in some limiting procedure. A arnot engine is any engine with the following properties that is, one, reversible, two, operates in a cycle, and three, all inputs all heat inputs outputs at two temperatures. OK, so lets go through the conditions. Reversible implies that at any stage, if change the directions of inputs and outputs, it will just go backward. o can go forward, backward, by reversing inputs, outputs. And so again, think of the case of our spring that we were pushing back and forth. n order for it to have this property of reversibility, should not only do it pulling down and up sufficiently slowly so that at each stage, can define its tension, but also as far as its connection to the outside world, have to do it frictionlessly, so that if were to reverse the direction of the force that have, it will either go forward or backward, reversing the path that it has. f there is any friction that is involved, then cannot do that. o essentially, this reversibility is some kind of a generalization of the frictionless condition that we would use in mechanics. The cycle part is easy. Essentially, it says that the start and end points are the same. And it kind of harks back to this statement that is part of either one of these formulations of the econd Law that is the net result or sole result is something. o we want the engine to essentially be back to where it was, so that when we are looking at changes that are involved, we say, OK, the engine is not part of the equation. t is where it was at the beginning and at the end of the cycle. Now theres something that didnt emphasize, but was and actually, was not really using, but you may have thought was using is that in all of these pictures that drew, said there is a hot place and theres a cold place. Now, didnt specify exactly what these are, and whether this corresponds to a particular temperature. Now, for the case of the arnot engine, have to be precise. have to say that this is always at one temperature. This is always at the other temperature. o these arnot engines are defined implicitly with two temperature labels corresponding to the hot part and the cold part. n principle, everything else that talked about could have had a range of temperatures. But the arnot engine says, OK, just two temperatures. OK, so for the arnot engine, if can now draw a diagram that would be kind of straight line version of what had before. And the arnot engine will be extracting a certain amount of heat, QH, from here, Q from here, doing a certain amount of work. And since it is reversible, could actually run it backward. could have W. could have QH, Q, and it would work as a refrigerator. Now, it is good to have at least one example that such a theoretical construct as a arnot engine can be made based on things that we have so far. And we can be explicit, and show that for the case of the ideal gas. o we said that a gas can represent in equivalent by a point in the pressurevolume diagram. o lets say that what is the working substance of this arnot engine is a gas. And part of this whole story is that it should be working between two isotherms the TH and T. But weve already established that we can have isotherms for the ideal gas that correspond to these curves that correspond to PV as constant. o could really choose one of the isotherms that we discussed before corresponds to TH. One corresponds to T. And can imagine taking a gas whose initial state is point A, and expanding it while maintaining it at this isotherm TH, and ending up at some point B. o as m expanding this, lets say we have put this gas. have this piston that contains this gas. y TH and T correspond to two baths that correspond to, if you like, lakes a temperature TH, lakes at temperature T. took my piston, put it into the hot bath, make it expand up to some point. And in the process, there is certainly a certain amount of heat that will go into the system, QH. o have taken QH out of the hot source. Now, next thing that want to do is to take this, and put it into the colder lake. But what the process is, is have to do it in a way that there is no heat exchange. o have to find the path that goes from the hot bath to the cold path to point , some point without any heat involved. Well have to ask how thats possible, and whats the structure of that path. Once m in the colder bath, then can expand my gas up to some point D. And clearly, have to choose this point D sufficiently precisely so that then, when take my gas from the cool to the original hot position, end up at precisely the location that started, so that have completed a cycle. o its necessary that should be able to construct these two paths, B and DA, that correspond to no heat exchange. And somehow m sure that can complete this cycle. o the paths B and DA are called adiabats, or adiabatic paths, in the sense that this is what you would get if you put your system in a container with these adiabatic walls that allows no exchange of heat. Now clearly, also want to do this sufficiently slowly so that infinitesimally, can draw this path as a series of points. And so dont want to do what was in the Joule expansion experiment, and suddenly expand the whole thing. have to do it sufficiently slowly so that the conditions of this arnot engine are satisfied. o along these paths, what do know? By definition dQ is 0. And dQ is dE minus dW along those paths. And since m performing these paths sufficiently slowly, can write the W as PdV minus PdV. Now, what you will show in the problem set is that dont need to make any assumption about the functional form of energy on P and V. Just knowing that m describing a system that is characterized by two coordinates, P and V, is sufficient for you to construct these curves. Turns out that if you have more than two coordinates three or four coordinates at this time you dont know that you will be able to do so. But were sticking with this. For simplicity, m going to choose this the form that have for the ideal gas, which know that its really a function only of the product PV. This is what we had used before. And again, so as to simplify the algebra, will use the form that is applicable to a monoatomic gas. And this really have no reason to do this, except that want to be able to get through this in two minutes with the algebra. o will write the energy to be 3/2 PV. o then for that particular choice, what have here is d of 3/2 PV plus PdV. And thats 3/2 PdV plus V, 3/2 VdP plus PdV, which is 5/2 PdV plus 3/2 VdP is 0. And you can rearrange that easily to dP over P plus 5/3 dV over V is 0. And this is simply the derivative of log of PV to the power of 5/3. And since this derivative is 0, you know that along the curves that correspond to no heat exchange, you have something like PV to the power of gamma in this case, gamma being 5/3 that is some constant. o after you have done your expansion, all you need is to, in this diagram, go continuously along the path that corresponds to this formula that we have indicated. And this formula describes a path that is distinct from the isotherm. o you start from somewhere. You start along some path. You are guaranteed to hit the other isotherm at some point. You start from the starting point. You go backward. Youre guaranteed to hit the isotherm at some point. And then you join those two points, and youve completed your cycle. And so you know that, at least for the case of the ideal gas, or actually for any twocoordinate system, you can construct a cardinal cycle. That is, put your material, go through a cycle that is reversible with all heat exchanges at two temperatures only. OK, now why is this idealization even useful? Well, we said that this arnot engine is stamped by really two temperatures TH and T. And the following theorem is important, which says that of all engines operating only between TH and T, the arnot engine is most efficient. o we are going to move away from this idealization that weve made, but so far, only in one small step. That is, we are still going to assume that all heat exchanges are done between TH and T. OK, and so lets say that we have some kind of an engine which does not have to be reversible. o because it does not have to be reversible, its not a arnot engine, but otherwise, operates completely between these two points. o it takes, lets say, heat QH prime, Q prime, and does a certain amount of work W. And so somebody comes and says, ve constructed this engine that is actually very good. ts better than your arnot engine. say, no, thats not possible. And the way that will prove it to you is as follows will take the output of your engine, connect it to a arnot engine, and since the arnot engine can be run backward, it will act as a refrigerator and extract heat and deposit heat precisely at the two temperatures that your engine is operating. o if somebody looks at the combination of these two, what do they see? The thing that they see for the combination is that there is some entity that is operating between TH and T. And there is some internal amount of work that is going on, and you dont care about that. But there is a certain amount of heat, QH prime minus QH that becomes Q prime minus Q. Now here we invoke lausius. lausius says that if you see heat going between two temperatures, it could have only gone from the direction hotter to colder, which means that this amount of heat has to be positive. QH prime should be greater than QH. o this is why lausius we know that this has to be case. o then what can do, is can divide both expressions by W. And then invert this thing. And if invert it, the inequality gets inverted. o get that W over QH prime is less than W over QH. And the lefthand side is the efficiency of my nonarnot engine. And the righthand side is the efficiency of my arnot engine. arnot And what ve shown is that the efficiency of the nonarnot engine has to be less than or equal to the efficiency of the arnot engine. OK, now, the next step is the following that all arnot engines operating between TH and T have the same efficiency. And the statement is, suppose have two different arnot engines arnot engine one and arnot engine two. can use one to run the other one backward, and would get that the efficiency of arnot engine one is less than or equal to arnot engine two, or the other way around that two is less than or equal to one. And hence, the only possibility is that they are the same, which is actually interesting, because told you that arnot engines are stamped by two temperatures, TH and T. And what it says is that they all have an efficiency that is a function, therefore, of these two temperatures only. t cannot depend on whether constructed my arnot engine out of an ideal gas. constructed it out of a rubber band that was pulling or pushing, or any other substance in the world that you can think about, as long as that substance can construct some kind of a arnot cycle, irrespective of what the coordinate systems are, what the isotherms look like, et cetera. Just working between two temperatures and two isotherms tells me that this reversible cycle will have this efficiency relating the amount of work and the amount of heat that these exchange as you go around. OK? Now, thats important because previously we wanted to define our temperature. And we had to rely on some property of the gas. We said OK, empirically we observe that for the gas, you have this relationship that when it is dilute, PV is proportional to T and we use that property of the gas to construct our ideal gas temperature scale. Well, thats OK, but it relies on some observations additionally to everything else that we had. Whereas now we have the means to define temperatures irrespective of any material. This is much more of a universal quantity. t doesnt depend on any substance, any particular structure. o its a much better way to rely for constructing a temperature scale. o in some sense, at least theoretically, we are going to abandon our previous temperature scale based on properties of the ideal gas, and reconstruct the temperature scale based on this. Any questions? OK. o this is what will be called the thermodynamic temperature scale. And what it amounts to is that somehow use the efficiency between two temperatures or vice versa use that there are two temperatures. Lets say one of them is a reference temperature. derive the temperature of the other one by the maximum efficiency of all engines that can operate between the reference temperature and the temperature that wish to measure. ts T. But that means before doing that, need to know at least something, or some kind of a postulate about the structure of this function of two variables. And cannot write any arbitrary function. t has to satisfy certain properties and symmetries. And can see what those properties are by putting a couple of these engines in series. o lets imagine that we have a structure where have the highest temperature T1, and intermediate temperature T2, and the lowest temperature T3. And put one arnot engine to operate between these two temperatures, and one to operate between these temperatures. And the first one is going to take heat that will called Q1. And release a heat that will call Q2, in the process doing a certain amount of work that will call W between 1 and 2. Now what will do is will use the entirety of that heat, Q2, to run the second arnot engine to temperature T3, reducing here the amount of heat, Q3, and doing a certain amount of work W23. learly what want to do is to say that when look at the combination of these things, what happens at temperature 2 is an intermediate. And the whole thing is equivalent to a arnot engine operating between T1 and T3. Again, the whole thing is reversible, operates between two temperatures, and since each one of them can be made to go back to where they started, is a cycle. o the whole thing is really equivalent to another arnot engine that takes Q1, deposits Q3, does a certain amount of work, W13, which is the sum of W12 plus W23. o clearly, these two different perspectives on the same operation will constrain some give some constraint between the form of the functions that will be describing the efficiencies of arnot engines. o lets follow that mathematically. OK, so the first one what do we have? By conservation of energy, have that Q2 is so this is let me be precise. This is arnot engine 1 tells me that Q2 is Q1 minus W12. All right, so how much heat have here is the difference between this and the amount of work that did conservation of energy. W12 is the efficiency times Q. That was how it was defined. o it is 1 minus the efficiency operating between T1 and T2 times Q1. What do we know about arnot engine number 2? arnot engine number 2 says that can look at Q3 as being the difference between Q2 minus the work that is done over here. The work is related to the heat through multiplying by the efficiency that is operating between T2 and T3. And if substitute for Q2 from the first equation, what do get? will get Q2 being Q1, 1 minus efficiency T1, T2, multiplying by the second bracket that is 1 minus efficiency T2 T3. But can also look at the composite arnot engine that is the sum total of them represented by the diagram on the right. And what that states just writing the whole thing same way is that this Q3 is the same as Q1 minus this amount of W13, which is the same thing as Q1, 1 minus the efficiency between T1 and T3. o suddenly you see that have two equations that relate Q3 and Q1 essentially two formulations of this ratio. And what that says is that the efficiency functions calculated between pairs of temperatures, when we look at triplet of them, have to be related by 1 minus efficiency between T1 and T3 is 1 minus efficiency between T1 and T2, 1 minus efficiency between T2 and T3. o when we are constructing our temperature scale based on properties of this efficiency function, we better choose and efficiency function that satisfies this equality. And very roughly again, where did this type of equality come from? t originated in the fact that this 1 minus eta was the ratio of two Qs. This was the ratio of Q2 over Q1, and then the other one was Q1 over Q3, and you would multiply them, and you would get Q2 over Q3, et cetera. And that gives you a hint as to how you should construct this function to satisfy the property that you want. f write the function as the ratio of some function of T2 divided by some function of T1, it would cancel in precisely the same way that when you were multiplying Q2s and Q1s, the cancellations occur. OK? o what need to do is to postulate that this is of this form. And then actually, m free to choose any function or form for F that want. o here we need to make a convention. And the convention that we make so this is this was not required is that this is really the functional form is linear. o this defines for you the thermodynamic temperature scale. Yes. ADENE: Yes, the part, the step right before the convention, where you wrote F of T1 over T1, is this also an assumption, or is it a necessity? PROFEOR: m, lets say, 99.9% sure that its a mathematical necessity. cant at this stage, think of precisely how to make that actually, yes, can make that rigor. o essentially think of writing this in the following way 1 minus eta of T1 and T2 equals to 1 minus eta of T1 and T3, 1 minus eta of T2 and T3. OK? The lefthand side depends on T1 and T2. The righthand side is the ratio of two functional forms involving T1 and T2 precisely as you have over here, as long as you regard T3 to be some arbitrary parameter or constant. OK? o thats the proof. All right, unfortunately, its like doing magic. Once you reveal the trick, it becomes not so interesting. Yes. ADENE: Does that mean more precisely if we assumed that T2 and T1 using F are empirical temperatures, and we define the different temperature to be F 2, something like that? PROFEOR: OK, what wanted to say next maybe answers to that. o why dont give that answer first, and then come back to you? s that so far, we have defined two temperatures. There is the thermodynamic temperature scale based on these efficiencies and there is one other statement, which is that this only defines the ratios. o need to also pick a reference temperature. And for reference temperature, pick the temperature of the coexistence of ice, steam, water, to be 273.16 degrees K, which is what have for the ideal gas scale. Because if dont do that, only know things up to some proportionality constant. f say that pick that reference temperature, then in principle what could do is, if somebody gives me a bath, can run a arnot engine between that bath of unknown temperature and this reference point, calculate the efficiency, 1 minus the efficiency, use this ratio 273.16 and have the temperature of my new object. o weve completed that thermodynamic temperature scale. f understood correctly the question that was posed, is, well, what about the empirical temperature? And was going to answer something different, which is not really what you were asking. o maybe you can repeat your question one more time. ADENE: f the, all of our computations where we assume that instead of Ts, there are some thetas. PROFEOR: Right. ADENE: And which means that we use the at last we define the thermodynamic temperature, it seems to have more precise. PROFEOR: Thats fine. o youre saying that what did with the Zeroth Law was to state that for somebody that exists in thermal equilibrium with a bath, can characterize it with the theta. And can go through the whole argument that had over here by constructing, lets say, arnot engines that operate between empirical temperatures defined as theta H and theta . And nowhere in the process have given you a number for what theta H and theta is up to this point. And at this point, defined efficiency, which have established is only a function of the empirical temperature operating between two isotherms, as some way through this process and this definition, giving an actual numerical value. o thats fine. Yes. ADENE: o should you also define temperature with ideal gas and infinite expansion? PROFEOR: Yes. ADENE: And doesnt it define what convention for effective we need to pick here? o for all the definitions to be consistent with each other? PROFEOR: OK, so now that becomes a different question. o think the first question was, lets not define any temperature scale up to this point. And this is the first time that we define a temperature scale. Now, your question is the one that wanted to originally answer, so thank you for the question, which is that actually did define for you a temperature scale through some property of the ideal gas. And indeed, that property of the ideal gas implicitly used here in the shape of the isotherms that had and constructing this energy functional in this fashion. Now you say well, OK, you defined temperature two different ways. Are they consistent with each other? And you will have a problem set that will ask the following question lets run an ideal gas arnot cycle between ideal gas temperatures theta H and theta . OK? Now you can and have drawn for you the form of this arnot cycle. sing PdV, you can calculate what amount of work goes into this at different stages. And the net work would be the area of this cycle. o W would give you the area of the cycle. You can calculate what QH is. And you can calculate the efficiency of this as a function theta H and theta . And what you will find, if you do things correctly, is that using all of these definitions for the shapes of these isotherms, you can calculate what the efficiency is. And the answer for your efficiency will come out to be theta H minus theta divided by theta H, which is precisely what you would have expected based on the arnot cycle. o then you can establish that the ratio of theta H to theta defined through the ideal gas temperature scale is the same as the ratio of the temperatures if you had defined through the thermodynamic. o the two scales, once you set the same point for both of them, become identical. Other questions? OK, so we went through all of this in order to ultimately get back to the story that was constructing at the beginning of the lecture, which was that d like to have the form ultimately for my energy. want to construct the energy. And if was looking only at mechanical systems, and there was no temperature in the world, would construct it as sum over i Ji dxi. But know that thats not enough, because there are processes by which can change the state by not doing any mechanical work go from one point in the diagram to another point in the diagram. o there must be something else. Now we said that it kind of makes a lot of sense that something else should be something like a temperature. And before that, really, the only thing that we had that was a form of temperature was the ideal gas temperature, or the empirical temperature that we had defined through the Zeroth Law. And neither of them had any nice connection to heat. Through this arnot engines, et cetera you have established some kind of a connection between heats, temperatures, et cetera. And thats why this thermodynamic temperature scale is useful. t is independent of any material. And its really the T that should put here. And so the next question is, what should put here? And that will also come through the econd Law. will develop that mostly the next time around, but ll give you in the next two or three minutes a preview. o the statement that we had was that the efficiency of any engine is less than the arnot engine, right? o lets imagine that we have some kind of a exchange process going between TH and T. And there is some engine dont know what that engine is that takes heat QH here, Q here, and does a certain amount of work. Now know that W over QH is less than or equal to 1 minus TH over T. And W is really the same thing as QH minus Q. o actually, could also have written this as 1 minus Q over QH is less than 1 minus T over T. can eliminate the 1, and rearrange things, yes? ADENE: ts T over TH. PROFEOR: 1 minus T over TH, thank you very much. Yes, and if you ever forget not that forgot miswrote but something to remember is that ultimately, was kind of hinting this if you ever forget Qs are proportional to Ts. o what have is that ideally, QH will be proportional to TH. Q will be proportional to T. o the difference between them would be proportional to the differences. Also, that means that if were to rearrange this slightly, what would get is that QH over TH plus minus Q over T has to be negative. o this is just a rearrangement of the whole thing. But this rearrangement wrote this as minus Q so as to look at things from the perspective of the engine. o the engine is something that goes through a cycle. As part of that cycle, it does some work. But what this expression has to do is the heat that goes into the system. o can regard minus Q as the heat that goes into the engine coming from a reservoir of temperature T, QH going through the engine, coming from a reservoir of temperature TH. And you have a relation such as this because the efficiency of any type of engine has to be less than this efficiency of the arnot engine that is stamped by the corresponding temperatures. OK, now this is kind of limited. And in order to be able to construct a general formula, we need to be able to make statements thats are relevant to arbitrary complex cycles and complex behaviors. o what we will start next time is to prove something that ultimately will allow us to quantify entropy. ts called lausius Theorem, which states the following imagine some kind of a generalization of your engine that takes place in some multidimensional space. o rather than really thinking about an engine, want to think about some substance. This was an ideal gas run through some nice cycle. But want to think about some substance that is described by multiple coordinate system. take the system from some point A, and the only requirement that place on it is that ultimately, do something, and come back to A. o it is a cycle. What dont even require is that this is a quasistatic process, so that the intermediate stages are welldefined in this coordinate space OK, so for any arbitrary cyclic transformation. OK, now this transformation well do is that at various stages just like that simple example well do work on the environment, take work from the environment. But most importantly from our perspective, there will be heat input into the system at various stages of the cycle. And m going to look at it from the perspective of what goes into the system, just like in this engine. o sometimes maybe this dQ is negative means that the system is really releasing heat, just like this engine was releasing the heat. Now, lausius Theorem says that if you integrate all around the cycle, these elements of heat that you input to the system throughout various stages of the cyclic transformation, and divide them by some T. And this is kind of something that needs definition. And is supposed to parametrize the cycle. Lets say goes from 0 to 1 as you go across the cycle. Generalizing that is negative. Next time, we will see if this system is nonequilibrium, what exactly mean by this T of . And we will see, somehow, that once define this, by doing part of this, can actually define an entropy function integral from A to B dQ over , and complete the definition of what needs to be put for the Q in order to give you an expression for T. OK.","The thing that they see for the combination is that there is some entity that is operating between TH and T. And there is some internal amount of work that is going on, and you dont care about that. And more generally then, we said that the changes that we obtain in energy are the sum of the two components the amount of work that you do on it, and that we know how to measure from our various mechanical prescriptions. That is, we are still going to assume that all heat exchanges are done between TH and T. OK, and so lets say that we have some kind of an engine which does not have to be reversible. o we want the engine to essentially be back to where it was, so that when we are looking at changes that are involved, we say, OK, the engine is not part of the equation. We said OK, empirically we observe that for the gas, you have this relationship that when it is dilute, PV is proportional to T and we use that property of the gas to construct our ideal gas temperature scale. o basically, you idealize this thing and isolate it from any other source that the amount of work and we can then calculate mechanical work the amount of mechanical work that you do is only a function of the initial and final states, and does not depend on how you supply the work that produces this, et cetera. o lets imagine that we have some kind of a exchange process going between TH and T. And there is some engine dont know what that engine is that takes heat QH here, Q here, and does a certain amount of work. And the first thing that we did was to state that it wholly depends on the existence of equilibrim is the first and foremost thing that we need to know, is that the objects that we are studying in thermodynamics are in equilibrium. And the way that will prove it to you is as follows will take the output of your engine, connect it to a arnot engine, and since the arnot engine can be run backward, it will act as a refrigerator and extract heat and deposit heat precisely at the two temperatures that your engine is operating. One of the things that will do throughout is to emphasize that one of the main materials that we are going to look at, and illustrate the concept with, is the gas. And you have a relation such as this because the efficiency of any type of engine has to be less than this efficiency of the arnot engine that is stamped by the corresponding temperatures. And hence, the only possibility is that they are the same, which is actually interesting, because told you that arnot engines are stamped by two temperatures, TH and T. And what it says is that they all have an efficiency that is a function, therefore, of these two temperatures only. Now, what you will show in the problem set is that dont need to make any assumption about the functional form of energy on P and V. Just knowing that m describing a system that is characterized by two coordinates, P and V, is sufficient for you to construct these curves. n order for it to have this property of reversibility, should not only do it pulling down and up sufficiently slowly so that at each stage, can define its tension, but also as far as its connection to the outside world, have to do it frictionlessly, so that if were to reverse the direction of the force that have, it will either go forward or backward, reversing the path that it has. o the statement that we had was that the efficiency of any engine is less than the arnot engine, right? And so what is happening is that theres a certain amount of heat, QH minus Q that is taken here that has to be equal to W, by conservation of energy. And we will see, somehow, that once define this, by doing part of this, can actually define an entropy function integral from A to B dQ over , and complete the definition of what needs to be put for the Q in order to give you an expression for T. OK. And the thing that is interesting is that you can take a look at how well you are able to do these processes, and then make this equation that we wrote up here to be an exact differential form. OK, so we went through all of this in order to ultimately get back to the story that was constructing at the beginning of the lecture, which was that d like to have the form ultimately for my energy. But what the process is, is have to do it in a way that there is no heat exchange. But if there is a shortcoming from the internal energy function that we had constructed previously, and the work that we compute for a particular process, we say that in that process, there was some heat that went into the system. Now what will do is will use the entirety of that heat, Q2, to run the second arnot engine to temperature T3, reducing here the amount of heat, Q3, and doing a certain amount of work W23. And before that, really, the only thing that we had that was a form of temperature was the ideal gas temperature, or the empirical temperature that we had defined through the Zeroth Law. And part of this whole story is that it should be working between two isotherms the TH and T. But weve already established that we can have isotherms for the ideal gas that correspond to these curves that correspond to PV as constant. And the convention that we make so this is this was not required is that this is really the functional form is linear. Now, when we describe this in the context of the gas, we noted that for all gases in the limit that they are dilute, the isotherms the places that would correspond in this case to always being in contact with the object is at a fixed temperature, form these hyperboles. arnot And what ve shown is that the efficiency of the nonarnot engine has to be less than or equal to the efficiency of the arnot engine. Now, lausius Theorem says that if you integrate all around the cycle, these elements of heat that you input to the system throughout various stages of the cyclic transformation, and divide them by some T. And this is kind of something that needs definition. What we will do immediately is to take some of this heat that has been generated by this machine and use it in a regular engine to create some amount of work. Well, we said that this arnot engine is stamped by really two temperatures TH and T. And the following theorem is important, which says that of all engines operating only between TH and T, the arnot engine is most efficient. And could have calculated what the work is, and used this formula to calculate the contribution of the work to the change in internal energy. Just working between two temperatures and two isotherms tells me that this reversible cycle will have this efficiency relating the amount of work and the amount of heat that these exchange as you go around. o there is a measure of the performance figure of merit, if you like, for this which we will label by omega, which is how much heat you were able to remove from the room given some amount of work or energy that was put in. o if you want to make an ideal refrigerator, what you want to do is to extract more and more Q for less and less W. o the ideal version would be where essentially, the limit of zero work, you would still be able to transfer heat from the room to the outside and get an ideal refrigerator. o what need to do is to postulate that this is of this form. And then this has to take an amount of heat QH, deposit an amount of heat, Q, which we will select to be, essentially, the temperature or the room from which the anti lausius machine is operating. All right, so how much heat have here is the difference between this and the amount of work that did conservation of energy. This empirical temperature and being in equilibrium means that there is this important function of the coordinates of the first system equal to some other functional or form potentially, depending on the coordinates of the second system. Now, your question is the one that wanted to originally answer, so thank you for the question, which is that actually did define for you a temperature scale through some property of the ideal gas. take the system from some point A, and the only requirement that place on it is that ultimately, do something, and come back to A. o it is a cycle. o then you can establish that the ratio of theta H to theta defined through the ideal gas temperature scale is the same as the ratio of the temperatures if you had defined through the thermodynamic. And since this derivative is 0, you know that along the curves that correspond to no heat exchange, you have something like PV to the power of gamma in this case, gamma being 5/3 that is some constant. Now, of course you know that can only do this if pull it on this spring sufficiently slowly, so that the work that do goes into changing the internal energy of the spring. And what you will find, if you do things correctly, is that using all of these definitions for the shapes of these isotherms, you can calculate what the efficiency is. f say that pick that reference temperature, then in principle what could do is, if somebody gives me a bath, can run a arnot engine between that bath of unknown temperature and this reference point, calculate the efficiency, 1 minus the efficiency, use this ratio 273.16 and have the temperature of my new object. derive the temperature of the other one by the maximum efficiency of all engines that can operate between the reference temperature and the temperature that wish to measure. And what that states just writing the whole thing same way is that this Q3 is the same as Q1 minus this amount of W13, which is the same thing as Q1, 1 minus the efficiency between T1 and T3. And you would say, well, if had this spring, and didnt know whether it was a Hookean or a nonlinear spring, what could do is could pull on it, calculate what the force is, and the displacement, and integrate f dx and use the formula that quite generally, the mechanical work is sum over i, lets say Ji dxi. And again, based on conservation of energy, we expect that this W to be less than QH by an amount that you are setting to the exhaust. For simplicity, m going to choose this the form that have for the ideal gas, which know that its really a function only of the product PV. o you could, for example, pull the spring at some velocity, and figure out what the amount of work is, and gradually reduce that velocity, and hope that the formula the values that you get are converging to something as you go to low velocity. o lets say that what is the working substance of this arnot engine is a gas. And m going to look at it from the perspective of what goes into the system, just like in this engine. And in particular, the version of the gas that is the ideal gas. And it suggests that if want to write a similar type of formula for heat, that the thing that should put here is temperature or empirical temperature, which is, again, a measure of thermal equilibrium. Then if you look at the combined system, what we see is that the combined system is doing the following this was equivalent to this. The work is related to the heat through multiplying by the efficiency that is operating between T2 and T3. And it kind of harks back to this statement that is part of either one of these formulations of the econd Law that is the net result or sole result is something. But what this expression has to do is the heat that goes into the system. He says that you cannot make an engine that takes a certain amount of heat and completely makes it to work without having any waste Q. o basically, it says that there is no ideal engine, and that your eta has to be less than 1. OK, now, the next step is the following that all arnot engines operating between TH and T have the same efficiency. And in the process, there is certainly a certain amount of heat that will go into the system, QH. And what you want to do is to make the best use of this set up. OK, now said that it would be ideal and what we would like to do, ultimately is to have a similar formula for dQ. Now you can and have drawn for you the form of this arnot cycle. can use one to run the other one backward, and would get that the efficiency of arnot engine one is less than or equal to arnot engine two, or the other way around that two is less than or equal to one. And so that immediately reminded us of conservation of energy, and a suggestion that there exists this function that depends on where you are in this coordinate representation that gives the total energy content of the system. And that work will be used to take heat, Q, out of some room and exhaust it to the atmosphere. And this conversion of heat to work is important to you. But forgetting the ideal part for the time being, the coordinates that we use to describe the gas with is V and pressure. And this is the first time that we define a temperature scale. And we can be explicit, and show that for the case of the ideal gas. For example, if we are thinking in terms of a spring and we pull on the spring so we are looking at the relationship between displacement and the force the result that we get how easy it is to pull on this depends on something else depends on temperature. And then the question that would immediately jump onto you is, what is the conjugate that have put for the displacement? constructed it out of a rubber band that was pulling or pushing, or any other substance in the world that you can think about, as long as that substance can construct some kind of a arnot cycle, irrespective of what the coordinate systems are, what the isotherms look like, et cetera. And then the question is, well, which one of them do you want to use? o can use this kind of formula if, in this type of diagram that indicates the state of the system, proceed sufficiently slowly so that can put points that corresponding to all intermediate states. But what is happening is that youre pulling Q out of the room, and exhausting, presumably, what you have here QH minus Q to the other side. lausius says that if you see heat going between two temperatures, it could have only gone from the direction hotter to colder, which means that this amount of heat has to be positive. And this really have no reason to do this, except that want to be able to get through this in two minutes with the algebra. But this is a process that by definition follow the path along which there was no input either in the form of work or heat into the system. And so you know that, at least for the case of the ideal gas, or actually for any twocoordinate system, you can construct a cardinal cycle. And actually, we saw that in order to ultimately be consistent with how we will define work, it is good to think of minus pressure as the appropriate force. o youre concerned with efficiency, which is defined to be the amount of work that youre going to extract out of a certain amount of heat that you expend in your coal, or whatever. And since will be kind of switching between using one or the other, it would be good if can show that they really are the same statement, and m really not making use of two different statements when use one or the other alternative. And indeed, that property of the ideal gas implicitly used here in the shape of the isotherms that had and constructing this energy functional in this fashion. What was important was that, of course, we want to relax the condition of making changes over which there is no heat exchange to the system. have to say that this is always at one temperature. t is where it was at the beginning and at the end of the cycle. Now, for the case of the arnot engine, have to be precise. Once m in the colder bath, then can expand my gas up to some point D. And clearly, have to choose this point D sufficiently precisely so that then, when take my gas from the cool to the original hot position, end up at precisely the location that started, so that have completed a cycle. o typical thing that you have for your coal engine is you have something that is a source of heat. o after you have done your expansion, all you need is to, in this diagram, go continuously along the path that corresponds to this formula that we have indicated. And so the next part of the story is to build that. learly what want to do is to say that when look at the combination of these things, what happens at temperature 2 is an intermediate.",0.1188118811881188
37,37,"assume from high school you know how to add and multiply complex numbers using the relation i squared equals negative one. m a little less certain that you remember how to divide them. hope you read last night by way of preparation for that, but since thats something were going to have to do a lot of a differential equations, so remember that the division is done by making use of the complex conjugate. o, if z is equal to a plus bi, some people write a plus ib, and sometimes ll do that too if its more convenient. Then, the complex conjugate is what you get by changing i to negative i. And, the important thing is that the product of those two is a real number. The product of these is a squared minus the quantity ib all squared, which makes a squared plus b squared because i squared is negative one. o, the product of those, thats what you multiply if you want to multiply this by something to make it real. You always multiplied by its complex conjugate. And thats the trick that underlines the doing of the division. o, for example, better hang onto these or ll never remember all the examples. uppose, for example, we wanted to calculate (two plus i) divided by (one minus 3 i). To calculate it means want to do the division; want to express the answer in the form a plus bi. What you do is multiply the top and bottom by the complex conjugate of the denominator in order to make it real. o, its (one plus 3i) divided by (one plus 3i), as they taught you in elementary school, that is one, in a rather odd notation; therefore, multiplying doesnt change the value of the fraction. And so, the denominator now becomes 1 squared plus 3 squared, which is ten. And, the numerator is, learn to do this without multiplying out four terms. You must be able to do this in your head. And, you always do it by the grouping, or post office method, whatever you want to call it, namely, first put down the real part, which is made out of two times one minus three times one. o, thats negative one. And then, the imaginary part, which is i times one. Thats one, coefficient one, plus 6i. o, that makes 7i. Now, some people feel this still doesnt look right, if you wish, and for some places and differential equations, it will be useful to write that as minus one tenth plus seven tenths i. And, now its perfectly clear that its in the form a plus bi. o, learn to do that if you dont know already. ts going to be important. Now, the main thing today is the polar representation, which sometimes they dont get to in high school. And if they do, its usually not in a grown upenough in a form for us to be able to use it. o, have to worry about that little bit. The polar representation, of course, is nominally just the switch to polar coordinates. f heres a plus bi, then this is r, and thats theta. And therefore, this can be written as, in the polar form, that would be r cosine theta plus i, or r cosine theta. Thats the A part. And, the B part is, the imaginary part is r sin(theta) times i. Now, it would be customary, at this point, to put the i in front, just because it looks better. The complex numbers are commutative, satisfied to commutative law of multiplication, which means it doesnt matter in multiplication whether you put i in front or behind. ts still the same answer. o, this would be r cosine theta plus i times r sine theta, which, of course, will factor out, and will make it cosine theta plus i sine theta. Now, it was Euler who took the decisive step and said, hey, look, m going to call that e to the i theta. Now, why did he do that? Because everything seemed to indicate that it should. But thats certainly worth the best color we have, which is what? We are getting low here. Okay, nonetheless, its worth pink. will even give him his due, Euler. ometimes its called Eulers formula, but it really shouldnt be. ts not a formula. ts a definition. o, in some sense, you cant argue with it. f you want to call putting a complex number in a power, and calling it that, you can. But, one can certainly ask why he did it. And the answer, guess, is that all the evidence seemed to point to the fact that it was the thing to do. Now, think its important to talk about a little bit because think its, in my opinion, if youre seeing this for the first time, even if you read about it last night, its a mysterious thing, and one needs to see it from every possible point of view. ts something you get used to. You will never see it in a sudden flash of insight. t will just get as familiar to you as more common arithmetic, and algebraic, and calculus processes are. But, look. What is it we demand? f youre going to call something an exponential, what is it we want an exponential to do, what gives an expression like this the right to be called e to the i theta? The answer is cant creep inside Eulers mind. t must have been a very big day of his life. He had a lot of big days, but when he realized that that was the thing to write down as the definition of e to the i theta. But, what is it one wants of an exponential? Well, the high school answer surely is you want it to satisfy the exponential law. Now, to my shock, realize a lot of people dont know. n my analysis class, these are some math majors, or graduate engineers in various subjects, and if say prove such and such using the exponential law, m sure to get at least half a dozen emails asking me, whats the exponential law? Okay, the exponential law is a to the x times a to the y equals a to the x plus y: the law of exponents. Thats the most important reason why, thats the single most important thing about exponents, are the way one uses them. And, this is the exponential function, called the exponential function because all this significant stuff is in the exponents. All right, so it should satisfy we want, first of all, the exponential law to be true. But thats not all. Thats a high school answer. An T answer would be, mean, why is e to the x such a popular function? Well, of course, it does satisfy the exponential law, but for us, an even more reasonable thing. ts the function, which, when you differentiate it, you get the same thing you started with. And, its apart from a constant factor, the only such function. Now, in terms of differential equations, it means that its the solution that e to the, lets be a little generous, make it e to the ax. No, better not to use x because complex numbers tend to be called x plus iy. Lets use t as a more neutral variable, which is standing outside the fray, as it were. t satisfies the relationship that its the solution, if you like, to the differential equation. Thats a fancy way of saying it. dy / dt equals a times y. Now, of course, that is not unique. We could make it unique by putting in an initial value. o, if want to get this function and not a constant times it, should make this an initial value problem and say that y of zero should be one. And now, will get only the function, e to the at. o, in other words, that characterizes this function. ts the only function in the whole world that has that property. Now, if youre going to call something e to the i theta, we want that to be true. o, here are my questions. s it true that e to the i theta one, lets use that, times e to the i theta two, see, m on a collision course here, but thats easily fixed. s that equal to e to the i (theta one plus theta two)? f that turns out to be so, thats a big step. What would we like to be true here? Well, will it be true that the derivative, with respect to t of e to the i theta, would like that to be equal to i times e to the i theta. o, question, question. think those are the two most significant things. Now, the nodes do a third thing, talk about infinite series. ince we havent done infinite series, anyway, its not officially part of the syllabus, the kind of power series that are required. But, will put it down for the sake of completeness, as people like to say. o, it should behave right. The infinite series should be nice. The infinite series should work out. There is no word for this, should work out, lets say. mean, whats the little music? s that some weird music idea, or is it only me that hears it? Yes, Lord. feel m being watched up there. This is terrible. o, theres one guy. Heres another guy. And, wont put a box around the infinite series, since m not going to say anything about it. Now, these things, in fact, are both true. Otherwise, why would be saying them, and why would Euler have made the formula? But, whats interesting to see is whats behind them. And, that gives you little practice also in calculating with the complex numbers. o, lets look at the first one. What will it say? t is asking the question. t says, please, calculate the product of these two things. Okay, do it, m told. will calculate the product of cosine theta one plus i cosine theta two ine. ine theta one. Thats e to the i theta one, right? o, that corresponds to this. The other factor times the other factor, cosine theta two plus i sine theta two. Okay, what does that come out to be? Well, again, we will use the method of grouping. Whats the real part of it? The real part of it is cosine theta one cosine theta two. And then, theres a real part, which comes from these two factors. ts going to occur with a minus sign because of the i squared. And, whats left is sine theta one sine theta two. And then, the imaginary part, ll factor out the i. And then, whats left, wont have to keep repeating the i. o, it will have to be sine theta one cosine theta two. And, the other factor will be cosine theta one sine theta two plus sine theta two cosine theta one. Well, it looks like a mess, but, again, high school to the rescue. What is this? The top thing is nothing in disguise, but its a disguised form of cosine (theta one plus theta two). And the bottom is sine of (theta one plus theta two). o, the product of these two things is this, and thats exactly the formula. n other words, this formula is a way of writing those two trigonometric identities for the cosine of the sum and the sine of the sum. nstead of the two identities taking up that much space, written one after the other, they take up as much space, and they say exactly the same thing. Those two trigonometric identities are exactly the same as saying that e to the i theta satisfies the exponential law. Now, people ask, you know, whats beautiful in mathematics? To me, thats beautiful. think thats great. omething long turns into something short, and its just as good, and moreover, connects with all these other things in the world, differential equations, infinite series, blah, blah, blah, blah, blah. Okay, dont have to sell Euler. He sells himself. Now, how about the other one? How about the other one? Now, thats obviously, havent said something because for one thing, how do you differentiate if theres theta here, and t down there. Okay, thats easily fixed. But, how do differentiate this? What kind of a guy is e to the i theta? Well, if write it out, take a look at what it is. ts cosine theta plus i sine theta. As theta varies, its a function. The variable is real. Theta is a real variable. ts angle in radians, but it runs from negative infinity to infinity. o, if you think of functions as a black box, whats going in is a real number. But, whats coming out is a complex number. o, schematically, here is the e to the i theta box, if you like to think that way, theta goes in, and thats real, and a complex number, this particular complex number goes out. o, one, wed call it, m not going to write this down because its sort of pompous and takes too long. But, it is a complex valued function of a real variable. You got that? p to now, we studied real functions of real variables. But now, real valued functions of real variables, those are the kind calculus is concerned with. But now, its a complexvalued function because the variable is real. But, the output, the value of the function is a complex number. Now, in general, such a function, well, maybe a better say, complexvalued, how about complexvalued function of a real variable, lets change the name of the variable. t is always a real variable. dont think we have complex time yet, although m sure there will be someday. But, the next Einstein appears. A complexvalued function of a real variable, t, in general, would look like this. t goes in, and what comes out? Well: a complex number, which would then have to write this way. n other words, the real part depends on t, and the imaginary part depends upon t. o, a general function looks like this, a general complexvalued function. This is just a special case of it, where the variable has a different name. But, the first function would be cosine t, and the second function would be sine t. o, my only question is, how do you differentiate such a thing? Well, m not going to fuss over this. The general definition is, with deltas and whatnot, but the end result of a perfectly fine definition is, you differentiate it by differentiating each component. The reason you dont have to work so very hard is because this is a real variable, and already know what it means to differentiate a function of a real variable. o, could write it this way, that the derivative of u plus iv, ll abbreviate it that way, this means the derivative, with respect to whatever variable, since didnt tell you what the variable in these functions were, well, dont have to tell you what m differentiating with respect to. ts whatever was there because you cant see. And the answer is, it would be the derivative of u plus i times the derivative of v. You differentiate it just the way you would if these were the components of a motion vector. You would get the velocity by differentiating each component separately. And, thats what youre doing here. Okay, now, the importance of that is that it at least tells me what it is have to check when check this formula. o, lets do it now that we know what this is. We know how to differentiate the function. Lets actually differentiate it. Thats fortunately, by far, the easiest part of the whole process. o, lets do it. o, whats the derivative? Lets go back to t, our generic variable. want to emphasize that these functions, when we write them as functions, that theta will almost never be the variable outside of these notes on complex numbers. t will normally be time or something like that, or x, a neutral variable like x. o, whats the derivative of e to the i theta? m hoping that it will turn out to be i e to the i theta, and that the yellow law may be true just as the green one was. Okay, lets calculate it. ts the derivative, with respect to, unfortunately can convert ts to thetas, but not thetas to ts. est la vie, okay. Times cosine t plus i sine t, and whats that? Well, the derivative of cosine t, differentiating the real and imaginary parts separately, and adding them up. ts negative sine t, plus i times cosine t. Now, lets factor out at the i, because it says if factor out the i, what do get? Well, now, the real part of whats left would be cosine t. And, how about the imaginary part? Do you see, it will be i sine t because i times i gives me that negative one. And, whats that? e to the it. i times e to the i t. o, that works too. What about the initial condition? No problem. What is y of zero? Whats the function at zero? Well, dont say right away, i times zero is zero, so it must be one. Thats illegal because, why is that illegal? ts because in that formula, you are not multiplying i times theta. mean, sort of, you are, but that formula is the meaning of e to the i theta. Now, it would be very nice if this is like, well, anyway, you cant do that. o, you have to do it by saying its the cosine of zero plus i times the sine of zero. And, how much is that? The sine of zero is zero. Now, its okay to say i times zero is zero because thats the way complex numbers multiply. What is the cosine of zero? Thats one. o, the answer, indeed, turns out to be one. o, this checks, really, from every conceivable standpoint down as indicated, also from the standpoint of infinite series. o, we are definitely allowed to use this. Now, the more general exponential law is true. m not going to say much about it. o, in other words, e to the a, this is really a definition. e to the (a plus ib) is going to be, in order for the general exponential law to be true, this is really a definition. ts e to the a times e to the ib. Now, notice when look at the at any complex number, so, in terms of this, the polar form of a complex number, to draw the little picture again, if here is our complex number, and here is r, and here is the angle theta, so the nice way to write this complex number is r e to the i theta. The e to the i theta is, now, why is that? What is the magnitude of this? This is r. The length of the absolute value, didnt talk about magnitude in argument. guess should have. But, its in the notes. o, r is called the modulus. Well, the fancy word is the modulus. And, we havent given the complex number a name. Lets call it alpha, modulus of alpha, and theta is called, its the angle. ts called the argument. didnt make up these words. There, from a tradition of English that has long since vanished, when was a kid, and you wanted to know what a play was about, you looked in the playbill, and it said the argument of the play, its that oldfashioned use of the word argument. Argument means the angle, and sometimes thats abbreviated by arg alpha. And, this is abbreviated, of course, as absolute value of alpha, its length. Okay, the notes give you a little practice changing things to a polar form. think we will skip that in favor of doing a couple of other things because thats pretty easy. But let me, you should at least realize when you should look at polar form. The great advantage of polar form is, particularly once youve mastered the exponential law, the great advantage of polar form is its good for multiplication. Now, of course, you know how to multiply complex numbers, even when they are in the artesian form. Thats the first thing you learn in high school, how to multiply a plus bi times c plus di. But, as you will see, when push comes to shove, you will see this very clearly on Friday when we talk about trigonometric inputs to differential equations, that the changing to complex numbers makes all sorts of things easy to calculate, and the answers come out extremely clear, whereas if we had to do it any other way, its a lot more work. And worst of all, when you finally slog through to the end, you fear you are none the wiser. ts good for multiplication because the product, so heres any number in its polar form. Thats a general complex number. ts modulus times e to the i theta times r two e to the i theta two Well, you just multiply them as ordinary numbers. o, the part out front will be r1 r2, and the e to the i theta parts gets multiplied by the exponential law and becomes e to the i (theta one plus theta two) which makes very clear that the multiply geometrically two complex numbers, you multiply the moduli, the rs, the absolute values, how long the arrow is from zero to the complex number, multiply the moduli, and add the arguments. o the new number, its modulus is the product of r1 and r2. And, its argument, its angle, polar angle, is the sum of the old two angles. And, you add the angles. And, you put down in your books angles, but m being photographed, so m going to write arguments. n other words, it makes the geometric content of multiplication clear, in a sense in which this is extremely unclear. From this law, blah, blah, blah, blah, blah, whatever it turns out to be, you have not the slightest intuition that this is true about the complex numbers. That first thing is just a formula, whereas this thing is insightful representation of complex multiplication. Now, d like to use it for something, but before we do that, let me just indicate how just the exponential notation enables you to do things in calculus, formulas that are impossible to remember from calculus. t makes them very easy to derive. A typical example of that is, suppose you want to, for example, integrate (e to the negative x) cosine x. Well, number one, you spend a few minutes running through a calculus textbook and try to find out the answer because you know you are not going to remember how to do it. Or, you run to a computer, and type in atlab and something. Or, you fish out your little pocket calculator, which will give you a formula, and so on. o, you have aides for doing that. But, the way to do it if youre on a desert island, and the way always do it because never have any of these little aides around, and cannot trust my memory, probably a certain number of you remember how you did it at high school, or how you did it in 18.01, if you took it here. You have to use integration by parts. But, its one of the tricky things thats not required on an exam because you had to use integration by parts twice in the same direction, and then suddenly by comparing the end product with the initial product and writing an equation. omehow, the value falls out. Well, thats tricky. And its not the sort of thing you can waste time stuffing into your head, unless you are going to be the integration bee during AP or something like that. nstead, using complex numbers is the way to do this. How do think of this, cosine x? What do, is think of that e to the negative x cosine x is the real part, the real part of what? Well, cosine x is the real part of e to the ix. o, this thing, this is real. This is real, too. But m thinking of it as the real part of e to the ix. Now, if multiply these two together, this is going to turn out to be, therefore, the real part of e to the minus x. ll write it out very pompously, and then will fix it. would never write this, you are you. Okay, its e to the minus x times, when write cosine x plus i sine x, so it is the real part of that is cosine x. o, its the real part of, write it this way for real part of e to the, factor out the x, and whats up there is (negative one plus i) times x. Okay, and now, so, the idea is the same thing is going to be true for the integral. This is going to be the real part of that, the integral of e to the (minus one plus i) times x dx. n other words, what you do is, this procedure is called complexifying the integral. nstead of looking at the original real problem, m going to turn it into a complex problem by turning this thing into a complex exponential. This is the real part of that complex exponential. Now, whats the advantage of doing that? imple. ts because nothing is easier to integrate than an exponential. And, though you may have some doubts as to whether the familiar laws work also with complex exponentials, assure you they all do. t would be lovely to sit and prove them. On the other hand, think after a while, you find it rather dull. o, m going to do the fun things, and assume that they are true because they are. o, whats the integral of e to the (minus one plus i) x dx? Well, if there is justice in heaven, it must be e to the (minus one plus i) times x divided by minus one plus i. n some sense, thats the answer. This does, in fact, give that. Thats correct. want the real part of this. want the real part because thats the way the original problem was stated. want the real part only. o, want the real part of this. Now, this is what separates the girls from the women. This is why you have to know how to divide complex numbers. o, watch how find the real part. write it this way. Normally when do the calculations for myself, would skip a couple of these steps. But this time, will write everything out. Youre going to have to do this a lot in this course, by the way, both over the course of the next few weeks, and especially towards the end of the term where we get into a complex systems, which involve complex numbers. Theres a lot of this. o, now is the time to learn to do it, and to feel skillful at it. o, its this times e to the negative x times e to the ix, which is cosine x plus i sine x. Now, m not ready, yet, to do the calculation to find the real part because dont like the way this looks. want to go back to the thing did right at the very beginning of the hour, and turn it into an a plus bi type of complex number. n other words, what we have to do is the division. o, the division is going to be, now, m going to ask you to do it in your head. multiply the top and bottom by negative one minus . What does that put in the denominator? One squared plus one squared: Two. And in the numerator, negative one minus i. This is the same as that. But now, it looks at the form a + bi. ts negative one over two minus i times one half. o, this is multiplied by e to the minus x and cosine x. o, if you are doing it, and practice a little bit, please dont put in all these steps. Go from here; well, would go from here to here by myself. aybe you shouldnt. Practice a little before you do that. And now, what do we do with this? Now, this is in a form to pick out the real part. We want the real part of this. o, you dont have to write the whole thing out as a complex number. n other words, you dont have to do all the multiplications. You only have to find the real part of it, which is what? Well, e to the negative x will be simply a factor. Thats a real factor, which dont have to worry about. And, in that category, can include the two also. o, only have to pick out the real part of this times that. And, whats that? ts negative cosine x. And, the other real part comes from the product of these two things. times negative i is one. And, whats left is sine x. o, thats the answer to the question. Thats the integral of e to the negative x * cosine x. Notice, its a completely straightforward process. t doesnt involve any tricks, unless you call going to the complex domain a trick. But, dont. As soon as you see in this course the combination of e to ax times cosine bx or sine bx, you should immediately think, and youre going to get plenty of it in the couple of weeks after the exam, you are going to get plenty of it, and you should immediately think of passing to the complex domain. That will be the standard way we solve such problems. o, youre going to get lots of practice doing this. But, this was the first time. Now, guess in the time remaining, m not going to talk about in the notes, i, R, at all, but would like to talk a little bit about the extraction of the complex roots, since you have a problem about that and because its another beautiful application of this polar way of writing complex numbers. uppose want to calculate. o, the basic problem is to calculate the nth roots of one. Now, in the real domain, of course, the answer is, sometimes theres only one of these, one itself, and sometimes there are two, depending on whether n is an even number or an odd number. But, in the complex domain, there are always n answers as complex numbers. One always has n nth roots. Now, where are they? Well, geometrically, its easy to see where they are. Heres the unit circle. Heres the unit circle. One of the roots is right here at one. Now, where are the others? Well, do you see that if place, lets take n equal five because thats a nice, dramatic number. f place these peptides equally spaced points around the unit circle, so, in other words, this angle is alpha. Alpha should be the angle. What would be the expression for that? f there were five such equally spaced, it would be one fifth of all the way around the circle. All the way around the circle is two pi. o, it will be one fifth of two pi in radians. Now, its geometrically clear that those are the five fifth roots because, how do multiply complex numbers? multiply the moduli. Well, they all have moduli one. o, if take this guy, lets call that complex number, oh, hate to give you, they are always giving you Greek notation. All right, why not torture you? Zeta. At least you will learn how to make a zeta in this period, small zeta, so thats zeta. Theres our fifth root of unity. ts the first one that occurs on the circle that isnt the trivial one, one. Now, do you see that, how would calculate zeta to the fifth? Well, if write zeta in polar notation, what would it be? The modulus would be one, and therefore it will be simply, the r will be one for it because its length is one. ts modulus is one. Whats up here? times that angle, and that angle is two pi over five. o, theres just, geometrically see where zeta is. And, if translate that geometry into the e to the i theta form for the formula, see that it must be that number. Now, lets say somebody gives you that number and says, hey, is this the fifth root of one? forbid you to draw any pictures. What would you do? You say, well, ll raise it to the fifth power. Whats zeta to the fifth power? Well, its e to the i two pi / five, and now, if think of raising that to the fifth power, by the exponential law, thats the same thing as putting a five in front of the exponent. o, this times five, and whats that? Thats e to the i times two pi. And, what is that? Well, its the angle. f the angle is two pi, ve gone all the way around the circle and come back here again. ve got the number one. o, this is one. ince the argument, two pi, is the same as an angle, its the same as, well, lets not write it that way. ts wrong. ts just wrong since two pi and zero are the same angle. o, could replace this by zero. Oh dear. Well, guess have to stop right in the middle of things. o, youre going to have to read a little bit about how to find roots in order to do that problem. And, we will go on from that point Friday.","Okay, its e to the minus x times, when write cosine x plus i sine x, so it is the real part of that is cosine x. o, its the real part of, write it this way for real part of e to the, factor out the x, and whats up there is (negative one plus i) times x. Okay, and now, so, the idea is the same thing is going to be true for the integral. This is going to be the real part of that, the integral of e to the (minus one plus i) times x dx. What do, is think of that e to the negative x cosine x is the real part, the real part of what? o, the part out front will be r1 r2, and the e to the i theta parts gets multiplied by the exponential law and becomes e to the i (theta one plus theta two) which makes very clear that the multiply geometrically two complex numbers, you multiply the moduli, the rs, the absolute values, how long the arrow is from zero to the complex number, multiply the moduli, and add the arguments. Well, will it be true that the derivative, with respect to t of e to the i theta, would like that to be equal to i times e to the i theta. The e to the i theta is, now, why is that? o, its this times e to the negative x times e to the ix, which is cosine x plus i sine x. Now, m not ready, yet, to do the calculation to find the real part because dont like the way this looks. e to the it. Then, the complex conjugate is what you get by changing i to negative i. And, the important thing is that the product of those two is a real number. Now, notice when look at the at any complex number, so, in terms of this, the polar form of a complex number, to draw the little picture again, if here is our complex number, and here is r, and here is the angle theta, so the nice way to write this complex number is r e to the i theta. This is the real part of that complex exponential. You only have to find the real part of it, which is what? Well, its e to the i two pi / five, and now, if think of raising that to the fifth power, by the exponential law, thats the same thing as putting a five in front of the exponent. What you do is multiply the top and bottom by the complex conjugate of the denominator in order to make it real. Now, if multiply these two together, this is going to turn out to be, therefore, the real part of e to the minus x. ll write it out very pompously, and then will fix it. Okay, the exponential law is a to the x times a to the y equals a to the x plus y: the law of exponents. And the answer, guess, is that all the evidence seemed to point to the fact that it was the thing to do. And the answer is, it would be the derivative of u plus i times the derivative of v. You differentiate it just the way you would if these were the components of a motion vector. Well, cosine x is the real part of e to the ix. And, the B part is, the imaginary part is r sin(theta) times i. Now, it would be customary, at this point, to put the i in front, just because it looks better. Now, guess in the time remaining, m not going to talk about in the notes, i, R, at all, but would like to talk a little bit about the extraction of the complex roots, since you have a problem about that and because its another beautiful application of this polar way of writing complex numbers. o, you have to do it by saying its the cosine of zero plus i times the sine of zero. o, schematically, here is the e to the i theta box, if you like to think that way, theta goes in, and thats real, and a complex number, this particular complex number goes out. The real part of it is cosine theta one cosine theta two. s it true that e to the i theta one, lets use that, times e to the i theta two, see, m on a collision course here, but thats easily fixed. A typical example of that is, suppose you want to, for example, integrate (e to the negative x) cosine x. Well, number one, you spend a few minutes running through a calculus textbook and try to find out the answer because you know you are not going to remember how to do it. As soon as you see in this course the combination of e to ax times cosine bx or sine bx, you should immediately think, and youre going to get plenty of it in the couple of weeks after the exam, you are going to get plenty of it, and you should immediately think of passing to the complex domain. o, the product of those, thats what you multiply if you want to multiply this by something to make it real. mean, sort of, you are, but that formula is the meaning of e to the i theta. And then, the imaginary part, ll factor out the i. And then, whats left, wont have to keep repeating the i. o, it will have to be sine theta one cosine theta two. And in the numerator, negative one minus i. This is the same as that. The reason you dont have to work so very hard is because this is a real variable, and already know what it means to differentiate a function of a real variable. e to the (a plus ib) is going to be, in order for the general exponential law to be true, this is really a definition. m hoping that it will turn out to be i e to the i theta, and that the yellow law may be true just as the green one was. And, you always do it by the grouping, or post office method, whatever you want to call it, namely, first put down the real part, which is made out of two times one minus three times one. Well, now, the real part of whats left would be cosine t. And, how about the imaginary part? Whats the real part of it? s that equal to e to the i (theta one plus theta two)? Now, in terms of differential equations, it means that its the solution that e to the, lets be a little generous, make it e to the ax. o, only have to pick out the real part of this times that. And, if translate that geometry into the e to the i theta form for the formula, see that it must be that number. f youre going to call something an exponential, what is it we want an exponential to do, what gives an expression like this the right to be called e to the i theta? But m thinking of it as the real part of e to the ix. Now, this is in a form to pick out the real part. Now, if youre going to call something e to the i theta, we want that to be true. But, as you will see, when push comes to shove, you will see this very clearly on Friday when we talk about trigonometric inputs to differential equations, that the changing to complex numbers makes all sorts of things easy to calculate, and the answers come out extremely clear, whereas if we had to do it any other way, its a lot more work. o, lets do it now that we know what this is. The modulus would be one, and therefore it will be simply, the r will be one for it because its length is one. Thats e to the i theta one, right? o, the division is going to be, now, m going to ask you to do it in your head. o, the product of these two things is this, and thats exactly the formula. o, could write it this way, that the derivative of u plus iv, ll abbreviate it that way, this means the derivative, with respect to whatever variable, since didnt tell you what the variable in these functions were, well, dont have to tell you what m differentiating with respect to. And, whats left is sine x. o, thats the answer to the question. And the bottom is sine of (theta one plus theta two). Youre going to have to do this a lot in this course, by the way, both over the course of the next few weeks, and especially towards the end of the term where we get into a complex systems, which involve complex numbers. But, the way to do it if youre on a desert island, and the way always do it because never have any of these little aides around, and cannot trust my memory, probably a certain number of you remember how you did it at high school, or how you did it in 18.01, if you took it here. o, if want to get this function and not a constant times it, should make this an initial value problem and say that y of zero should be one. o, now is the time to learn to do it, and to feel skillful at it. ts modulus times e to the i theta times r two e to the i theta two Well, you just multiply them as ordinary numbers. want the real part of this. o, want the real part of this.",0.1336325966850828
38,38,"ANNONER: The following program is brought to you by altech. YAER ABOTAFA: Welcome back. Last time, we introduced neural networks, and we started with multilayer perceptrons, and the idea is to combine perceptrons using logical operations like ORs and ANDs, in order to be able to implement more sophisticated boundaries than the simple linear boundary of a perceptron. And we took a final example, where we were trying to implement a circle boundary in this case, and we realized that we can actually do this at least approximate it if we have a sufficient number of perceptrons. And we convinced ourselves that combining perceptrons in a layered fashion will be able to implement more interesting functionalities. And then we faced the simple problem that, even for a single perceptron, when the data is not linearly separable, the optimization finding the boundary based on data is a pretty difficult optimization problem. ts combinatorial optimization. And therefore, it is next to hopeless to try to do that for a network of perceptrons. And therefore, we introduced neural networks that came in as a way of having a nice algorithm for multilayer perceptrons, by simply softening the threshold. nstead of having it as just going from 1 to +1, it would go from 1 to +1 gradually using a sigmoid function, in this case the tanh. And if the signal which is given by this amount the usual signal that goes into the perceptron is large, large negative or large positive, the tanh approximates 1 or +1. o we get the decision function we want. And if s is very small, this is almost linear tanh(s) is linear. And the most important aspect about it is that its differentiable its a smooth function, and therefore the dependency of the error in the output on the parameters w_ij will be a wellbehaved function, for which we can apply things like gradient descent. And the neural network looks like this. t starts with the input, followed by a bunch of hidden layers, followed by the output layer. And we spent some time trying to argue about the function of the hidden layers, and how they transform the inputs into a particularly useful nonlinear transformation, as far as implementing the output is concerned, and the question of interpretation. And then we introduced the backpropagation algorithm, which is applying stochastic gradient descent to neural networks. Very simply, it decides on the direction along every coordinate in the w space, using the very simple rule of gradient descent. And in this case, you only need two quantities. One of them is x_i, that was implemented using this formula, the forward formula, so to speak, going from layer l minus 1 to layer l. And then there is another quantity that we defined, which was called delta, that is computed backwards. You start from layer l, and then go to layer l minus 1. And the formula is strikingly similar to the formula in the forward thing, but instead of the nonlinearity being applied, you multiply by something. And once you get all the deltas and xs by a forward and a backward run, then you simply can decide on the move in every weight, according to this very simple formula that involves the xs and the deltas. And the simplicity of the backpropagation algorithm, and its efficiency, are the reasons why neural networks have become very popular as a standard tool of implementing functions that need machine learning in industry, for quite some time now. Today, m going to start a completely new topic. ts called overfitting, and it will take us three full lectures to cover overfitting and the techniques that go with it. And the techniques are very important, because they apply to almost any machine learning problem that youre going to see. And they are applied on top of any algorithm or model you use. o you can use neural networks or linear models, et cetera. But the techniques that were going to use here, which are regularization and validation, apply to all of these models. o this is another layer of techniques for machine learning. And overfitting is a very important topic. t is fair to say that the ability to deal with overfitting is what separates professionals from amateurs in machine learning. Everybody can fit, but if you know what overfitting is, and how to deal with it, then you have an edge that someone who doesnt know the fundamentals would not be able to comprehend. o the outline today is, first, we are going to start what is the notion? what is overfitting? And then we are going to identify the main culprit for overfitting, which is noise. And after observing some experiments, we will realize that noise covers more territory than we thought. Theres actually another type of noise, which we are going to call deterministic noise. ts a novel notion that is very important for overfitting in machine learning, and were going to talk about it a little bit. And then, very briefly, m going to give you a glimpse into the next two lectures by telling you how to deal with overfitting. And then we will be ready, having diagnosed what the problem is, to go for the cures regularization next time, and validation the time after that. OK. Lets start by illustrating the situation where overfitting occurs. o lets say we have a simple target function. Lets take it to be a 2ndorder target function, a parabola. o my input space is the real numbers. have only a scalar input x. And theres a value y, and have this target that is 2ndorder. We are going to generate five data points from that target, in order to learn from. This is an illustration. Lets look at the five data points. As you see, the data points look like they belong to the curve, but they dont seem to belong perfectly to the curve. o there must be noise, right? This is a noisy case, where the target itself the deterministic part of the target is a function, and then there is added noise. ts not a lot of noise, obviously very small amount. But nonetheless, it will affect the outcome. o we do have a noisy target in this case. Now, if just told you that you have five points, which is the case you face when you learn. The target disappears, have five points, and you want to fit them. Going back to your math, you realize, want to fit five points. aybe should use a 4thorder polynomial will do it, right? You have five parameters. o lets fit it with 4thorder polynomial. This is the guy who doesnt know machine learning, by the way. o say, m going to use the 4thorder polynomial. And what will the fit look like? Perfect fit, in sample. And you measure your quantities. The first quantity is E_in. uccess! We achieved zero training error. And then when you go for the outofsample, you are comparing the red curve to the blue curve, and the news is not good. m not even going to calculate it, its just huge. This is a familiar situation for us, and we know what the deal is. The point want to make here is that, when you say overfitting, overfitting is a comparative term. t must be that one situation is worse than another. You went further than you should. And there is a distinction between overfitting, and just bad generalization. o the reason m calling this overfitting is because, if you use, lets say, 3rdorder polynomial, you will not be able to achieve zero training error, in general. But you will get a better E_out. Therefore, the overfitting here happened by using the 4thorder instead of the 3rdorder. You went further. Thats the key. And that point is made even more clearly, when you talk about neural networks and overfitting within the same model. n the case of overfitting with 3rdorder polynomial versus 4thorder polynomial, you are comparing two models. Here, m going to take just neural networks, and ll show you how overfitting can occur within the same model. o lets say we have a neural network, and it is fitting noisy data. Thats a typical situation. o you run your backpropagation algorithm with a number of epochs, and you plot what happens to E_in, and you get this curve. an you see this curve at all? Let me try to magnify it, hoping that it will become clearer. A little bit better. This is the number of epochs. You start from an initial condition, a random vector. And then you run stochastic gradient descent, and evaluate the total E_in at the end of every epoch, and you plot it. And it goes down. t doesnt go to zero. The data is noisy. You dont have enough parameters to fit it perfectly. But this looks like a typical situation, where E_in goes down. Now, because this is an experiment, you have set aside a test set that you did not use in training. And what you are going to do, you are going to take this test set and evaluate what happens outofsample. Not only at the end, but as you go. Just to see, as train, am making progress outofsample or not? Youre definitely making progress insample. o you plot the outofsample, and this is what you get. o this is estimated by a test set. Now, there are many things you can say about this curve, and one of them is, in the beginning when you start with a random w, in spite of the fact that youre using a full neural network, when you evaluate on this point, you have only one hypothesis that does not depend on the data set. This is the random w that you got. o its not a surprise that E_in and E_out are about the same value here. Because they are floating around. As you go down the road, and start exploring the weight space by going from one iteration to the next, youre exploring more and more of the space of weights. o you are getting the benefit, or the harm, of having the full neural network model, gradually. n the beginning here, you are only exploring a small part of the space. o if you can think of an effective V dimension as you go, if you can define that, then there is an effective V dimension that is growing with time until it gets after you have explored the whole space, or at least potentially explored the whole space, if you had different data sets then you have the effective V dimension, will be the total number of free parameters in the model. o the generalization error, which is the difference between the red and green curve, is getting worse and worse. Thats not a surprise. But there is a point, which is an important point here, which happens around here. Let me now shrink this back, now that you know where the curves are. And lets look at where overfitting occurs. Overfitting occurs when you knock down E_in, so you get a smaller E_in, but E_out goes up. f you look at these curves, you will realize that this is happening around here. Now there is very little, in terms of the difference of generalization error, before the blue line and after the blue line. Yet am making a specific distinction, that crossing this boundary went into overfitting. Why is that? Because up till here, can always reduce the E_in, and in spite of the fact that E_out is following suit with very diminishing returns, its still a good idea to minimize E_in. Because you are getting smaller E_out. The problems happen when you cross, because now you think youre doing well, you are reducing E_in, and you are actually harming the performance. Thats what needs to be taken care of. o thats where overfitting occurs. n this situation, it might be a very good idea to be able to detect when this happens, and simply stop at that point and report that, instead of reporting the final hypothesis you will get after all the iterations, right? Because in this case, youre going to get this E_out instead of that E_out, which is better. And indeed, the algorithm that goes with that is called early stopping. And it will be based on validation. And although its based on validation, it really is a regularization, in terms of putting the brakes. o now we can see the relative aspect of overfitting. And overfitting can happen when you compare two things, whether the two things are two different models, or two instances within the same model. And we look at this and say that if there is overfitting, wed better be able to detect it, in order to stop earlier than we would otherwise, because otherwise we will be harming ourselves. o this is the main story. Now lets look at what is overfitting as a definition, and what is the culprit for it. Overfitting, as a criterion, is the following. ts fitting the data more than is warranted. And this is a little bit strange. What would be more than is warranted? mean, we are in machine learning. We are the business of fitting data. o can fit the data. keep fitting it. But there comes a point, where this is no longer good. Why does this happen? What is the culprit? The culprit, in this case, is that youre actually fitting the noise. The data has noise in it, and you are trying to look at the finite sample set that you got, and youre trying to get it right. n trying to get it right, you are inadvertently fitting the noise. This is understood. can see that this is not good. At least, its not useful at all. Fitting the noise, theres no pattern to detect in the noise, so fitting the noise cannot possibly help me outofsample. However, if it was only just useless, we would be OK. We wouldnt be having this lecture. Because you think, give the data, the data has the signal and the noise. cannot distinguish between them. just get x and get y. y has a component which is a signal, and a component which is noise, but get just one number. cannot distinguish between the two. And am fitting them. And now m going to fit the noise. Lets look at it this way. m in the business of fitting. cannot distinguish the two. Fitting the noise is the cost of doing business. f its just useless, wasted some effort, but nothing bad happened. The problem really is that its harmful. ts not a question of being useless, and thats a big difference. Because machine learning is machine learning. f you fit the noise insample, the learning algorithm gets a pattern. t imagines a pattern, and extrapolates that outofsample. o based on the noise, it gives you something outofsample and tells you this is the pattern in the data, obviously, which it isnt. And that will obviously worsen your outofsample, because its taking you away from the correct solution. o you can think of the learning algorithm in this case, when detecting a pattern that doesnt exist, the learning algorithm is hallucinating. Oh, theres a great pattern, and this is what it looks like, and it reports it, and eventually, obviously that imaginary thing ends up hurting the performance. o lets look at a case study. And the main reason for the case study, because we vaguely now understand that its a problem of the noise, so lets see how does the noise affect the situation? an we get overfitting without noise? What is the deal? o m going to give you a specific case. m going to start with a 10thorder target. 10thorder target means 10thorder polynomial. m always working on the real numbers. The input is a scalar, and m defining polynomials based on that, and m going to take 10thorder target. The 10thorder target, one of them looks like this. You choose the coefficient somehow, and you get something like that. A fairly elaborate thing. And then you generate data, and the data will be noisy, because we want to investigate the impact of noise on overfitting. Lets say m going to generate 15 data points in this case. o this is what you get. Lets look at these points. The noise here is not trivial as it was last time. Theres a difference. Obviously, these are not lying on the curve. o there is a noise that is contributing to that. Now the other guy, which is a 50th order, is noiseless. That is, m going to generate a 50thorder polynomial, so its obviously much more elaborate than the blue curve here, but m not going to add noise to it. m going to generate also 15 points from this guy, but the 15 points, as you will see, perfectly lie on the curve. This is all of them here. o this is the data, this is the target, and the data lies on the target. These are two interesting cases. One of them is a simple target, so to speak. Added noise, that makes it complicated. This one is complicated in a different way. ts a highorder target to begin with, but there is no noise. These are the two cases that m going to try to investigate overfitting in. We are going to have two different fits for each target. We are in the business of overfitting. We have to have comparative models. o m going to have two models to fit every case. And see if get overfitting here, and get it here. This is the first guy that we saw before. The simple target with noise. And this guy is the other one, which is the complex target without noise. 10thorder, 50thorder. Well just refer to them as a noisy loworder target, and a noiseless highorder target. This is what we want to learn. Now, what are we going to learn with? Were going to learn with two models. One of them is the same thing we have a 2ndorder polynomial that were going to use to fit. Thats our model. And were going to have a 10thorder polynomial These are the two guys that we are going to use. Heres what happens with the 2ndorder fit. You have the data points, and you fit them, and its not surprising. For the 2nd order, its a simple curve, and it tries to find a compromise. Here we are applying mean squared error, so this is what you get. Now, lets analyze the performance of this fellow. What m going list here, as you see, m going to say, what is the insample error, what is the outofsample error, for the 2nd order which is already here, and the 10th order, which havent shown yet. The insample error in this case is 0.05. This is a number. Obviously, it depends on the scale. ts some number. When you get the outofsample version, not surprisingly, its bigger, because this one fit the data. The other one is outofsample, so its going to be bigger. But the difference is not dramatic, and this is the performance you get. Now lets apply the 10thorder fit. You already foresee what a problem can exist here. The red curve sees the data, tries to fit that, uses all the degrees of freedom it has it has 11 of them and then it gets this guy. And when you look at the insample error, obviously the insample error must be smaller than the insample error here. You have more to fit and you fit it better, so you get smaller insample error. And what is outofsample error? Just terrible. o this is patently a case of overfitting. When you went from 2nd order to 10th order, the insample error indeed went down. The outofsample error went up. Way up. o you say, this confirms what we have said before. We are fitting the noise. And you can see here that youre actually fitting the noise. You can see the red curve is trying to go for these guys, and you know that these guys are off the target. Therefore, the red curve is bending particularly, in order to capture something that is really noise. o this is the case. Here its a little bit strange, because here we dont have any noise. And we also have the same models. Were going to take the same two models. We have 2nd order and 10th order, fitting here. Lets see how they perform here. Well, this is the 2ndorder fit. Again, thats what you expect from a 2ndorder fit. And you look at the insample error and outofsample error, and they are OK ballpark fine. You get some error, and the other one is bigger than it. Now we go for the 10th order, which is the interesting one. This is the 10th order. You need to remember that the 10th order is fitting a 50th order. o it really doesnt have enough parameters to fit, if we had all the glory of the target function in front of us. But we dont have all the glory of the target function. We have only 15 points. o it does as good a job as possible for fitting. And when we look at the insample error, definitely the insample error is smaller than here. Because we have more. ts actually extremely small. t did it really, really, well. And then when you go for the outofsample. Oh, no! You see, this is squared error. o these guys, when you go down and when you go up, kill you. And indeed they did. o this is overfitting galore. And now you ask yourself, you just told us about noise and not noise. This is noiseless, right? Why did we get overfitting here? We will find out that the reason we are getting overfitting here, because actually this guy has noise. But its not your usual noise. ts another type of noise. And getting that notion down is very important to understand the situations in practice, where you are going to get overfitting. You could be facing a completely noiseless, in the conventional sense, situation, and yet there is overfitting, because you are fitting another type of noise. o lets look at the irony in this example. Here is the first example the noisy simple target. o you are learning a 10thorder target, and the target is noisy. And m not showing the target here, m showing the data points together with the two fits. Now lets say that tell you that the target is 10th order, and you have two learners. One of them is O, and one of them is R O for overfitting, and R is for restricted, as it turns out. And you tell them, guys, m not going to tell you what the target is, because if tell you what the target is, this is no longer machine learning. But let me help you out a little bit. The target is a 10thorder polynomial. And m going to give you 15 points. hoose your model. Fair enough? The information given does not depend on the data set, so its a fair thing. The first learner says, know that the target is 10th order. Why not pick a 10thorder model? ounds like a good idea. And they do this, and they get the red curve, and they cry and cry and cry! The other guy said, oh, its 10thorder model? Who cares? How many points do you have? 15. OK, 15. am going to take a 2nd order, and am actually pushing my luck, because 2nd order is 3 parameters, have 15 points, the ratio is 5. omeone told us a rule of thumb that it should be 10. m flirting with danger. But cannot use a line when you are telling me the thing is 10th order, so let me try my luck with 2nd. Thats what you do. And they win. o its a rather interesting irony, because there is a thought in peoples mind that you try to get as much information about the target function, and put it in the hypothesis set. n some sense this is true, for certain properties. But if you are matching the complexity, here the guy who actually took the 10thorder target, and decided to put the information all too well in the hypothesis m taking a 10thorder hypothesis set lost. o again, we know all too well now. The question is, you match the data resources, rather than the target complexity. There will be other properties of the target function, that we will take to heart. ymmetry and whatnot, there are a bunch of hints that we can take. But the question of complexity is not one of the things that you just apply the general idea of: let me match the target function. Thats not the case. n this case, you are looking at generalization issues, and you know that generalization issues depend on the size and the quality of the data set. Now, the example that just gave you, we have seen it before when we introduced learning curves, if you remember what those were. Those were, yeah, m going to put how E_in and E_out change with a number of examples. And gave you something, and told you that this is an actual situation well see later, and this is the situation. o this is the case where you take the 2ndorder polynomial model, H_2, and the inevitable error, which is the black line, comes now not only from the limitations of the model an inability for a 2nd order to replicate a 10th order, which is the target in this case but also because there is noise added. Therefore, theres an amount of error that is inevitable because of the noise. But the model is very limited. The generalization is not bad, which is the difference between the two curves. And if you have more examples, the two curves will converge, as they always do, but they converge to the inevitable amount of error, which is dictated by the fact that youre using such a simple model in this case. And when we looked at the other case, also introduced in this case this was the 10thorder fellow. o the 10thorder fellow is you can fit a lot, so the insample error is always smaller than here. That is understood. The outofsample error starts by being terrible, because you are overfitting. And then it goes down, and it converges to something that is better, because that carries the ability of H_10 to approximate a 10th order, which should be perfect, except that we have noise. o all of this actually is due to the noise added to the examples. And the gray area is the interesting part for us. Because in the gray area, the insample error for the more complex model is smaller. ts smaller always, but we are observing it in this case. And the outofsample error is bigger. Thats what defines the gray area. Therefore in this gray area, very specifically, overfitting is happening. f you move from the simpler model to the bigger model, you get better insample error and worse outofsample error. Now we realize that this guy is not going to lose forever. The guy who chose the correct complexity is not going to lose forever. They lost only because of the number of examples that was inadequate. f the number of examples is adequate, they will win handily. Like here if you look here, you end up with an outofsample error far better than you would ever get here. But now have enough examples, in order to be able to do that. Now, we understand overfitting. And we understand that overfitting will not happen for all the numbers of examples, but for a small number of examples where you cannot pin down the function, then you suffer from the usual bad generalization that we saw. Now, we notice that we get overfitting even without noise, and we want to pin it down a little bit. o lets look at this case. This is the case of the 50thorder target, the higherorder target that doesnt have any noise conventional noise, at least. And these are the two fits. And theres still an irony, because here are the two learners. The first guy chose the 10th order, the second guy chose the 2nd order. And the idea here is the following. You told me that the target now doesnt have noise. Right? That means dont worry about overfitting. Wrong. But well know why. o given the choices, m going to try to get close to the 50th order, because have a better chance. f choose the 10th order, someone else chooses 2nd order, m closer to the 50th, so think will perform better. At least thats the concept. o you do this, and you know that there is no noise, so you decide on this idea, and again you get bad performance. And you ask yourself, this is not my day. tried everything, and seem to be making the wise choice, and m always losing. And why is this the case, when there is no noise? And then you ask, is there really no noise? And that will lead us to defining that there is an actual noise in this case, and well analyze it and understand what it is about. o will take these two examples, and then make a very elaborate experiment. And will show you the results of that experiment. will encourage you, if you are interested in the subject, to do simulate this experiment. All the parameters are given. And it will give you a very good feel for overfitting, because now we are going to look at the figure, and have no doubt in our mind that overfitting will occur whenever you actually encounter a real problem. And therefore, you have to be careful. ts not like constructed a particular funny case. No, if you average over a huge number of experiments, you will find that overfitting occurs in the majority of the cases. o lets look at the detailed experiment. m going to study the impact of two things the noise level, which already conceptually convinced myself that its related to overfitting, and the target complexity, just because it does seem to be related. Not sure why, but it seems like when took a complex target, albeit noiseless, still got overfitting, so let me see what the target complexity does. We are going to take, as general target function m going to describe what it is, and m going to add noise to it. The noise is a function of x. o m just getting it generically, and as always, we have independence from one x to another. n spite of the fact that the parameters of the noise distribution depend on x can have different noise for different points in the space the realization of epsilon is independent from one x to another. That is always the assumption. When we have different data points, they are independent. o this is the thing, and m going to measure the level of noise by the energy in that noise, and were going to call it sigma squared. m taking the expected value of epsilon to be 0. f there were an expected value, would put it in the target, so will remain with 0. And then theres fluctuation around it, and the fluctuation either could be big, large sigma squared, or small. And m quantifying it with sigma squared. No particular distribution is needed. You can say Gaussian, and indeed applied Gaussian in the experiment. But for the statement, you just need the energy of that. Now lets write it down. want to make the target function more complex, at will. o m going to make it higherorder polynomial. Now have another parameter, pretty much like the sigma squared. have another parameter which is capital Q, the order of the polynomial. m calling it Q_f, because it describes the target complexity of f, just to remember that its related to f. And what do, define a polynomial, which is the sum of coefficients times a power of x, from q equals 0 to Q, so its indeed a Qthorder polynomial, and add the noise here. Now, in order to run the experiment right, m going to normalize this quantity, such that the energy here is always 1. And the reason do that is because want the sigma squared to mean something. The signal to noise ratio is always what means something. o if normalize the signal to energy 1, then can say sigma squared is really the amount of noise. And if you look at this, it is not easy to generate interesting polynomials using this formula. Because if you pick these guys at random lets say independent coefficients at random, in order to generate a general target, these guys are the powers of x. o you start with the x, and then the parabola, and then the 3rd order, and then the 4th order, and then the 5th order. Very, very boring guys. One of them is doing this way, and the other one is doing this way, and they get steeper and steeper. o if you combine them with random coefficients, you will almost always get something that looks this way, or something that looks this way. And the other guys dont play a role, because this one dominates. The way to get interesting guys here is, instead of generating the alpha_qs here as random, you go for a standard set of polynomials, which are called Legendre polynomials. Legendre polynomials are just polynomials with specific coefficients. There is nothing mysterious about them, except that the choice of the coefficients is such that, from one order to the next, theyre orthogonal to each other. o its like harmonics in a sinusoidal expansion. f you take the 1storder Legendre, then the 2nd, and the 3rd, and the 4th, and you take the inner product, you see they are 0. They are orthogonal to each other, and you normalize them to get energy 1. Because of this, if you have a combination of Legendres with random coefficients, then you get something interesting. All of a sudden, you get the shape. And when you are done, it is just a polynomial. All you do, you collect the guys that happen to be the coefficients of x, the coefficients of x squared, coefficients of x cubed, and these will be your alphas. Nothing changed in the fact that m generating a polynomial. just was generating the alphas in a very elaborate way, in order to make sure that get interesting targets. Thats all there is to it. As far as we are concerned, we generated guys that have this form and happened to be interesting representative of different functionalities. o in this case we have the noise level. Thats one parameter that affects overfitting. We have potentially the target complexity seems to be affecting overfitting. At least we are conjecturing that it is. And the final guy that affects overfitting is the number of data points. f give you more data points, you are less susceptible to overfitting. Now d like to understand the dependency between these. And if we go back to the experiment we had, this is just one instance of those, where the target complexity here is 10. use the 10thorder polynomial, so Q_f is 10. The noise is whatever the distance between the points and the curve is. Thats what captures sigma squared. And the data size here is 15. have 15 data points. o this is one instance. m basically generating at will random instances of that, in order to see if the observation of overfitting persists. Now, how am going to measure overfitting? m going to define an overfit measure, which is a pretty simple one. Were fitting a data set from x_1, y_1 to x_N, y_N. And we are using two models, our usual two models. Nothing changed. We either use 2ndorder polynomials, or the 10thorder polynomials. And if going from the 2ndorder polynomial to the 10thorder polynomial gets us in trouble, then we are overfitting. And we would like to quantify that. When you compare the outofsample errors of the two models, you have a final hypothesis from H_2, and this is the fit the green curve that you have seen. And another final hypothesis from the other model, which is the red curve the wiggly guy. f you want to define an overfit measure based on the two, what you do is you get the outofsample error for the more complex guy, minus the out of sample error for the simple guy. Why is this an overfit measure? Because if the more complex guy is worse, it means its outofsample error is bigger, and you get a positive number, large positive if the overfitting is terrible. And if this is negative, it means that actually the more complex guy is doing better, so you are not overfitting. Zero means that they are the same. o now have a number in my mind that measures the level of overfitting in any particular setting. And if you apply this to, again, the same case we had before, you look at here, and the outofsample error for the red is terrible. The outofsample error of green is nothing to be proud of, but definitely better. And the overfit measure in this case will be positive, so we have overfitting. Now lets look at the result of running this for tens of millions of iterations. Not epochs iterations. omplete runs. Generate the target, generate the data set, fit both, look at the overfit measure. Repeat 10 million times, for all kinds of parameters. o you get a pattern for what is going on. This is what you get. First, the impact of sigma squared. m going to have a plot in which you get N, the number of examples, and the level of noise, sigma squared. And on the plot, m going to give a color depending on the intensity of the overfit. That intensity will be depending on the number of points, and the level of the noise that you have. And this is what you get. First lets look at the color convention. o 0 is green. f you get redder, theres more overfitting. f you get bluer, there is less overfitting. Now looked at the number of examples, and picked interesting range. f you go, this is 80, 100, and 120 points. o what happens to 40? All of them are dark red. Terrible overfitting. And if you go beyond that, you have enough examples now not to overfit, so its almost all blue. o m just giving you the transition part of it. You look at it. There is a noise level. As increase the noise level, overfitting worsens. Why is that? Because if pick any number of examples, lets say 100. f had 100, and it had that little noise, m doing fine. Doing fine in terms of not overfitting. And as go, get into the red region, and then get deeply into the red region. o this tells me, indeed, that overfitting worsens with sigma squared. By the way, for all of the targets here, picked a fixed complexity. 20. 20thorder polynomial. fixed it because just wanted a number, and wanted only to relate the noise to the overfitting. o thats what m doing here. When change the complexity, this will be the other plot. For this guy, we get something that is nice, and its really according to what we expect. As you increase the number of points, the overfitting goes down. As you increase the level of noise, the overfitting goes up. That is what we expect. Now lets go for the impact of Q_f, because that was the mysterious part. There was no noise and we are getting overfitting. s this going to persist? What is the deal? This is what you get. o here, we fixed the level of noise. We fixed it at sigma squared equals 0.1. Now we are increasing the target complexity, from trivial to 100thorder polynomial. Thats a pretty serious guy. And we are plotting the same range for the number of points, from 80, 100, 120. Thats where it happens. And you can see that overfitting occurs significantly. And it worsens also with the target complexity. Because lets say, you look at this guy. f you look at this guy, you are here in the green, and gets red, and then it gets darker red. Not as pronounced as in this case. But you do get the overfitting effect by increasing the target complexity. And when the number of examples is bigger, then theres less overfitting, as you expect it to be. But if you go high enough guess its getting lighter blue, green, yellow. Eventually, it will get to red. And if you look at these two guys, the main observation is that the red region is serious. Overfitting is real and here to stay, and we have to deal with it. ts not like an individual case there. Now, there are two things you can derive from these two figures. The first thing is that there seems to be another factor, other than conventional noise lets call it conventional noise for the moment that affects overfitting. And we want to characterize that. That is the first thing we derive. The second thing we derive is a nice logo for the course! Thats where it came from. o now lets look at noise, and look at the impact of noise. And you can notice that noise is between quotation marks here, because now were going to expand our horizon about what constitutes noise. Here are the two guys. And in the first case, we are going now to call it stochastic noise. Noise is stochastic, but obviously we are calling it stochastic because the other guy will not be stochastic. And theres absolutely nothing to add here. This is what we expect. Were just calling it a name. Now we are going to call whatever effect that is done by having a more complex target here, we are going also to call it noise. But it is going to be called deterministic noise. Because there is nothing stochastic about it. Theres a particular target function. just cannot capture it, so it looks like noise to me. And we would like to understand what deterministic noise is about. However, if you look at it, and now you speak in terms of stochastic noise and deterministic noise, and you would like to see what affects overfitting. o, we put it in a box. First observation: if have more points, have less overfitting. f you move from here to here, things get bluer. f you move from here to here, things get bluer. have less overfitting. econd thing: if increase the stochastic noise increase the energy in the stochastic noise the overfitting goes up. ndeed, if go from here to here, things get redder. And finally, with deterministic noise, which is vaguely associated in my mind with the increase of target complexity, also increase the overfitting. f go from here to here, am getting redder. Albeit have to travel further, and its a bit more subtle, but the direction is that get more overfitting as get more deterministic noise, whatever that might be. o now, lets spend some time just analyzing what deterministic noise is, and why it affects overfitting the way it does. Lets start with the definition. What is it? t will be actually noise. f tell you what is the stochastic noise, you will say, heres my target, and there is something on top of it. That is what call stochastic noise. o the deterministic noise will be the same thing, except that it captures something deterministic. ts the part of the target that your hypothesis set cannot capture. o lets look at the picture. Here is the picture. This is your target, the blue guy. You take a hypothesis set that lets say simple, and you look for the guy that best approximates f. Not in the learning sense. You actually try very hard to find the best possible approximation. Youre still not going to get f, because your hypothesis set is limited, but the best guy will be sitting there, and it will fail to pick certain part of the target. And that is the part we are labeling the deterministic noise. And if you think from an operational point of view, if you are that hypothesis, noise is all the same. ts something cannot capture. Whether couldnt capture it, because theres nothing to capture as in stochastic noise or couldnt capture it, because m limited in capturing, and this have to consider as out of my league. Both of them are noise, as far as m concerned. omething cannot deal with. This is how we define it. And then we ask, why are we calling it noise? ts a little bit of a philosophical issue. But lets say that you have a young sibling your kid brother has just learned fractions. o they used to have just 1, 2, 3, 4, 5, 6. Theyre not even into negative numbers, and they learn fractions, and now theyre very excited. They realize that theres more to numbers than just 1, 2, 3. o you are the big brother. You are big altech guy. o you must know more about numbers. They come ask you, tell me more about numbers. Now, in your mind, you probably can explain to them negative numbers a little bit by deficiency. Real numbers, just intuitively continuous. You are not going to tell them about limits, or anything like that. Theyre too young for that. But you probably are not going to tell them about complex numbers, are you? Because their hypothesis set is so limited that complex numbers, for them, would be completely noise. And the problem with explaining something that people cannot capture is that they will create a pattern that really doesnt exist. And then you tell them complex number, and they really cant comprehend it, but they got the notion. o now its the noise. They fit the noise, and they tell you, is 7.34521 a complex number? Because in their minds they just got on to a tangent. o youre better off just killing that part. And giving them a simple thing that they can learn, because the additional part will actually mislead them. islead them, as in noise. o this is our idea, that if have a hypothesis set, and there is part of the target that cannot capture, theres no point in trying to capture it, because when you try to capture it, you are detecting a false pattern that you cannot extrapolate, given your limitations. Thats why its called noise. Now the main differences between deterministic noise and stochastic noise both of them can be plotted, a realization but the main differences are, the first thing is that deterministic noise depends on your hypothesis set. For the same target function, if you use a more sophisticated hypothesis set, the deterministic noise will be smaller, because you were able to capture more. Obviously, the stochastic noise will be the same. Nothing can capture it, so all hypotheses are the same. We cannot capture it, and therefore its noise. The other thing is that, if give you a particular point x, deterministic noise is a fixed amount, which is the difference between the value of the target at that point and the best hypothesis approximation you have. f gave you stochastic noise, then you are generating this at random. And if give you two instances of x, the same x, the noise will change from one occurrence to another, whereas here, its the same. Nonetheless, they behave exactly the same for machine learning, because invariably we have a given data set. Nobody changes xs on us, and give us another realization of the x. We just have the xs given to us together with the labels. o this doesnt make a difference for us. And we settle on a hypothesis set. Once you settle on a hypothesis set, the deterministic noise is as bad as the stochastic noise. ts something that we cannot capture, and it depends on something that we have already fixed, so it doesnt depend on anything. o in a given learning situation, they behave the same. Now, lets see the impact on overfitting. This is what we have seen before. This is the case where we have increasing target complexity, so increasing deterministic noise in the terminology we just introduced, and the number of points. And red means overfitting, so this is how much overfitting is there. And we are looking at deterministic noise, as it relates to the target complexity. Because the quantitative thing we had is target complexity. We defined what a realization of deterministic noise is, but its not clear to us what quantity we should measure out of deterministic noise, in order to tell us that this is the level of noise that results in overfitting, yet. We have the one in the case of stochastic noise very easily. We just take the energy of it. o here we realize that as you increase the target complexity, the deterministic noise increases, which is the overfitting phenomenon that we observe increases. But youll notice theres something interesting here. t doesnt start until you get to 10. Because this was overfitting of what? The 10th order versus the 2nd order. o if youre going to start having deterministic noise, youd better go above 10, so that there is something that you cannot approximate. This is the part where its there. o here, wouldnt say proportional, but it definitely increases with the target complexity, and it decreases with N as we expect. Now for the finite N, you suffer the same way you suffer from the stochastic noise. We have declared that deterministic noise is the part that your hypothesis set cannot capture. o what is the problem? f cannot capture it, it wont hurt me, because when try to fit, wont capture it anyway. No. You cannot capture it in its entirety. But if give you only a finite sample, then you only get few points, and you may be able to capture a little bit of the stochastic noise, or the deterministic noise in this case. Again, if have 10 points if you give me a million points, and even if there is stochastic noise, theres nothing can do to capture the noise. Let me remind you of the example we gave in linear regression. We took linear regression and said, lets say that we are learning a linear function. o linear regression would be perfect in this case. This is the target. And then we added noise to the examples, so instead of getting the points perfectly on that line, you get points right or left. And then we tried to use linear regression to fit it. f you didnt have any noise, linear regression would be perfect in this case. Now, since theres noise, and it doesnt really see the line it only sees those guys, it eats a little bit into the noise, and therefore gets deviated from the target. And that is why you are getting worse performance than without the noise. Now, if have 10 points, linear regression will have easy time eating into that, because there isnt much to fit. There are only 10 guys, and maybe theres some linear pattern there. f get a million points, the chances are wont be able to fit any of them at all, because they are noise all over the place, and cannot find a compromise using my few parameters, and therefore will end up really not being affected by them. n the infinite case, cannot get anything. They are noise, and cannot fit them. They are out of my ability. But the problem is that once you have a finite sample, youre given the unfortunate ability to be able to fit the noise, and you will indeed fit it. Whether its stochastic that it doesnt make sense or deterministic, that there is no point in fitting it, because you know in your hypothesis set, there is no way to generalize outofsample for it. t is out of your ability. o the problem here is that for the finite N, you get to try to fit the noise, both stochastic and deterministic. Now, let me go quickly through a quantitative analysis that will put deterministic noise and stochastic noise in the same equation, so that they become clear. Remember biasvariance? That was a few lectures ago. What was that about? We had a decomposition of the expected outofsample error into two terms. And this is the expected value of outofsample error. remember, this is the hypothesis we get, and we have dependency on the data set that got us. We compare it to the target function, and we get the expected value with respect to those. And that ended up being a variance, which tells me how far am from the centroid within the hypothesis set, and that means that theres a variety of things get based on D. And the other one is how far the centroid is from the target, which tells me the bias of my hypothesis set from the target. And the leap of faith we had is that this quantity, which is the average hypothesis you get over all data sets, is about the same as the best hypothesis in the hypothesis set. o we had that. And in this case, f was noiseless in this analysis. Now, d like to add noise to the target, and see how this decomposition will go, because this will give us a very good insight into the role of stochastic noise versus deterministic noise. o we add noise. And were going to plot it red, because we want to pay attention to it, and because we are going to get the expected values with respect to it. o y now is the realization, the target plus epsilon. And m going to assume that the expected value of the noise is 0. Again, if the expected value is something else, we put that in the target, and leave the part which is pure fluctuation outside, and call that epsilon. Now would like to repeat the analysis, more quickly, obviously, with the added noise. Here is the noise term. First, this is what we started with. o m comparing what you get in your hypothesis, in a particular learning situation, to the target. But now the target is noisy. o the first thing is to replace this fellow by the noisy version, which is y. know that y has f of x, plus the noise. Thats what m comparing to. And now, because y depends on the noise, m not only getting the averaging with respect to the data set, m also getting the average with respect to the realization of the noise. o m getting expected value with respect to D and epsilon epsilon affecting y. o you expand this, and this is just rewriting it. f of x plus epsilon is y, so m writing it this way. And we do the same thing we did before, but just carrying this around, until we see where it goes. o what did we do? We added and subtracted the centroid the average hypothesis, remember in preparation for getting squared terms, and cross terms. And here we have the epsilon added to the mix. And then we write it down. And in the first case, we get the squared, so we put these together and put them as squared. We take these two guys together, and put them as squared. And this guy by itself, we put it as squared. We will still have cross terms, but these are the ones that m going to focus on. And then we have more cross terms than we had before, because theres epsilon in it. But the good news is that, if you get the expected value of the cross terms, all of them will go to 0. The ones that used to go to 0 will go to 0. The other ones will go to 0, because the expected value of epsilon goes to 0, and epsilon is independent of the other random thing here, which is the data set. Data set is generated. ts noise is generated. Epsilon is generated on the test point x, which is independent, and therefore you will get 0. o its very easy to argue that this is 0, and you will get basically the same decomposition with this fellow added. o lets look at it. Well, well see that there are actually two noise terms that come up. This is the variance term. Let me put it. This is the bias term. And this is the added term, which is just sigma squared, the energy of the noise. Let me just discuss this a little bit. We had the expected value with respect to D, and with respect to epsilon. And then, remember that we take the expected value with respect to x, average over all the space, in order to get just the bias and variance, rather than the bias of x of your test point. o did that already. o every expectation is with respect to the data set, with respect to the input point, and with respect to the realization of the noisy epsilon. But m keeping the guys that survive, because the other guys epsilon doesnt appear here, so the thing is constant with respect to it, so take it out. Here, neither epsilon nor D appears here, so just leave it for simplicity. And here, D doesnt appear, but epsilon and x appear, so do it this way. could put the more elaborate notation, but just wanted to keep it simple. Now, look at this decomposition. We have the moving from your hypothesis to the centroid, from the centroid to the target proper, and then from the target proper to the actual output, which has a noise aspect to it. o its again the same thing of trying to approximate something, and putting it in steps. Now if you look at the last quantity, that is patently the stochastic noise. The interesting thing is that there is another term here which is corresponding to the deterministic noise. And that is this fellow. Thats another name for the bias. Why is that? Because our leap of faith told us that this guy, the average, is about the same as the best hypothesis. o we are measuring how the best hypothesis can approximate f. Well, this tells me the energy of deterministic noise. And this is why its deterministic noise. And putting it this way it gives you the solid ground to treat them the same. Because if you increase the number of examples, you may get better variance. There is more examples, so you dont float around fitting all of them. o the red region, that used to be the variance, shrinks and shrinks. These guys are both inevitable. There is nothing you can do about this, and theres nothing you can do about this given a hypothesis set. o these are fixed. But again, in the biasvariance, remember the approximation was overall approximation. We took the entire target function, and the entire hypothesis. We didnt look at particular data points. We looked at approximation proper, and thats why these are inevitable. You tell me what the hypothesis set is, well, thats the best can do. And this is the best can do as far as the noise, which is just not predicting anything in the noise. Now, both the deterministic noise and the stochastic noise will have a finite version on the data points, and the algorithm will try to fit them. And thats why this guy gets a variety. Because depending on the particular fit of those, you will get one or another. o these guys affect the variance, by making the fit more susceptible to going in more places. Depending on what happens, will go this way and that way not because its indicated by the target function want to learn, but just because there is a noise present in the sample that am blindly following, because cant distinguish noise from signal, and therefore end up with more variety, and end up with worse variance and overfit. Now very briefly, m going to give you a lead into the next two lectures. We understand what overfitting is, and we understand that its due to noise. And we understand that noise is in the eye of the beholder, so to speak. There is stochastic noise, but theres another noise which is not really noise, but depends on which hypothesis looks at it. t looks like noise to some, and not look like noise to other, and we call that deterministic noise. And we saw experimentally that it affects overfitting. o how do we deal with overfitting? What does it mean to deal with overfitting? We want to avoid it. We dont want to spend more energy fitting, and get worse outofsample error, whether by choice of a model, or by actually optimizing within a model, like we did with neural networks. There are two cures. One of them is called regularization, and that is best described as putting the brakes. o overfitting you are going, going, going, going, and you hurt yourself. o all m doing here is, m just making sure that you dont go all the way. And when you do that, m going to avoid overfitting this way. The other one is called validation. What is the cure in this case for overfitting? You check the bottom line, and make sure that you dont overfit. ts a different philosophy. That is, the reason m overfitting is because m going for E_in, and m minimizing it, and m going all the way. say, no, wait a minute. E_in is not a very good indication for what happens. aybe theres another way to be able to tell what is actually happening out of sample, and therefore avoid overfitting, because you can check on what is happening in the real quantity you care about. o these are the two approaches. ll give you just an appetizer a very short appetizer for putting the brakes the regularization part, which is the subject of next lecture. Remember this curve? Thats what we started with. We had the five points, we had the 4thorder polynomial, we fit, and we ended up in trouble. And we can describe this as free fit, that is, fit all you can. o fit all you can, five points, ll take 4thorder polynomial, go for it, get this, and thats what happens. Now, putting the brakes means that youre going to not allow yourself to go all the way, and you are going to have a restrained fit. The reason m showing this is because its fairly dramatic. You will think that need this curve is so incredibly bad that you think you really need to do something dramatic in order to avoid that. But here, what m going to do, m just going to make you fit, and m actually going to make you fit using a 4thorder polynomial. ll give you that privilege. But m going to prevent you from fitting the points perfectly. m going to put some friction in it, such that you cannot get exactly to the points. And the amount of brake m going to put here is so minimal, its laughable. When you go for your car service, they measure the brake, and they tell you, oh, the brake is 70%, et cetera, and then when it gets to 40%, they tell you you need to do something about the brake. The brakes here are about 1%. o if this was a car, you would be braking here, and you would be stopping in Glendale! ts like completely ridiculous. But that little amount of brake will result in this. Totally dramatic. Fantastic fit. The red curve is a 4thorder polynomial, but we didnt allow it to fit all the way. And you can see that its not fitting all the way, because it really is not getting the points right. ts getting there, but not exactly. o we dont have to do much to prevent the overfitting. But we need to understand what is regularization, and how to choose it, et cetera. And this well talk about next time. And then the time after that, were going to talk about validation, which is the other prescription. will stop here, and we will take questions after a short break. Lets start the QA, and well start with a question in house. TDENT: o on previous lecture we spoke about stochastic gradient descent, and we say that we should choose point by point, and move in the direction of gradient of error in this point. PROFEOR: Negative of the gradient, yes. TDENT: o the question is, how important is it to choose points randomly? mean, we can choose them just from the list first point, second point, and so on? PROFEOR: Yeah. Depending on the runs, it could be no difference at all, or it could be a real difference. And the best way to think of randomization in this case is that its an insurance policy. Theres something about the pattern that is detrimental in a particular case. You are always safe by picking the points at random, because theres no chance that the random thing will have a pattern eventually, if you keep doing it. o in many cases, you just run through examples 1 through N, 1 through N. 1 through N, and you will be fine. ome cases, you take a random permutation. ome cases even, you stay true to picking the point at random, and you hope that the representation of a point will be the same, in the long run. n my own experience, there is little difference in a typical case. Every now and then, theres a funny case. And therefore, you are safer using the stochastic presentation the random presentation of the examples in order to be able not to fall into the trap in those cases. Yeah. Theres another question in house. TDENT: Hi, Professor. have a question about slide 4. ts about neural networks. dont understand how do you draw the outofsample error on that plot? PROFEOR: OK. n general, you cannot, obviously, draw the outofsample error. f you could draw it, you will just pick it. This is a case where, give you a data set, and you decide to set aside part of the data set for testing. o you are not involving it at all in the training. And what you do, you go about your training, and at the end of every epoch, when you evaluate the insample error on the entire batch, which is the green curve here, you also evaluate, for that set of weights the frozen weights at the end of the epoch you evaluate that on the test set, and you get a point. And because that point is not involved in the training, it becomes an outofsample point, and that gets the red point. And you go down. Now, theres an interesting tricky point here, because if you decide at some point to maybe, look at the red curve. Now am going to stop where the red curve is minimum. TDENT: Yes. PROFEOR: OK? Now at that point, the set that used to be a test set is no longer a test set, because now it has just been involved in a decision regarding training. Becomes slightly contaminated, becomes a validation set, which were going to talk about when we talk about validation. but that is really the premise. TDENT: OK. understand. Also, can slide 16? PROFEOR: lide 16. TDENT: didnt follow that. Why the two noises are the same, for the same learning problem. PROFEOR: Theyre the same in the sense that they are part of the outputs that m being given, or that m trying to predict. And that part, cannot predict regardless of what do. n the case of stochastic noise, its obvious. Theres nothing to predict there, so whatever do, miss it. n the case here, its particular to the hypothesis set that have. o take a hypothesis set, and look in a nonlearning scenario, look at the target function and choose your best scenario. You choose, this is my best hypothesis, which we called here h star. f you look at the difference between h star and f, the difference is a part which cannot capture, because the best could do is h star. o the remaining part is what m referring to as deterministic noise, and it is beyond my ability given my hypothesis set. o thats why they are the same the same in the sense of unreachable as far as my resources are concerned. TDENT: OK. n a real problem, do we know the complexity of the target function? PROFEOR: n general, no. We also dont know the particulars of the noise. We know that the problem is noisy, but we cannot identify the noise. We cannot, in most cases, even measure the noise. o the purpose here is to understand that, even in the case of a noiseless target in the conventional sense, there is something that we can identify conceptually identify that does affect the overfitting. And even if we dont know the particulars of it, we will have to put in place the guards, in order to avoid overfitting. That was the goal here, rather than try to Any time you see the target function drawn, you should immediately have an alarm bell that this is conceptual, because you never actually see the target function in a real learning situation. TDENT: Oh. o, thats why the two noises are equal, then. Because we dont know the target function, so we dont know which part is deterministic. PROFEOR: Yeah. f knew the target, and if knew the noise, then the situation would be good, but then dont need machine learning. already have that. TDENT: Thank you. PROFEOR: o we go for the questions from the outside? ODERATOR: Yeah. Quick conceptual question. s it OK to say that the deterministic noise is the part of reality that is too complex to be modeled? PROFEOR: t is definitely part of the reality that part. And basically, its our failure to model it is what made it noise, as far as we are concerned. o obviously you can, in some sense, model it by going to a bigger hypothesis set. The bigger hypothesis set will have a closer h star to the target, and therefore the difference will be small. But the situation pertains to the case where you already chose the hypothesis set according to prescriptions of V dimension, number of examples, and other considerations. And given that hypothesis set, you already concede that even if the target is noiseless, there is part of it which behaves as noise, as far as m concerned. And will have to treat it as such, when consider overfitting and the other considerations. ODERATOR: Also, is it fair to say that overtraining will cause overfitting? PROFEOR: think they probably are synonymous. Overfitting is relative. Overtraining will be relative within the same model, if try to give it a definition. That you overtrain, so you already settled on the model, and youre overtraining it. The case of neural network would be overtraining. The case of choosing the 3rdorder polynomial versus the 4thorder polynomial will not really be overtraining, but it will be overfitting. ts all technicalities, but just to answer the question. ODERATOR: Practically, when you implement these algorithms, and theres also some approximation, maybe due to the floatingpoint number or something. o is this another source of error? Does it produce overfitting? Or is it PROFEOR: ts Formally speaking, yes, its another source. But it is so minute with respect to the other guys, that its never mentioned. We have another inhouse question. TDENT: A couple of lectures ago, we spoke about 3rd linear model, which is logistic regression. PROFEOR: You said the 3rd linear model? TDENT: Yes. o the question is, is it true that initially have data which is completely linearly separable? o the points marked some points are marked 1, and some are +1, and there is a plane which separates them. s it true that applying this learning model, youre never stuck in a local minimum and get 0 insample error? PROFEOR: OK. This is a very specific question about logistic regression. f the thing is completely clean, then you obviously can get closer and closer to having the probability being perfect, by having bigger and bigger weights. o there is a minimum. And again, its a unique minimum. Except that the minimum is at infinity, in terms of the size of the weight. But this doesnt bother you, because you are really going to stop at some point when the gradient is small, according to your specification. And you can specify this any way you want. o the goal is not necessarily to arrive at the minimum. Which hardly ever happens, even if the thing is not at infinity. But get close enough, in the sense that the value is close to the minimum, and therefore you achieve the small error that you want. ODERATOR: an you resolve again the contradiction of when you increase the complexity of the model, you should be reducing your bias, and hence your deterministic noise? o here we had an example when we had H well, H_10 had more error than H_2. PROFEOR: H_10 had total error more than H_2. f we were doing the approximation game, H_10 would be better. We had three terms in the biasvariance. f we were only going by these two, then there is no question that the bigger model, H_10, will win. Because this is for all, and this one will be better for H_10 than H_2, because H_10 is closer to the target we want, and therefore we will be making smaller error. This is not the source of the problem of overfitting. This is just identifying terms in the biasvariance decomposition, biasvariancenoise decomposition in this case, that correspond to the different types of noise. The problem of overfitting happens here. And that happens because of the finitesample version of both. That is, get N points in which there is a contribution of noise coming from the stochastic and coming from the deterministic. On those points, the algorithm will try to fit that noise, in spite of the fact that if it knew, it wouldnt, because it knows that theyre out of reach. But it gets a finite sample, and it can use its resources to try to fit part of that noise, and that is what is causing overfitting. And that ends up being harmful, and so harmful in the H_10 case, that the harm offsets the fact that m closer to the target function. That doesnt help me very much. Because, same thing we said before, Lets say theres H_10. And the target function is sitting here. That doesnt do me much good if my algorithm, and the distraction of the noise, leads me to go in that direction. will be further from the target function than another guy who, only working with this, remained in the confines and ended up being closer to the target function. ts a question of the variance term that results in overfitting, not this guy, in spite of the fact that these guys contain both types of noise contributing to their value. But their value is static. t doesnt change with N, and it has nothing to do with the overfitting aspect. ODERATOR: n the case of polynomial fitting, a way to avoid the overfitting could be to use piecewise linear piecewise linear functions around each point. o it is a method of regularization? Or is it PROFEOR: OK. Depends on the number of degrees of freedom you have. You can have piecewise linear, which is really horrible. ts like something you cant tell. t depends on how many pieces. f you have as many pieces as there are points, you can see what the problem is. o it really is, what is the V dimension of your model? And can take it if its piecewise linear, and have only four parameters, then dont worry too much that its piecewise linear. only worry about the four parameters aspect of it. 10thorder polynomial was bad because of the 11 parameters, not because of other factor. But anything you do to restrict your model, in terms of the fitting, can be called regularization. And there are some good methods and bad methods, but they are all regularization, in terms of putting the brakes. ODERATOR: ome practical question is, how do you usually get the profile of the outofsample error? Do you sacrifice points, or PROFEOR: OK. This is obviously a good question. When we talk about validation validation has an impact on overfitting. ts used to do that. But its also used in model selection in general. And because of that, its very tempting to say, m going to use validation, and m going to set aside a number of points. But obviously, the problem is that when you set aside a number of points, you deprive yourself from a resource that you could have used for training, in order to arrive at a better hypothesis. o theres a tradeoff, and well discuss that tradeoff in very specific terms, and find ways to go around it, like crossvalidation. But this will be the subject of the lecture on validation, coming up soon. ODERATOR: n the example of the color plots, here the order of the polynomial is a good indication of the V dimension, right? PROFEOR: These are the plots. What is the question? ODERATOR: Here, Q_f is directly related to the V dimension, right? PROFEOR: The target complexity has nothing to do with the V dimension. ts the target. m talking about different targets. The V dimension has to do only with the two fellows we are using. We are using H_2 and H_10, 2ndorder polynomials and 10thorder polynomials. o if we take the degrees of freedom as being a V dimension, they will have different V dimension. And the discrepancy in the V dimension, given the same number of examples, is the reason why we have discrepancy in the outofsample error. But you also have a discrepancy in the insample error. And the case of overfitting is such that the insample error is moving in one direction, and the outofsample moving in another direction. o the only relevant thing in this plot to the V dimension is the fact that the two models have different V dimensions, H_2 and H_10. ODERATOR: guess you never really have a measure on the target complexity, like in practice? PROFEOR: orrect. This was an illustration. And even in the case of the illustration, when we had explicitly a definition of the target complexity, it wasnt completely clear how to map this into energy of deterministic noise, a counterpart for sigma squared here. This is completely clean. And as you can see, because of that, the plot is very regular. Here, first we define this in a particular case, in order to be able to run an experiment. econd, in terms of that, its not clear what is can you tell me what is the energy of the deterministic noise here? Theres quite a bit of normalization that was done. o when we normalize the target in order to make sigma squared meaningful, we sacrifice the fact the target now is sandwiched between limited range. And therefore the amount of energy, of whatever the deterministic noise is, will be limited, regardless of how complex is the target is. o there is a compromise we had to do, in order to be able to find these plots. However, the moral of the story here is that theres something about the target complexity that behaved in the same way, as far as overfitting is concerned, as noise. And we identified it as deterministic noise. We didnt quantify it further. And it will be ts possible to quantify it. You can get the energy for this and that, and you can do it. But these are research topics. As far as we are concerned, in a real situation, we wont be able to identify either the stochastic noise or the deterministic noise. We just know theyre there. We know the impact of overfitting. And we will be able to find methods in order to be able to cure the overfitting, without knowing all of the specifics that we could possibly know about the noise involved. ODERATOR: Do you ever measure the is there some similar kind of measure of the model complexity, of the target function? Do you ever use a V dimension for that? PROFEOR: Not explicitly. One can apply it. You say what is the model that would include the target function? And then, based on the inclusion of the target function, you can say that this is the complexity of that model. The analysis we use is such that the complexity of the target function doesnt come in, in terms of the V analysis. But there are other methods. There are other approaches, other than the V analysis, where the target complexity matters. o didnt particularly spend time trying to capture the complexity of the target function until this moment, where the complexity of the target function could translate to something in the biasvariance decomposition, and that has an impact on overfitting and generalization. ODERATOR: think thats it. PROFEOR: We will see you on Thursday.","o this is the thing, and m going to measure the level of noise by the energy in that noise, and were going to call it sigma squared. o this is the case where you take the 2ndorder polynomial model, H_2, and the inevitable error, which is the black line, comes now not only from the limitations of the model an inability for a 2nd order to replicate a 10th order, which is the target in this case but also because there is noise added. The other thing is that, if give you a particular point x, deterministic noise is a fixed amount, which is the difference between the value of the target at that point and the best hypothesis approximation you have. And if you apply this to, again, the same case we had before, you look at here, and the outofsample error for the red is terrible. o the problem here is that for the finite N, you get to try to fit the noise, both stochastic and deterministic. And then, based on the inclusion of the target function, you can say that this is the complexity of that model. Now, there are many things you can say about this curve, and one of them is, in the beginning when you start with a random w, in spite of the fact that youre using a full neural network, when you evaluate on this point, you have only one hypothesis that does not depend on the data set. And this is what you get. This is the target. This is a noisy case, where the target itself the deterministic part of the target is a function, and then there is added noise. That intensity will be depending on the number of points, and the level of the noise that you have. And the leap of faith we had is that this quantity, which is the average hypothesis you get over all data sets, is about the same as the best hypothesis in the hypothesis set. o you plot the outofsample, and this is what you get. o this is the data, this is the target, and the data lies on the target. However, if you look at it, and now you speak in terms of stochastic noise and deterministic noise, and you would like to see what affects overfitting. You can get the energy for this and that, and you can do it. And what you do, you go about your training, and at the end of every epoch, when you evaluate the insample error on the entire batch, which is the green curve here, you also evaluate, for that set of weights the frozen weights at the end of the epoch you evaluate that on the test set, and you get a point. econd, in terms of that, its not clear what is can you tell me what is the energy of the deterministic noise here? m going to have a plot in which you get N, the number of examples, and the level of noise, sigma squared. o this is our idea, that if have a hypothesis set, and there is part of the target that cannot capture, theres no point in trying to capture it, because when you try to capture it, you are detecting a false pattern that you cannot extrapolate, given your limitations. We defined what a realization of deterministic noise is, but its not clear to us what quantity we should measure out of deterministic noise, in order to tell us that this is the level of noise that results in overfitting, yet. The data has noise in it, and you are trying to look at the finite sample set that you got, and youre trying to get it right. And in the first case, we are going now to call it stochastic noise. And you tell them, guys, m not going to tell you what the target is, because if tell you what the target is, this is no longer machine learning. When you compare the outofsample errors of the two models, you have a final hypothesis from H_2, and this is the fit the green curve that you have seen. But the difference is not dramatic, and this is the performance you get. And this is the added term, which is just sigma squared, the energy of the noise. And why is this the case, when there is no noise? f tell you what is the stochastic noise, you will say, heres my target, and there is something on top of it. But it gets a finite sample, and it can use its resources to try to fit part of that noise, and that is what is causing overfitting. But the problem is that once you have a finite sample, youre given the unfortunate ability to be able to fit the noise, and you will indeed fit it. f you want to define an overfit measure based on the two, what you do is you get the outofsample error for the more complex guy, minus the out of sample error for the simple guy. And that is the part we are labeling the deterministic noise. o based on the noise, it gives you something outofsample and tells you this is the pattern in the data, obviously, which it isnt. And you can see that its not fitting all the way, because it really is not getting the points right. And if we go back to the experiment we had, this is just one instance of those, where the target complexity here is 10. use the 10thorder polynomial, so Q_f is 10. Epsilon is generated on the test point x, which is independent, and therefore you will get 0. o its very easy to argue that this is 0, and you will get basically the same decomposition with this fellow added. And then when you go for the outofsample, you are comparing the red curve to the blue curve, and the news is not good. And that will lead us to defining that there is an actual noise in this case, and well analyze it and understand what it is about. And this is the best can do as far as the noise, which is just not predicting anything in the noise. What m going list here, as you see, m going to say, what is the insample error, what is the outofsample error, for the 2nd order which is already here, and the 10th order, which havent shown yet. That is, the reason m overfitting is because m going for E_in, and m minimizing it, and m going all the way. o this is the case. Now, both the deterministic noise and the stochastic noise will have a finite version on the data points, and the algorithm will try to fit them. And then you generate data, and the data will be noisy, because we want to investigate the impact of noise on overfitting. But the good news is that, if you get the expected value of the cross terms, all of them will go to 0. m calling it Q_f, because it describes the target complexity of f, just to remember that its related to f. And what do, define a polynomial, which is the sum of coefficients times a power of x, from q equals 0 to Q, so its indeed a Qthorder polynomial, and add the noise here. s it OK to say that the deterministic noise is the part of reality that is too complex to be modeled? And now m going to fit the noise. And m going to assume that the expected value of the noise is 0. And the main reason for the case study, because we vaguely now understand that its a problem of the noise, so lets see how does the noise affect the situation? And if this is negative, it means that actually the more complex guy is doing better, so you are not overfitting. We are going to take, as general target function m going to describe what it is, and m going to add noise to it. n spite of the fact that the parameters of the noise distribution depend on x can have different noise for different points in the space the realization of epsilon is independent from one x to another. But the question of complexity is not one of the things that you just apply the general idea of: let me match the target function. But if give you only a finite sample, then you only get few points, and you may be able to capture a little bit of the stochastic noise, or the deterministic noise in this case. And therefore the amount of energy, of whatever the deterministic noise is, will be limited, regardless of how complex is the target is. o the purpose here is to understand that, even in the case of a noiseless target in the conventional sense, there is something that we can identify conceptually identify that does affect the overfitting. This is the case where we have increasing target complexity, so increasing deterministic noise in the terminology we just introduced, and the number of points. And this guy is the other one, which is the complex target without noise. o this is what you get. This is what you get. This is what you get. The other ones will go to 0, because the expected value of epsilon goes to 0, and epsilon is independent of the other random thing here, which is the data set. One of them is the same thing we have a 2ndorder polynomial that were going to use to fit. remember, this is the hypothesis we get, and we have dependency on the data set that got us. And were going to have a 10thorder polynomial These are the two guys that we are going to use. o you do this, and you know that there is no noise, so you decide on this idea, and again you get bad performance. And we understand that overfitting will not happen for all the numbers of examples, but for a small number of examples where you cannot pin down the function, then you suffer from the usual bad generalization that we saw. Now lets say that tell you that the target is 10th order, and you have two learners. And then it goes down, and it converges to something that is better, because that carries the ability of H_10 to approximate a 10th order, which should be perfect, except that we have noise. However, the moral of the story here is that theres something about the target complexity that behaved in the same way, as far as overfitting is concerned, as noise. And were going to plot it red, because we want to pay attention to it, and because we are going to get the expected values with respect to it. The interesting thing is that there is another term here which is corresponding to the deterministic noise. This is not the source of the problem of overfitting. And now, because y depends on the noise, m not only getting the averaging with respect to the data set, m also getting the average with respect to the realization of the noise. Now lets look at what is overfitting as a definition, and what is the culprit for it. This is the case of the 50thorder target, the higherorder target that doesnt have any noise conventional noise, at least. And what you are going to do, you are going to take this test set and evaluate what happens outofsample. Because this is for all, and this one will be better for H_10 than H_2, because H_10 is closer to the target we want, and therefore we will be making smaller error. And it will give you a very good feel for overfitting, because now we are going to look at the figure, and have no doubt in our mind that overfitting will occur whenever you actually encounter a real problem. And even if we dont know the particulars of it, we will have to put in place the guards, in order to avoid overfitting. And that ended up being a variance, which tells me how far am from the centroid within the hypothesis set, and that means that theres a variety of things get based on D. And the other one is how far the centroid is from the target, which tells me the bias of my hypothesis set from the target. Depending on what happens, will go this way and that way not because its indicated by the target function want to learn, but just because there is a noise present in the sample that am blindly following, because cant distinguish noise from signal, and therefore end up with more variety, and end up with worse variance and overfit. Overfitting is real and here to stay, and we have to deal with it. And we understand that noise is in the eye of the beholder, so to speak. And when the number of examples is bigger, then theres less overfitting, as you expect it to be. o didnt particularly spend time trying to capture the complexity of the target function until this moment, where the complexity of the target function could translate to something in the biasvariance decomposition, and that has an impact on overfitting and generalization. The noise is whatever the distance between the points and the curve is. Youre still not going to get f, because your hypothesis set is limited, but the best guy will be sitting there, and it will fail to pick certain part of the target. o all of this actually is due to the noise added to the examples. Because if you pick these guys at random lets say independent coefficients at random, in order to generate a general target, these guys are the powers of x. o you start with the x, and then the parabola, and then the 3rd order, and then the 4th order, and then the 5th order. And then, remember that we take the expected value with respect to x, average over all the space, in order to get just the bias and variance, rather than the bias of x of your test point. And if you have more examples, the two curves will converge, as they always do, but they converge to the inevitable amount of error, which is dictated by the fact that youre using such a simple model in this case. o in this case we have the noise level. And the most important aspect about it is that its differentiable its a smooth function, and therefore the dependency of the error in the output on the parameters w_ij will be a wellbehaved function, for which we can apply things like gradient descent. Everybody can fit, but if you know what overfitting is, and how to deal with it, then you have an edge that someone who doesnt know the fundamentals would not be able to comprehend. And the discrepancy in the V dimension, given the same number of examples, is the reason why we have discrepancy in the outofsample error. Whether its stochastic that it doesnt make sense or deterministic, that there is no point in fitting it, because you know in your hypothesis set, there is no way to generalize outofsample for it. n the case here, its particular to the hypothesis set that have. The culprit, in this case, is that youre actually fitting the noise. Now, d like to add noise to the target, and see how this decomposition will go, because this will give us a very good insight into the role of stochastic noise versus deterministic noise. And even in the case of the illustration, when we had explicitly a definition of the target complexity, it wasnt completely clear how to map this into energy of deterministic noise, a counterpart for sigma squared here. You can see the red curve is trying to go for these guys, and you know that these guys are off the target. But get close enough, in the sense that the value is close to the minimum, and therefore you achieve the small error that you want. And then we are going to identify the main culprit for overfitting, which is noise. You take a hypothesis set that lets say simple, and you look for the guy that best approximates f. Not in the learning sense. We compare it to the target function, and we get the expected value with respect to those. And the final guy that affects overfitting is the number of data points. Now, putting the brakes means that youre going to not allow yourself to go all the way, and you are going to have a restrained fit. n this situation, it might be a very good idea to be able to detect when this happens, and simply stop at that point and report that, instead of reporting the final hypothesis you will get after all the iterations, right? And we will be able to find methods in order to be able to cure the overfitting, without knowing all of the specifics that we could possibly know about the noise involved. o its a rather interesting irony, because there is a thought in peoples mind that you try to get as much information about the target function, and put it in the hypothesis set. We know that the problem is noisy, but we cannot identify the noise. The noise is a function of x. o m just getting it generically, and as always, we have independence from one x to another. o the first thing is to replace this fellow by the noisy version, which is y. know that y has f of x, plus the noise. The analysis we use is such that the complexity of the target function doesnt come in, in terms of the V analysis. We have the moving from your hypothesis to the centroid, from the centroid to the target proper, and then from the target proper to the actual output, which has a noise aspect to it. And when you do that, m going to avoid overfitting this way. o the only relevant thing in this plot to the V dimension is the fact that the two models have different V dimensions, H_2 and H_10. That is, m going to generate a 50thorder polynomial, so its obviously much more elaborate than the blue curve here, but m not going to add noise to it. You have the data points, and you fit them, and its not surprising. But obviously, the problem is that when you set aside a number of points, you deprive yourself from a resource that you could have used for training, in order to arrive at a better hypothesis. o if you can think of an effective V dimension as you go, if you can define that, then there is an effective V dimension that is growing with time until it gets after you have explored the whole space, or at least potentially explored the whole space, if you had different data sets then you have the effective V dimension, will be the total number of free parameters in the model. And getting that notion down is very important to understand the situations in practice, where you are going to get overfitting. m going to study the impact of two things the noise level, which already conceptually convinced myself that its related to overfitting, and the target complexity, just because it does seem to be related. And given that hypothesis set, you already concede that even if the target is noiseless, there is part of it which behaves as noise, as far as m concerned. We have the one in the case of stochastic noise very easily. Because you think, give the data, the data has the signal and the noise. o here we realize that as you increase the target complexity, the deterministic noise increases, which is the overfitting phenomenon that we observe increases. You say what is the model that would include the target function? That was the goal here, rather than try to Any time you see the target function drawn, you should immediately have an alarm bell that this is conceptual, because you never actually see the target function in a real learning situation. And if you think from an operational point of view, if you are that hypothesis, noise is all the same. On those points, the algorithm will try to fit that noise, in spite of the fact that if it knew, it wouldnt, because it knows that theyre out of reach. For the same target function, if you use a more sophisticated hypothesis set, the deterministic noise will be smaller, because you were able to capture more. Now, if just told you that you have five points, which is the case you face when you learn. And we are looking at deterministic noise, as it relates to the target complexity. o it really is, what is the V dimension of your model? And as you can see, because of that, the plot is very regular. f you take the 1storder Legendre, then the 2nd, and the 3rd, and the 4th, and you take the inner product, you see they are 0. And then we added noise to the examples, so instead of getting the points perfectly on that line, you get points right or left. And if give you two instances of x, the same x, the noise will change from one occurrence to another, whereas here, its the same. We will find out that the reason we are getting overfitting here, because actually this guy has noise. What is the cure in this case for overfitting? But if you are matching the complexity, here the guy who actually took the 10thorder target, and decided to put the information all too well in the hypothesis m taking a 10thorder hypothesis set lost. o you are learning a 10thorder target, and the target is noisy. For this guy, we get something that is nice, and its really according to what we expect. And you can see here that youre actually fitting the noise. Again, if the expected value is something else, we put that in the target, and leave the part which is pure fluctuation outside, and call that epsilon. o it really doesnt have enough parameters to fit, if we had all the glory of the target function in front of us. And we look at this and say that if there is overfitting, wed better be able to detect it, in order to stop earlier than we would otherwise, because otherwise we will be harming ourselves. aybe theres another way to be able to tell what is actually happening out of sample, and therefore avoid overfitting, because you can check on what is happening in the real quantity you care about. And gave you something, and told you that this is an actual situation well see later, and this is the situation. And the overfit measure in this case will be positive, so we have overfitting. We are fitting the noise. And we would like to understand what deterministic noise is about. ODERATOR: n the example of the color plots, here the order of the polynomial is a good indication of the V dimension, right? Now, in order to run the experiment right, m going to normalize this quantity, such that the energy here is always 1. The point want to make here is that, when you say overfitting, overfitting is a comparative term. We are in the business of overfitting. We understand what overfitting is, and we understand that its due to noise. And because of that, its very tempting to say, m going to use validation, and m going to set aside a number of points. PROFEOR: Theyre the same in the sense that they are part of the outputs that m being given, or that m trying to predict. You get some error, and the other one is bigger than it. This is the random w that you got. o there is a noise that is contributing to that. And we took a final example, where we were trying to implement a circle boundary in this case, and we realized that we can actually do this at least approximate it if we have a sufficient number of perceptrons. You have more to fit and you fit it better, so you get smaller insample error. There is nothing mysterious about them, except that the choice of the coefficients is such that, from one order to the next, theyre orthogonal to each other. The red curve sees the data, tries to fit that, uses all the degrees of freedom it has it has 11 of them and then it gets this guy. And if going from the 2ndorder polynomial to the 10thorder polynomial gets us in trouble, then we are overfitting. t looks like noise to some, and not look like noise to other, and we call that deterministic noise. And this is the expected value of outofsample error. And the case of overfitting is such that the insample error is moving in one direction, and the outofsample moving in another direction. Because if the more complex guy is worse, it means its outofsample error is bigger, and you get a positive number, large positive if the overfitting is terrible. o the reason m calling this overfitting is because, if you use, lets say, 3rdorder polynomial, you will not be able to achieve zero training error, in general. o the outline today is, first, we are going to start what is the notion? But the techniques that were going to use here, which are regularization and validation, apply to all of these models. You tell me what the hypothesis set is, well, thats the best can do. Now if you look at the last quantity, that is patently the stochastic noise. And if you look at these two guys, the main observation is that the red region is serious. ts a question of the variance term that results in overfitting, not this guy, in spite of the fact that these guys contain both types of noise contributing to their value.",0.1229676185271212
39,39,"tanford niversity. Okay, so lets get going. Welcome back to the second class of 224N /Ling 284, Natural Language Processing with Deep Learning. o this class is gonna be almost the complete opposite of the last class. o in the last class, it was a very high level picture of sort of trying from the very top down. ort of say a little bit about what is natural language processing, what is deep learning, why its exciting, why both of them are exciting and how d like to put them together? o for todays class were gonna go completely to the opposite extreme. Were gonna go right down to the bottom of words, and were gonna have vectors, and were gonna do baby math. Now for some of you this will seem like tedious repetitive baby math. But think that there are probably quite a few of you for which having some math review is just going to be useful. And this is really the sort of foundation on which everything else builds. And so if you dont have sort of straight the fundamentals right at the beginning of how you can use neural networks on the sort of very simplest kind of structures, its sort of really all over from there. o what d like to do today is sort of really go slowly and carefully through the foundations of how you can start to do things with neural networks in this very simple case of learning representations for words. And hope thats kind of a good foundation that we can build on forwards. And indeed thats what were gonna keep on doing, building forward. o next week Richard is gonna keep on doing a lot of math from the ground up to try, and really help get straight some of the foundations of Deep Learning. Okay, so this is basically the plan. o tiny bit word meaning and no, Tiny bit on word meaning then start to introduce this model of learning word vectors called Word2vec. And this was a model that was introduced by Thomas ikolov and colleagues at Google in 2013. And so there are many other ways that you could think about having representations of words. And next week, Richards gonna talk about some of those other mechanisms. But today, wanna sort of avoid having a lot of background and comparative commentary. o m just gonna present this one way of doing it. And youd also pretty study the good way of doing it, so its not a bad one to know. Okay, so then after that, were gonna have the first or was it gonna be one of the features of this class. We decided that all the evidence says that students cant concentrate for 75 minutes. o we decided wed sort of mix it up a little, and hopefully, also give people an opportunity to sort of get more of a sense of what some of the exciting new work thats coming out every month in Deep Learning is. And so what were gonna do is have one TA each time, do a little research highlight. Which will just be sort of a like a verbal blog post of telling you a little bit about some recent paper and why its interesting, exciting. Were gonna start that today with Danqi. Then after that, wanna go sort of carefully through the word to vec objective function gradients. Refresher little on optimization, mention the assignment, tell you all about Word2vec thats basically the plan, okay? o we kinda wonder sort of have word vectors as mentioned last time as a model of word meaning. Thats a pretty controversial idea actually. And just wanna give kind of a few words of context before we dive into that and do it anyway. Okay, so if you look up meaning in a dictionary cuz a dictionary is a storehouse of word meanings after all. What the Websters dictionary says is meaning is the idea that is represented by a word, phrase, etc. The idea that a person wants to express by using words, signs, etc, etc. n some sense, this is fairly close to what is the commonest linguistic way of thinking of meaning. o standardly in linguistics, you have a linguistic sign like a word, and then it has things that it signifies in the world. o if have a word like glasses then its got a signification which includes these and there are lots of other pairs of glasses can see in front of me, right? And those things that it signifies, the denotation of the term glasses. That hasnt proven to be a notion of meaning thats been very easy for people to make much use of in computational systems for dealing with language. o in practice, if you look at what computational systems have done for meanings of words over the last several decades. By far the most common thing thats happened is, people have tried to deal with the meaning of words by making use of taxonomic resources. And so if theyre English, the most famous taxonomic resource is WordNet. And its famous, maybe not like Websters is famous. But its famous among computational linguists. Because its free to download a copy and thats much more useful than having a copy of Websters on your shelf. And it provides a lot of taxonomy information about words. o this little bit of Python code. This is showing you getting a hold of word net using the nltk which is one of the main Python packages for nlp. And so then m asking it for the word panda, not the Python package Panda, the word panda. Then m saying, well tell me about the hypernym the kind of things that its the kind of. And so for Panda its sort of heading up through carnivores, placentals, mammals up into sort of abstract types like objects. Or on the right hand side, m sort of asking for the word good, will tell me about synonyms of good. And part of what your finding there is, well WordNet is saying, well the word good has different senses. o for each sense, let me tell you some synonyms for each sense. o one sense, the second one is sort of the kind of good person sense. And theyre suggesting synonyms like honorable and respectable. But there are other ones here where this pair is good to eat and thats sort of meaning is ripe. Okay, so you get this sort of sense of meaning. Thats been, thats been a great resource, but its also been a resource that people have found in practice. ts hard to get nearly as much value out of it as youd like to get out of it. And why is that? mean there are a whole bunch of reasons. mean one reason is that at this level of this sort of taxonomic relationships, you lose an enormous amount of nuance. o one of those synonym sets for good was adept, expert, good, practiced, proficient, skillful. But mean, it seems like those mean really different things, right? t seems like saying m an expert at deep learning. eans something slightly different to m good at deep learning. o theres a lot of nuance there. Theres a lot of incompleteness in WordNet so for a lot of the ways that people, se words more flexibly. o if say m a deeplearning ninja, or something like that, that thats not in WordNet at all. What kind of things you put into these synonym sets ends up very subjective, right? Which sense distinctions you make and which things you do and dont say are the same, its all very unclear. t requires, even to the extent that its made, its required many person years of human labor. And at the end of the day, its sort of, its kind of hard to get anything accurate out of it in the way of sort of word similarities. Like kind of feel that proficient is more similar to expert than good, maybe. But you cant get any of this kind of stuff out of WordNet. Okay, so therefore, thats sort of something of a problem. And its part of this general problem of discrete, or categorical, representations that started on last time. o, the fundamental thing to note is that for sorta just about all NLP, apart from both modern deep learning and a little bit of neural net work NLP that got done in the 1980s, that its all used atomic symbols like hotel, conference, walk. And if we think of that from our kind of jaundiced neural net direction, using atomic symbols is kind of like using big vectors that are zero everywhere apart from a one and one position. o what we have, is we have a lot of words in the language that are equivalent to our symbols and were putting a one in the position, in the vector, that represents the particular symbol, perhaps hotel. And these vectors are going to be really, really long. mean, how long depends on how you look at it. o sometimes a speech recognizer might have a 20,000 word vocabulary. o itd be that long. But, if were kinda building a machine translation system, we might use a 500,000 word vocabulary, so thats very long. And Google released sort of a 1terabyte corpus of web crawl. Thats a resource thats been widely used for a lot of NLP. And while the size of the vocabulary in that is 13 million words, so thats really, really long. o, its a very, very big vector. And so, why are these vectors problematic? m sorry, m not remembering my slides, so should say my slides first. Okay, so this is referred to in neural net land as onehot in coding because theres just this one on zero in the vector. And so, thats the example of a localist representation. o why is this problematic? And the reason why its problematic is it doesnt give any inherent notion of relationships between words. o, very commonly what we want to know is when meanings and words and phrases are similar to each other. o, for example, in a web search application, if the user searches for Dell notebook battery size, wed like to match a document that says Dell laptop battery capacity. o we sort of want to know that notebooks and laptops are similar, and size and capacity are similar, so this will be equivalent. We want to know that hotels and motels are similar in meaning. And the problem is that if were using onehot vector encodings, they have no natural notion of similarity. o if we take these two vectors and say, what is the dot product between those vectors, its zero. They have no inherent notion of similarity. And, something just wanna stress a little, since this is important, is note this problem of symbolic encoding applies not only to traditional rule base logical approaches to natural language processing, but it also applies to basically all of the work that was done in probabilistic statistical conventional machine learning base natural language processing. Although those Latin models normally had real numbers, they had probabilities of something occurring in the context of something else, that nevertheless, they were built over symbolic representations. o that you werent having any kind of capturing relationships between words and the models, each word was a nation to itself. Okay, so thats bad, and we have to do something about it. Now, as ve said, theres more than one thing that you could do about it. And so, one answer is to say, okay gee, we need to have a similarity relationship between words. Lets go over here and start building completely separately a similarity relationship between words. And, of course, you could do that. But m not gonna talk about that here. What instead m going to talk about and suggest is that what we could do is we could explore this direct approach, where the representation of a word encodes its meaning in such a way that you can just directly read off from these representations, the similarity between words. o what were gonna do is have these vectors and do something like a dot product. And that will be giving us a sense of the similarity between words. Okay, so how do we go about doing that? And so the way we gonna go about doing that is by making use of this very simple, but extremely profound and widely used, NLP idea called distributional similarity. o this has been a really powerful notion. o the notion of distributional similarity is that you can get a lot of value for representing the meaning of a word by looking at the context in which it appears and doing something with those contexts. o, if want to know what the word banking means, what m gonna do is find thousands of instances of the word banking in text and m gonna look at the environment in which each one appeared. And m gonna see debt problems, governments, regulation, Europe, saying unified and m gonna start counting up all of these things that appear and by some means, ll use those words in the context to represent the meaning of banking. The most famous slogan that you will read everywhere if you look into distributional similarity is this one by JR Firth, who was a British linguist, who said, you shall know a word by the company it keeps. But this is also really exactly the same notion that Wittgenstein proposed in his later writings where he suggested a use theory of meaning. Where, somewhat controversially, this not the main stream in semantics, he suggested that the right way to think about the meaning of words is understanding their uses in text. o, essentially, if you could predict which textual context the word would appear in, then you understand the meaning of the word. Okay, so thats what were going to do. o what we want to do is say for each word were going to come up for it a vector and that dense vector is gonna be chosen so that itll be good at predicting other words that appear in the context of this word. Well how do we do that? Well, each of those other words will also have a word that are attached to them and then well be looking at sort of similarity measures like dot product between those two vectors. And were gonna change them as well to make it so that good at being able to be predicted. o it all kind off gets a little bit recursive or circular, but were gonna come up with this clever algorithm to do that, so that words will be able to predict their context words and viceversa. And so m gonna go on and say a little bit more about that. But let me just underline one bit of terminology that was appearing before in the slide. o we saw two keywords. One was distributional, which was here. And then weve had distributed representations where we have these dense vectors to represent the meaning of the words. Now people tend to confuse those two words. And theres sort of two reasons they confuse them. One is because they both start with distribute and so theyre kind of similar. And the second reason people confuse them is because they very strongly cooccur,. o that distributed representations and meaning have almost always, up until now, been built by using distributional similarity. But did just want people to gather that these are different notions, right? o the idea of distributional similarity is a theory about semantics of word meaning that you can describe the meaning of words by as a use theory of meaning, understanding the context in which they appear. o distributional contrasts with, way back here when said but didnt really explain, denotational, right? The denotational idea of word meaning is the meaning of glasses is the set of pairs of glasses that are around the place. Thats different from distributional meaning. And distributed then contrasts with our onehot word vector. o the onehot word vectors are localist representation where youre storing in one place. Youre saying here is the symbol glasses. ts stored right here whereas in distributed representations were smearing the meaning of something over a large vector space. Okay, so thats part one. And were now gonna sorta be heading into part two, which is what is Word2vec? Okay, and so ll go almost straight into this. But this is sort of the recipe in general for what were doing for learning neural word embeddings. o were gonna define a model that aims to predict between a center word and words that appear in its context. Kind of like we are here, the distributional wording. And well sort of have some, perhaps probability measure or predicts the probability of the context given the words. And then once we have that we can have a loss function as to whether we do that prediction well. o ideally wed be able to perfectly predict the words around the word so the minus t means the words that arent word index t so the words around t. f we could predict those perfectly from t wed have probability one so wed have no loss but normally we cant do that. And if we give them probability a quarter then well have sort of three quarters loss or something, right? o well have a loss function and well sort of do that in many positions in a large corpus. And so our goal will be to change the representations of words so as to minimize our loss. And at this point sort of a miracle occurs. ts sort of surprising, but true that you can do no more than set up this kind of prediction objective. ake it the job of every words word vectors to be such that theyre good at predicting their words that appear in their context or vice versa. You just have that very simple goal and you say nothing else about how this is gonna be achieved, but you just pray and depend on the magic of deep learning. And this miracle happens and outcome these word vectors that are just amazingly powerful at representing the meaning of words and are useful for all sorts of things. And so thats where we want to get into more detail and say how that happens. Okay. o that representation was meant to be meaning all words apart from the wt, yes, what is this w minus t mean? m actually not gonna use that notation again in this lecture. But the w minus t, minus is sometimes used to mean everything except t. o wt is my focus word, and w minus t is in all the words in the context. Okay, so this idea that you can learn low dimensional vector representations is an idea that has a history in neural networks. t was certainly present in the 1980s, parallel distributed processing era including work by Rumelhart on learning representations by backpropagating errors. t really was demonstrated for word representations in this pioneering early paper by Yoshua Bengio in 2003 and neural probabilistic language model. mean, at the time, sort of not so many people actually paid attention to this paper, this was sort of before the deep learning boom started. But really this was the paper where the sort of showed how much value you could get from having distributed representations of words and be able to predict other words in context. But then as things started to take off that idea was sort of built on and revived. o in 2008, ollobert and Weston started in the sort of modern direction by saying, well, if we just want good word representations, we dont even have to necessarily make a probabilistic language model that can predict, we just need to have a way of learning our word representations. And thats something thats then being continued in the model that m gonna look at now, the word2vec model. That the emphasis of the word2vec model was how can we build a very simple, scalable, fast to train model that we can run over billions of words of text that will produce exceedingly good word representations. Okay, word2vec, here we come. The basic thing word2vec is trying to do is use theory of meaning, predict between every word and its context words. Now word2vec is a piece of software, mean, actually inside word2vec its kind of a sort of a family of things. o there are two algorithms inside it for producing word vectors and there are two moderately efficient training methods. o for this class what m going to do is tell you about one of the algorithms which is a skipgram method and about neither of the moderately efficient training algorithms. nstead m gonna tell you about the hopelessly inefficient training algorithm but is sort of the conceptual basis of how this is meant to work and that the moderately efficient ones, which ll mention at the end. And then what youll have to do to actually make this a scalable process that you can run fast. And then, today is also the day when were handing out assignment one and ajor part of what you guys get to do in assignment one is to implement one of the efficient training algorithms, and to work through the method one of those efficient training algorithms. o this is the picture of the skipgram model. o the idea of the skipgram model is for each estimation step, youre taking one word as the center word. o thats here, is my word banking and then what youre going to do is youre going to try and predict words in its context out to some window size. And so, the model is going to define a probability distribution that is the probability of a word appearing in the context given this center word. And were going to choose vector representations of words so we can try and maximize that probability distribution. And the thing that well come back to. But its important to realize is theres only one probability distribution, this model. ts not that theres a probability distribution for the word one to the left and the word one to the right, and things like that. We just have one probability distribution of a context word, which well refer to as the output, because its what we, produces the output, occurring in the context close to the center word. s that clear? Yeah, okay. o thats what we kinda wanna do so were gonna have a radius m and then were going to predict the surrounding words from sort of positions m before our center word to m after our center word. And were gonna do that a whole bunch of times in a whole bunch of places. And we want to choose word vectors such as that were maximizing the probability of that prediction. o what our loss function or objective function is is really this J prime here. o the J prime is saying were going to, so were going to take a big long amount of text, we take the whole of Wikipedia or something like that so we got big long sequence of words, so there are words in the context and real running text, and were going to go through each position in the text. And then, for each position in the text, were going to have a window of size 2m around it, m words before and m words after it. And were going to have a probability distribution that will give a probability to a word appearing in the context of the center word. And what wed like to do is set the parameters of our model so that these probabilities of the words that actually do appear in the context of the center word are as high as possible. o the parameters in this model of these theta here that show here and here. After this slide, kinda drop the theta over here. But you can just assumed that there is this theta. What is this theta? What is theta is? ts going to be the vector representation of the words. The only parameters in this model of the vector representations of each word. There are no other parameters whatsoever in this model as youll see pretty quickly. o conceptually this is our objective function. We wanna maximize the probability of this predictions. n practice, we just slightly tweak that. Firstly, almost unbearably when were working with probabilities and we want to do maximization, we actually turn things into log probabilities cuz then all that products turn into sums and our math gets a lot easier to work with and so thats what ve done down here. Good points. And the question is, hey, wait a minute youre cheating, windows size, isnt that a parameter of the model? And you are right, this is the parameter of the model. o guess was a bit loose there. Actually, it turns out that there are several hyper parameters of the model, so did cheat. t turns out that there are a few hyper parameters of the model. One is Windows sized and it turns out that well come across a couple of other fudge factors later in the lecture. And all of those things are hyper parameters that you could adjust. But lets just ignore those for the moment, lets just assume those are constant. And given those things arent being adjusted, the only parameters in the model, the factor representations of the words. What m meaning is that theres sort of no other probability distribution with its own parameters. Thats a good point. buy that one. o weve gone to the log probability and the sums now and, and then rather than having the probability of the whole corpus, we can sort of take the average over each positions so ve got 1 on T here. And thats just sort of a making it per word as sort of a kinda normalization. o that doesnt affect whats the maximum. And then, finally, the machine learning people really love to minimize things rather than maximizing things. And so, you can always swap between maximizing and minimizing, when youre in plus minus land, by putting a minus sign in front of things. And so, at this point, we get the negative log likelihood, the negative log probability according to our model. And so, thats what we will be formally minimizing as our objective function. o if there were objective function, cost function, loss function, all the same, this negative log likelihood criterion really that means that were using this our crossentropy loss which is gonna come back to this next week so wont really go through it now. But the trick is since we have a one hot target, which is just predict the word that actually occurred. nder that criteria the only thing thats left in cross entropy loss is the negative probability of the true class. Well, how are we gonna actually do this? How can we make use of these word vectors to minimize that negative log likelihood? Well, the way were gonna do it is were gonna come with the probably distribution of context word, given the center word, which is constructed out of our word vectors. And so, this is what our probability distribution is gonna look like. o just to make sure were clear on the terminology m gonna use forward from here. o c and o are indices in the space of the vocabulary, the word types. o up here, the t and the t plus j, where in my text there are positions in my text. Those are sort of words, 763 in words 766 in my text. But here o and c in my vocabulary words have word types and so have my p for words 73 and 47 in my vocabulary words. And so, each word type theyre going to have a vector associated with them so u o is the vector associated with context word in index o and vc is the vector thats associated with the center word. And so, how we find this probability distribution is were going to use this, whats called a oftmax form, where were taking dot products between the the two word vectors and then were putting them into a oftmax form. o just to go through that kind of maximally slowly, right? o weve got two word vectors and were gonna dot product them, which means that we so take the corresponding terms and multiply them together and sort of sum them all up. o may adopt product is sort of like a loose measure of similarity so the contents of the vectors are more similar to each other the number will get bigger. o thats kind of a similarity measure through the dot product. And then once weve worked out dot products between words were then putting it in this oftmax form. o this oftmax form is a standard way to turn numbers into a probability distribution. o when we calculate dot products, theyre just numbers, real numbers. They could be minus 17 or 32. o we cant directly turn those into a probability distribution so an easy thing that we can do is exponentiate them. Because if you exponentiate things that puts them into positive land so its all gonna be positive. And thats a good basis for having a probability distribution. And if you have a bunch of numbers that come from anywhere that are positive and you want to turn them into a probability distribution thats proportional to the size of those numbers, theres a really easy way to do that. Which is you sum all the numbers together and you divide through by the sum and that then instantly gives you a probability distribution. o thats then denominated that is normalizing to give a probability and so when you put those together, that then gives us this form that were using as our oftmax form which is now giving us a probability estimate. o thats giving us this probability estimate here built solely in terms of the word vector representations. s that good? Yeah. That is an extremely good question and was hoping to delay saying that for just a minute but youve asked and so will say it. Yes, you might think that one word should only have one vector representation. And if you really wanted to you could do that, but it turns out you can make the math considerably easier by saying now actually each word has two vector representation that has one vector representation when it synthesis the word. And it has another vector representation when its a context word. o thats formally what we have here. o the v is the center word vectors, and the u are the context word vectors. And it turns out not only does that make the math a lot easier, because the two representations are separated when you do optimization rather than tied to each other. ts actually in practice empirically works a little better as well, so if your life is easier and better, who would not choose that? o yes, we have two vectors for each word. Any other questions? Yeah, so the question is, well wait a minute, you just said this was a way to make everything positive, but actually you also simultaneously screwed with the scale of things a lot. And thats true, right? The reason why this is called a oftmax function is because its kind of close to a max function, because when you exponentiate things, the big things get way bigger and so they really dominate. And so this really sort of blows out in the direction of a max function, but not fully. ts still a sort of a soft thing. o you might think that thats a bad thing to do. Doing things like this is the most standard underlying a lot of math, including all those super common logistic regressions, you see another classs way of doing things. o its a good way to know, but people have certainly worked on a whole bunch of other ways. And there are reasons that you might think theyre interesting, but wont do them now. Yes? Yeah, so the question was, when m dealing with the context words, am paying attention to where they are or just their identity? Yeah, where they are has nothing to do with it in this model. ts just, what is the identity of the word somewhere in the window? o theres just one probability distribution and one representation of the context word. Now you know, its not that thats necessarily a good idea. There are other models which absolutely pay attention to position and distance. And for some purposes, especially more syntactic purposes rather than semantic purposes, that actually helps a lot. But if youre sort of more interested in just sort of word meaning, it turns out that not paying attention to position actually tends to help you rather than hurting you. Yeah. Yeah, so the question is how, wait a minute, is there a unique solution here? ould there be different rotations that would be equally good? And the answer is yes, there can be. think we should put off discussing this cuz actually theres a lot to say about optimization in neural networks, and theres a lot of exciting new work. And the one sentence headline is its all good news, people spent years saying that minimal work ought to be a big problem and it turns out its not. t all works. But think we better off talking about that in any more detail. Okay, so yeah this is my picture of what the skip gram model ends up looking like. ts a bit confusing and hard to read, but also ve got it thrown from left to right. Right, so we have the center word thats a one hot vector. We then have a matrix of the representations of center words. o if we kind of do a multiplication of this matrix by that vector. We just sort of actually select out the column of the matrix which is then the representation of the center word. Then what we do is we have a second matrix which stores the representations of the context words. And so for each position in the context, show three here because that was confusing enough. Were going to multiply the vector by this matrix which is the context word representations. And so we will be picking out sort of the dot products of the center word with each context word. And its the same matrix for each position, right? We only have one context word matrix. And then these dot products, were gonna soft max then turn into a probability distribution. And so our model, as a generative model, is predicting the probability of each word appearing in the context given that a certain word is the center word. And so if we are actually using it generatively, it would say, well, the word you should be using is this one here. But if there is sort of actual ground truth as to what was the context word, we can sort of say, well, the actual ground truth was this word appeared. And you gave a probability estimate of 0.1 to that word. And so thats the basis, so if you didnt do a great job at prediction, then theres going to be some loss, okay? But thats the picture of our model. Okay, and so what we wanna do is now learn parameters, these word vectors, in such a way that we do as good a job at prediction as we possibly can. And so standardly when we do these things, what we do is we take all the parameters in our model and put them into a big vector theta. And then were gonna say were gonna do optimization to change those parameters so as to maximize objective function of our model. o what our parameters are is that for each word, were going to have a little d dimensional vector, when its a center word and when its a context word. And so weve got a vocabulary of some size. o were gonna have a vector for aardvark as a context word, a vector for art as a context word. Were going to have a vector of aardvark as a center word, a vector of art as a center word. o our vector in total is gonna be of length 2dV. Theres gonna be a big long vector that has everything that was in what was shown in those matrices before. And thats what we then gonna be saying about optimizing. And so after the break, m going to be so going through concretely how we do that optimization. But before the break, we have the intermission with our special guest, Danqi hen. Hi, everyone. m Danqi hen, and m the head TA of this class. o today will start our first research highlight session, and will introduce you a paper from Princeton. The title is A imple but Toughtobeat Baseline for entence Embeddings. o today we are learning the word vector representations, so we hope these vectors can encode the word meanings. But our central question in natural language processing, and also this class, is that how we could have the vector representations that encode the meaning of sentences like, natural language processing is fun. o with these sentence representations, we can compute the sentence similarity using the inner product of the two vectors. o, for example, exico wishes to guarantee citizens safety, and, exico wishes to avoid more violence. o we can use the vector representation to predict these two sentences are pretty similar. We can also use this sentence representation to use as features to do some sentence classification task. For example, sentiment analysis. o given a sentence like, natural language processing is fun, we can put our classifier on top of the vector representations and predict if sentiment is positive. Hopefully this is right, so. o there are a wide range of measures that compose word vector representations into sentence vector representations. o the most simple way is to use the bagofwords. o the bagofwords is just like the vector representation of the natural language processing. ts a average of the three single word vector representations, the natural, language, and processing. Later in this quarter, well learn a bunch of complex models, such as recurrent neural nets, the recursing neural nets, and the convolutional neural nets. But today, for this paper from Princeton, want to introduce that this paper introduces a very simple unsupervised method. That is essentially just a weighted bagofwords sentence representation plus remove some special direction. will explain this. o they have two steps. o the first step is that just like how we compute the average of the vector representations, they also do this, but each word has a separate weight. Now here, a is a constant. And the p(w), it means the frequency of this word. o this basically means that the average representation down weight the frequent words. Thats the very simple tep 1. o for the tep 2, after we compute all of these sentence vector representations, we compute the first principal components and also subtract the projections onto this first principle component. You might be familiar with this if you have ever taken 229 and also learned PA. o thats it. Thats their approach. o in this paper, they also give a probabilistic interpretation about why they want to do this. o basically, the idea is that given the sentence representation, the probability of the limiting or single word, theyre related to the frequency of the word. And also related to how close the word is related to this sentence representation. And also theres a 0 term that means common discourse vector. Thats usually related to some syntax. o, finally, the results. o first, they take context parents on the sentence similarity and they show that this simple approach is much better than the average of word vectors, all the TFDF rating, and also all the performance of other sophisticated models. And also for some supervised tasks like sentence classification, theyre also doing pretty well, like the entailment and sentiment task. o thats it, thanks. Thank you. Okay, Okay, so, and well go back from there. All right, so now were wanting to sort of actually work through our model. o this is what we had, right? We had our objective function where we wanna minimize negative log likelihood. And this is the form of the probability distribution up there, where we have these sort of word vectors with both center word vectors and context word vectors. And the idea is we want to change our parameters, these vectors, so as to minimize the negative log likelihood item, maximize the probability we predict. o if thats what we want to do, how can we work out how to change our parameters? Gradient, yes, were gonna use the gradient. o, what were gonna have to do at this point is to start to do some calculus to see how we can change the numbers. o precisely, what well going to want to do is to say, well, we have this term for working out log probabilities. o, we have the log of the probability of the word t plus j word t. Well, what is the form of that? Well, weve got it right here. o, we have the log of v maybe can save a line. Weve got this log of this. And then, what were gonna want to do is that were going to want to change this so that we have, m sorry, minimized in this objective. o, lets suppose we sort of look at these center vectors. o, what were gonna want to do is start working out the partial derivatives of this with respect to the center vector which is then, going to give us, how we can go about working out, in which way to change this vector to minimize our objective function. Okay, so, we want to deal with this. o, whats the first thing we can do with that to make it simpler? ubtraction, yeah. o, this is a log of a division so, we can turn that into a log of a subtraction, and then, we can do the partial derivatives separately. o, we have the derivative with Vc of the log of the exp of u0^T vc and then, weve got minus the log of the sum of w equals 1 to V of exp of u w^T vc. And at that point, we can separate it into two pieces, right, cuz when theres addition or subtraction we can do them separately. o, we can do this piece 1 and we can do the, work out the partial derivatives of this piece 2. o, piece 1 looks kind of easy so, lets start here. o, whats the first thing should do to make this simpler? Easy question. ancel some things out, log and x inverses of each other so, they can just go away. o, for 1, we can say that this is going to be the partial derivative with respect to Vc of u0^T vc. Okay, thats looking kind of simpler so, what is the partial derivative of this with respect to vc? u0, so, this just comes out as u0. Okay, and so, mean, effectively, this is the kind of level of calculus that youre gonna have to be able to do to be okay on assignment one thats coming out today. o, its nothing that life threatening, hopefully, youve seen this before. But nevertheless, we are here using calculus with vectors, right? o, vc here is not just a single number, its a whole vector. o, thats sort of the ath 51, E 100 kind of content. Now, if you want to, you can pull it all apart. And you can work out the partial derivative with respect to Vc, some index, k. And then, you could have this as the sum of l = 1 to d of (u0)l (Vc)l. And what will happen then is if youre working out of with respect to only one index, then, all of these terms will go away apart from the one where k equals l. And youll sort of end up with that being the (uo)k term. And mean, if things get confusing and complicated, think it can actually, and your brain is small like mine, it can actually be useful to sort of go down to the level of working it out with real numbers and actually have all the indices there and you can absolutely do that and it comes out the same. But a lot of the time its sort of convenient if we can just stay at this vector level and work out vector derivatives, okay. o, now, this was the easy part and weve got it right there and well come back to that, okay. o then, the trickier part is we then, go on to number 2. o now, if we just ignore the minus sign for a little bit, so, well subtract it afterwards, weve then got the partial derivatives with respect to vc of the log of the sum from w = 1 to v of the exp of uw^T vc, okay. Well, how can we make progress with this half? Yeah, so thats right, before youre going to do that? The chain rule, okay, so, our key tool that we need to know how to use and well just use everywhere is the chain rule, right? o, neural net people talk all the time about backpropagation, it turns out that backpropagation is nothing more than the chain rule with some efficient storage of partial quantities so that you dont keep on calculating the same quantity over and over again. o, its sort of like chain rule with memorization, that is the backpropagation algorithm. o, now, key tool is the chain rule so, what is the chain rule? o, within saying, okay, well, what overall are we going to have is some function where were taking f(g(u)) of something. And so, we have this inside part z and so, what were going to be doing is that were going to be taking the derivative of the outside part then, with the value of the inside. And then, were gonna be taking the derivative of the inside part o for this here, so the outside part, heres our F. And then heres our inside part Z. o the outside part is F, which is a log function. And so the derivative of a log function is the one on X function. o that were then gonna be having that this is 1 over the sum of w equals 1 to V of the exp of uw^T vc. And then were going to be multiplying it by, what do we get over there. o we get the partial derivative with respect to With respect to vc, of This inside part. The sum of, and its a little trickier. We really need to be careful of indices so were gonna get in the bad mess if we have W here, and we reuse W here. We really need to change it into something else. o were gonna have X equals 1 to V. And then weve got the exp of X, transpose V. o thats made a little bit of progress. We want to make a bit more progress here. o whats the next thing were gonna do. Distribute the derivative. This is just adding some stuff. We can do the same trick of we can do each part of the derivative separately. o X equals 1 to big V of the partial derivative with respect to V of the exp of ux^T vc. Okay, now we wanna keep going What can we do next. The chain rule again. This is also the form of heres our F and heres our inner values V which is in turn sort of a function. Yeah, so we can apply the chain rule a second time and so we need the derivative of X. Whats the derivative of X. X, so this part here is gonna be staying. The sum of X equals 1 to V of the partial derivative. Hold on no. Not that one, moving that inside. o its still exp at its value of X T V. And then were having the partial derivative with respect to V of XT V. And then weve got a bit more progress to make. o we now need to work out what this is. o whats that. Right, so thats the same as sort of back over here. At this point this is just going to be, that s coming out as X. And here we still have the sum of X equals 1 to V of the X of X T V. o at this point we kind of wanna put this together with that. uz were still, stopped writing that. But we have this one over the sum of W equals 1 to V of the exp of W, transpose V. an we put those things together in a way that makes it prettier. o can move this inside this sum. uz this is just the sort of number thats a multiplier thats distributed through. And in particular when do that, can start to sort of notice this interesting thing that m going to be reconstructing a form that looks very like this form. orry, leaving this part up aside. t looks very like the oftmax form that started off with. And so can then be saying that this is the sum from X equals 1 to V of the exp of X transpose V over the sum of W equals 1 to V. o this is where its important that have X and W with different variables of the X of W transpose V times of X. And so well, at that point, thats kind of interesting cuz, this is kind of exactly the form that started of with, for my softmax probability distribution. o what were doing is we. What were doing is that that part is then being the sum over X equals one to V of the probability of . t was wait. The probability of O given the probability of X given times X. o thats what were getting from the denominator. And then we still had the numerator. The numerator was zero. What we have here is our final form is 0 minus that. And if you look at this a bit its sort of a form that you always get from these softmax style formulations. o this is what we observed. There was the actual output context word appeared. And this has the form of an expectation. o what were doing is right here. Were calculating expectation though were working out the probability of every possible word appearing in the context, and based on that probability we get taking that much of that X. o this is in some, this is the expectation vector. ts the average over all the possible context vectors, weighted by their likelihood of occurrence. Thats the form of our derivative. What were going to want to be doing is changing the parameters in our model. n such a way that these become equal cause thats when were then finding the maximum and minimum for us to minimize. Okay and so that gives us the derivatives in that model. Does that make sense? Yeah, thats gonna be question. Anyway, so precisely doing things like this is what will expect you to do for assignment one. And ll take the question, but let me just mention one point. o in this case, ve only done this for the V, the center vectors. We do this to every parameter of the model. n this model, our only other parameters are the context vectors. Were also gonna do it for those. ts very similar cuz if you look at the form of the equation, theres a certain symmetry between the two. But were gonna do it for that as well but m not gonna do it here. Thats left to you guys. Question. Yeah. From here to here. Okay. o. o, right, so this is a sum right? And this is just the number at the end of the day. o can divide every term in this sum through by that number. o thats what m doing. o now ve got my sum with every term in that divided through by this number. And then say, wait a minute, the form of this piece here is precisely my softmax probably distribution, where this is the probability of x given . And so then m just rewriting it as probability of x given c. Where that is meaning, kind of did double duty here. But thats sort of meaning that youre using this probability of x given c using this probability form. Yeah, the probability that x occurs as a context word of center word c. Well, weve just assumed some fixed window size . o maybe our window size is five and so were considering sort of ten words, five to the left, five to the right. o thats a hypergrameter, and that stuffs nowhere. Were not dealing with that, we just assume that Gods fixed that for us. The problem, so its done at each position. o for any position, and all of them are treated equivalently, for any position, the probability that word x is the word that occurs within this window at any position given the center word was of . Yeah? All right, so the question is, why do we choose the dot product as our basis for coming up with this probability measure? And you know think the answer is theres no necessary reason, that there are clearly other things that you could have done and might do. On the other hand, kind of think in terms of Vector Algebra its sort of the most obvious and simple thing to do. Because its sort of a measure of the relatedness and similarity. mean sort of said loosely it was a measure of similarity between vectors. omeone could have called me on that because f you say, well wait a minute. f you dont control for the scale of the vectors, you can make that number as big as you want, and that is true. o really the common measure of similarity between vectors is the cosine measure. Where what you do is in the numerator. You take a dot product and then you divide through by the length of the vectors. o youve got scale and variance and you cant just cheat by making the vectors bigger. And so, thats a bigger, better measure of similarity. But to do that you have to do a whole lot more math and its not actually necessary here because since youre sort of predicting every word against every other word. f you sort of made one vector very big to try and make some probability of word k being large. Well the consequence would be it would make the probability of every other word be large as well. o you kind of cant cheat by lengthening the vectors. And therefore you can get away with just using the dot product as a kind of a similarity measure. Does that sort of satisfy? o yes. mean, its not necessary, right? And if we were going to argue, you could sort of argue with me and say no look, this is crazy, because by construction, this means the most likely word to appear in the context of a word is itself. That doesnt seem like a good result, because presumably different words occur. And you could then go from there and say well no lets do something more complex. Why dont we put a matrix to mediate between the two vectors to express what appears in the context of each other, it turns out you dont need to. Now one thing of course is since we have different representations for the context and center word vectors, its not necessarily true that the same word would be highest because therere two different representations. But in practice they often have a lot of similarity between themselves not really that thats the reason. ts more that its sort of works out pretty well. Because although it is true that youre not likely to get exactly the same word in the context, youre actually very likely to get words that are pretty similar in meaning. And are strongly associated and when those words appear as the center word, youre likely to get your first word as a context word. And so at a sort of a macro level, you are actually getting this effect that the same words are appearing on both sides. ore questions, yeah, there are two of them. dont know. Do do the behind person first and then the in front person? o havent yet done gradient descent. And maybe should do that in a minute and will see try then. Okay? o that truth is well, weve just clicked to the huge amount text. o if our word at any position, we know what are the five words to the left and the five words to the right and thats the truth. And so were actually giving some probability estimate to every word appearing in that context and we can say, well, actually the word that appeared there was household. What probability did you give to that and theres some answer. And so, thats our truth. Time is running out, so maybe d sort of just better say a little bit more before we finish which is sort of starting to this optimization. o this is giving us our derivatives, we then want to use our derivatives to be able to work out our word vectors. And mean, m gonna spend a super short amount time on this, the hope is through 221, 229 or similar class. Youve seen a little bit of optimization and youve seen some gradient descent. And so, this is just a very quick review. o the idea is once we have gradient set at point x that if what we do is we subtract off a little fraction of the gradient, that will move us downhill towards the minimum. And so if we then calculate the gradient there again and subtract off a little fraction of it, well sort of start walking down towards the minimum. And so, thats the algorithm of gradient descent. o once we have an objective function and we have the derivatives of the objective function with respect to all of the parameters, our gradient descent algorithm would be to say, youve got some current parameter values. Weve worked out the gradient at that position. We subtract off a little fraction of that and that will give us new parameter values which we will expect to be give us a lower objective value, and well walk towards the minimum. And in general, that is true and that will work. o then, to write that up as Python code, its really sort of super simple that you just go in this while true loop. You have to have some stopping condition actually where you evaluating the gradient of given your objective function, your corpus and your current parameters, so you have the theta grad and then youre sort of subtracting a little fraction of the theta grad after the current parameters and then you just repeat over. And so the picture is, so the red lines that are sort of the contour lines of the value of the objective function. And so what you do is when you calculate the gradient, its giving you the direction of the steepest descent and you walk a little bit each time in that direction and you will hopefully walk smoothly towards the minimum. Now the reason that might not work is if you actually take a first step and you go from here to over there, youve greatly overshot the minimum. o, its important that alpha be small enough that youre still walking calmly down towards the minimum and then all work. And so, gradient descent is the most basic tool to minimize functions. o its the conceptually first thing to know, but then the sort of last minute. What wanted to explain is actually, we might have 40 billion tokens in our corpus to go through. And if you have to work out the gradient of your objective function relative to a 40 billion word corpus, thats gonna take forever, so youll wait for an hour before you make your first gradient update. And so, youre not gonna be able train your model in a realistic amount of time. o for basically, all neural nets doing naive batch gradient descent hopeless algorithm, you cant use that. ts not practical to use. o instead, what we do s used stochastic gradient descent. o, the stochastic gradient descent or GD is our key tool. And so what thats meaning is, so we just take one position in the text. o we have one center word and the words around it and we say, well, lets adjust it at that one position work out the gradient with respect to all of our parameters. And using that estimate of the gradient in that position, well work a little bit in that direction. f you think about it for doing something like word vector learning, this estimate of the gradient is incredibly, incredibly noisy, because weve done it at one position which just happens to have a few words around it. o the vast majority of the parameters of our model, we didnt see at all. o, its a kind of incredibly noisy estimate of the gradient. walking a little bit in that direction isnt even guaranteed to have make you walk downhill, because its such a noisy estimate. But in practice, this works like a gem. And in fact, it works better. Again, its a win, win. ts not only that doing things this way is orders of magnitude faster than batch gradient descent, because you can do an update after you look at every center word position. t turns out that neural network algorithms love noise. o the fact that this gradient descent, the estimate of the gradient is noisy, actually helps GD to work better as an optimization algorithm and neural network learning. And so, this is what were always gonna use in practice. have to stop there for today even though the fire alarm didnt go off. Thanks a lot.","And so can then be saying that this is the sum from X equals 1 to V of the exp of X transpose V over the sum of W equals 1 to V. o this is where its important that have X and W with different variables of the X of W transpose V times of X. And so well, at that point, thats kind of interesting cuz, this is kind of exactly the form that started of with, for my softmax probability distribution. o, we have the log of the probability of the word t plus j word t. Well, what is the form of that? o what we want to do is say for each word were going to come up for it a vector and that dense vector is gonna be chosen so that itll be good at predicting other words that appear in the context of this word. And what wed like to do is set the parameters of our model so that these probabilities of the words that actually do appear in the context of the center word are as high as possible. At this point this is just going to be, that s coming out as X. And here we still have the sum of X equals 1 to V of the X of X T V. o at this point we kind of wanna put this together with that. And so, the model is going to define a probability distribution that is the probability of a word appearing in the context given this center word. ts not that theres a probability distribution for the word one to the left and the word one to the right, and things like that. And so, we have this inside part z and so, what were going to be doing is that were going to be taking the derivative of the outside part then, with the value of the inside. And you can work out the partial derivative with respect to Vc, some index, k. And then, you could have this as the sum of l = 1 to d of (u0)l (Vc)l. And what will happen then is if youre working out of with respect to only one index, then, all of these terms will go away apart from the one where k equals l. And youll sort of end up with that being the (uo)k term. o then, the trickier part is we then, go on to number 2. o now, if we just ignore the minus sign for a little bit, so, well subtract it afterwards, weve then got the partial derivatives with respect to vc of the log of the sum from w = 1 to v of the exp of uw^T vc, okay. What were doing is that that part is then being the sum over X equals one to V of the probability of . And this is the form of the probability distribution up there, where we have these sort of word vectors with both center word vectors and context word vectors. o we have one center word and the words around it and we say, well, lets adjust it at that one position work out the gradient with respect to all of our parameters. o, what were gonna want to do is start working out the partial derivatives of this with respect to the center vector which is then, going to give us, how we can go about working out, in which way to change this vector to minimize our objective function. And were going to have a probability distribution that will give a probability to a word appearing in the context of the center word. Well, the way were gonna do it is were gonna come with the probably distribution of context word, given the center word, which is constructed out of our word vectors. And you are right, this is the parameter of the model. And then, what were gonna want to do is that were going to want to change this so that we have, m sorry, minimized in this objective. o what we have, is we have a lot of words in the language that are equivalent to our symbols and were putting a one in the position, in the vector, that represents the particular symbol, perhaps hotel. Okay, and so, mean, effectively, this is the kind of level of calculus that youre gonna have to be able to do to be okay on assignment one thats coming out today. Okay, so then after that, were gonna have the first or was it gonna be one of the features of this class. o that were then gonna be having that this is 1 over the sum of w equals 1 to V of the exp of uw^T vc. o the notion of distributional similarity is that you can get a lot of value for representing the meaning of a word by looking at the context in which it appears and doing something with those contexts. We just sort of actually select out the column of the matrix which is then the representation of the center word. o the v is the center word vectors, and the u are the context word vectors. o the J prime is saying were going to, so were going to take a big long amount of text, we take the whole of Wikipedia or something like that so we got big long sequence of words, so there are words in the context and real running text, and were going to go through each position in the text. o what d like to do today is sort of really go slowly and carefully through the foundations of how you can start to do things with neural networks in this very simple case of learning representations for words. What instead m going to talk about and suggest is that what we could do is we could explore this direct approach, where the representation of a word encodes its meaning in such a way that you can just directly read off from these representations, the similarity between words. And so if you dont have sort of straight the fundamentals right at the beginning of how you can use neural networks on the sort of very simplest kind of structures, its sort of really all over from there. And at the end of the day, its sort of, its kind of hard to get anything accurate out of it in the way of sort of word similarities. Were calculating expectation though were working out the probability of every possible word appearing in the context, and based on that probability we get taking that much of that X. o this is in some, this is the expectation vector. And mean, if things get confusing and complicated, think it can actually, and your brain is small like mine, it can actually be useful to sort of go down to the level of working it out with real numbers and actually have all the indices there and you can absolutely do that and it comes out the same. And so our model, as a generative model, is predicting the probability of each word appearing in the context given that a certain word is the center word. o weve gone to the log probability and the sums now and, and then rather than having the probability of the whole corpus, we can sort of take the average over each positions so ve got 1 on T here. The only parameters in this model of the vector representations of each word. o if our word at any position, we know what are the five words to the left and the five words to the right and thats the truth. And so we will be picking out sort of the dot products of the center word with each context word. o thats what we kinda wanna do so were gonna have a radius m and then were going to predict the surrounding words from sort of positions m before our center word to m after our center word. And if we were going to argue, you could sort of argue with me and say no look, this is crazy, because by construction, this means the most likely word to appear in the context of a word is itself. Yeah, so we can apply the chain rule a second time and so we need the derivative of X. Whats the derivative of X. X, so this part here is gonna be staying. And so what thats meaning is, so we just take one position in the text. And if you have a bunch of numbers that come from anywhere that are positive and you want to turn them into a probability distribution thats proportional to the size of those numbers, theres a really easy way to do that. o in 2008, ollobert and Weston started in the sort of modern direction by saying, well, if we just want good word representations, we dont even have to necessarily make a probabilistic language model that can predict, we just need to have a way of learning our word representations. We just have one probability distribution of a context word, which well refer to as the output, because its what we, produces the output, occurring in the context close to the center word. And so were actually giving some probability estimate to every word appearing in that context and we can say, well, actually the word that appeared there was household. o, what were gonna have to do at this point is to start to do some calculus to see how we can change the numbers. o basically, the idea is that given the sentence representation, the probability of the limiting or single word, theyre related to the frequency of the word. But we have this one over the sum of W equals 1 to V of the exp of W, transpose V. an we put those things together in a way that makes it prettier. o, we have the derivative with Vc of the log of the exp of u0^T vc and then, weve got minus the log of the sum of w equals 1 to V of exp of u w^T vc. o thats here, is my word banking and then what youre going to do is youre going to try and predict words in its context out to some window size. And so standardly when we do these things, what we do is we take all the parameters in our model and put them into a big vector theta. But really this was the paper where the sort of showed how much value you could get from having distributed representations of words and be able to predict other words in context. o what our parameters are is that for each word, were going to have a little d dimensional vector, when its a center word and when its a context word. Then what we do is we have a second matrix which stores the representations of the context words. o the first step is that just like how we compute the average of the vector representations, they also do this, but each word has a separate weight. And if you really wanted to you could do that, but it turns out you can make the math considerably easier by saying now actually each word has two vector representation that has one vector representation when it synthesis the word. Okay, thats looking kind of simpler so, what is the partial derivative of this with respect to vc? And were going to choose vector representations of words so we can try and maximize that probability distribution. And we want to choose word vectors such as that were maximizing the probability of that prediction. o, if want to know what the word banking means, what m gonna do is find thousands of instances of the word banking in text and m gonna look at the environment in which each one appeared. And the idea is we want to change our parameters, these vectors, so as to minimize the negative log likelihood item, maximize the probability we predict. And so the picture is, so the red lines that are sort of the contour lines of the value of the objective function. And so if we are actually using it generatively, it would say, well, the word you should be using is this one here. Okay, and so what we wanna do is now learn parameters, these word vectors, in such a way that we do as good a job at prediction as we possibly can. And so what you do is when you calculate the gradient, its giving you the direction of the steepest descent and you walk a little bit each time in that direction and you will hopefully walk smoothly towards the minimum. ts going to be the vector representation of the words. And this is just the number at the end of the day. o the idea of distributional similarity is a theory about semantics of word meaning that you can describe the meaning of words by as a use theory of meaning, understanding the context in which they appear. And so, how we find this probability distribution is were going to use this, whats called a oftmax form, where were taking dot products between the the two word vectors and then were putting them into a oftmax form. o, now, this was the easy part and weve got it right there and well come back to that, okay. And the p(w), it means the frequency of this word. Were going to multiply the vector by this matrix which is the context word representations. And so, this is what our probability distribution is gonna look like. Okay, so thats what were going to do. o we decided wed sort of mix it up a little, and hopefully, also give people an opportunity to sort of get more of a sense of what some of the exciting new work thats coming out every month in Deep Learning is. And then, today is also the day when were handing out assignment one and ajor part of what you guys get to do in assignment one is to implement one of the efficient training algorithms, and to work through the method one of those efficient training algorithms. o the idea is once we have gradient set at point x that if what we do is we subtract off a little fraction of the gradient, that will move us downhill towards the minimum. Okay, so thats bad, and we have to do something about it. f you dont control for the scale of the vectors, you can make that number as big as you want, and that is true. o once we have an objective function and we have the derivatives of the objective function with respect to all of the parameters, our gradient descent algorithm would be to say, youve got some current parameter values. o, for 1, we can say that this is going to be the partial derivative with respect to Vc of u0^T vc. Well, each of those other words will also have a word that are attached to them and then well be looking at sort of similarity measures like dot product between those two vectors. We then have a matrix of the representations of center words. But a lot of the time its sort of convenient if we can just stay at this vector level and work out vector derivatives, okay.",0.100727949889961
40,40,"OK. ve got some matrix A. ts an n by k matrix. Lets say its not just any n by k matrix. This matrix A has a bunch of columns that are all linearly independent. o, a1. a2, all the way through ak are linearly independent. They are linearly independent columns. Let me write that down. a1, a2, all the column vectors of A. All the way through ak are linearly independent. Now, what does that mean? That means that the only solution to x1 times a1 plus x2 times a2, plus all the way to xk times ak. The only solution to this is all of these xs have to be 0. o, all xis must be equal to 0. Thats what linear independence implies. Or another way to write it is all the solutions to this equation x1, x2, all the way down to xk equaling the zero vector. That all the solutions to this are all of these entries have to be equal to 0. This is just another way of writing this right there. Weve seen it multiple times. Thats the zero vector right there. o if all of these have to be 0, thats like saying that the only solution to ax is equal to 0, is x is equal to the zero vector. Or another way to say it this is all coming out of the fact that this guys columns are linearly independent. o linear independence of columns. Based on that, we can say, since the only solution to ax is equal to 0 is x is equal to the zero vector, we know that the null space of a must be equal to the zero vector. Or its a set with the just the zero vector in it. And that is all a bit of review. Now, n by k. We dont know its dimensions. t may or may not be a square matrix. o we dont know, necessarily, whether its invertible and all of that. But maybe we can construct an invertible matrix with it. o, lets study a transpose times a. a transpose times a. A is an n by k matrix. A transpose will be a k by n matrix. o, A transpose a is going to be a k by k matrix. o its a square matrix. o thats a nice place to start for an invertible matrix. o lets see if it is actually invertible. We dont know anything about A. All we know is its columns are linearly independent. Lets see if A transpose a is invertible. Essentially, to show that its invertible, if we can show that all of its columns are linearly independent, then well know its invertible. f we have any and ll get back to this at the end of the video. But if you have a square matrix with linearly independent columns remember, the linearly independent columns all are associated with pivot columns when you put them in reduced row echelon form. o if you have a square matrix, then youre going to have exactly so if its a k by k matrix, that means youre going to have k that means that the reduced row echelon form of a matrix will have k pivot columns and be k by k. And be a square k by k matrix. And theres only one k by k matrix with k pivot columns. And thats the identity matrix. The k by k identity matrix. And if when you do something to reduce row echelon form, and it you got the identity matrix, that means that your matrix is invertible. could have probably left that to the end of the video, but just want to show you. f we can show that we already know that this guys square, that a transpose A is a square matrix. f we can show that, given that a has linearly independent columns, that a transpose times A also has linearly independent columns, and given the columns are linearly independent, and its a square matrix, that tells us that when we put it into reduced row echelon form, well get the identity matrix. And that tells us that this thing would be invertible. Lets see if we can prove that all of this guys columns are linearly independent. o lets say have some vector V. Lets say my vector V is a member of the null space of a transpose A. That means that if take a transpose A times my vector v, m going to get the zero vector. Fair enough? Now, what happens if multiply both sides of the equation times the transpose of this guy? o ll get a v transpose actually let me just do it right here. multiply v transpose on this side, and v transpose on this side. You could view this as a matrix vector product. Right? Or, in general, if you take a row vector times a column vector, its essentially their dot product. o this righthand side of the equation, you dot anything with the zero vector. That is just going to be the zero vector. Now what is the lefthand side of this going to be? Weve seen this before. f you have the transpose of we can view this as, even though its a transpose of a vector, you can view it as a it is a row factor, but you could also view it as a matrix. Right? Lets say v is a k by 1 matrix. v transpose will be a 1 by k matrix. Weve seen this before. That that is equal to the reverse product, the transpose of the reverse product. Or if we take the product of two things and transpose it, thats the same thing as taking the reverse product of the transposes of either of those two matrices. o given that, we can replace this right here with a times a vector v transpose and were multiplying this vector times av times this vector right here. And that is going to be equal to the zero vector. Now, what is this? f m taking some vectors transpose, and lets say this is a vector. Remember, even though have a matrix vector product right here, when multiply a matrix times this vector, it will result in another vector. o this is a vector, and this is a vector right here. And if take some vector and multiply its transpose times that vector weve seen this before. That is the same thing as y dot y. These two statements are identical. o this thing right here is the same thing as av dot av. And so what does the righthand side equal? The righthand side is going to be equal to 0. Actually let me just make a correction up here. When take v transpose times the zero vector, v transpose is going to have k elements. And then the zero vector is also going to have k elements. And when take this product thats like dotting it. Youre taking the dot product of v and 0. o this is a dot product of v with the zero vector which is equal to zero, the scalar zero. o this right heres the scalar zero. want to make sure clarify that. t wouldnt make sense otherwise. o the righthand side, when multiply the zero vector times the transpose of v, gets just the number zero. No vector zero there. o this av dot av is going to be equal to 0. Or we could say that the magnitude, or the length, of av squared is equal to 0. Or that tells us that av has to be equal to 0. The only vector whose length is 0, is the zero vector. o av let me switch colors. sing that a little bit too much. o we know that av must be equal to 0, to the zero vector. This must be equal to the zero vector since its length is 0. Now, we started off with saying v is a member of the null space of a transpose A. v can be any member of the null space of a transpose A. But then from that assumption, it turns out that V also has to be a member of the null space of A. That av is equal to 0. Lets write that down. f v is a member of the null space of a transpose A, then v is a member of the null space of a. Now, our null space of A, because As columns are linearly independent, it only contains one vector. t only contains the zero vector. o, if this guys a member of the null space of A transpose A, and he has to be a member of the null space of A, theres only one thing he can be. Theres only one entry there. o then v has to be equal to the zero vector. Or another way to say that is, any v thats in our null space of a transpose A has to be the zero vector. Or the null space of a transpose A is equal to the null space of a which is equal to just the zero factor sitting there. Now, what does that do for us? That tells us that the only solution to a transpose A times some vector x equal to zero, this says that the only solution is the zero vector is equal to the zero vector. Right? Because the null space of a transpose A is the same as the null space of a. And that just has the zero vector in it. The null space is just the solution to this. o if the only solution to the null space is this, that means that the columns of a transpose A are linearly independent. You could, essentially, write all of the linear combinations of the columns by the weights of the entries of x. We actually did that at the beginning. ts the same argument we used up here. o if all of their columns are linearly independent, and said it over here, a transpose A has linearly independent columns, and its a square matrix, that was from the definition of it. o we now know that A transpose A if were to put it let me do this way. That tells me that the reduced row echelon form of a transpose A is going to be equal to the k by k identity matrix which tells me that a transpose A is invertible. Which is a pretty neat result. started with the matrix that has linearly independent columns. o it wasnt just any matrix. t wasnt just any run of the mill matrix. t did have linearly independent columns, but it might have weird dimensions. ts not necessarily a square matrix. But could construct a square matrix. a transpose A with it. And we now know that it also has linearly independent columns. ts a square matrix. And therefore it is invertible.","o if the only solution to the null space is this, that means that the columns of a transpose A are linearly independent. And that is going to be equal to the zero vector. Based on that, we can say, since the only solution to ax is equal to 0 is x is equal to the zero vector, we know that the null space of a must be equal to the zero vector. That tells us that the only solution to a transpose A times some vector x equal to zero, this says that the only solution is the zero vector is equal to the zero vector. That is just going to be the zero vector. o if all of these have to be 0, thats like saying that the only solution to ax is equal to 0, is x is equal to the zero vector. That tells me that the reduced row echelon form of a transpose A is going to be equal to the k by k identity matrix which tells me that a transpose A is invertible. Because the null space of a transpose A is the same as the null space of a. And that just has the zero vector in it. Or another way to say that is, any v thats in our null space of a transpose A has to be the zero vector. o lets say have some vector V. Lets say my vector V is a member of the null space of a transpose A. That means that if take a transpose A times my vector v, m going to get the zero vector. o we know that av must be equal to 0, to the zero vector. Now, we started off with saying v is a member of the null space of a transpose A. v can be any member of the null space of a transpose A. But then from that assumption, it turns out that V also has to be a member of the null space of A. That av is equal to 0. f we can show that, given that a has linearly independent columns, that a transpose times A also has linearly independent columns, and given the columns are linearly independent, and its a square matrix, that tells us that when we put it into reduced row echelon form, well get the identity matrix. Or the null space of a transpose A is equal to the null space of a which is equal to just the zero factor sitting there. Youre taking the dot product of v and 0. o this is a dot product of v with the zero vector which is equal to zero, the scalar zero. o if you have a square matrix, then youre going to have exactly so if its a k by k matrix, that means youre going to have k that means that the reduced row echelon form of a matrix will have k pivot columns and be k by k. And be a square k by k matrix. o, A transpose a is going to be a k by k matrix. o this is a vector, and this is a vector right here. o if all of their columns are linearly independent, and said it over here, a transpose A has linearly independent columns, and its a square matrix, that was from the definition of it. The null space is just the solution to this. o then v has to be equal to the zero vector. Or another way to say it this is all coming out of the fact that this guys columns are linearly independent. That all the solutions to this are all of these entries have to be equal to 0. This must be equal to the zero vector since its length is 0. When take v transpose times the zero vector, v transpose is going to have k elements. The only solution to this is all of these xs have to be 0. o, all xis must be equal to 0. f v is a member of the null space of a transpose A, then v is a member of the null space of a. Now, our null space of A, because As columns are linearly independent, it only contains one vector.",0.3248898678414097
41,41,"Well, lets get started. The topic for today is orry. Thank you. For today and the next two lectures, we are going to be studying Fourier series. Today will be an introduction explaining what they are. And, calculate them, but thought before we do that ought to least give a couple minutes oversight of why and where were going with them, and why theyre coming into the course at this place at all. o, the situation up to now is that weve been trying to solve equations of the form y double prime plus a y prime, constant coefficient secondorder equations, and the f of t was the input. o, we are considering inhomogeneous equations. This is the input. And so far, the response, then, is the solution equals the corresponding solution, y of t, maybe with some given initial conditions to pick out a special one we call the response, the response to that particular input. And now, over the last few days, the inputs have been, however, extremely special. For input, the basic input has been an exponential, or sines and cosines. And, the trouble is that we learn how to solve those. But the point is that those seem extremely special. Now, the point of Fourier series is to show you that they are not as special as they look. The reason is that, lets put it this way, that any reasonable f of t which is periodic, it doesnt have to be even very reasonable. t can be somewhat discontinuous, although not terribly discontinuous, which is periodic with period, maybe not the minimal period, but some period two pi. Of course, sine t and cosine t have the exact period two pi, but if change the frequency to an integer frequency like sine 2t or sine 26 t, two pie would still be a period, although would not be the period. The period would be shorter. The point is, such a thing can always be represented as an infinite sum of sines and cosines. o, its going to look like this. Theres a constant term you have to put out front. And then, the rest, instead of writing, its rather long to write unless you use summation notation. o, will. o, its a sum from n equal one to infinity integer values of n, in other words, of a sine and a cosine. ts customary to put the cosine first, and with the frequency, the n indicates the frequency of the thing. And, the bn is sine nt. Now, why does that solve the problem of general inputs for periodic functions, at least if the period is two pi or some fraction of it? Well, you could think of it this way. ll make a little table. ll make a little table. Lets look at, lets put over here the input, and here, ll put the response. Okay, suppose the input is the function sine nt. Well, in other words, if you just solve the problem, you put a sine nt here, you know how to get the answer, find a particular solution, in other words. n fact, you do it by converting this to a complex exponential, and then all the rigmarole weve been going through. o, lets call the response something. Lets call it y. d better index it by n because it, of course, is a response to this particular periodic function. o, n of t, and if the input is cosine nt, that also will have a response, yn. Now, really cant call them both by the same name. o, why dont we put a little s up here to indicate that thats the response to the sine. And here, ll put a little c to indicate what the answer to the cosine. Youre feeding cosine nt, what you get out is this function. Now what? Well, by the way, notice that if n is zero, its going to take care of a constant term, too. n other words, the reason there is a constant term out front is because that corresponds to cosine of zero t, which is one. Now, suppose input instead an cosine nt. All you do is multiply the answer by an. ame here. ultiply the input by bn. You multiply the response. Thats because the equation is a linear equation. And now, what am going to do? m going to add them up. f add them up from the different ends and take a count also, the n equals zero corresponding to this first constant term, the sum of all these according to my Fourier formula is going to be f of t. Whats the sum of this, the corresponding responses? Well, thats going to be summation a n y n c t plus b n y n, the response to the sine. That will be the sum from one to infinity, and there will be some sort of constant term here. Lets just call it c1. o, in other words, if this input produces that response, and these are things which we can calculate, were led by this formula, Fouriers formula, to the response to things which otherwise we would have not been able to calculate, namely, any periodic function of period two pi will have, the procedure will be, youve got a periodic function of period two pi. Find its Fourier series, and ll show you how to do that today. Find its Fourier series, and then the response to that general f of t will be this infinite series of functions, where these things are things you already know how to calculate. They are the responses to sines and cosines. And, you just formed the sum with those coefficients. Now, why does that work? t works by the superposition principle. o, this is true. The reason can do the adding and multiplying by constant, m using the superposition principle. f this input produces that response, then the sum of a bunch of inputs produces the sum of the corresponding responses. And, why is that? Why can use the superposition principle? Because the ODE is linear. ts okay, since the ODE is linear. Thats what makes all this work. Now, so what were going to do today is will show you how to calculate those Fourier series. will not be able to use it to actually solve any differential equation. t will take us pretty much all the period to show how to calculate a Fourier series. And, okay, so m going to solve differential equations on onday. Wrong. probably wont even get to it then because the calculation of a Fourier series is a sufficient amount of work that you really want to know all the possible tricks and shortcuts there are. nfortunately, they are not very clever tricks. They are just obvious things. But, it will take me a period to point out those obvious things, obvious in my sense if not in yours. And, finally, the third day, well solve differential equations. will actually carry out the program. But the main thing were going to get out of it is another approach to resonance because the things that we are going to be interested in are picking out which of these terms may possibly produce resonance, and therefore a very crazy response. ome of the terms in the response suddenly get a much bigger amplitude than this than you would normally have thought they had because its picking out resonant terms in the Fourier series of the input. Okay, well, thats a big mouthfu. Lets get started on calculating. o, the program today is calculate the Fourier series. Given f of t periodic, having two pi as a period, find its Fourier series. How, in other words, do calculate those coefficients, an and bn. Now, the answer is not immediately apparent, and its really quite remarkable. think its quite remarkable, anyway. ts one of the basic things of higher mathematics. And, what it depends upon are certain things called the orthogonality relations. o, this is the place where youve got to learn what such things are. Well, think it would be a good idea to have a general definition, rather than immediately get into the specifics. o, m going to call u of x, u of t, think will use, since Fourier analysis is most often applied when the variable is time, think will stick to independent variable t all period long, if remember to, at any rate. o, these are two continuous, or not very discontinuous functions on minus pi. Lets make them periodic. Lets say two pi is a period. o, functions, for example like those guys, sine t, sine nt, sine 22t, and so on, say two pi is a period. Well, want them really on the whole real axis, not there. Define for all real numbers. Then, say that they are orthogonal, perpendicular. But nobody says perpendicular. Orthogonal is the word, orthogonal on the interval minus pi to pi if the integral, so, two are orthogonal. Well, these two functions, if the integral from minus pi to pi of u of t v of t, the product is zero, thats called the orthogonality condition on minus pi to pi. Now, well, its just the definition. would love to go into a little song and dance now on what the definition really means, and what its application, why the word orthogonal is used, because it really does have something to do with two vectors being orthogonal in the sense in which you live it in 18.02. ll have to put that on the ice for the moment, and whether get to it or not depends on how fast talk. But, you probably prefer talk slowly. o, lets compromise. Anyway, thats the condition. And now, what say is that that Fourier, that blue Fourier series, what finding the coefficients an and bn depends upon is this theorem that the collection of functions, as look at this collection of functions, sine nt for any value of the integer, n, of course can assume n is a positive integer because sine of minus nt is the same as sine of nt. And, cosine mt, lets give it a different, so dont want you to think they are exactly the same integers. o, this is a big collection of functions, as n runs from one to infinity Here, could let m be run from zero to infinity because cosine of zero t means something. ts a constant, one that any two distinct ones, two distinct, you know, how can two things be not different? Well, you know, you talk about two coincident roots. m just killing, doing a little overkill. Any two distinct ones of these, two distinct members of the set of this collection of, dont know, theres no way to say that, any two distinct ones are orthogonal on this interval. Of course, they all have two pi as a period for all of them. o, they form into this general category that m talking about, but any two distinct ones are orthogonal on the interval for minus pi to pi. o, if integrate from minus pi to pi sine of three t times cosine of four t dt, answer is zero. f integrate sine of 3t times the sine of 60t, answer is zero. The same thing with two cosines, or a sine and a cosine. The only time you dont get zero is if you integrate, if you make the two functions the same. Now, how do you know that you could not possibly get the answer is zero if the two functions are the same? f the two functions are the same, then m integrating a square. A square is always positive. m integrating a square. A square is always positive, and therefore cannot get the answer, zero. But, in the other cases, might get the answer zero. And the theorem is you always do. Okay, so, why is this? Well, there are three ways to prove this. ts like many fundamental facts in mathematics. There are different ways of going about it. By the way, along with the theorem, probably should have included, so, m far away. But you might as well include, because were going to need it. What happens if you use the same function? f take equal to V, and in that case, as ve indicated, youre not going to get the answer, zero. But, what you will get is, so, in other words, m just asking, what is the sine of n t squared. Thats a case where two of them are the same. use the same function. Whats that? Well, the answer is, its the same as what you will get if you integrate the cosine, cosine squared n t dt. And, the answer to either one of these is pi. Thats something you know how to do from 18.01 or the equivalent thereof. You can integrate sine squared. ts one of the things you had to learn for whatever exam you took on methods of integration. Anyway, so m not going to calculate this out. The answer turns out to be pi. All right, now, the ways to prove it are you can use trig identities. And, m asking you in one of the early problems in the problem set, identities, identities for the product of sine and cosine, expressing it in a form in which its easy to integrate, and you can prove it that way. Or, you can use, if you have forgotten the trigonometric identities and want to get some more exercise with complex you can use complex exponentials. o, m asking you how to, in another part of the same problem m asking you how to do it, do one of these, at any rate, using complex exponentials. And now, m going to use a mysterious third method another way. m going to use the ODE. m going to do that because this is the method. ts not just sines and cosines which are orthogonal. There are masses of orthogonal functions out there. And, the way they are discovered, and the way you prove theyre orthogonal is not with trig identities and complex exponentials because those only work with sines and cosines. t is, instead, by going back to the differential equation that they solve. And thats, therefore, the method here that m going to use here because this is the method which generalizes to many other differential equations other than the simple ones satisfied by sines and cosines. But anyway, that is the source. o, the way the proof of these orthogonality conditions goes, so m not going to do that. And, m going to assume that m is different from n so that m not in either of these two cases. What it depends on is, whats the differential equation that all these functions satisfy? Well, its a different differential equation depending upon the value of n, but they look at essentially the same. These satisfy the differential equation, in other words, what they have in common. The differential equation is, lets call it u. t looks better. ts going to look better if you let me call it u. u double prime plus, well, n squared, so for the function sine n t cosine n t, satisfy u double prime plus n squared times u. n other words, the frequency is n, and therefore, this is a square of the frequency is what you put here, equals zero. n other words, what these functions have in common is that they satisfy differential equations that look like that. And the only thing thats allowed to vary is the frequency, which is allowed to change. The frequency is in this coefficient of u. Now, the remarkable thing is thats all you need to know. The fact that they satisfy the differential equation, thats all you need to know to prove the orthogonality relationship. Okay, lets try to do it. Well, need some notation. o, m going to let un and vm be any two of the functions. n other words, ll assume m is different from n. For example, this one could be sine nt, and that could be sine of mt, or this could be sine nt and that could be cosine of mt. You get the idea. Any two of those in the subscript indicates whether what the n or the m is that are in that. Any two, and mean really two, distinct, well, if say that m is not n, then they positively have to be different. o, again, its overkill with my twosness. And, what m going to calculate, well, first of all, from the equation, m going to write the equation this way. t says that u double prime is equal to minus n squared u. Thats true for any of these guys. Of course, here, it would be v double prime is equal to minus m squared times v. You have to make those simple adjustments. And now, what were going to calculate is the integral from minus pi to pi of un double prime times vm dt. Now, just bear with me. Why am going to do that? cant explain what m going to do that. But you wont ask me the question in five minutes. But the point is, this is highly unsymmetric. The u is differentiated twice. The v isnt. o, those two functions but there is a way of turning them into an expression which looks extremely symmetric, where they are the same. And the way to do that is want to get rid of one of these primes here and put one on here. The way to do that is if you want to integrate one of these guys, and differentiate this one to make them look the same, thats called integration by parts, the most important theoretical method you learned in 18.01 even though you didnt know that it was the most important theoretical method. Okay, were going to use it now as a basis for Fourier series. Okay, so m going to integrate by parts. Now, the first thing you do, of course, when you integrate by parts is you just do the integration. You dont do differentiation. o, the first thing looks like this. And, thats to be evaluated between negative pi and pi. n doing integration by parts between limits, minus what you get by doing both. You do both, the integration and the differentiation. And, again, evaluate that between limits. Now, m just going to B my way through this. This is zero. dont care what the uns, which un you picked and which vm you picked. The answer here is always going to be zero. nstead of wasting six boards trying to write out the argument, let me wave my hands. Okay, its clear, for example, that a v is a sine, sine mt. Of course its zero because the sine vanishes at both pi and minus pi. f the un were a cosine, after differentiate it, it became a sine. And so, now its this side guy thats zero at both ends. o, the only case in which we might have a little doubt is if this is a cosine, and after differentiation, this is also a cosine. n other words, it might look like cosine, after, this cosine nt times cosine mt. But, claim that thats zero, too. Why? Because the cosines are even functions, and therefore, they have the same value at both ends. o, if subtract the value evaluated at pi, and subtract the value of minus pi, again zero because have the same value at both ends. o, by this entirely convincing argument, no matter what combination of sines and cosines have here, the answer to that part will always be zero. o, by calculation, but thought calculation; its just a waste of time to write anything out. You stare at it until you agree that its so. And now, ve taken, by this integration by parts, ve taken this highly unsymmetric expression and turned it into something in which the u and the v are treated exactly alike. Well, good, thats nice, but why? Why did go to this trouble? Okay, now were going to use the fact that this satisfies the differential equation, in other words, that u double prime is equal to minus n, m sorry, should have subscripted this. f thats the solution, then this is equal to, times. You have to put in a subscript otherwise. The n wouldnt matter. All right, m now going to take that expression, and evaluate it differently. un double prime vm dt is equal to, well, un double prime, because it satisfies the differential equation is equal to that. o, what is this? This is minus n squared times the integral from negative pi to pi, and m replacing un double prime by minus n squared un. pulled the minus n squared out. o, its un here, and the other factor is vm dt. Now, thats the proof. Huh? What do you mean thats the proof? Okay, well, ll first state it, why intuitively thats the end of the argument. And then, ll spell it out a little more detail, but the more detail you make for this, the more obscure it gets instead of, look, just showed you that this is symmetric in u and v, after you massage it a little bit. Here, m calculating it a different way. s this symmetric in u and v? Well, the answer is yes or no. s this symmetric at u and v? No. Why? Because of the n. The n favors u. We have what is called a paradox. This thing is symmetric in u and v because can show it is. And, its not symmetric in u and v because can show it is. can show its not symmetric because it favors the n. Now, theres only one possible resolution of that paradox. Both would be symmetric if what were true? Pardon? Negative pi. All right, let me write it this way. Okay, never mind. You see, the only way this can happen is if this expression is zero. n other words, the only way something can be both symmetric and not symmetric is if its zero all the time. And, thats what were trying to prove, that this is zero. But, instead of doing it that way, let me show you. This is equal to that, and therefore, two things according to Euclid, two things equal to the same thing are equal to each other. o, this equals that, which, in turn, therefore, equals what would have gotten. m just saying the symmetry of different way, what would have gotten if had done this calculation. And, that turns out to be minus m squared times the integral from minus pi to pi of un vm dt. o, these two are equal because they are both equal to this. This is equal to that. This equals that. Therefore, how can this equal that unless the integral is zero? Hows that? Remember, m is different from n. o, what this proves is, therefore, the integral from negative pi to pi of un vm dt is equal to zero, at least if m is different from n. Now, there is one case didnt include. Which case didnt include? un times un is not supposed to be zero. o, in that case, dont have to worry about, but there is a case that didnt. For example, something like the cosine of nt times the sine of nt. Here, the m is the same as the n. Nonetheless, am claiming that this is zero because these arent the same function. One is a cosine. Why is that zero? an you see mentally that thats zero? entally? Well, this is trying to be in another life, its trying to be one half the sine of two nt, right? And obviously the integral of sine of two nt is zero between minus pi and pi because you integrate it, and it turns out to be zero. You integrate it to a cosine, which has the same value of both ends. Well, that was a lot of talking. f this proof is too abstract for you, wont ask you to reproduce it on an exam. You can go with the proofs using trigonometric identities, and/or complex exponentials. But, you ought to know at least one of those, and for the problem set m asking you to fool around a little with at least two of them. Okay, now, what has this got to do with the problem we started with originally? The problem is to explain this blue series. o, our problem is, how, from this, am going to get the terms of this blue series? o, given f of t, two pi s a period. Find the an and the bn. Okay, lets focus on the an. The bn is the same. Once you know how to do one, you know how to do the other. o, heres the idea. Again, it goes back to the something you learned at the very beginning of 18.02, but dont think it took. But maybe some of you will recognize it. o, what m going to do is write it. Heres the term were looking for here, this one. Okay, and there are others. ts an infinite series that goes on forever. And now, to make the argument, ve got to put it one more term here. o, m going to put in ak cosine kt. dont mean to imply that that k could be more than n, in which case should have written it here. could have also used equally well bk sine kt here, and could have put it there. This is just some other term. This is the an, and this is the one we want. And, this is some other term. Okay, all right, now, what you do is, to get the an, what you do is you multiply everything through by, you focus on the one you want, so its dot, dot, dot, dot, dot, and you multiply by cosine nt. o, its ak cosine kt times cosine nt. Of course, that gets multiplied, too. But, the one we want also gets multiplied, an. And, it becomes, when multiply by cosine nt, cosine squared nt, and now, hope you can see whats going to happen. Now, oops, didnt multiply the f of t, sorry. ts the oldest trick in the book. now integrate everything from minus, so dont endlessly recopy. ll integrate by putting it up in yellow chalk, and you are left to your own devices. This is definitely a colored pen type of course. Okay, so, you want to integrate from minus pi to pi? Good. Just integrate everything on the right hand side, also, from minus pi to pi. Plus, these are the guys just to indicate that havent, they are out there, too. And now, what happens? Whats this? Zero. Every term is zero because of the orthogonality relations. They are all of the form, a constant times cosine nt times something different from cosine nt, sine kt, cosine kt, or even that constant term. All of the other terms are zero, and the only one which survives is this one. And, whats its value? The integral from minus pi to pi of cosine squared, put that up somewhere. ts right here, down there? t is pi. o, this term turns into an pi, an, dragged along, but this, the integral of the square of the cosine turns out to be pi. And so, the end result is that we get a formula for an. What is an? an is, well, an times pi, all these terms of zero, and nothing is left but this lefthand side. And therefore, an times pi is the integral from negative pi to pi of f of t times cosine nt dt. But, thats an times pi. Therefore, if want just an, have to divide it by pi. And, thats the formula for the coefficient an. The argument is exactly the same if you want bn, but will write it down for the sake of completeness, as they say, and to give you a chance to digest what ve done, you know, 30 seconds to digest it. ine nt dt. And, thats because the argument is the same. And, the integral of sine squared nt is also pi. o, theres no difference there. Now, theres only one little caution. t have to be a little careful. This is n one, two, and so on, and this is also n one, two, and unfortunately, the constant term is a slight exception. We better look at that specifically because if you forget it, you can get them to gross, gross, gross errors. How about the constant term? uppose repeat the argument for that in miniature. There is a constant term plus other stuff, a typical other stuff, an cosine, lets say. How am going to get that constant term? Well, if you think of this as sort of like a constant times, the reason is the constant is because its being multiplied by cosine zero t. o, that suggests should multiply by one. n other words, what should do is simply integrate this from negative pi to pi, f of t dt. Whats the answer? Well, this integrated from minus pi to pi is how much? ts c zero times two pi, right? And, the other terms all give me zero. Every other term is zero because if you integrate cosine nt or sine nt over a complete period, you always get zero. There is as much area above the axis or below. Or, you can look at two special cases. Anyway, you always get zero. ts the same thing with sine here. o, the answer is that c zero is equal to, is a little special. You dont just put n equals zero here because then you would lose a factor of two. o, c zero should be one over two pi times this integral. Now, there are two kinds of people in the world, the ones who learn two separate formulas, and the ones who just learn two separate notations. o, what most people do is they say, look, want this to be always the formula for a zero. That means, even when n is zero, want this to be the formula. Well, then you are not going to get the right leading term. nstead of getting c zero, youre going to get twice it, and therefore, the formula is, the Fourier series, therefore, isnt written this way. ts written f you want an a zero there, calculate it by this formula. Then, youve got to write not c zero, but a zero over two. think you will be happiest if have to give you advice. think youll be happiest remembering a single formula for the ans and bns, in which case you have to remember that the constant leading term is a zero over two if you insist on using that formula. Otherwise, you have to learn a special formula for the leading coefficient, namely one over two pi instead of one over pi. Well, am really going to calculate a Fourier series in four minutes? Not very likely, but ll give it a brave college try. Anyway, you will be doing a great deal of it, and your book has lots and lots of examples, too many, in fact. t ruined all the good examples by calculating them for you. But, will at least outline. Do you want me to spend three minutes outlining a calculation just so you have something to work on in the next boring class you are in? Lets see, so ll just put a few key things on the board. would advise you to sit still for this. Otherwise youre going to hack it, and take twice as long as you should, even though knew youve been up to 3:00 in the morning doing your problem set. heer up. got up at 6:00 to make up the new one. o, were even. This should be zero here. o, heres minus pi. Heres pi. Heres one, negative one. The function starts out like that, and now to be periodic, it then has to continue on in the same way. o, think thats enough of its path through life to indicate how it runs. This is a typical squareaway function, sometimes its called. ts an odd function. t goes equally above and below the axis. Now, the integrals, when you calculate them, the an is going to be, okay, look, the an is going to turn out to be zero. Let me, instead, and you will get that with a little hacking. m much more worried about what youll do with the bns. Also, next onday youll see intuitively that the an is zero, in which case you wont even bother trying to calculate it. How about the bn, though? Well, you see, because the function is discontinuous, so, this is my input. y f of t is that orange discontinuous function. The bn is going to be, have to break it into two parts. n the first part, the function is negative one. And there, will be integrating from minus pi to pi of the function, which is minus one times the sine of nt dt. And then, theres another part, sorry, minus pi to zero. The other part integrate from zero to pi of what? Well, f of t is now plus one. And so, simply integrate sine nt dt. Now, each of these is a perfectly simple integral. The only question is how you combine them. o, this is, after you calculate it, it will be (one minus cosine n pi) all over n. And, this part will turn out to be (one minus cosine n pi) over n also. And therefore, the answer will be two minus two cosine, two over n times, right, two minus, two times (one minus cosine n pi) over n. No, okay, now, whats this? This is minus one if n is odd. ts plus one if n is even. Now, either you can work with it this way, or you can combine the two of them into a single expression. ts minus one to the nth power takes care of both of them. But, the way the answer is normally expressed, it would be minus two over n, two over n times, if n is even, get zero. f n is odd, get two. o, times two, if n is odd, and zero if n is even. o, its four over n, or its zero, and the final series is a sum of those coefficients times the appropriate cosine or sine? ine terms because the cosine terms were all coefficients, all turned out to be zero. m sorry didnt have the chance to do that calculation in detail. But, think thats enough sketch for you to be able to do the rest of it yourself.","And obviously the integral of sine of two nt is zero between minus pi and pi because you integrate it, and it turns out to be zero. And there, will be integrating from minus pi to pi of the function, which is minus one times the sine of nt dt. Now, how do you know that you could not possibly get the answer is zero if the two functions are the same? And now, what say is that that Fourier, that blue Fourier series, what finding the coefficients an and bn depends upon is this theorem that the collection of functions, as look at this collection of functions, sine nt for any value of the integer, n, of course can assume n is a positive integer because sine of minus nt is the same as sine of nt. This is the an, and this is the one we want. Now, the integrals, when you calculate them, the an is going to be, okay, look, the an is going to turn out to be zero. Well, these two functions, if the integral from minus pi to pi of u of t v of t, the product is zero, thats called the orthogonality condition on minus pi to pi. ts going to look better if you let me call it u. u double prime plus, well, n squared, so for the function sine n t cosine n t, satisfy u double prime plus n squared times u. n other words, the frequency is n, and therefore, this is a square of the frequency is what you put here, equals zero. Well, the answer is, its the same as what you will get if you integrate the cosine, cosine squared n t dt. would love to go into a little song and dance now on what the definition really means, and what its application, why the word orthogonal is used, because it really does have something to do with two vectors being orthogonal in the sense in which you live it in 18.02. ll have to put that on the ice for the moment, and whether get to it or not depends on how fast talk. And now, what were going to calculate is the integral from minus pi to pi of un double prime times vm dt. f add them up from the different ends and take a count also, the n equals zero corresponding to this first constant term, the sum of all these according to my Fourier formula is going to be f of t. Whats the sum of this, the corresponding responses? Remember, m is different from n. o, what this proves is, therefore, the integral from negative pi to pi of un vm dt is equal to zero, at least if m is different from n. Now, there is one case didnt include. And the way to do that is want to get rid of one of these primes here and put one on here. But, what you will get is, so, in other words, m just asking, what is the sine of n t squared. m going to do that because this is the method. o, in other words, if this input produces that response, and these are things which we can calculate, were led by this formula, Fouriers formula, to the response to things which otherwise we would have not been able to calculate, namely, any periodic function of period two pi will have, the procedure will be, youve got a periodic function of period two pi. The frequency is in this coefficient of u. Now, the remarkable thing is thats all you need to know. And, the answer to either one of these is pi. Okay, now were going to use the fact that this satisfies the differential equation, in other words, that u double prime is equal to minus n, m sorry, should have subscripted this. o, this is, after you calculate it, it will be (one minus cosine n pi) all over n. And, this part will turn out to be (one minus cosine n pi) over n also. And therefore, an times pi is the integral from negative pi to pi of f of t times cosine nt dt. o, the answer is that c zero is equal to, is a little special. The only time you dont get zero is if you integrate, if you make the two functions the same. And, what m going to calculate, well, first of all, from the equation, m going to write the equation this way. The way to do that is if you want to integrate one of these guys, and differentiate this one to make them look the same, thats called integration by parts, the most important theoretical method you learned in 18.01 even though you didnt know that it was the most important theoretical method. Well, if you think of this as sort of like a constant times, the reason is the constant is because its being multiplied by cosine zero t. o, that suggests should multiply by one. And, m asking you in one of the early problems in the problem set, identities, identities for the product of sine and cosine, expressing it in a form in which its easy to integrate, and you can prove it that way. n other words, the reason there is a constant term out front is because that corresponds to cosine of zero t, which is one. All of the other terms are zero, and the only one which survives is this one. And, that turns out to be minus m squared times the integral from minus pi to pi of un vm dt. The answer here is always going to be zero. o, this term turns into an pi, an, dragged along, but this, the integral of the square of the cosine turns out to be pi. This is n one, two, and so on, and this is also n one, two, and unfortunately, the constant term is a slight exception. And here, ll put a little c to indicate what the answer to the cosine. This is equal to that. Any two of those in the subscript indicates whether what the n or the m is that are in that. o, if integrate from minus pi to pi sine of three t times cosine of four t dt, answer is zero. Here, the m is the same as the n. Nonetheless, am claiming that this is zero because these arent the same function. probably wont even get to it then because the calculation of a Fourier series is a sufficient amount of work that you really want to know all the possible tricks and shortcuts there are. And, thats what were trying to prove, that this is zero. The argument is exactly the same if you want bn, but will write it down for the sake of completeness, as they say, and to give you a chance to digest what ve done, you know, 30 seconds to digest it. The function starts out like that, and now to be periodic, it then has to continue on in the same way. Find its Fourier series, and then the response to that general f of t will be this infinite series of functions, where these things are things you already know how to calculate. And, thats because the argument is the same. The other part integrate from zero to pi of what? But, the way the answer is normally expressed, it would be minus two over n, two over n times, if n is even, get zero. Well, in other words, if you just solve the problem, you put a sine nt here, you know how to get the answer, find a particular solution, in other words. Now, so what were going to do today is will show you how to calculate those Fourier series. And thats, therefore, the method here that m going to use here because this is the method which generalizes to many other differential equations other than the simple ones satisfied by sines and cosines. But the main thing were going to get out of it is another approach to resonance because the things that we are going to be interested in are picking out which of these terms may possibly produce resonance, and therefore a very crazy response. Now, why does that solve the problem of general inputs for periodic functions, at least if the period is two pi or some fraction of it? Now, the first thing you do, of course, when you integrate by parts is you just do the integration. And, the integral of sine squared nt is also pi. o, why dont we put a little s up here to indicate that thats the response to the sine. This is zero. Well, by the way, notice that if n is zero, its going to take care of a constant term, too. And, m going to assume that m is different from n so that m not in either of these two cases. Well, thats going to be summation a n y n c t plus b n y n, the response to the sine. Well, this is trying to be in another life, its trying to be one half the sine of two nt, right? think youll be happiest remembering a single formula for the ans and bns, in which case you have to remember that the constant leading term is a zero over two if you insist on using that formula. o, the only case in which we might have a little doubt is if this is a cosine, and after differentiation, this is also a cosine. o, n of t, and if the input is cosine nt, that also will have a response, yn. And therefore, the answer will be two minus two cosine, two over n times, right, two minus, two times (one minus cosine n pi) over n. No, okay, now, whats this? You see, the only way this can happen is if this expression is zero. n other words, what should do is simply integrate this from negative pi to pi, f of t dt. This is the input. And then, ll spell it out a little more detail, but the more detail you make for this, the more obscure it gets instead of, look, just showed you that this is symmetric in u and v, after you massage it a little bit. Okay, so, you want to integrate from minus pi to pi? And, why is that? Of course its zero because the sine vanishes at both pi and minus pi. The reason is that, lets put it this way, that any reasonable f of t which is periodic, it doesnt have to be even very reasonable. The bn is going to be, have to break it into two parts. Now, the point of Fourier series is to show you that they are not as special as they look. This thing is symmetric in u and v because can show it is. The integral from minus pi to pi of cosine squared, put that up somewhere. This is equal to that, and therefore, two things according to Euclid, two things equal to the same thing are equal to each other. o, what is this?",0.1291087766858692
42,42,"ANNONER: The following program is brought to you by altech. YAER ABOTAFA: Welcome back. Last time, we talked about radial basis functions, and the functional form of the hypothesis in that model is the superposition of a bunch of Gaussians, centered around mu_k. And we had two models, or two versions of that model, one of them where the centers are fewer than the number of data points, which is the most common one, in which case we need to come up with the value of the centers, mu_k, and learn the values of w_k. And it turned out to be a very simple algorithm in that case, where you use unsupervised learning to get the mu_ks, the centers, by clustering the input points without reference to the label that they have. And after you do that, it becomes a very simple linear model where you get the w_ks, the parameters, using the usual pseudoinverse. And in the other case, where we used as many centers as there are data points, and the centers were the data points, there was obviously no first step. And in that case, in order to get the w_k, we actually used the real inverse rather than the pseudoinverse. One of the interests of radial basis functions they are very popular functions to use in machine learning, but one of the most important features about them is how they relate to so many aspects of machine learning. o d like to go through this, because its actually very instructive and it puts together some of the notions we had. o let me magnify this a bit. Radial basis functions have this as the building block, the Gaussian, and they are related to nearest neighbor. n the case of nearest neighbor, you have a data point, one of your points in the training set, and it influences the region around it. o everything in the region around it in the input space inherits the label of that point, until you get to a point which is closer to another data point, and then you switch to that point. o you can think now of RBF as a soft version of that. The point affects the points around it, but its not black and white. ts not full effect and then zero effect. ts gradually diminishing effect. ts also related to neural networks, thinking of this as the activation in the hidden layer, as we saw last time. And the activation for the neural networks in the hidden layer was a sigmoid. And the main conceptual difference between the two in this case is that this is local. t takes care of one region of the space at the time, whereas this is global. That thing affects points regardless of the value of the signal, and you get the effect of a function by getting the differences between these different sigmoids. Then we had the relationship to V, which is very easy because in the case of V, we had an outright RBF kernel. o there was simply a very easy way to compare them because they use the same kernel, except that there were many interesting differences. For example, when we use the RBF, we cluster the points, we determine the centers according to an unsupervised learning criterion. And in the case of V, the centers, if youre going to call them that, happen to be the support vectors in which the output is very much consulted in deciding what these support vectors are. And the support vectors happen to be around the separating boundary, whereas the centers here happen to be all over the input space, in order to represent different clusters of the inputs. The two remaining relations as far as RBF are concerned are regularization and unsupervised learning. nsupervised learning is easy, because that is the utility we had in order to cluster the points and find the centers. o you look at the points, and then you try to find the representative center for them such that when you put a radial basis function around that point, it captures the contribution of those points, and then more or less dies out, or at least is not as effective when it goes far away, and this is another center that does the same. The interesting aspect was regularization because, it seems on face value, its a completely different concept. RBF is a model. Regularization is a method that we apply on top of any model. But it turns out that RBFs were derived in the first place in function approximation using just a consideration of regularization. o you have a bunch of points, you want to interpolate and extrapolate them, and you dont want the curve to be too wiggly. o you capture a smoothness criterion using a function of derivatives, and then when you solve for them, you find that the interpolation is done by Gaussians, which gives you the RBFs. o this is what this model does. Today, were going to switch gears completely and in a very pleasant way. f you think about it, we have gone through lots of math, and lots of algorithms, and lots of homework, and all of that, and think we paid our dues and we earned the ability to do some philosophy, if you will. o were going to look at learning principles without very strong appeal to math, because we have very strong math foundation to stand on already. And well try to understand the concepts, and relate these concepts as they appear in machine learning, because they also appear in other fields in science in general, and they are fascinating concepts in their own right. And when we put them in the context of machine learning, they assume a real meaning and a real understanding that will help us understand the principles in general. o the three principles, the usual label for them is Occams razor, sampling bias, and data snooping. And you may be familiar with some of them, and we have already alluded to data snooping in one of the lectures. And if you look at them, Occams razor relates to the model. Both of these guys relate to the data. One of them has to do with collecting the data, and the other one has to do with handling the data. And well take them one at a time, and see what they are about and how they apply to machine learning and so on. o lets start with Occams razor. There is a recurring theme in machine learning, and in science, and in life in general that less is more. impler is better, and so on. And there are so many manifestations of that, and just chose one of the most famous quotes. put ""quote"" between quotes because its not really a quote. He didnt say that in so many words, but at least, thats what people keep quoting Einstein as saying. And it says that an explanation of the data so you are running an experiment, you collect the data, and you want to make an explanation of the data. The explanation could be E equals squared, or something else. o you are trying to find an explanation of the data, and here is a condition about what the explanation should be like. t should be as simple as possible, but no simpler. Very wise words. As simple as possible, thats the Occams razor part. No simpler, because now you are violating the data. You have to be able to explain the data. o this is the rule. And that quote, in one manifestation or another, has occurred in history. saac Newton has something that is similar, and a bunch of them, but m going to quote the one that survived the test of time, which is Occams razor. o lets first explain what the razor is. Well, a razor is this. You have to write ""Occam"" on it in order to become Occams razor! And the idea here is symbolic. o the notion of the razor is the following. You have an explanation of the data, and you have your razor. o what you do, you keep trimming the explanation to the bare minimum that is still consistent with the data, and when you arrive at that, then you have the best possible explanation. And its attributed to William of Occam in the 14th century, so it goes back quite a bit. What we would like to do, wed like to state the principle of Occams razor, and then zoom in, in order to make it concrete. Rather than just a nice thing to have, wed like to really understand what is going on. o lets look at the statement. The statement, in English, not in mathematics, says that the simplest model that fits the data is also the most plausible. And we put it in a box, because its important. o, first thing to realize about this statement is that it is neither precise nor selfevident. ts not precise, because really dont know what simplest means. We need to pin that down. Right? know that the simplest model is nice, but m saying something more than just nice. m saying its most plausible. t is the most likely to be true for explaining the data. That is a statement, and you actually need to argue why this is true. ts not wishful thinking that we just use the simple, and things will be fine. There is something said here. o there are two questions to answer, in order to make this concrete. The two questions are, the first one is, what does it mean for a model to be simple? t turns out to be a complex question, but we will see that its actually manageable in very concrete terms. The second question is, how do we know that this is the case? How do we know that simpler is better, in terms of performance? o well take one question at a time, and address it. First question, simple means exactly what? Now, you look at the literature and complexity is all over the place. ts a very appealing concept with very big variety of definitions, but the definitions basically belong to two categories. When you measure the complexity, there are basically two types of measures of complexity. And my goal here is to be able to convince you that they actually are talking about more or less the same thing, in spite of being inherently different conceptually. The first one is a complexity of an object, in our case, a hypothesis h or the final hypothesis g. That is one object, and we can say that this is a complex hypothesis or a simple hypothesis. The other set of definitions have to do with the complexity of a set of objects. n our case, the hypothesis set. We say that this is a complex hypothesis set, complex model, and so on. And we did have concretely a measure of complexity of small h and a measure of complexity of big H, and if you remember, we actually used the same symbol for them. t was Omega. Omega here was the penalty for model complexity when we did the V analysis, and Omega here was the regularization term. This is the one we add in the augmented error, in order to capture the complexity of what we end up with. o we already have a feel that there is some kind of correspondence, and if you look at the different definitions outside, there are many definitions of the complexity of an object, and m going to give you two from different worlds. One of them is DL, stands for inimum Description Length. And the other one, which is simple, is the order of a polynomial. Let me take the minimum description length. o the idea is that give you an object and you try to specify the object, and you try to specify it with as few bits as possible. The fewer the bits you can get away with, the simpler the object in your mind. o the measure of complexity here is how few bits can get away with, in specifying that object? And lets take just an example, in order to be able to relate to that. Lets say m looking at an integer that happens to be a million digits, a million decimal digits. Huge numbers, any numbers. Now, m trying to find the complexity of individual numbers of that length. There will be different complexities. o let me give you one number which is, lets say, 10 to the million minus 1, in order to make it a million digits. o lets say 10 to the million minus 1. Now, 10 to the million minus 1 is 99999999, a million times, right? n spite of the fact that this is a million in length, it is a simple object because you were able to describe it as ""10 to the million minus 1"". That is not a very long description, right? And therefore, because you managed to get a short description, the object is simple in your mind. This is very much related to Kolmogorov complexity. The only difference between Kolmogorov complexity and minimum description length is that minimum description length is more friendly. t doesnt depend on computability and other issues. But this is the notion. And you can see that when we describe the complexity of an object, that complexity is an intrinsic property of the object. Order of a polynomial is simpler to understand. tell you there is a 17thorder polynomial versus a 100thorder polynomial, and you already can see that the object is more complex when you have a higher order. And indeed, this was our definition of the complexity of the target, if you recall, when we were running the experiments of deterministic noise. n that case, we needed to generate target functions of different complexity, and the way we did it, we just increased the order of the polynomial as our measure of the complexity of that object. Now we come to the complexity of a class of objects. Well, there are notions running around that actually define that, and m going to quote two of them, very famous. The entropy is one, and the one we are most familiar with, which is the V dimension. Now, these guys apply to a set of objects. For example, the entropy. You run an experiment, you consider all possible outcomes of the experiment, the probabilities that go with them, and you find one collective function that captures the probability, sum of p logarithm of 1 over p, and that becomes your entropy and that describes the disorder, the complexity, whatever you want, of the class of objects, each outcome being one object. n the case of the V dimension, it applies directly to the notion we are most familiar with. t applies to a hypothesis set, and it looks at the hypothesis set as a whole, and produces one number that describes the diversity of that hypothesis set. And the diversity in that case we measure as the complexity. o if you look at one object from that set, and you look at this measure of complexity, now that measure of complexity is extrinsic with respect to that object. t depends on what other guys belong to the same category. Thats how measure the complexity of it, whereas in the first one, didnt want to be a member of anything. just looked at that object, and tried to find an intrinsic property of that object that captures the complexity. o these are the two categories you will find in the literature. Now, when we think of simple as far as Occams razor, as far as different quotes are concerned, we are thinking of a single object. tell you E equals squared, or looked at the board, P V equals n R T, and that is a simple statement. You dont look at what other alternatives were there to explain the data. You just look at that object intrinsically, and that is what you think of as the measure of complexity. When you do the math in order to prove Occams razor in one version or another, the complexity you are using is actually the complexity of the set of objects. And we have seen that already. We looked at the V dimension, for example, in order to prove something of an Occams nature in this course already, and that captured the complexity of a set of objects. o this is a little bit worrying, because the intuitive concept is one thing, and the mathematical proofs deal with another. But the good news is that the complexity of an object and the complexity of a set of objects, as we described in this slide, are very much related, almost identical. And here is the link between them: counting. ouldnt be simpler. Here is the idea. Lets say we are using the minimum description length, which is very popular and versatile. o it takes l bits to specify a particular object, h. m taking the objects here to be h, because m in machine learning. The objects are hypotheses, so use that. Now, the measure of complexity in this term is that the complexity of this fellow is l bits, because that is my definition. Now, this implies something. This implies that if look at all the guys that are similar to this object in terms of complexity, they also happen to have l bits worth of minimum description. How many of them are there? Well, 2^l, right? And now you can look at the set of all similar objects, and you call it H, and you have one of 2^l as the description of an object here, and you can take the ""1 of 2^l"" as the description of the complexity of that set. o now we are establishing something in our mind. omething is being complex in its own right, when its one of many. omething is simple in its own right, when its one of few. That is the link that makes us able to use this side for the proofs, and make a claim on this side. t is not an exact correspondence, but it is an overwhelmingly valid correspondence. Now these are with bits, and can pin it down exactly. How about realvalued parameters? Lets look at our 17thorder polynomial. You can look at a 17thorder polynomial, and you can see that because its 17th order, it goes up and down and up and down, and that looks complex. But also, because if its a 17th order polynomial, its one of many, in the realm of infinity in this case, because having 17 parameters to choose makes me able to choose a whole bunch of guys that belong to the same category. o the class of 17thorder polynomials is big, and therefore, its not only that the individual is complex, the set is also complex. There are exceptions to this rule, and one notable exception was a deliberate exception. And we wanted something that looks complex, so that it does our job of fitting, but is one of few. And therefore, we are not going to pay the full price for it being complex, and that was our good old friend V. Remember this fellow? This looks complex all right, but its actually not really complex because its defined only by very few support vectors. And therefore in spite of the fact that it looks complex, its really one of few, and that is what we achieve by the support vector machines. Now, let us take this in our mind, that we are going to use the complexity of an object as the same as the complexity of the set of objects that the object naturally belongs to, and we will see some ramifications. o now m going to give you the first puzzle of the lecture. There are 5 puzzles in this lecture, so you need to pay attention, and each puzzle makes a point. And the first one has to do with this complexity, so lets look at the puzzle. The puzzle has to do with a football oracle, someone who can predict football games perfectly. You watch onday night football, you want to know the result, and something happens onday morning. You get a letter in the mail. You open the letter. Hi. Today, the home team will win. Or, the home team will lose. You dont make much of it, just some character sent something. ts not a big deal. You watch the game, and its a good call. OK, interesting. 50%, lucky. Next onday, another letter, another prediction. And the funny thing is that he predicted either the home team will win or not, and it was very long odds. Everybody thought the other way around. And at the end of the game, the guy was right, and the guy was right for 5 weeks in a row. Now you are really very curious, and you are eagerly waiting in the 6th week in the morning of onday to see where the letter is. You have a perfect record. Now comes the letter. The letter says: you want more predictions? Pay me $50. Very simple question: hould you pay? The question is easily answered, because now the scams are so many that the default, just dont look at anything. There must be something to it. But really want to pin down what is it, because that is the message we are carrying out. o the idea here is that no, you shouldnt, and the guy is really not predicting anything. And the reason for that is the following. Hes not sending letters to you only. Hes sending letters to 32 people. n the first game, for half of them, he said that the home team will lose. The second one, he said the home team will win. Now, because he did that, he is sure that some of the guys will get the correct answer. o the game is played, and the home team loses. o in the second week, he goes for the guys where he was right, and sends half of them that the home team will lose, and the other half, the home team will win. Now, he had plans to send the other guys as well something similar, except that its hopeless now because he already lost with them, so theyre not going to pay him the $50. o just for the memory, this is what would have been sent. There are no letters sent here, but he would have gone zero one, zero one. And he waits for the game, and out comes: the home team won. o you can see who hes going to send letters to now, right? The other guys are a lost cause. This would have been sent to them, but thats OK. And he waits, and what happens this time? The home team lost. And therefore, here is your next letter. Home team won. Here is your next letter. Only two people are surviving from this thing. And here is the result, the home team won. Now at that point, the guy sent how many letters? 32 plus 16 plus 8 plus 4 plus 2, so about 64, 63 to be exact. The postage on that, writing the letter, he probably spent $30 on that. And hes charging you, the lucky guy out of the 32, $50. Thats a money making proposition. Very nice, and its understood and illegal, by the way! But the interesting thing here is to understand, why is this related to what weve just talked about? You thought the prediction ability was great because you only saw your letters. There is one hypothesis, and it got it right perfectly. The problem is that actually, the hypothesis set is very complex, and therefore the prediction value is meaningless. You just didnt know. You didnt see the hypothesis set. o now we understand what is the complexity of an object. Now we go to the question, why is simpler better? o the first thing to understand is that we are not saying that simpler is more elegant. impler is more elegant, but this is not the statement of Occams razor. Occams razor is stating that simpler will have better outofsample performance. Thats a concrete statement. n all honesty, if Occam said that you take the more complex guy and it will give you better outofsample error, will take the more complex one, thank you. am after performance. m not after elegance here. ts nice that the elegant guy happens also to be better, but we need to establish that it is actually better. And there is a basic argument. t manifests itself in many ways, and we have already run one in this course during the theory. And you put some assumptions, and theres a formal proof under idealized conditions of the following. nstead of going through any formal proofs quite a variety of them, am extracting the crux of the proof. What is the point being made? And m going to relate it to the proof that we ourselves ran. o here is highorder steps. There are fewer simple hypotheses than complex ones. That is what we established from the definition of complexity. And in our case, that was captured by the growth function. You probably have forgotten what this is, long ago. This was taking N points, finding what your hypothesis set can generate in terms of different patterns on those N points, we call dichotomies. o if it can generate everything like the postal guy, then its a huge hypothesis set. f it can generate few of them, then its a simple hypothesis, and its measured by that growth function, and that resulted in the V dimension. Remember all of that? o now, fine. Fewer simple hypotheses than complex ones. OK, then what? The next thing is because there are fewer ones, it is less likely to fit a given data set. That is, you have N points, and youre going to generate labels. Lets say you generate them at random, and you ask yourself, what are the chances that my hypothesis set will fit? Well, if it has few of those guys, obviously that goes down, and the probability, if you take it uniformly, simply would be the growth function divided by 2^N. f my growth function is polynomial, then very quickly, the probability of fitting a given data set is very small. OK, fine, can buy that. o now thats nice, but you want to convince me now that simpler is better in fit. Here, you told me that cannot fit. o what is the point? The punchline in all of those is that if something is less likely, then when it does happen, its more significant. And there are many manifestations of this, even when you define the entropy that alluded to. A probability of an event is p. What is the information associated with that particular point? The smaller the probability, the bigger the information, the bigger the surprise when it happens. And indeed, you define the term as being logarithm 1 over p. o if p is very small, tons of bits of information. f something half the time will happen, half the time will not happen, its just 1 bit. ts not a big deal. And, looking back at the postal scam, the only difference between someone believing in the scam and someone having the big picture is the fact that the growth function, from your point of view when you received the letters, was 1. You thought you were the only person. Here is one hypothesis, and you got it right, and you gave a lot of value for that because this is unlikely to happen. On the other hand, the reality of it is that the growth function is actually 2^N and this is certain to happen, so when it happens, its meaningless. Lets look at a scientific experiment, where a fit is meaningless. o you are running an experiment, or you ask people to run an experiment, to establish whether conductivity of a particular metal is linear in the temperature. can design an experiment for that. o you go and you ask two scientists to conduct experiments, and they go, and they come back with the following results. Here is the first scientist. Took the metal, but they had a dinner appointment, so they were in a hurry, so they got 2 points and drew the line and gave you this. The second guy had a supper appointment, so had more time to do it, so did it 3 times, and then the line. have a very specific question, which is: what evidence do they provide for the hypothesis that conductivity is indeed linear in the temperature? What is clear without thinking too much is that this guy provided more evidence than this guy. t is interesting to realize that this guy provided nil, none, nada. Why is that? Because obviously, 2 points can always be connected by a line. o the notion that goes with this is called falsifiability. f your data has no chance of falsifying your assertion, then by the same token, it does not provide any evidence for that assertion. You have to have a chance of falsifying your assertion, in order to be able to draw the evidence. This is called the axiom of nonfalsifiability, and in some sense, its equivalent to the arguments we have done so far. And in our terms, the linear model is just way too complex for the size of the data set, which is 2, to be able to generalize at all. And therefore, there is no evidence here. n this case, this guy could have been falsified if the red point came here. Therefore, he actually provides an evidence. This is the point. This guy could not have been falsified. o now we go to the next notion, which is sampling bias. ts a very interesting notion, and its tricky. And by the way, if you look at all of these principles, its not like theyre just concepts, and nice, and relate to other fields. They also provide you with red flags when youre doing machine learning. For example, when you use Occams razor, what does it mean? t means that beware of fitting the data with complex models, because it looks great in sample and you are very encouraged, and when you go out of sample, you know what happens. You know all too well by the theory we have done. imilarly, when we talk about sampling bias and later, data snooping, there are traps that we need to avoid when we practice machine learning. o lets look at sampling bias, and we start with a puzzle. Here is the puzzle. t has to do with the presidential election, not this one. But in 1948, this was the first presidential election after World War , which was a big deal, and the two people who ran was Truman, who was currently President, and he ran against Dewey. And it was very close in terms of people will take opinion polls, and its not clear who is going to win. o now, one newspaper ran a phone poll, and what they did is ask people how they actually voted. o this is not before the election asking, what do you think? This is the night of the election, after the election closed, they actually called people picked at random at their home, asked them: who did you vote for? Black and white. Dewey or Truman, et cetera? They collected the thing, and they applied some statistical thing or Hoeffding or some other quantity, and came with the conclusion that Dewey has won decisively. Decisively doesnt mean he won by 60%. Decisively means that he won above the error bar. The probability that the opposite is true is diminishingly small. And the result was so obvious that they decided to be the first to break the news, and they printed their newspaper declaring: Great. OK, so Dewey won. What happens when someone wins an election? They have a victory rally. o lets look at the victory rally. One problem. Victory rally was Truman, and you can see the big smile on the guys face. o what happened? Well, polls are polls and there is always a probability, and this and that. No, thats not the issue here. Thats the key. o dont blame delta for it. delta? What was delta again? Weve been doing techniques for a while. forgot all about the theory. o lets remind you what delta was. We were talking about the discrepancy between insample, the poll, outof sample, the general population, the actual vote, and we were asking ourselves, what is the probability that this will be bigger than something, such that the result is flipped? You thought it was Dewey winning, and it turned out to be Truman. And that turned out to be less than or equal to delta, and delta is expressed in terms of epsilon, N, and whatnot. o in principle, it is possible, although not very probable, that the newspaper was just incredibly unlucky. Now, the statement is very interesting. No, the newspaper was not unlucky. f they did the poll again and again and again, with 10 times the sample, or 100 times the sample, they will get exactly the same thing. OK?! o what is the problem? The problem is the following. There is a bias in the poll they conducted and it is because of a rather laughable reason. n 1948, phones were expensive. That means that households that had phones used to be richer, and richer people at that point favored Dewey more than the general population did. o there was a sampling bias. There was always the case the population they were asking actually favored Dewey. The sample was very reflective of the general population, of that mini general population. The problem is that, that general population is not the overall general population. And that brings us to the statement of the sampling bias principle. t says that if the data is sampled in a biased way, then learning will produce a similarly biased outcome. Learning is not an oracle, not like the football oracle. Learning sees the world through the data you give it. m a learning algorithm, here is the data. You give me skewed data, m going to give you a skewed hypothesis. m doing my job. m trying to fit the data. o this is always the case, and then you realize that there is always a problem in terms of making sure that the data is actually representative of what you want. o again, we put this in a box. Thats the second principle, so its important. And lets look at a practical example in learning. n financial forecasting, people use machine learning a lot, and sometimes when you look at the markets, the markets are completely crazy. A rumor comes out and the market goes this way, et cetera. And you are a technical person, you are trying to find an intrinsic pattern in the time series. o you decide, m going to use the normal conditions of the market. o m going to take periods of the market where the market was normal, and then there is actually a pattern when people buy, buy, buy, and sell, sell, sell, something happens, or whatever you are going to discover using your linear regression or other learning algorithm. And you do this. And then you deploy it, and when you test it, you test it in the real market, and realize that now there is a sampling bias. n spite of the fact that you were very happy insample, you actually forgot a part of the market, and you dont know whether that part will be terrible for you, great for you, or neutral for you. You just dont know. Thats what sampling bias does. The newspaper could have done this poll and, by their sheer luck, the general population thinks the same of Truman and Dewey as the small sample they talked about, in which case the result would have come out and they would have never discovered that they made a mistake. o sampling bias makes you vulnerable, at the mercy of the part that you didnt touch. n this case, you didnt touch the market in certain conditions, and if it does happen, all bets are off. One way to deal with sampling bias is matching the distributions. ts a very interesting technique, and its actually applied in practice. m going to mention that. o what is the idea? The idea is that you have a distribution on the input space, in your mind, and there was one assumption in Hoeffding and V inequality and all of that. They didnt make too many assumptions, but one assumption they certainly made is that you pick the points for training from the same distribution you pick for testing. That was the only thing that they require. o when you have sampling bias, that is violated. And therefore, you try to say dont have the same distribution. have data picked from some distribution, and m going to deliver the hypothesis to the customer, and theyre going to test it in other conditions. What do do? What you do, you try to match the distributions. You dont reach for the distributions and match them. You do something that will effectively make them match. And you look at this, and lets say that this is the training distribution, and the test distribution is off a little bit. This is a probability density function. Both of them are Gaussian. One of them is off and with a different sigma. o what you do, if you have access to those if someone tells you what the distributions are and then gives you a sample, there is a way by either giving different weights for the training data, or resampling the training data, to get another set as if it was pulled from the other distribution. ts a fairly simple method. Very seldom that you actually have the explicit knowledge of the probability distributions, so its not that useful in practice, but in principle, you can see that it can be done. And the price you pay for it is that you had 100 examples. When you are done with this scaling and resampling or whatever method you use, the effective size now is 90. o you lose a little bit in terms of the independence of the points, and therefore, you get effectively a smaller sample because of it. But at least, you deal with the sampling bias that you wanted to deal with. Now, this method works, and even if you dont know the distribution, there are ways to try to infer the distribution that work. But it doesnt work if there is a region in the input space where the probability is zero for training, nothing will be sampled from that part, but you are going to test on it. There is a probability of getting a point there, very much like guys without a phone. That happened to have zero probability in the sample, but they dont have zero probability in the general population. And in that case, there is nothing that can be done in terms of matching, because obviously you dont know what happened in that part. On the other hand, in many other cases, there is a simple procedure, which is actually very useful in practice. f you look at, for example, the Netflix competition, one of the things you realize is that have the data set, its a huge data set, 100 million points. And then m going to test your hypothesis on the final guys, the final ratings. o its a much smaller set. And the interesting aspect about it is that if you look at the distribution of the general ratings, the 100 million, it really is different from the distribution of these guys. Therefore, the question came up, can do something during the training such that make the 100 million look as if they were pulled from the distribution of the last guys? Very interesting question, has a very concrete answer, and the 100 million become 10 million, not that you are throwing away points, but you are weighting them such that when you are done, they look smaller than a set. But then you are actually matched to that, and you can get a dividend in performance. o there is a cure for sampling bias in certain cases, and there is no cure in other cases, in which all you can do is admit that you dont know how your system will perform in the parts that were not sampled. That would be fatal if you are doing a presidential poll, but may not be as fatal when you are doing machine learning, because all you are going to do, you are going to warn against using this system within that particular subdomain. Third puzzle, try to detect sampling bias here. redit approval. We have seen that before. Thats a running example in the course, so let me remind you what that was. The bank wants to approve credit automatically. t goes for the historical records of customers who applied before, and they were given credit cards, so you have a benefit of, lets say, 3 or 4 years worth of credit behavior. And you look back at their inputs, and the inputs in those cases were simply the information they provided at the time they applied for credit, because this is the information that will be available from a new customer. And you get something like that. This is the application. You also have the output, which is simply you go back and see whatever the credit behavior is and you ask yourself, did they make money for me? Because its not only credit worthiness, that you are a reliable person. ts also that some people who are flirting with disaster are very profitable for the bank, because they max out and they pay this ridiculous percentage, so they make a lot of money as long as they dont default. Once they default, its a problem. o theres a question of just, did you make profit or not? Thats a question. And m going to approve future customers if expect that they will make profit for me. Thats the deal. Where is the sampling bias? We probably alluded to it in one form or another. The problem is that youre using historical data of customers you approved, because these are the only ones you actually have credit behavior on. o the guys who applied, and you rejected them, are not part of this sample. And when you are done, you are going to have a system that applies to a new applicant. You do not know a priori whether that applicant will be approved or not, according to your old criteria. o it could belong to the population that was never part of your training sample. Now, this is one case where the sampling bias is not that terrible in terms of effect, not in terms of characterizing what is going on. You have a part of the population, and they have zero probability in terms of training, and nonzero probability in terms of testing. ts good, oldfashioned sampling bias. But the point is that banks tend to be a bit aggressive in providing credit because, as mentioned, the borderline guys are very profitable. o you dont want to just be conservative and cut them off, because youre going to be losing revenue. Because of this, the boundary that you are talking about is pretty much represented by the guys you already accepted. You already made mistakes in what you accepted. o when you get that boundary, the chances are the guys you missed out will be deep on one side. You got all the support vectors, if you want, so the interior points dont matter. They matter a little bit, but actually, that system with the sampling bias does pretty good on future guys. By evidence that you reject someone, how do you know that its good because you rejected it? They apply somewhere else, and they make the other guy lose money, so you realize that your decision was good. o you can verify, if you have a consortium of banks, whether actually that sampling bias here has an impact, or doesnt have an impact. Final topic, data snooping, the sweetest of all. Well, its the sweetest because it is so tricky, and manifests itself in so many ways. Let me first state the principle. The principle says, if a data set has affected any step of the learning process, then the ability of the same data set to assess the outcome has been compromised. Very simply stated. The principle doesnt forbid you from doing anything. You can do whatever you want. Just realize that if you use a particular data set, whether its the whole, or a subset or whatever, use it to navigate into m going to do this, m going to choose this model, m going to choose this lambda, m going to do this, m going to reject this, whatever it is. You made a decision, then when you have an outcome from the learning process and you use the same data set that affected the choice of that, the ability to fairly assess the performance of the outcome has been compromised by the fact that this was chosen according to the data set. think this is completely understood by us, having gone through the course. We put it in a box, and then we make the statement that this is the most common trap for practitioners, by and large. ve dealt with Wall treet firms quite a bit in my career, and there are lots of people who are using machine learning, and it is rather incredible how they manage to datasnoop. And there is a good reason for it, because when you datasnoop, you end up with better performance, you think, because thats why you snooped. looked at the data, chose a better model. The other guy didnt look at the data, and they are struggling with the model, and they are not getting the same insample, and am ahead. t looks very tempting to do. And its not just looking at the data. The problem is that there are many ways to fall into the trap, and they are all happy ways. o if you think of it as landmines, it is actually happy landmines. You very cheerfully step on the mine, because you think you are doing well. o you need to be very careful. And because it has different manifestations, what m going to do now, m going to go through examples of data snooping. ome of them we have seen before, and some of them we havent. And then you will get the idea. What should avoid, and what kind of discipline or compensation should have, in order to be able not to suffer from the consequences of data snooping? o the first way of data snooping, we have seen before, is looking at the data. o m borrowing something from our experience. Remember the nonlinear transform? Yeah. o you have a data set like this, and lets say you didnt even look at the data and you decided that, m going to use a 2ndorder transform. o this is the transform, you take a full 2nd order. You apply it, and you look at the outcome, and this is good. managed to get zero insample error. What is the price m paying for generalization? One, two, three, four, five, six. Thats an estimate for the V dimension, so thats the compromise between this six and however many points, et cetera. o you realize, fit the data well but dont like the fact that its six. dont have too many points, so my handle on generalization is not good. o let me try to do better, at least in your mind. o what you do is say, wait a minute, didnt need all of these guys. could have gone with just this guy, knowing that this is the origin. All you need to do is just x_1 squared and x_2 squared. This is just a circle centered at the origin. Why do need the other funny stuff? This would be if m going for a more elaborate set. o now one, two, three, now have V dimension of three, so m better. Of course, we know better, but m just playing along. And then you get carried away and say, can even do this. ts not an ellipse, its a circle, so can just add up x_1 squared and x_2 squared as one coordinate, and then have two. And you see what the problem is, and the problem is what we mentioned before. What you are really doing, you are a learning algorithm in your own right, but free of charge. Thats the problem. You are looking at the data, and you are zooming in, and youre zooming in. Youre learning. Youre learning. You are narrowing down the hypotheses, and then leaving the final learning algorithm just to get you the radius. Yeah, big deal. Well, the problem is that you are charging now for a V dimension of two, which is the last part of the learning cost, which is choosing the coefficients here. But you didnt charge for the fact that you are a learning algorithm, and you took the data into consideration, and you kept zooming in from a bigger hypothesis set. You didnt charge for the full V dimension of that. Now, it is very important to realize that the problem here is that the snooping here involves the data set. Because what happens when you look at the data set? You are vulnerable to designing your model, or your choices in the learning, according to the idiosyncrasies of the data set. And therefore, you may be doing well on that data set, but you dont know whether you will be doing in another, independently generated data set from the same distribution, which would be your outofsample, so thats the key. On the other hand, you are completely allowed, encouraged, ordered to look at all other information related to your target function and input space, except for the realization of the data set that you are going to use for training, unless you are going to charge accordingly. o here is the deal. omeone comes in, ask him, how many inputs do you have? What is the range of the inputs? How did you measure the inputs? Are they physically correlated? Do you know of any properties that can apply? s it monotonic in this? All of this is completely valid and completely important for you in order to zoom in correctly, because right now, you are not using the data. You are not subject to overfitting the data. You are using properties of the target function and the input space proper, and therefore improving your chances of picking a correct model. The problem starts when you look at the data set and not charge accordingly, very specifically. Here is another puzzle. This one is financial forecasting. Befitting. o right now, there will be data snooping somewhere here, and you need to look out for it. n this case, this is a real situation with real data. You are predicting the exchange rate between the dollar versus the British pound. o you have eight years worth of daily trading, where you just simply take the change from day to day. And eight years would be about 2,000 points. There are about 250 trading days per year, at least when the data was collected. And what you are planning to do is the following. You look here. Let me magnify it. This is your input for the prediction, and this is your output. o r is the rate. o you dont look at the rate in the absolute, you look at delta rate, the difference between the rate today and the rate yesterday. Thats what youre trying to predict. Youre asking yourself whether its going up or down every day, and by how much. o you get delta, and you get delta for the 20 days before, hoping that a particular pattern of up and down in the exchange rate will make it more likely that todays change, which hasnt happened yet you are deciding to either buy or sell at the open whether this will be positive or negative and by how much. o if you make a certain prediction, then you can obviously capitalize on that, and make predictions according to that. And if you are right more often than not, you will be making money because you are losing less often than winning if you have the right objective function. o this is the case. What happens here is that now you have the 2,000 points, so for every day, there is a change, delta r. And what you do first, you normalize the data to zero mean and unit variance. And then after that, you have this array of 2,000 points. You create training set and test set. o the training set in this case, you take 1,500 points, 1,500 days. o every day now, you take the day, and you take the previous 20 days as their input. That becomes your training. And for the test, you picked it at random, not the last ones, just to make sure that there is no funny stuff, change in this or that. You just want to see if something is inherent, so just to be on the safe side, you did it randomly. And then you take 500 points in order to test on. o right now, out of the 2,000 array of points, you have a big array of 20 points input, one output, 20 points input, one output, 1,500 of those. And on the other side on the test, 20 points input, one output, 20 inputs, one output, 500 of those. This is for the test. Thats the game. o you go on with the training. You train your system on the training set, and to make sure, because you heard of data snooping, these guys are in a lock. You didnt look at the data at any point. You just carried all of this automatically, and then when you are done and you froze the final hypothesis, you open the safe, you get the test data, and you see how you did. And this is how you did. You train only on D_train, you test on D_test, and this is what you get. m not saying how often you got it right, but m actually saying that you put a trade according to the prediction, and m asking you how much money you made. o for the 500 points, sometimes you win, sometimes you lose, but you win more often than you lose, which is good. And at the end of two years worth thats what 500 days would be you would have made a respectable 22% unleveraged, so thats pretty good. o you are very happy, and now having done that, you go to the bank and tell them have this great prediction system. Here is the system. m going to sell it for you, and guarantee that it will be you do the error bars and whatever. And they go, and they go live, and they lose money, and they sue you, and all of that. o you ask yourself, what went wrong? What went wrong is that there is snooping. And whats interesting is, where exactly is the snooping? o there are many things: random, the fact that used inputs that happened to be outputs to the other guy? No, no, thats legitimate. m just really getting the pattern. You just go around it, and it is really remarkably subtle, to the level where you can fall into that very, very easily, and here is where the snooping happened. The snooping happened when you normalized. What? had the daily rates, right? 2,000 of them. have the change. All of that is legitimate. Now, slipped a fast one by you hope did when told you, first you normalize this to zero mean and unit variance. t looked like an innocent step, because you get them to a nice numerical range, and some methods will actually ask you to please put the data normalized, because its sensitive to the dynamic range of the data. The problem is that did this before separated the training from the testing. o took into consideration the mean and variance of the test set. That extremely slight snooping into whats supposed to be the test set, supposed not to affect anything, has affected me, but by just a mean and How could it possibly make a difference? Well, if you didnt do that, you split the data first. You took the training set only, and you did the normalization. And whatever the mu and sigma squared that did the normalization for the training set, you took them frozen and applied them to the test set so that they live in the same range of values. And you did the training now and the test without any snooping. nder those conditions, this is what you would have gotten. o no wonder you lost money. All the money you made is because you sniffed on the average of the outofsample. And the average matters, because if you think about it, lets say that the dollar had a trend of going up. That will affect the mean, but you dont know that at least, you dont know it for the outofsample unless you got something outofsample. o m not saying normalization is a bad idea. Normalization is a super idea. Just make sure that whatever parameters you use for normalization are extracted exclusively from what you call a training set, and then you are safe. Otherwise, you will be getting something that you are not entitled to get. Easy to think about, if you are actually thinking: m going to deploy this system. dont have the test set. o if you dont have the test set, you cannot possibly use those points in order to normalize. o use only things that you will actually be able to use when you deploy the system. n this case, you have only the training. Now, the third manifestation of data snooping comes from the reuse of the data set. That is also very common. o what you do, give you a homework problem. Oh, am very excited about neural networks. Let me try neural networks. Oops, they didnt work. heard support vector machines are better. Let me try them. Yeah, did, but it was the wrong kernel. Let me use the RBF kernel. Oh, maybe m just using too sophisticated a model. Let me go back to the linear models, and just use a nonlinear transformation. And eventually, using the same data set, you will succeed. And the best way to describe it is a very nice quote in machine learning. t says, ""f you torture the data long enough, it will confess"", but exactly the same way that a confession would mean nothing in this case. o the problem here is that when you do this, you are increasing the V dimension without realizing it. used neural networks and it didnt work, and then used support vector machines with this and that. Guess what is the final model you used in order to learn? The union of all of the above. ts just that some of them happened to be rejected by the learning algorithm. Thats fine, but this is the resource you had. o you think of the V dimension, and the V dimension is of the total learning model. Again, as we will see, there will be remedies for data snooping, and there is a question of its not like have to try a system, and when fail, just quit. Thats not what is being said. ts just asking you to account for what you have been doing. Dont be fooled into thinking that can do whatever, and then the final guy that use with a very simple model, after all the wisdom that accumulated from the data, is the V dimension that m going to charge. That just doesnt work. The interesting thing is that this could happen, not because you used the data, but because others used the data. Oh my God, its really terrible here. Heres the deal. You decide to try your methods on some data set. o you go to one of the data sets available on the internet, lets say for heart attacks or something, and you say, am very aware of data snooping, right? m not going to look at the data, m not going to normalize using the data. m going to get the data, and put them in a safe, and close the safe, and will just do my homework before even touch the data. And your homework is in the form of reading papers about other people who used the data set. You want to get the wisdom, so you use this, and you find that people realize that Boltzmann machines dont work in this case. The best kernel for the V happens to be polynomial of order three, whatever it is. o you collect it, and you look at it, and then you have your own arsenal of things. o as a starting point, you put a baseline based on the experience you got, and you say that m going now to modify it. Now you open the safe and get the data. Now you realize what happened. You didnt look at the data, but you used something that was affected by the data, through the work of others. o in that case, dont be surprised that if all you did was determine a couple of parameters, thats the only thing you added to the deal, and you got a great performance. And you say, have two parameters, V dimension is 2, have 7,000 points. must be doing great out of sample, and you go out of sample, and it doesnt happen. Doesnt happen because actually, its not the two parameters, its all the decisions that led to that model. And the key problem in all of those is always to remember that you are matching a particular data set too well. You are now married to that data set. You kept trying things, et cetera, and after a while, you know exactly what to do with this data set. f someone comes and generates another data set from the same distribution, and you look at it, it will look completely foreign to you. What happens? t used to be that whenever these two points are close, there is always a point in the same line far away. Thats obviously an idiosyncrasy of the thing. Now you give me a data set that doesnt have that. That must be generated from a different distribution. No, its generated from the same distribution. You just got too much into this data set, to the level where you are starting to fit funny stuff, fitting the noise. There are two remedies for data snooping, and m going to do this, and then give you the final puzzle, and call it a day. You avoid it, or you account for it. Thats it. o avoiding it is interesting. t really requires strict discipline. o ll tell you a story from my own lab. We were working on a problem, and performance was very critical, and we were very excited about what we are having, all the ingredients that make you go for data snooping. You just want to push it a little bit. We realized that this is the case, so we had that discipline that well take the data the first thing we did, we sampled points at random, put them in the safe, and then the rest of the guys you can use for your training, validation, whatever you want. o at some point, one of my colleagues who was working on the problem declared that they already have the final hypothesis ready. t was a neural network at that point. o now was the safe keeper, so now m supposed to give them the test points, in order to see what the performance is like. smelled a rat, so what decided, asked them, could you please send me the weights of the final hypothesis before send you the data set? That was the requirement, because now its completely clear. Hes committed to one final hypothesis. f send him the data set and he says it performed great, can verify that because he has already sent me that. ts a question of causality in this case. And the problem is that it is not that difficult to come Here is the data set, and what you really had, you had the candidate, but you had three other guys that are in the running. And then you look at the data, and you decide, maybe get one from the running, et cetera. You can do very little. And in particular, in financial applications, its extremely vulnerable, because its so noisy. t is very easy when you fit the noise a little bit, you will make much better performance than you will ever get from the pattern, so you had better be extremely careful. And therefore, you have a discipline that really is completely waterproof that you did not datasnoop. Accounting for data snooping is not that bad, because we already did a theory, and when we have a finite number of hypotheses we are choosing from for validation, we know the level of contamination. Even if its an infinite one, we have the V dimension. We had very nice guidelines to tell us how much contamination happened. The most vulnerable part is looking at the data, because its very difficult to model yourself and say, what is the hypothesis set that explored, in order to come up with that model by looking at the data? o because accounting is very difficult, thats why keep raising a flag about looking at the data. But if you can account, by all means, thats all you need to do. Look at the data all you want, just charge accordingly, and you will be completely safe as far as machine learning is concerned. Final puzzle, and we call it a day. And we are still in data snooping, so maybe this has to do with data snooping, but it also has to do with sampling bias, so its an interesting puzzle. This is a case where you are testing the longterm performance of a famous strategy in trading, which is called ""buy and hold"". What does it mean? You buy and hold. You dont keep m going to sell today, because its going down. No, you just buy, and sit on it, forget about it. ts like a pension plan or something. And five years later, you look at it and see what happens. o you want to see how much money you make out of this. o what you do is you decide to use 50 years worth of data. Thats usually a good life span in a professional life, so that will cover how much money you make at the time you retire, from the time you start contributing to it. o here is the way you do the test. You want the test to be as broad as possible, so you go for the P 500. You take all currently traded stocks, the 500 of them. And then you go back, and you assume that you strictly applied a buy and hold for all of them. o dont be tempted to say that m going now to modify it, because this guy crashed at some point, so if sold and then bought again, would make more money. No, no, no. ts buy and hold we are testing. That was frozen. o you do this, and then you compute, and you find that you will make fantastic profit. And you compute, if do this you are now young in your career and apply it, by the time retire, will have a couple of yachts and will do this. ts a wonderful thing. an you see the problem? You are very well trained now, so you can detect it. The problem is there is a sampling bias, formally speaking, because you looked at the currently traded stock. That obviously excludes the guys that were there and took a dive, and that obviously puts you at a very unfair advantage. And its interesting that people do treat this not as a sampling bias but as a data snooping, in spite of the fact that it doesnt fit our definition of data snooping. t does fit the definition of snooping, because you looked at the future when you are here. ts as if you are looking 50 years from now, and someone tells you which stocks will be traded at that point. o thats not allowed. But nonetheless, some people will treat this as data snooping. n our context, this is formally just sampling bias, and sampling bias that happens to be created or caused by a form of snooping. will stop here, and we will take questions after a short break. Lets start the QA. ODERATOR: n the last one homework that people were using LBV, it emphasized the fact that data should be scaled, so why did we not discuss this in the course, or what? PROFEOR: There are many things did not discuss in the course. had a budget of 18 lectures, and chose what consider to be the most important. There is a question of input data processing, and there is a question not only of normalization, its also a question of decorrelation of inputs and whatnot, which is a practical matter. And the fact that did not cover something doesnt mean that its not important. t just means that its a constrained optimization problem, and you have the solution, and have to have a feasible solution. o thats what have. think we have an inhouse question. TDENT: Thanks. Professor, you mentioned that if you reuse the same data set to compare between different models, its a form of data snooping. o how do we know what form of model is better? PROFEOR: The part of it which is formally data snooping is the part where you used the failure of the previous model to direct you to the choice of the new model, without accounting for the V dimension of having done that. o effectively, its not you that looked at the data, but the previous model looked at the data and made a decision, and you didnt charge for it. o that is the datasnooping aspect of it. f you did this as a formal hierarchy. You start out, here is the data set, dont look at it. m going to start with support vector machines with RBF, and then if fail, m going to do this, et cetera. And given that this is my hierarchy, the effective V dimension is whatever, this is completely legitimate. The snooping part is using the data for something without accounting for it in this case, using the data for rejecting certain models and directing yourself to other models. TDENT: Yes. o by accounting for the data snooping, do you mean you consider the effective V dimension of your entire model, and use a much larger data set for your entire model? PROFEOR: Youll get the V dimension, so if the V dimension is so big that the current number, the amount of data set, wont give you any generalization, the conclusion is that wont be able to generalize unless get more data, which is what youre suggesting. o the basic thing is that you are going to learn, and you are going to finally hand a hypothesis to someone. What do you expect in terms of performance? Data snooping makes you much more optimistic than you should, because you didnt charge for things that you should have charged for. Thats the only statement being made. TDENT: s there a possibility that data snooping will make you pessimistic, will make you more conservative? PROFEOR: can probably construct deliberate scenarios under which this is the case, but in all the problems that have seen, people are always eager to get good performance. That is the inherent bias, and that is what directs you toward something optimistic, because you do something that gets you smaller insample error, and you think now that this insample error is relevant, but you didnt account for what it cost you to get to that insample error. o its always in the optimistic direction. TDENT: Yes. Thank you. PROFEOR: ure. ODERATOR: Assuming that there is sampling bias, can you discuss how can you get around it? PROFEOR: o we discussed it a little bit. f there is a sampling bias, if you know the distributions, you can let me look at the so in this case, lets say that give you these distributions. What this means, you generated the data according to the blue curve, and therefore, you will get some data here. o what is clear, for example, is that the data that correspond to the center of the red curve, which is the test, are underrepresented in the training set. And on the other hand, the data that are here are overrepresented. The blue curve is much bigger, it will give you some samples. t will hardly ever be the case that you will get that sample from the testing. o what you do, you devise a way of scaling, or giving importance not scaling the y value, just scaling the emphasis of the examples such that you compensate for this discrepancy, as if you are coming from here, and there are some resampling methods to do the same effect. o this is one approach. The other approach, which is in the absence of those guys, is to look at the input space in terms of coordinates. Lets say that with the case of the Netflix, you look at, for example, users rated a certain number of movies. ome of them are heavy users, and some of them are light users. o you put how many movies a user rated, and you try to see that in the training and in the test, you have equivalent distribution as far as the number of ratings are concerned. And you look for another coordinate and a third coordinate, and you try to match these coordinates. This is an attempt to basically take a peek at the distribution, the real distributions that we dont know, in terms of the realization along coordinates that we can relate to. o there are some methods to do that. Basically, you are compensating by doing something to the training set you have, to make it look more like it was coming from the test distribution. ODERATOR: s there any counter example to Occams razor? PROFEOR: s there ODERATOR: ounter example to Occams razor or not? PROFEOR: ts statistically speaking in what we can take a case where violate the marriage between the complexity of an object and the complexity of the set that belongs to the object. o can take one hypothesis which is extremely sophisticated in terms of the minimum description length or the order of the polynomial, but it happens to be the only hypothesis in my hypothesis set. Now, if this happens to be close to your target function, you will be doing great, in spite of the fact that its complex. o can create things where start violating certain things like that. But in the absence of further knowledge, and in very concrete statistical terms, Occams razor holds. o the idea is that when you use something simpler, on average, you will be getting a better performance. Thats the conclusion here. ODERATOR: pecifically talking about applications in computer vision and the idea of sampling bias comes to mind, is there any particular method used there to correct this, or just any of the things we discussed? PROFEOR: think its the same as discussed, just applied to the domain. ometimes the method becomes very particular when you look at what type of features you extract in a particular domain, and therefore, it gets modified in that way. But the principle of it is that you take the data points from your sample, and give them either different weight or different resampling, such that you replicate what would have happened if you were sampling from the test distribution. ODERATOR: think thats it. PROFEOR: Very good. Well see you on Thursday.","And now you can look at the set of all similar objects, and you call it H, and you have one of 2^l as the description of an object here, and you can take the ""1 of 2^l"" as the description of the complexity of that set. And the problem is that it is not that difficult to come Here is the data set, and what you really had, you had the candidate, but you had three other guys that are in the running. We realized that this is the case, so we had that discipline that well take the data the first thing we did, we sampled points at random, put them in the safe, and then the rest of the guys you can use for your training, validation, whatever you want. You made a decision, then when you have an outcome from the learning process and you use the same data set that affected the choice of that, the ability to fairly assess the performance of the outcome has been compromised by the fact that this was chosen according to the data set. You just carried all of this automatically, and then when you are done and you froze the final hypothesis, you open the safe, you get the test data, and you see how you did. What happens here is that now you have the 2,000 points, so for every day, there is a change, delta r. And what you do first, you normalize the data to zero mean and unit variance. o this is always the case, and then you realize that there is always a problem in terms of making sure that the data is actually representative of what you want. Now, let us take this in our mind, that we are going to use the complexity of an object as the same as the complexity of the set of objects that the object naturally belongs to, and we will see some ramifications. You just look at that object intrinsically, and that is what you think of as the measure of complexity. o what you do, if you have access to those if someone tells you what the distributions are and then gives you a sample, there is a way by either giving different weights for the training data, or resampling the training data, to get another set as if it was pulled from the other distribution. o what is clear, for example, is that the data that correspond to the center of the red curve, which is the test, are underrepresented in the training set. When you do the math in order to prove Occams razor in one version or another, the complexity you are using is actually the complexity of the set of objects. PROFEOR: The part of it which is formally data snooping is the part where you used the failure of the previous model to direct you to the choice of the new model, without accounting for the V dimension of having done that. Now, it is very important to realize that the problem here is that the snooping here involves the data set. And the interesting aspect about it is that if you look at the distribution of the general ratings, the 100 million, it really is different from the distribution of these guys. o you look at the points, and then you try to find the representative center for them such that when you put a radial basis function around that point, it captures the contribution of those points, and then more or less dies out, or at least is not as effective when it goes far away, and this is another center that does the same. And then you deploy it, and when you test it, you test it in the real market, and realize that now there is a sampling bias. The most vulnerable part is looking at the data, because its very difficult to model yourself and say, what is the hypothesis set that explored, in order to come up with that model by looking at the data? When you are done with this scaling and resampling or whatever method you use, the effective size now is 90. o you lose a little bit in terms of the independence of the points, and therefore, you get effectively a smaller sample because of it. On the other hand, the reality of it is that the growth function is actually 2^N and this is certain to happen, so when it happens, its meaningless. And in our terms, the linear model is just way too complex for the size of the data set, which is 2, to be able to generalize at all. You apply it, and you look at the outcome, and this is good. The idea is that you have a distribution on the input space, in your mind, and there was one assumption in Hoeffding and V inequality and all of that. But the principle of it is that you take the data points from your sample, and give them either different weight or different resampling, such that you replicate what would have happened if you were sampling from the test distribution. And we had two models, or two versions of that model, one of them where the centers are fewer than the number of data points, which is the most common one, in which case we need to come up with the value of the centers, mu_k, and learn the values of w_k. You run an experiment, you consider all possible outcomes of the experiment, the probabilities that go with them, and you find one collective function that captures the probability, sum of p logarithm of 1 over p, and that becomes your entropy and that describes the disorder, the complexity, whatever you want, of the class of objects, each outcome being one object. And it says that an explanation of the data so you are running an experiment, you collect the data, and you want to make an explanation of the data. o you have a data set like this, and lets say you didnt even look at the data and you decided that, m going to use a 2ndorder transform. f there is a sampling bias, if you know the distributions, you can let me look at the so in this case, lets say that give you these distributions. f you look at, for example, the Netflix competition, one of the things you realize is that have the data set, its a huge data set, 100 million points. And you look at this, and lets say that this is the training distribution, and the test distribution is off a little bit. And whatever the mu and sigma squared that did the normalization for the training set, you took them frozen and applied them to the test set so that they live in the same range of values. Dont be fooled into thinking that can do whatever, and then the final guy that use with a very simple model, after all the wisdom that accumulated from the data, is the V dimension that m going to charge. On the other hand, you are completely allowed, encouraged, ordered to look at all other information related to your target function and input space, except for the realization of the data set that you are going to use for training, unless you are going to charge accordingly. That is the inherent bias, and that is what directs you toward something optimistic, because you do something that gets you smaller insample error, and you think now that this insample error is relevant, but you didnt account for what it cost you to get to that insample error. o what you do, you keep trimming the explanation to the bare minimum that is still consistent with the data, and when you arrive at that, then you have the best possible explanation. Here is one hypothesis, and you got it right, and you gave a lot of value for that because this is unlikely to happen. Well, the problem is that you are charging now for a V dimension of two, which is the last part of the learning cost, which is choosing the coefficients here. And it turned out to be a very simple algorithm in that case, where you use unsupervised learning to get the mu_ks, the centers, by clustering the input points without reference to the label that they have. n that case, we needed to generate target functions of different complexity, and the way we did it, we just increased the order of the polynomial as our measure of the complexity of that object. And for the test, you picked it at random, not the last ones, just to make sure that there is no funny stuff, change in this or that. But it doesnt work if there is a region in the input space where the probability is zero for training, nothing will be sampled from that part, but you are going to test on it. o the problem here is that when you do this, you are increasing the V dimension without realizing it. n the case of nearest neighbor, you have a data point, one of your points in the training set, and it influences the region around it. o you put how many movies a user rated, and you try to see that in the training and in the test, you have equivalent distribution as far as the number of ratings are concerned. Now, this is one case where the sampling bias is not that terrible in terms of effect, not in terms of characterizing what is going on. And you see what the problem is, and the problem is what we mentioned before. o in that case, dont be surprised that if all you did was determine a couple of parameters, thats the only thing you added to the deal, and you got a great performance. o here is the way you do the test. o we already have a feel that there is some kind of correspondence, and if you look at the different definitions outside, there are many definitions of the complexity of an object, and m going to give you two from different worlds. There are two remedies for data snooping, and m going to do this, and then give you the final puzzle, and call it a day. Now, the measure of complexity in this term is that the complexity of this fellow is l bits, because that is my definition. n spite of the fact that you were very happy insample, you actually forgot a part of the market, and you dont know whether that part will be terrible for you, great for you, or neutral for you. All of this is completely valid and completely important for you in order to zoom in correctly, because right now, you are not using the data. t means that beware of fitting the data with complex models, because it looks great in sample and you are very encouraged, and when you go out of sample, you know what happens. But you didnt charge for the fact that you are a learning algorithm, and you took the data into consideration, and you kept zooming in from a bigger hypothesis set. And therefore in spite of the fact that it looks complex, its really one of few, and that is what we achieve by the support vector machines. o if you look at one object from that set, and you look at this measure of complexity, now that measure of complexity is extrinsic with respect to that object. o effectively, its not you that looked at the data, but the previous model looked at the data and made a decision, and you didnt charge for it. The second question is, how do we know that this is the case? You have an explanation of the data, and you have your razor. And the other one, which is simple, is the order of a polynomial. You didnt look at the data, but you used something that was affected by the data, through the work of others. Very seldom that you actually have the explicit knowledge of the probability distributions, so its not that useful in practice, but in principle, you can see that it can be done. The problem is that actually, the hypothesis set is very complex, and therefore the prediction value is meaningless. PROFEOR: ts statistically speaking in what we can take a case where violate the marriage between the complexity of an object and the complexity of the set that belongs to the object. And you may be familiar with some of them, and we have already alluded to data snooping in one of the lectures. We looked at the V dimension, for example, in order to prove something of an Occams nature in this course already, and that captured the complexity of a set of objects. And in the case of V, the centers, if youre going to call them that, happen to be the support vectors in which the output is very much consulted in deciding what these support vectors are. And you do this. But the good news is that the complexity of an object and the complexity of a set of objects, as we described in this slide, are very much related, almost identical. And what you are planning to do is the following. And the first one has to do with this complexity, so lets look at the puzzle. n spite of the fact that this is a million in length, it is a simple object because you were able to describe it as ""10 to the million minus 1"". o what you do, you devise a way of scaling, or giving importance not scaling the y value, just scaling the emphasis of the examples such that you compensate for this discrepancy, as if you are coming from here, and there are some resampling methods to do the same effect. You just go around it, and it is really remarkably subtle, to the level where you can fall into that very, very easily, and here is where the snooping happened. And in that case, there is nothing that can be done in terms of matching, because obviously you dont know what happened in that part. And therefore, you may be doing well on that data set, but you dont know whether you will be doing in another, independently generated data set from the same distribution, which would be your outofsample, so thats the key. You want to get the wisdom, so you use this, and you find that people realize that Boltzmann machines dont work in this case. Basically, you are compensating by doing something to the training set you have, to make it look more like it was coming from the test distribution. o you get delta, and you get delta for the 20 days before, hoping that a particular pattern of up and down in the exchange rate will make it more likely that todays change, which hasnt happened yet you are deciding to either buy or sell at the open whether this will be positive or negative and by how much. o can take one hypothesis which is extremely sophisticated in terms of the minimum description length or the order of the polynomial, but it happens to be the only hypothesis in my hypothesis set. And you compute, if do this you are now young in your career and apply it, by the time retire, will have a couple of yachts and will do this. m going to sell it for you, and guarantee that it will be you do the error bars and whatever. The first one is a complexity of an object, in our case, a hypothesis h or the final hypothesis g. That is one object, and we can say that this is a complex hypothesis or a simple hypothesis. You also have the output, which is simply you go back and see whatever the credit behavior is and you ask yourself, did they make money for me? You start out, here is the data set, dont look at it. This implies that if look at all the guys that are similar to this object in terms of complexity, they also happen to have l bits worth of minimum description. What this means, you generated the data according to the blue curve, and therefore, you will get some data here. The other guy didnt look at the data, and they are struggling with the model, and they are not getting the same insample, and am ahead. And its interesting that people do treat this not as a sampling bias but as a data snooping, in spite of the fact that it doesnt fit our definition of data snooping. Now you are really very curious, and you are eagerly waiting in the 6th week in the morning of onday to see where the letter is. PROFEOR: Youll get the V dimension, so if the V dimension is so big that the current number, the amount of data set, wont give you any generalization, the conclusion is that wont be able to generalize unless get more data, which is what youre suggesting. This is the one we add in the augmented error, in order to capture the complexity of what we end up with. o you think of the V dimension, and the V dimension is of the total learning model. The entropy is one, and the one we are most familiar with, which is the V dimension. The two questions are, the first one is, what does it mean for a model to be simple? o everything in the region around it in the input space inherits the label of that point, until you get to a point which is closer to another data point, and then you switch to that point. The other approach, which is in the absence of those guys, is to look at the input space in terms of coordinates. And the key problem in all of those is always to remember that you are matching a particular data set too well. Because what happens when you look at the data set? o this is the case. o there is a cure for sampling bias in certain cases, and there is no cure in other cases, in which all you can do is admit that you dont know how your system will perform in the parts that were not sampled. The other set of definitions have to do with the complexity of a set of objects. o now was the safe keeper, so now m supposed to give them the test points, in order to see what the performance is like. o the basic thing is that you are going to learn, and you are going to finally hand a hypothesis to someone. This is an attempt to basically take a peek at the distribution, the real distributions that we dont know, in terms of the realization along coordinates that we can relate to. And this is how you did. And you can see that when we describe the complexity of an object, that complexity is an intrinsic property of the object. o you are very happy, and now having done that, you go to the bank and tell them have this great prediction system. o the idea is that give you an object and you try to specify the object, and you try to specify it with as few bits as possible. The interesting thing is that this could happen, not because you used the data, but because others used the data. Now, if this happens to be close to your target function, you will be doing great, in spite of the fact that its complex. saac Newton has something that is similar, and a bunch of them, but m going to quote the one that survived the test of time, which is Occams razor. Again, as we will see, there will be remedies for data snooping, and there is a question of its not like have to try a system, and when fail, just quit. o you collect it, and you look at it, and then you have your own arsenal of things. nsupervised learning is easy, because that is the utility we had in order to cluster the points and find the centers. o right now, there will be data snooping somewhere here, and you need to look out for it. We were talking about the discrepancy between insample, the poll, outof sample, the general population, the actual vote, and we were asking ourselves, what is the probability that this will be bigger than something, such that the result is flipped? o you are trying to find an explanation of the data, and here is a condition about what the explanation should be like. And then you look at the data, and you decide, maybe get one from the running, et cetera. One of them has to do with collecting the data, and the other one has to do with handling the data.",0.0789901077808947
43,43,"Welcome back. Were finally using the Laplace Transform to do something useful. n the first part of this problem, we just had this fairly straightforward differential equation. And know its a little bit frustrating right now, because youre like, this is such an easy one to solve using the characteristic equation. Why are we doing Laplace Transforms? Well just want to show you that they can solve even these problems. But later on there are going to be classes of problems that, frankly, our traditional methods arent as good as the Laplace Transform. But anyway, how did we solve this? We just took the Laplace Transform of both sides of this equation. We got all of this hairy mess. We used the property of the derivative of functions, where you take the Laplace Transform, and we ended up, after doing a lot of algebra essentially, we got this. We got the Laplace Transform of y is equal to this thing. We just took the Laplace Transform of both sides and manipulated algebraically. o now our task in this video is to figure out what ys Laplace Transform is this thing? And essentially what were trying to do, is were trying to take the inverse Laplace Transform of both sides of this equation. o another way to say it, we could say that y if we take the inverse Laplace Transform of both sides we could say that y is equal to the inverse Laplace Transform of this thing. 2s plus 13, over s squared plus 5s plus 6. Now well eventually actually learn the formal definition of the inverse Laplace Transform. How do you go from the s domain to the t domain? Or how do you go from the frequency domain to the time domain? Were not going to worry about that right now. What were going to do is were going to get this into a form that we recognize, and say, oh, know those functions. Thats the Laplace Transform of whatever and whatever. And then well know what y is. o lets try to do that. o what were going to use is something that you probably havent used since Algebra two, which is think when its taught in, you know, eighth, ninth, or 10th grade, depending. And you finally see it now in differential equations that it actually has some use. Let me write it. Were going to use partial fraction expansion. And ll do a little primer on that, in case you dont remember it. o anyway, lets just factor the bottom part right here. And youll see where m going with this. o if factor the bottom, get s plus 2 times s plus 3. And what we want to do, is we want to rewrite this fraction as the sum of 2 guess you could call it partial fractions. think thats why its called partial fraction expansion. o we want to write this as a sum of A over s plus 2, plus B over s plus 3. And if we can do this, then and bells might already be ringing in your head we know that these things that look like this are the Laplace Transform of functions that weve already solved for. And ll do a little review on that in a second. But anyway, how do we figure out A and B? Well if we were to actually add A and B, if we were to lets do a little aside right here so if we said that A so if we were to give them a common denominator, which is this, s plus 2 times s plus 3. Then what would A become? Wed have to multiply A times s plus 3, right? o wed get As plus 3A. This, as ve written it right now, is the same thing as A over s plus 2. You could cancel out an s plus 3 in the top and the bottom. And now were going to add the B to it. o plus ll do that in a different color plus well, if we have this as the denominator, we could multiply the numerator and the denominator by s plus 2, right? To get B times s, plus 2B, and thats going to equal this thing. And all did is added these two fractions. Nothing fancier than there. That was Algebra two. Actually, think should do an actual video on that as well. But thats going to equal this thing. 2s plus 13, all of that over s plus 2 times s plus 3. Notice in all differential equations, the hairiest parts always the algebra. o now what we do is we match up. We say, well, lets add the s terms here. And we could say that the numerators have to equal each other, because the denominators are equal. o we have A plus Bs plus 3A plus 2B is equal to 2s plus B. o the coefficient on s, on the righthand side, is 2. The coefficient on the lefthand side is A plus B, so we know that A plus B is equal to 2. And then on the righthand side, we see 3A plus 2B must be equal to oh, this is a 13. Did say B? This is a 13. Thats a 13. t looks just like a B, right? That was 2s plus 13. Anyway, so on the righthand side get, it was 3A plus 2B is equal to 13. Now we have two equations with two unknowns, and what do we get? know this is very tiresome, but itll be satisfying in the end. Because youll actually solve something with the Laplace Transform. o lets multiply the top equation by 2, or lets just say minus 2. o we get minus 2A minus 2B equals minus 4. And then we get add the two equations you get A is equal to these cancel out A is equal to 9. Great. f A is equal to 9, what is B equal to? B is equal to 9 plus what is equal to 2? Or 2 minus 9 is minus 7. And we have done some serious simplification. Because now we can rewrite this whole expression as the Laplace Transform of y is equal to A over s plus 2, is equal to 9 over s plus 2, minus 7 over s plus 3. Or another way of writing it, we could write it as equal to 9 times 1 over s plus 2, minus 7 times 1 over s plus 3. Why did take the trouble to do this? Well hopefully, youll recognize this was actually the second Laplace Transform we figured out. What was that? ll write it down here just so you remember it. t was the Laplace Transform of e to the at, was equal to 1 over s minus a. That was the second Laplace Transform we figured out. o this is interesting. This is the Laplace Transform of what? o if we were to take the inverse Laplace Transform actually let me just stay consistent. o that means that this is the Laplace Transform of y, is equal to 9 times the Laplace Transform of what? f we just do pattern matching, if this is s minus a, then a is minus 2. o 9 times the Laplace Transform of e to the minus 2t. Does that make sense? Take this, put it in this one, which we figured out, and you get 1 over s plus 2. And let me clean this up a little bit, because m going to need that real estate. ll write this. ll leave that there, because well still use that. And then we have minus 7 times this is the Laplace Transform of what? This is the Laplace Transform of e to the minus 3t. This pattern matching, youre like, wow, if you saw this, you would go to your Laplace Transform table, if you didnt remember it, youd see this. Youre like, wow, that looks a lot like that. just have to figure out what a is. have s plus 3. have s minus a. o in this case, a is equal to minus 3. o if a is equal to minus 3, this is the Laplace Transform of e to the minus 3t. o now we can take the inverse Laplace actually, before we do that. We know that because the Laplace Transform is a linear operator and actually now can delete this down here we know that the Laplace Transform is a linear operator, so we can write this. And you normally wouldnt go through all of these steps. just really want to make you understand what were doing. o we could say that this is the same thing as the Laplace Transform of 9e to the minus 2t, minus 7e to the minus 3t. Now we have something interesting. The Laplace Transform of y is equal to the Laplace Transform of this. Well if thats the case, then y must be equal to 9e to the minus 2t, minus 7e to the minus 3t. And never proved to you, but the Laplace Transform is actually a 1:1 Transformation. That if a functions Laplace Transform, if take a function against the Laplace Transform, and then if were take the inverse Laplace Transform, the only function whose Laplace Transform that that is, is that original function. ts not like two different functions can have the same Laplace Transform. Anyway, a couple of things to think about here. Notice, we had that thing that kind of looked like a characteristic equation pop up here and there. And we still have to solve a system of two equations with two unknowns. Those are both things that we had to do when we solve an initial value problem, when we use just traditional, the characteristic equation. But here it happened all at once. And frankly it was a little bit hairier because we had to do all this partial fraction expansion. But its pretty neat. The Laplace Transform got us something useful. n the next video ll actually do a nonhomogeneous equation, and show you that the Laplace Transform applies equally well there. o its kind of a more consistent theory of solving differential equations, instead of kind of guessing solutions, and solving for coefficients and all of that. ee you in the next video.","o that means that this is the Laplace Transform of y, is equal to 9 times the Laplace Transform of what? The Laplace Transform of y is equal to the Laplace Transform of this. And then we have minus 7 times this is the Laplace Transform of what? This is the Laplace Transform of what? We got the Laplace Transform of y is equal to this thing. This is the Laplace Transform of e to the minus 3t. have s plus 3. have s minus a. o in this case, a is equal to minus 3. o if a is equal to minus 3, this is the Laplace Transform of e to the minus 3t. o another way to say it, we could say that y if we take the inverse Laplace Transform of both sides we could say that y is equal to the inverse Laplace Transform of this thing. o we could say that this is the same thing as the Laplace Transform of 9e to the minus 2t, minus 7e to the minus 3t. Because now we can rewrite this whole expression as the Laplace Transform of y is equal to A over s plus 2, is equal to 9 over s plus 2, minus 7 over s plus 3. f we just do pattern matching, if this is s minus a, then a is minus 2. o 9 times the Laplace Transform of e to the minus 2t. t was the Laplace Transform of e to the at, was equal to 1 over s minus a. That was the second Laplace Transform we figured out. The coefficient on the lefthand side is A plus B, so we know that A plus B is equal to 2. B is equal to 9 plus what is equal to 2? And then we get add the two equations you get A is equal to these cancel out A is equal to 9. We know that because the Laplace Transform is a linear operator and actually now can delete this down here we know that the Laplace Transform is a linear operator, so we can write this. o plus ll do that in a different color plus well, if we have this as the denominator, we could multiply the numerator and the denominator by s plus 2, right? Well if we were to actually add A and B, if we were to lets do a little aside right here so if we said that A so if we were to give them a common denominator, which is this, s plus 2 times s plus 3. And what we want to do, is we want to rewrite this fraction as the sum of 2 guess you could call it partial fractions. To get B times s, plus 2B, and thats going to equal this thing. And essentially what were trying to do, is were trying to take the inverse Laplace Transform of both sides of this equation. o we have A plus Bs plus 3A plus 2B is equal to 2s plus B. o the coefficient on s, on the righthand side, is 2. f A is equal to 9, what is B equal to?",0.2658227848101265
44,44,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: All right. Good morning, everyone. Lets get started. o were going to start 6.046 in earnest today. Were going to start with our first module on divide and conquer. Youve all seen divide and conquer algorithms before. erge sort is a classic divide and conquer algorithm. m going to spend just a couple minutes talking about the paradigm, give you a slightly more general setting than merge sort. And then well get into two really cool divide and conquer problems in the sense that these are problems for which divide and conquer works very well mainly, convex hall and median finding. o before get started on the material, let me remind you that you should be signing up for a recitation section on tellar. And please do that even if you dont plan on attending sections. Because we need that so we can assign your problem sets to be graded, OK? o thats our way of partitioning problem sets as well. And then the other thing is problem set one is going to go out today. And that its a one week problem set. All problem sets are going to be a week in duration. Please read these problem sets the day that they come out. pend 5, 10 minutes reading them. ome things are going to look like theyre magic, that theyre how could possibly prove this? f you think about it for a bit, itll become obvious. We promise you that. But get started early. Dont get started at 7:00 P when we have 11:59 P deadline on Thursday, all right? That four hours or five hours of time may not be enough to go from magical to obvious, OK? o lets get started with the paradigm associated with divide and conquer. ts just a beautiful notion that you can break up the problem into smaller parts and somehow compose the solutions to the smaller parts. And of course, the details are going to be whats important when we take a particular problem instance. But lets say were given a problem of size n. Were going to divide it into a sub problems ll put that in quotes so you know its a symbol a sub problems of size n over b. And here, a is an integer. And a is going to be greater than or equal to 1. t could be two. t could be three. t could be four. This is the generalization alluded to. And b does not have to be two or even an integer. But it has to be strictly greater than one. Otherwise, theres no notion of divide and conquer. Youre not breaking things up into smaller problems. o b should be strictly greater than one. o thats the general setting. And then youll solve each sub problem recursively. And the idea here is that once the sub problems become really small, they become constant size, its relatively easy to solve them. You can just do exhaustive search. f you have 10 elements and youre doing effectively a cubic search, well, 10 cubed is 1,000. Thats a constant. Youre in great shape as long as the constants are small enough. And so youre going to recurse until these problems get small. And then typically this is not true for all divide and conquer approaches. But for most of them, and certainly the ones were going to cover today, the smarts is going to be in the combination step when you combine these problems, the solutions of these sub problems, into the overall solution. And so thats the story. Typically, what happens in terms of efficiency is that you can write a recurrence thats associated with this divide and conquer algorithm. And you say t of n, which is a running time, for a problem of size n is going to be a times tfn over b and this is a recurrence plus the work that you need to do for the merge operational or the combine. This is the same as merge. And so you get a recurrence. And youre not quite done yet in terms of the analysis. Because once you have the recurrence, you do have to solve the recurrence. And its usually not that hard and certainly its not going to be particularly difficult for the divide and conquer examples that were going to look, at least today. But we also have this theorem thats called the master theorem that is essentially something where you can fairly mechanically plug in the as and the bs and whatever you have there maybe its theta n, maybe its theta n square and get the solution to the recurrence. m actually not going to do that today. But youll hear once again about the massive theorem tomorrow in section. And its a fairly straightforward template that you can use for most of the divide and conquer examples were going to look at in 046 with one exception that well look at in median finding today that will simply give you the solution to the recurrence, OK? o youve see most of these things before. Thats a little bit of setup. And so lets dive right in into convex hull, which is my favorite problem when it comes to using divide and conquer. o convex hull, got a little prop here which will save me from writing on the board and hopefully be more understandable. But the idea here is that in this case, we have a two dimensional problem with a bunch of points in a two dimensional plane. You can certainly do convex hull for three dimensions, many dimensions. And convexity is something that is a fundamental notion in optimization. And maybe well get to that in 6046 in advanced topics, maybe not. But in the context of todays lecture, what were interested in doing is essentially finding an envelope or a hull associated with a collection of points on a two dimensional plane. And this hull obviously is going to be something, as you can guess, that encloses all of these points, OK? o what have here, if make this string taut enough this is not working so well, but think you get the picture. All right, so thats not a convex hull. This is not a convex hull for the reason that have a bunch of points outside of the hull. All right, so let me just that is a convex hull. And now if start stretching like that or like this or like that, thats still a convex hull, OK? o thats the game. We have to find an algorithm. And we look at a couple of different ones that will find all of these segments that are associated with this convex hull, OK? o this is a segment thats part of the convex hull. Thats a segment thats part of the convex hull. f, in fact, had something like this and this was stretched out because have those two points outside the convex hull, this may still be a segment thats part of the electronics hall but this one is not, right? o thats the game here is to find these segments. o if youre going to working with segments or tangents theyre going to be used synonymously all of the tangents or segments associated with the entirety of the convex hull and we have to discover them. And only input that we have is the set of pointx xiy coordinates. And theres just a variety of algorithms that you can use to do this. The one that wish had time to explain but ll just mention is whats called a gift wrapping algorithm. You might not have done this, but guarantee you said you probably have taken a misshapen gift, right, and tried to wrap it in gift wrapping paper. And when youre doing that, youre essentially if youre doing it right youre essentially trying to find the convex hull of this three dimensional structure. Youre trying to tighten it up. Youre trying to find the minimum amount of gift wrapping paper. m not sure if youve ever thought about minimizing gift wrapping paper, but you should have. And thats the convex hull of this three dimensional shape. But well stick to two dimensions because well have to draw things on the board. o let me just spec this out a bit. ve been given endpoints in a plane. And those set of points are s, xi, yi such that i equals 1, 2 to n. And were just going to assume here, just to make things easy because we dont want to have segments that are null or segments that are a little bit different because theyre discontinuous. But were going to assume that no two have the same xcoordinate. This is just a matter of convenience. And no two have the same ycoordinate. And then finally, no three in a line. Because we want to be able to look at pairs of points and find these segments. And it just gets kind of inconvenient. You have to do special cases if there of them are on a line. And so the convex hull itself is the smallest polygon containing all points in s. And were going to call that ch of s convex hull of s. TDENT: mallest convex polygon. PROFEOR: The smallest convex polygon thank you. And so just as an example on the board, when you have something like this, youre going to have your convex hull being that. This one is inside of it. These two points are inside of it. And all the other ones form the hull. And so we might have p, q, r, s, t, u. And v and x are inside of the hull. Theyre not part of the specification of ch of s, which havent quite told you how were going to specify that. But the way youre going to specify that is simply by representing it as a sequence of points that are on the boundary on the hull in clockwise order. And you can think of this as being a doubly linked list in terms of the data structure that youd use if you coded this up. o in this case, it would be p to q to r to s. Youre going to start with t in this case. ts a doubly linked list. o you could conceivably start with anything. But thats the representation of the convex hull. And were going to use clockwise just because we want to be clear on as to what order were enumerating these points. ts going to become important when we do the divide and conquer algorithm. o lets say that we didnt care about divide and conquer just for the heck of it and gave you a bunch of points over here. an you think of a simple forget efficiency for just a couple of minutes. an you think of a simple algorithm that would generate the segments of the convex hull? For example, do not want to generate this segment vx. f think of a segment as being something that is defined by two points, then dont want to generate the segment vx because clearly the segment is not part of the convex hull. But whereas the segment pq, qr, rs, et cetera, theyre all part of the convex hull, right? o what is the obvious brute force algorithm, forgetting efficiency, that given this set of points will generate one by one the segments of the convex hull? Anybody? Did you have your head up? No? Go ahead. Yep. TDENT: Draw the line and check how many other lines intersect with it. PROFEOR: Draw the line and check how many lines it intersects with. TDENT: Yeah. PROFEOR: s there think you got you draw the line. Thats good, right? TDENT: ADENE: PROFEOR: Well but you want to do a little more. Yeah, go ahead. TDENT: For every pair of points you see, make a halfplane and see where they complete all of their other points. PROFEOR: Ah, so thats good. Thats good. Thats good. All right, so the first person who breaks the ice here always gets a Frisbee. orry man. At least only hit the lecturer no liability considerations here. OK, now m getting scared. Right, so think theres a certain amount of when throw this, am going to choke or not, right? But its going to get higher when one of you guys in the back answers a question. o youre exactly right. And you draw a line. And then you just look at it. And you look at the half plane. And if all the points are to one side, it is a segment of the convex hull. f theyre not, its not a segment beautiful. All right, are we done? an we go and enjoy the good weather outside? No, weve got ways to go here. o this is not the segment whereas one let me draw that. should draw these in a dotted way. This is not a segment. This is not a segment. This is a segment. And violated my rule of these three not being in a straight line. o ll move this over here. And then thats a segment and so on and so forth, OK? Right? TDENT: ts no longer a side with the ones below it. PROFEOR: m sorry? TDENT: t would have to go directly to the bottom one from the left one. PROFEOR: Oh, youre right. Thats a good point. Thats an excellent point. O what happened here was when moved that out exactly right. Thank you. This is good. o when moved this out here, what happened was and drew this well, this one here, my convex hull, changed. The problem specification changed on me. t was my fault. But then what would happen, of course, is as move this, that would become the segment that was part of the convex hull, OK? o sorry to confuse people. But what we have here in terms of an algorithm, if leave the points the same, works perfectly well. o let me just leave the points the same and just quickly recap, which is, m going to take a pair of points. And m going to draw and let me just draw this in a dotted fashion first. And m going to say thats the segment. And m going to take a look at that line and say this breaks up the plane into two half planes. Are all about points on one side? And if the answer is yes, m going to go ahead and, boom, say that is a segment of my convex hull. f the answers is no, like in this case, m going to drop that segment, OK? o now lets talk about complexity. Lets say that there are n points here. And how many segments do have? have O n square theta n square segments. And what is the complexity of the test? What is the complexity of the test thats associated with, once ve drawn the segments, deciding whether the segment is going to be a tangent which is part of the convex hull or not? What is the complexity? TDENT: O n. PROFEOR: O n exactly right. o on test complexity and so we got over theta n cubed complexity, OK? o it makes sense to do divide and conquer if you can do better than this. Because this is a really simple algorithm. The good news is we will be able to do better than that. And now that we have a particular algorithm m not quite ready to show you that yet. Now that we have a particular algorithm, we can think about how we can improve things. And of course were going to use divide and conquer. o lets go ahead and do that. And so generally, the divide and conquer, as mentioned before, in most cases, the division is pretty straightforward. And thats the case here as well. All the fun is going to be in the merge step. Right, so what were going to do, as you can imagine, is were going to take these points. And were going to break them up. And the way were going to break them up is by dividing them into half lengths. Were going to just draw a line. And were going to say everything to the left of the line is one sub problem, everything to the right of the line is another sub problem, go off and find the convex hull for each of the sub problems. f you have two points, youre done, obviously. ts trivial. And at some point, you can say m just going to deal with brute force. f we can go down to order n cubed, if n is small, can just apply that algorithm. o it doesnt even have to be the base case of n equals 1 or n equals 2. Thats a perfectly fine thing to do. But you could certainly go with n equals 10, as mentioned before, and run this brute force algorithm. And so at that point, you know that you can get down to small enough size sub problems for which you can find the convex hull efficiently. And then youve got these two convex hulls which are clearly on two different half planes because thats the way you defined them. And now youve got to merge them. And thats where all the fun is, OK? o lets just write this out again. Youre going to sort the points by xcoordinates. And were going to do this once and for all. We dont have to keep sorting here because were just going to be partitioning based on xcoordinates. And we can keep splitting based on xcoordinates because we want to generate these halflengths, right? o if we can do those once and for all and for the input set , were going to divide into the left half A and right half B by the xcoordinates. And then were going to compute H of A and H of B recursively. And then were going to combine. o the only difference here from what we had before is the specification of the division. t looked pretty generic. ts similar to the paradigm that wrote before. But ve specified exactly how m going to break this up. o lets start with the merge operation. Were going to spend most of our time specing that. And again, theres many ways you could do the merge. And we want the most efficient way. Thats obviously going to determine complexity. o, big question how to merge. o what have here, if look at the merge step, is ve created my two sub problems corresponding to these two half planes. And what have here is lets say ve generated, at this point, a convex hull associated with each of these sub problems. o what have here is a1, a2. m going to go clockwise to specify the convex hull. And the other thing that m going to do is in the sub problem case, my starting point is going to be for the left sub problem, the coordinate that has the highest x value, OK? o thats a1 in this case the highest x value going over. x is increasing to the right. And for the right half of the problem, its going to be the coordinate that has the lowest x value. And m going to go clockwise in both of these cases. o when you see an ordering associated with the subscripts for these points, start with a1 or b1 and then go clockwise. And thats how we number this so just notational, nothing profound here. o got these two convex hulls these sub hulls, if you will. And what need to do now is merge them together. And you can obviously look at this and its kind of obvious what the overall convex hull is, right? But the key thing is, m going to have to look at each of the pairs of points that are associated with this and that and try to generate the tangents, the new tangents, that are not part of the sub hulls, but theyre part of the overall hull, right? o in this case, you can imagine an algorithm that is going to kind of do what this brute force algorithm does except that its looking at a point from here and a point from here. o you could imagine that m going to do a pairwise generation of segments. And then m going to check to see whether these segments are actually tangents that are part of the overall convex hull or not. o what would do here is d look at this. And is that going to be part of the overall hull? No, and precisely why not? omeone tell me why this segment a1 b1 is not part of the overall hull? Yeah, go ahead. TDENT: f we were to draw a line through the whole thing there would be one on both sides. PROFEOR: Exactly right thats exactly right. o here you go. o thats not part of it. Now, if look at this well, same reason thats not part of it. n this case and this is a fairly obvious example. m going to do something thats slightly less obvious in case you get your hopes up that we have this trivial algorithm, OK? This is looking good, right? Thats supposed to be a straight line, by the way. o a4 b2 mean, thats looking good, right? Because all the points are on one side. o a4 b2 is our upper tangent. Right, so our upper tangent is something that were going to define as if look at each of these things, m going to say they have a yij. OK, what is yij? yij is the ycoordinate. of the segment that m looking at, the ij segment. o this yij is for ai and bj. o what have here is y42 out here. And this is for the upper tangent, yij is going to be maximum, right? Because thats essentially something which would ensure me that there are no points higher than that, right? o if go up all the way and find this that has the maximum yij, that is going to be my upper tangent. Because only for that will have no points ahead of that, OK? o yij is upper tangent. This is going to be maximum. And m not going to write this down, but it makes sense that the lower tangent is going to have the lowest yij. Are we all good here? Yeah, question. TDENT: o am just wondering, couldnt hear what she said why we moved out a1 b1. PROFEOR: OK, so good. Let me that reason we moved out a1 b1 is because if just drew a1 b1 like this and m extrapolating this. This is again supposed to be a straight line. Then you clearly see that there are points on either side of the a1 b1 segment when you look at the overall problem, correct? You see that on a1 b1, b2 is on this side, b3 is on this side if just extend this line all the way to infinity in both directions. And that violates the requirement that the segment be part of the overall hull, OK? That make sense? Good. o, everybody with me? o clearly, theres a trivial merge algorithm here. And the trivial merge algorithm is to look at not every pair of points every ab pair, right? Every aibj pair. And so what is the complexity of doing that? f have n total points, the complexity would be would be in square, right? Because maybe d have half here and half here, ignore constants. And you could say, well, its going to be n squared divided by 4, but thats theta n squared. o theres an obvious merge algorithm that is theta n square looking at all pairs of points. And when mean all pairs of points, mean like an a and a b. Because want to pick a pair when go left of that dividing line and then right of the dividing line. But either way, its theta n square, OK? o now you look at that and you go, huh. an do a better? What if just went for the highest a point and the highest b point and just, no, thats it? m done constant time. Wouldnt that be wonderful? Yeah, wonderful, but incorrect, OK? Right, so what is an example. And so this is something that spent a little bit of time last night concocting. o m like you guys too. do my problem set the night before. Well, dont do as do. Do as say. But ve done this before. o thats the difference. But this particular example is new. o what have here is m going to show you why theres not a trivial algorithm, OK, that got to get these angles right that you cant just pick the highest points and keep going, right? And then that would be constant time. o thats my a over here. And lets assume that have my dividing line like that. And then what m going to do here and hope get this right is m going to have something like this, like that. And then m going to have b1 here clockwise so b2, b3, and b4. o as you can see here, if look at a4 a little adjustment necessary. OK, so if look at that, a4 to b1 versus mean, just eyeball it. A3 to b1 right, is a4 to b1 going to be the upper tangent? No, right? o now a3 is lower than a4. You guys see that? And b1 is lower than b2, right? o its clear that if just took a4 to b2 that it will not be an upper tangent. Everybody see that? Yep, all right, good. o we cant have a constant time algorithm. We have theta and square in the back. o it is there something maybe theta n? How would we do this merge and find the upper tangent by being a little smarter about searching for pairs of points that give us this maximum yij? mean, the goal here is simple. At some level, if you looked at the brute force, would generate each of these things. would find the yj intercepts associated with this line. And just pick the maximum. And the constant time algorithm doesnt work. The theta n squared algorithm definitely works. But we dont like it. o there has to be something in between. o, any ideas? Yeah, back there. TDENT: o... had a question. PROFEOR: No, youre just finding no, youre maximizing the yij. o for once you have this segment so the question was, isnt the obvious merge algorithm theta n cubed, right? And my answer is no, because the theta n extra factor came from the fact that you had to check every point, every endpoint, to see on which side of the plane it was. Whereas here, what m doing is ve got this one line here that is basically y equals 0, if you like, or y equals some m sorry, x equals 0 or x equals some value. And just need to, once have the equation for the line associated with a4 b1 or a4 b2, just have to find the intercept of it, which is constant time, right? And then once find the intercept of it, just maximize that intercept to get my yij. o m good, OK? o its only theta n squared, right? Good question. o this is actually quite very, very, very clever. This particular algorithm is called the two finger algorithm. And do have multiple fingers, but its going to work a lot better if borrow Erics finger. And were going to demonstrate to you the two finger algorithm for merging these two convex hulls. And then well talk about the complexity of it. And my innovation again last night was to turn this from a twofinger algorithm. Not only did have the bright idea of using Eric decided it was going to become the two finger an string algorithm. o this is wild. This is my contribution to 046 lore come on. o the way the two finger algorithm works this pseudo code should be incomprehensible. f you just look at it and you go, what, right? But this demo is going to clear everything up. Right so heres what you do. o now were going to do a demo of the merge algorithm that is a clever merge algorithm than the one that uses order n square time. And its correct. ts going to get you the correct upper tangent and what we are starting at here is with Eriks left finger on A1, which is defined to be the point thats closest to the vertical line that you see here, the one that has the highest xcoordinate. And my finger is on B1, which is the point that has the smallest Xcoordinate on the right hand side subhull. And what we do is we compute, for the segment A1 B1, we compute by Yij, in this case Y11, which is the intercept on the vertical line that you see here that Erik just marked with a red dot. And you can look at the pseudocode over on, to my right if face the board. And what happens now is m going to move clockwise, and m going to go from B1 to B4. And what happened here? Did the Yij increase or decrease? Well, as you can see it decreased. And so m going to go back to B1. And were not quite done with this step here. Eriks going to go counterclockwise over to A4. And were going to check again, yeah, keep the string taught, check again whether Yij increased or decreased and as is clear from here Yij increased. o now we move to this point. And as of this moment we think that A4 B1 has the highest Yij. But we have a while loop. Were going to have to continue with this while loop, and now what happens is, m going to go from B1 clockwise again to B4. And when this happens, did Yij increase or decrease? Well it decreased. o m going to go back to B1 and Erik now is going to go counterclockwise to A3. And as you can see Y31 increased a little bit, so were going to now stop this iteration of the algorithm and were at A3 B1, which we think at this point is our upper tangent, but lets check that. tart over again on my side B1 to B4, what happened? Well Yij decreased. o m going to go back to B1. And then Eriks going to try. Hes going conterclockwise, hes going to go A3 to A2 and, well, big decrease in Yij. Now Erik goes back to A3. At this point weve tried both moves, my clockwise move and Eriks counterclockwise move. y move from B1 to B4 and Eriks move from A3 to A2. o weve converged, were out of the while loop, A3 B1 for this example is our upper tangent. All right. You can have your finger back Erik. o the reason this works is because we have a convex hull here and a convex hull here. We are starting with the points that are closest to each other in terms of A1 being the closest to this vertical line, B1 being the closest to this vertical line, and we are moving upward in both directions because went clockwise and Erik went counterclockwise. And thats the intuition of why this algorithm works. Were not going to do a formal proof of this algorithm, but the monotonicity property corresponding to the convexity of this subhull and the convexity of the subhull essentially can give you a formal proof of correctness of this algorithm, but as said we wont cover that in 046. o all that remains now is to look at our pseudocode which matches the execution that you just saw and talk about the complexity of the pseudocode. o what is the complexity of this algorithm? ts order n, right? o what has happening here, if you look at this while loop, is that while have two counters, m essentially looking at two operations per loop. And either one of those counters is guaranteed to increment through the loop. And so since have in this case p points, in one case p plus q equals n so lets say had p points here and have q points here. And got p plus q equals n. And got a theta n merge simply because m going to be running through and incrementing as long as m in the loop, m going to be incrementing either the i or the j. And the maximum they can go to are p and q before bounce out of the loop or before they rotate around. And so thats why this is theta n. And so you put it all together in terms of what the merge corresponds to in terms of complexity and put that together with the overall divide and conquer. We have a case where this is looking like a recurrence that youve seen many a time t of n. ve broken it up into two sub problems. o have 2. And could certainly choose this l over here thats my line l to be such that have a good partition between the two sets of points. Now, if choose l to be all the way on the right hand side, then have this large sub problem makes no sense whatsoever. o what can do theres nothing thats stopping me when ve sorted these points by the xcoordinates to do the division such that theres exactly the same number, assuming an even number of points n, exactly the same number on the left hand side or the right hand side. But can get that right roughly certainly within one very easily. o thats where the n over 2 comes from, OK? n the next problem that well look at, the median finding problem, well find that trying to get the sub problems to be of roughly equal size is actually a little difficult, OK? But want to point out that in this particular case, its easy to get sub problems that are half the size because youve done the sorting. And then you just choose the line, the vertical line such that youve got a bunch of points that are on either side. And then in terms of the merge operation, we have 2t n over 2 plus theta n. People recognize this recurrence? ts the old merge sort recurrence. o we did all of this in well, its not merge sort. learly the algorithm is not merge sort. We got the same recurrence. And so this is theta n log n so a lot better than theta nq. And theres no convex hull algorithm thats in the general case better than this. Even the gift wrapping algorithm that mentioned to you, with the right data structures, it gets down to that in terms of theta n log n, but no better. OK, so good. Thats pretty much what had here. Again, like said, happy to answer questions about the correctness of this loop algorithm for merge later. Any other questions associated with this? TDENT: Question. Yeah, back there. TDENT: f the input is recorded by x coordinates, can you do better than ? PROFEOR: No, you cant, because mean, the n log n for the presorting, mean, theres another theta n log n for the sorting at the top level. And we didnt actually use that, right? o the question was, can we do better if the input was pre sorted? And actually did not even use the complexity of the sort. We just matched it in this case. o theta n log n and then you can imagine maybe that you could do a theta n sort if these points were small enough and you rounded them up and you could use a bucket sort or a counting sort and lower that. o this theta n log n is kind of fundamental to the divide and conquer algorithm. The only way you can improve that is by making a merge process thats even faster. And we obviously tried to cook up a theta one merge process. But that didnt work out, OK? TDENT: But are there algorithms that ? PROFEOR: First if you assume certain things about the input, youre absolutely, right? o one thing youll discover in algorithms in 6046 as well is that were never satisfied. OK, so just said, oh, you cant do better than theta n log n. But thats in the general case. And think mentioned that. Youre on the right track. f the input is pre sorted, you can take that away no, it doesnt help in that particular instance if you have general settings. But if you the two dimensional case if the hull, all the segments have a certain characteristic not quite planar, but something thats a little more stringent than that you could imagine that you can do improvements. dont know if any compelling special case input for convex hull from which you can do better than theta n log n. But thats a fine exercise for you, which is in what cases, given some structure on the points, can do better than theta n log n? o thats something that keeps coming up in the algorithm literature, if you can use that, OK? Yeah, back there question. TDENT: Wheres your step? You also have to figure out which lines to remove from each of your two... PROFEOR: Ah, good point. And youre exactly, absolutely right. And just realized that skipped that step, right? Thank you so much. o the question was, how do remove the lines? And its actually fairly straightforward. Lets keep this up here. And we dont need this incomprehensible pseudo code, right? o lets erase that. And thank you for asking that question. o its a little simple cut and paste approach where lets say that find the upper tangent ai bj. And find the lower tangent. Lets call it ak bm. And in this particular instance, what do have? have a1, a2, a3, a4 as being one of my sub hulls. And then have b1, b2, b3, b4 as the other one. Now, what did we determine to be the upper tangent? Was it a3 b1? Right, a3 b1? o a3 b1 was my upper tangent. And guess it was a1 a1 b4? A1 b4 was my lower tangent. o the big question is, now that ve found these two, how do generate the collect representation of the overall convex hull? And so it turns out that you have to do this and then the complexity of this is important as well. And you need to do whats called a cut and paste thats associated with this where were going to just look at this and that. o if were going to have these two things, then weve got to generate a list of points. Now, clearly a4 is not going to be part of that, right? A4 is not going to be part of the overall hull. What is it that we want? We want something like a1, a2, a3, b1, b2, b3, b4, right? But theres a point that we have to discard here. Agree? And so the way we do this is very mechanical. Thats the good news here. mean, you dont have to look at it pictorially. just made that up looking at eyeballing it. learly, a computer doesnt have eyeballs, right? And so what were going to do is were going to say the first link in general, the first link is ai to bj. Because thats my upper tangent, OK? And in this case, its going to be a3 d1, OK? And then m going to go down the b list until you see bm, which is the lower tangent. Youre on the b list. o youre looking for the lower tangent point. And then youre going to jump until you see bm. You link it to ak, OK? You link it to ak and continue until you return to ai. And then you have your circular list, OK? o what you see here is you have a3 here. o m going to go ahead and write out the execution of what just wrote here. o have a3. And m going to go jump over to b1. o m going to write down b1. Then m going to along the bs until get to b4. n this case, m going to include all of the bs. o got b1, b2, b3, b4. And then m going to jump from b4 to a1 because thats part of my lower tangent. And got a1 here, a2. And then m back to a3, which is great. Because then m done, OK? And so exactly what said happened, thank goodness, which is we dropped a4 but we kept all the other points. Does that answer your question? Good. What is the complexity of cut and paste? ts order n. m just walking through these lists. o theres no hidden complexity here, OK? Good, good thank you. You definitely deserve a Frisbee. n fact, you deserve two, right? Where are you? oh, could you stand up? Yeah, right two colors. All right. Oh, so he well, you can give it to him if you like. o good, thank you. o are we done? Are we done with convex hull? OK, good. o lets go on and do median finding. Very different very different set of issues here. till on divide and conquer, but a very different set of issues. The specification here is, of course, straightforward. You can think of it as just want a better algorithm than sorting and looking for the median at the particular position in over two position, for example. Lets say n is odd. And its floor of n over 2. You can find that median. Right, so its pretty easy if you can do sorting. But were never satisfied with using a standard algorithm. f we think that we can do better than that. o the whole game here is going to be m going to find the median. And want to do it in better than theta n log n time. OK, so thats what median finding is all about. Youre going to use divide and conquer for this. And so in general, were going to define, given a set of n numbers, define rank of x as the numbers in the set that are greater than m sorry, less than or equal to x. mean, you could have defined it differently. Were going to go with less than or equal to. o in general, the rank, of course, is something that could be used very easily to find the median. o if you want to find the element of rank n plus 1 divided by 2 floor, thats what we call the lower median. And n plus 1 divided by 2 ceiling is the upper median. And they may be the same if n is odd. But thats what we want. o you can think of it as its not median finding, but finding elements with a certain rank. And we want to do this in linear time, OK? o were going to apply divide and conquer here. And as always, the template can be instantiated. And the devil is in the details of either division or merge. And we had most of our fun with convex hull on the merge operation. t turns out most of the fun here with respect to median finding is in the divide, OK? o what want is the definition of a select routine that takes a set of numbers s. And this is the rank. o want a rank i. And that i might be n over 2 well, floor of n plus 1 over 2, whatever? And so what does the divide and conquer look like? Well, the first thing you need to do is divide. And as of now, were just going to say youre going to pick some element x belonging to s. And this choice is going to be crucial. But at this point, m not ready to specify this choice yet, OK? o were going to have to do this cleverly. And then what were going to do is were going to compute on k, which is the rank of x, and generate two sub arrays such that want to find the fifth highest element. want to find the median element. want to find the 10th highest element. o have to keep track of what happens in the sub problems. Because the sub problems are going to determine, depending on how many elements are inside those sub problems, which can only determine after ve solved those sub problems. m going to have to collect that information and put it together in the merge operation. o if want to find the 10th highest element and ve broken it up relatively arbitrarily, its quite possible that the 10th highest element is going to be discovered in the left one or the right one. And have to show that its the 10th highest. And it might be that theres four elements in the left and five on the right that are lets see. f defined the rank as less than or equal to x, theres four on the left and five on the right that are smaller. And thats why this is the 10th highest element. And thats essentially what we have to look at. o b and c are going to correspond to the sub arrays that you can clearly eliminate one of them. You can count the number of elements in b, count the number of elements in c. And you can eliminate one of them in this recursion as youre discovering this element with the correct rank in this case, i. o let me write the rest of this out and make sure were all on the same page. What have here pictorially is ve generated b here and c. o this is all of b and thats all of c. have k minus 1 elements here in b. And lets say have n minus k elements in c. And m going to do essentially take once ve selected a particular element, m going to look at all of the elements that are less than it and put it into the array b. m going to look at all the elements that are better than it. Lets assume all elements are unique. m going to put all of them into c. And m going to recur on b and c. Those two are my sub problems. But what have to do is once recur and discover the ranks of the sub problems, have to put them together. o what have here is if k equals i so computed the rank and realized that if k equals equals i, should say if k equals i, then m going to just return x. m done at this point. got lucky. picked an element x and it magically ended up having the correct rank, OK? Not always going to happen. And so in other case, if k is greater than i, then going to return select bi. o what ve done here is if k is greater than i, then m going to say, oh, so now m going to have to find the element in b. know that its going to be in b because k is greater than i. And ve got to find the exact position depending on what i is over here. But its going to be somewhere between 1 and k minus 1. And then the last case is if k is less than i, then this is a little more tricky. m going to turn on c of i minus k, OK? o what happens here is that my k is the rank for the x that looked at over here is less than i. o know that m going to find this element that m looking for in c. But if just look at c, dont want to look at c and look for an element of rank i within c, right? That doesnt make sense because m looking for an element of rank i in the overall array that was given to me. o have to subtract out the k elements that correspond to x and all of the k minus 1 elements that are in b to go figure out exactly what position or rank m looking for in the sub array corresponding to c, OK? o, people buy that. o thats just a small, little thing that you have to keep in mind as you do this. o thats pretty straightforward, looking pretty good. And you say, well, am done here? And as you can imagine, the answer is no, because we havent specified this value. Now, can someone tell me, at least from an efficiency standpoint, what might happen, what were looking for here? As you can imagine, we want to improve on theta n log n. And so you could you say, well, m happy with theta n. That theta n complexity algorithm is better than a theta n log n complexity algorithm, which is kind of in the bag. Because we know how to sort and we know how to index. o we want a theta n algorithm. Now, if you take this and if just picked, lets say, the biggest element kept picking x to be n or n minus 1 or just picked a constant value. picked x to be in the middle. picked the index. can always pick an element based on its index. can always go for the middle one. o what is the worst case complexity of this algorithm? f dont specify or give you this arbitrary selection corresponding to x belonging to s, what is the worst case complexity of this algorithm? Yeah, go ahead. TDENT: N squared. PROFEOR: N squared why is that? TDENT: Because if you take like the least element. PROFEOR: Yep. TDENT: How do you compare like N o against the other analysis? PROFEOR: Exactly right. Thats exactly right. o what happens is that youre doing a bunch of work here with this theta n work. Right here, this is theta n work, OK? o given that youre doing theta n work here, you have to be really careful as to how you pick the x element. o what might happen is that you end up picking the x over here. And given the particular rank youre looking for, you have to now youre left with a large array that has n minus 1 elements in the worst case. You started with n. You did not go to n over 2 and n over 2, which is what divide and conquer is all about even n over b, OK? You went to n minus 1. And then you go to n minus 2. And you go to n minus 3 because youre constantly picking this is worst case analysis. Youre constantly picking these sub arrays to be extremely unbalanced. o when the sub arrays are extremely unbalanced, you end up doing theta n work in each level of the recursion. And those theta ns, because youre going down all the way from n to one, are going to be theta n square when you keep doing that, OK? o thanks for that analysis. And so this is theta n squared if you have a batch selection. o we wont talk about randomized algorithms, but the problem with randomized algorithms is that the analysis will be given a probability distribution. And itll be expected time. What we want here is a deterministic algorithm that is guaranteed to run in worst case theta n. o we want a deterministic way of picking x belonging to s such that all of this works out and when we get our recurrence and we solve it, somehow magically were getting fully balanced partitions firmly balanced sub problems in the sense that its not n minus 1 and 1. ts something like it could even be n over 10 and 9n over 10. But as long as you guarantee that, youre shaking things down geometrically. And the asymptotics is going to work out. but the determinism is what we need. And so were going to pick x cleverly. And we dont want the rank x to be extreme. o this is not the only way you could do it, but this is really very clever. Theres a deterministic way. And youre going to see some arbitrary constants here. And well talk about them once ve described it. But what were going to do is were going to arrange s into columns of size 5, right? Were going to take this single array. And were going to make it a two dimensional array where the number of rows is five and the number of columns that you have is n over 5 the ceiling in this case. And then were going to sort it each column, big elements on top. And were going to do this in linear time. And you might say, how did that happen? Well, theres only five elements. o its linear. You could do whatever you wanted. You could do n raised to four. But its five raised to four and its constants. Dont you love theory? o then were going to find what were going to call the median of medians. o m going to explain this. This works for arbitrary rank, but its a little easier to focus in on the median to just explain the particular example. Because as you can see, theres an intricacy here associated with the break up. And so here we go. m going to draw out a picture. And were going to try and argue that this deterministic strategy that ll specify gives you fairly balanced partitions in all cases, OK? o what we see here is we see pictorially, you see columns of length five. Each of these dots corresponds to a number. This one dimensional array got turned into a two dimensional right. o got four full columns. And its suddenly possible, given n, that my fifth column is not full, right? o thats certainly possible. o thats why have that up here. t so what ve here is m going to lay them out this way. And m going to look at that. m going to look at the middle elements of each of these n over five columns. Thats exactly what m going to look at. Now, if look at what want, what want over here is this x. f want to find m going to find the median of medians. o is x. Now, it is true the first that these columns m just putting that up here imagining that thats x. Thats not guaranteed to be x because the columns themselves arent well, these columns are sorted. And what m going to have to guarantee, of course, is that when go find this median of medians is that it ends up being something that gives me balanced partitions. o maybe say a little bit more before explain whats going on. Each of these columns is sorted. And s is arranged into columns of size 5 like just said here. These are the medians, OK? f look at determining the medians and say that once ve determined this x, which ve discovered that its the median, then this is right there in the middle. Theres going to be a bunch of columns to the left of it, a bunch of elements to the left of it, and a bunch of elements to the right of it. And in this case, have five columns. could have had more. t happens to be the third one. o the idea is that once find this median of medians, which corresponds to this x number, can say that all of the columns these all correspond to columns that have their median element greater than x. These correspond to columns that have their median element less than x, OK? o what have here in this picture is that these elements here are going to be greater than x. And these elements here are going to be less than x. o let me clear. Whats happened here is weve not only sorted all of the columns such that you have large elements up here. Each of these five columns have been sorted that way. On top of that, ve discovered the particular column that corresponds to the medians of medians. And this is my x over here. And it may be the case that these columns arent sorted. This one may be larger than that or vice versa same thing over there. have no idea. But its guaranteed that once find this median that do know all of the columns that have elements in this position that are less than this x. And know columns that in this position have elements that are greater than x, OK? Yep. TDENT: houldnt the two elements below x also be computed less than x. PROFEOR: Youre exactly right. would have probably been able to get the same asymptotic complexity if dropped those because had a constant number. But youre absolutely exactly right. o the point that the question was just redrew it. These two are clearly less than x as well because theyre part of the sorting. And thats essentially have here. Now, my goal here and you can kind of see from here as to where were headed. What ve down here by this process of sorting each column and finding the median of medians is that found this median of medians such that theres a bunch of columns on the left. And roughly half of those elements in those columns are less than x. And there are a bunch of columns on the right. And roughly half of those columns have elements that are greater than x. o what now have to do is to do a little bit of math to show you exactly what the recurrence is. And let me do that over here. o thats the last thing that we have to do. probably wont solve the recurrence, but that can wait until tomorrow. The recurrence will be something thats not particularly difficult to solve. o want to now make a more quantitative argument that the variable being n as to how many elements are guaranteed to be greater than x. And essentially what m saying, which is m writing out what have on that picture there, half of the n over 5 groups contribute at least three elements greater than x except for one group with possibly less than five elements, which is the one that have all the way to the right, and one group that contains x. o for all the other columns, m going to get three elements that are greater than x. And so if you write that out, this says there are at least three n over 10, because have half of all of those groups, minus 2. And m not counting perfectly accurately here, but have an at least. o this should all be fine. 3n over 1d 3 times n over 10 minus 2 elements are strictly greater than x. And that comes from that picture. m going to be able to say the same thing for less than x as well. cant count the one. Depending on how things go, maybe could have played around and subtracted 1 instead of a 2 in the latter case. But m just being conservative here. t is clear that m going to have a bunch of columns that are full columns, that are going to be contributing three elements that are greater than x. And in this case, have, well, two of them here for the less than x. And got one for the greater than x. o thats all that m seeing over here with respect to the balance of the partitions. And it turns out thats enough. t turns out all have to do with this observation is to go off and run the recurrence. And were going to get an efficient algorithm. Yep. TDENT: hould it not be like greater than or equal to, because theres... PROFEOR: No, theres nothing thats equal. TDENT: o you are saying, thats all you need. PROFEOR: Yeah. Yeah, assume that so, convenience, yeah. Theres always a little bit of convenience thrown in here. We will assume that the a has unique elements. o theres nothing thats x, OK? Good. o the recurrence, once you do that, is t of n equals were going to just say its order one for n less than or equal to 140. Where did that come from? Well, like 140. ts just a large number. t came from the fact that youre going to see 10 minus 3, which is 7. And then you want to multiply that by 2. o some reasonably large number were going to go off and were going to assume thats a constant. o you could sort those 140 numbers and find the median or whatever rank. ts all constant time once you get down to the base case. o you just want it to be large enough such that you could break it up and you have something interesting going on with respect to the number of columns. o dont worry much about that number. The key thing here is the recurrence, all right? And this is what we have spent the rest of our time on. And ll just write this out and explain where these numbers came from. o thats our recurrence for n less than or equal to 140. And else, youre going to do this. o what is going on here? What are all of these components corresponding to this recurrence? Really quickly, this is simply something that says m finding the median of medians. m finding some element that has a certain rank. o this median of medians is going to be running on n over 5 columns. o ve got this there are n over 5 columns here. And m going to be calling this algorithm recursively, the median finding algorithm, to do that finding the median of medians. This thing over here is m going to be discarding at least regardless of what do. Because have these two statements here, take the overall n. And m going to discard. n my paradigm over here, m either going to go with b or m either going to go with c depending on what m looking for. And given that b and c are not completely unbalanced, m going to be discarding 3n over 10 minus 6 elements, which simply corresponds to me ignoring the ceiling here and multiplying the 3 out. o thats 3n over 10 minus 6. o then have 7n over 10 plus 6. Thats the maximum size partition that m going to recur on. ts only going to be exactly one of them, as you can see from that. ts either else. ts not recurring on both of them. ts recurring on one of them. o thats where the 7n over 10 plus 6 comes from. And then you ask where does this theta n come from. Well, the theta n comes from the fact that do have to do some sorting. ts constant time sorting for every column, OK? Because its only five elements. o m going to do constant time sorting. But theres order n columns. Because its then its n over 5 columns. o this is the sorting of all of the columns, all right? o thats it. And ll just leave you with you cannot apply the master theorem for solving this particular recurrence. But if you make the observation and youll see this in section. You make the observation that n over 5 plus 7n over 10 is actually less than n. o you get 0.2n here and 0.7n there. Thats actually less than n. This thing runs in linear time. And youll see that in section tomorrow. o this whole thing is theta n time. ee you next time.","What have here pictorially is ve generated b here and c. o this is all of b and thats all of c. have k minus 1 elements here in b. And lets say have n minus k elements in c. And m going to do essentially take once ve selected a particular element, m going to look at all of the elements that are less than it and put it into the array b. m going to look at all the elements that are better than it. t is clear that m going to have a bunch of columns that are full columns, that are going to be contributing three elements that are greater than x. And in this case, have, well, two of them here for the less than x. And got one for the greater than x. o thats all that m seeing over here with respect to the balance of the partitions. ts going to get you the correct upper tangent and what we are starting at here is with Eriks left finger on A1, which is defined to be the point thats closest to the vertical line that you see here, the one that has the highest xcoordinate. o what ve done here is if k is greater than i, then m going to say, oh, so now m going to have to find the element in b. know that its going to be in b because k is greater than i. And ve got to find the exact position depending on what i is over here. And so it turns out that you have to do this and then the complexity of this is important as well. But the key thing is, m going to have to look at each of the pairs of points that are associated with this and that and try to generate the tangents, the new tangents, that are not part of the sub hulls, but theyre part of the overall hull, right? o want to now make a more quantitative argument that the variable being n as to how many elements are guaranteed to be greater than x. And essentially what m saying, which is m writing out what have on that picture there, half of the n over 5 groups contribute at least three elements greater than x except for one group with possibly less than five elements, which is the one that have all the way to the right, and one group that contains x. o for all the other columns, m going to get three elements that are greater than x. And so if you write that out, this says there are at least three n over 10, because have half of all of those groups, minus 2. And then what m going to do here and hope get this right is m going to have something like this, like that. o what happens here is that my k is the rank for the x that looked at over here is less than i. o know that m going to find this element that m looking for in c. But if just look at c, dont want to look at c and look for an element of rank i within c, right? o what have here is m going to show you why theres not a trivial algorithm, OK, that got to get these angles right that you cant just pick the highest points and keep going, right? And you say t of n, which is a running time, for a problem of size n is going to be a times tfn over b and this is a recurrence plus the work that you need to do for the merge operational or the combine. And were going to make it a two dimensional array where the number of rows is five and the number of columns that you have is n over 5 the ceiling in this case. And then what were going to do is were going to compute on k, which is the rank of x, and generate two sub arrays such that want to find the fifth highest element. And is that going to be part of the overall hull? o if go up all the way and find this that has the maximum yij, that is going to be my upper tangent. And the other thing that m going to do is in the sub problem case, my starting point is going to be for the left sub problem, the coordinate that has the highest x value, OK? And if all the points are to one side, it is a segment of the convex hull. And roughly half of those columns have elements that are greater than x. o what now have to do is to do a little bit of math to show you exactly what the recurrence is. Were not going to do a formal proof of this algorithm, but the monotonicity property corresponding to the convexity of this subhull and the convexity of the subhull essentially can give you a formal proof of correctness of this algorithm, but as said we wont cover that in 046. o all that remains now is to look at our pseudocode which matches the execution that you just saw and talk about the complexity of the pseudocode. And were going to say everything to the left of the line is one sub problem, everything to the right of the line is another sub problem, go off and find the convex hull for each of the sub problems. And its a fairly straightforward template that you can use for most of the divide and conquer examples were going to look at in 046 with one exception that well look at in median finding today that will simply give you the solution to the recurrence, OK? What is the complexity of the test thats associated with, once ve drawn the segments, deciding whether the segment is going to be a tangent which is part of the convex hull or not? And this is for the upper tangent, yij is going to be maximum, right? What we want here is a deterministic algorithm that is guaranteed to run in worst case theta n. o we want a deterministic way of picking x belonging to s such that all of this works out and when we get our recurrence and we solve it, somehow magically were getting fully balanced partitions firmly balanced sub problems in the sense that its not n minus 1 and 1. ts something like it could even be n over 10 and 9n over 10. o have to subtract out the k elements that correspond to x and all of the k minus 1 elements that are in b to go figure out exactly what position or rank m looking for in the sub array corresponding to c, OK? And as you can see Y31 increased a little bit, so were going to now stop this iteration of the algorithm and were at A3 B1, which we think at this point is our upper tangent, but lets check that. And so thats why this is theta n. And so you put it all together in terms of what the merge corresponds to in terms of complexity and put that together with the overall divide and conquer. And m not going to write this down, but it makes sense that the lower tangent is going to have the lowest yij. o what have here in this picture is that these elements here are going to be greater than x. And these elements here are going to be less than x. o let me clear. And you need to do whats called a cut and paste thats associated with this where were going to just look at this and that. And as of now, were just going to say youre going to pick some element x belonging to s. And this choice is going to be crucial. Right, so what were going to do, as you can imagine, is were going to take these points. And so just as an example on the board, when you have something like this, youre going to have your convex hull being that. And were going to do this once and for all. o you just want it to be large enough such that you could break it up and you have something interesting going on with respect to the number of columns. You can count the number of elements in b, count the number of elements in c. And you can eliminate one of them in this recursion as youre discovering this element with the correct rank in this case, i. o let me write the rest of this out and make sure were all on the same page. o in this case, you can imagine an algorithm that is going to kind of do what this brute force algorithm does except that its looking at a point from here and a point from here. And you can obviously look at this and its kind of obvious what the overall convex hull is, right? And this hull obviously is going to be something, as you can guess, that encloses all of these points, OK? o the recurrence, once you do that, is t of n equals were going to just say its order one for n less than or equal to 140. But we also have this theorem thats called the master theorem that is essentially something where you can fairly mechanically plug in the as and the bs and whatever you have there maybe its theta n, maybe its theta n square and get the solution to the recurrence. And m going to say thats the segment. And just need to, once have the equation for the line associated with a4 b1 or a4 b2, just have to find the intercept of it, which is constant time, right? And it might be that theres four elements in the left and five on the right that are lets see. And for the right half of the problem, its going to be the coordinate that has the lowest x value. But its guaranteed that once find this median that do know all of the columns that have elements in this position that are less than this x. And know columns that in this position have elements that are greater than x, OK? f, in fact, had something like this and this was stretched out because have those two points outside the convex hull, this may still be a segment thats part of the electronics hall but this one is not, right? And what we do is we compute, for the segment A1 B1, we compute by Yij, in this case Y11, which is the intercept on the vertical line that you see here that Erik just marked with a red dot. And what m going to have to guarantee, of course, is that when go find this median of medians is that it ends up being something that gives me balanced partitions. o this is the sorting of all of the columns, all right? But for most of them, and certainly the ones were going to cover today, the smarts is going to be in the combination step when you combine these problems, the solutions of these sub problems, into the overall solution. Right, so our upper tangent is something that were going to define as if look at each of these things, m going to say they have a yij. Theres going to be a bunch of columns to the left of it, a bunch of elements to the left of it, and a bunch of elements to the right of it. And m going to look at that. o if we can do those once and for all and for the input set , were going to divide into the left half A and right half B by the xcoordinates. And if the answer is yes, m going to go ahead and, boom, say that is a segment of my convex hull. o in this case, it would be p to q to r to s. Youre going to start with t in this case. o the whole game here is going to be m going to find the median. And so what is the complexity of doing that? But the way youre going to specify that is simply by representing it as a sequence of points that are on the boundary on the hull in clockwise order. And those theta ns, because youre going down all the way from n to one, are going to be theta n square when you keep doing that, OK? And what happens now is m going to move clockwise, and m going to go from B1 to B4. f you just look at it and you go, what, right? o what is the complexity of this algorithm? And got p plus q equals n. And got a theta n merge simply because m going to be running through and incrementing as long as m in the loop, m going to be incrementing either the i or the j. And the maximum they can go to are p and q before bounce out of the loop or before they rotate around. Were going to have to continue with this while loop, and now what happens is, m going to go from B1 clockwise again to B4. o the idea is that once find this median of medians, which corresponds to this x number, can say that all of the columns these all correspond to columns that have their median element greater than x. These correspond to columns that have their median element less than x, OK? And could certainly choose this l over here thats my line l to be such that have a good partition between the two sets of points. Now, if look at what want, what want over here is this x. f want to find m going to find the median of medians. A3 to b1 right, is a4 to b1 going to be the upper tangent? You started with n. You did not go to n over 2 and n over 2, which is what divide and conquer is all about even n over b, OK? And so we might have p, q, r, s, t, u. And v and x are inside of the hull. As you can imagine, we want to improve on theta n log n. And so you could you say, well, m happy with theta n. That theta n complexity algorithm is better than a theta n log n complexity algorithm, which is kind of in the bag. But want to point out that in this particular case, its easy to get sub problems that are half the size because youve done the sorting. But if you the two dimensional case if the hull, all the segments have a certain characteristic not quite planar, but something thats a little more stringent than that you could imagine that you can do improvements. And those set of points are s, xi, yi such that i equals 1, 2 to n. And were just going to assume here, just to make things easy because we dont want to have segments that are null or segments that are a little bit different because theyre discontinuous. f look at determining the medians and say that once ve determined this x, which ve discovered that its the median, then this is right there in the middle. This is not a convex hull for the reason that have a bunch of points outside of the hull. But what were going to do is were going to arrange s into columns of size 5, right? But then what would happen, of course, is as move this, that would become the segment that was part of the convex hull, OK? o m going to go back to B1 and Erik now is going to go counterclockwise to A3. Now, clearly a4 is not going to be part of that, right? A4 is not going to be part of the overall hull. o this is a segment thats part of the convex hull. And its usually not that hard and certainly its not going to be particularly difficult for the divide and conquer examples that were going to look, at least today. o if youre going to working with segments or tangents theyre going to be used synonymously all of the tangents or segments associated with the entirety of the convex hull and we have to discover them. All the fun is going to be in the merge step. o is x. Now, it is true the first that these columns m just putting that up here imagining that thats x. Thats not guaranteed to be x because the columns themselves arent well, these columns are sorted. f think of a segment as being something that is defined by two points, then dont want to generate the segment vx because clearly the segment is not part of the convex hull. o thats the last thing that we have to do. What ve down here by this process of sorting each column and finding the median of medians is that found this median of medians such that theres a bunch of columns on the left. ts only going to be exactly one of them, as you can see from that. And m going to be calling this algorithm recursively, the median finding algorithm, to do that finding the median of medians. o b and c are going to correspond to the sub arrays that you can clearly eliminate one of them. And were going to use clockwise just because we want to be clear on as to what order were enumerating these points. n the next problem that well look at, the median finding problem, well find that trying to get the sub problems to be of roughly equal size is actually a little difficult, OK? m going to do something thats slightly less obvious in case you get your hopes up that we have this trivial algorithm, OK? And we look at a couple of different ones that will find all of these segments that are associated with this convex hull, OK? And what is the complexity of the test? But what have to do is once recur and discover the ranks of the sub problems, have to put them together. o if want to find the 10th highest element and ve broken it up relatively arbitrarily, its quite possible that the 10th highest element is going to be discovered in the left one or the right one. And thats essentially what we have to look at. And then you want to multiply that by 2. o some reasonably large number were going to go off and were going to assume thats a constant. And so m going to go back to B1. o lets say that we didnt care about divide and conquer just for the heck of it and gave you a bunch of points over here. And then m going to check to see whether these segments are actually tangents that are part of the overall convex hull or not. o now were going to do a demo of the merge algorithm that is a clever merge algorithm than the one that uses order n square time. And so at that point, you know that you can get down to small enough size sub problems for which you can find the convex hull efficiently. Even the gift wrapping algorithm that mentioned to you, with the right data structures, it gets down to that in terms of theta n log n, but no better. And so what were going to do is were going to say the first link in general, the first link is ai to bj. o then were going to find what were going to call the median of medians. t turns out all have to do with this observation is to go off and run the recurrence. And were going to do this in linear time. And this is my x over here. m going to put all of them into c. And m going to recur on b and c. Those two are my sub problems. And that violates the requirement that the segment be part of the overall hull, OK? And thats the case here as well. Then you clearly see that there are points on either side of the a1 b1 segment when you look at the overall problem, correct? o thats just a small, little thing that you have to keep in mind as you do this. f the answers is no, like in this case, m going to drop that segment, OK? And then m going to go down the b list until you see bm, which is the lower tangent. o were going to have to do this cleverly. And my answer is no, because the theta n extra factor came from the fact that you had to check every point, every endpoint, to see on which side of the plane it was. And a is going to be greater than or equal to 1. t could be two. o this is not the only way you could do it, but this is really very clever. And this is what we have spent the rest of our time on. And what have here is lets say ve generated, at this point, a convex hull associated with each of these sub problems. f defined the rank as less than or equal to x, theres four on the left and five on the right that are smaller. And then the other thing is problem set one is going to go out today. o what you see here is you have a3 here. o given that youre doing theta n work here, you have to be really careful as to how you pick the x element. And you can look at the pseudocode over on, to my right if face the board. You see that on a1 b1, b2 is on this side, b3 is on this side if just extend this line all the way to infinity in both directions. And we want to do this in linear time, OK? And so in general, were going to define, given a set of n numbers, define rank of x as the numbers in the set that are greater than m sorry, less than or equal to x. mean, you could have defined it differently. And theres just a variety of algorithms that you can use to do this. Typically, what happens in terms of efficiency is that you can write a recurrence thats associated with this divide and conquer algorithm.",0.147177066946338
45,45,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer highquality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. ANA BELL: o this next exercise is which of the following is allowed in Python. o x plus y is equal to 2. And again, these are all things that you can just type into Anaconda, which is our DE for the class, and see whether it works or not. o this the first one is probably not right lets increase it a little bit is not right because the left side needs to be a variable. This is not right because its an expression. This is not right because 2 is not a proper variable name. And xy that ones actually going to be OK because its not multiplication, its just the variable xy. o 50% said xy is equal to 2, and thats right. o none of these. o just to make sure that you guys understand, xy, this last one here, is the right one because xy is not x times y, like in math. ts just one variable name that has two characters, x and y.","This is not right because 2 is not a proper variable name. o just to make sure that you guys understand, xy, this last one here, is the right one because xy is not x times y, like in math. o this the first one is probably not right lets increase it a little bit is not right because the left side needs to be a variable. And xy that ones actually going to be OK because its not multiplication, its just the variable xy.",0.3644859813084112
46,46,"n this lesson were going to introduce you to stack data structure. Data structures, as we know, are ways to store and organize data in computers. o far, in the series we have discussed some of the data structures. We have talked about arrays and linked lists. Now in this lesson we are going to talk about stacks and we are going to talk about stack as abstract data type or ADT. When we talk about a data structure as abstract data type, we talk only about the features or operations available with the data structure. We do not go into implementation details. o basically we define the data structured only as a mathematical or logical model. Well go into implementation of stack in later lessons. n this lesson, were going to talk only about stack ADT. o we are only going to have a look at the logical view of stack. tack as a data structure in computer science is not very different from stack as a way of organizing objects, in real world. Here are some examples of stack from real world: First figure is of a stack of dinner plates. econd figure is of a mathematical puzzle, called tower of hanoi, where we have three rods or three pegs and multiple disks and the game is about moving a stack of discs, from one peg to another with this constraint that, a disc can not go on top of a smaller disc. Third figure is of a pack of Tennis balls. tack basically is a collection with this property, that an item in the stack must be inserted or removed, from the same end that we call the top of stack. n fact this is not just a property, this is a constraint or restriction. Only the top of a stack is accessible and any item has to be inserted or removed from the top. A stack is also called last in first out collection. ost recently added item in a stack has to go out first. n the first example, you will always pick up a dinner plate from top of the stack and if you will have to put a plate back into the stack, you will always put it back on top of the stack. You can argue, that can slip out a plate from in between without actually removing the plates on the top. o the constraint that should take out a plate always from the top is not strictly enforced. For the sake of argument, this is fine. You can say this. n other two examples where we have discs in a pag, and tennis balls in this box that can open only from one side, there is no way you can take out an item from in between. Any insertion of removal has to happen from top. You can not slip out an item from in between. You can take out an item, but for that you will have to remove all the items on top of that item. Lets now formally define stack as an abstract data tape. A stack is a list or collection with the restriction that insertion and deletion can be performed only from one end, that we call the top of stack. Lets now define the interface or operations available with stack ADT. There are two fundamental operations available with a stack. An insertion is called a push operation. push operation can insert or push some item X onto the stack. Another operation, second operation is called pop. pop is removing the most recent item from the stack, most recent element from the stack. push and pop are the fundamental operations and there can be few more. Typically there is one operation called top, that simply returns the element at top of the stack. And there can be an operation to check wheather a stack is empty or not. o this operation will return true if the stack is empty, false otherwise. o push is inserting an element on top of stack and pop is removing an element from top of stack. We can push or pop only one element at a time. All these operations that have written here can be performed in constant time, or in other words their time complexity is O(1). Remember an element that is pushed or inserted last on to a stack, is popped or removed first. o stack is called last in first out structure, what goes in last comes out first. last in first out, in short is called LFO. Logically a stack is represented something like this: As a three sided figure, as a container open from one side. This is representation of an empty stack. Lets name this stack s. Lets say this figure is representing a stack of integers. Right now the stack is empty. will perform push and pop operations to insert and remove integers from the stack. will first write down the operations here and then show you what will happen in the logical representation. Lets first perform a push. want to push number 2 on to the stack. The stack is empty right now, so we can not pop anything. After the push, stack will look something like this: There is only one integer in the stack, so of course its on top. Lets push another integer. This time, want to push number 10. And now lets say we want to perform a pop. The integer at top right now is 10. With a pop, it will be removed from the stack. Lets do few more push. just pushed 7 and 5 onto the stack. At this stage, if will call top operation, it will return me number 5. sEmpty will return me false. At this stage, a pop will remove 5 from the stack. As you can see the element, the integer which is coming last, is going out first, Thats why we call stack last in first out data structure. We can pop till the stack gets empty. One more pop, and stack will be empty. o this pretty much is stack data structure. Now one obvious question can be what are the real scenarios where stack helps us. Lets list down some of the applications of stack. tack data structure is used for execution of function calls in a program. We have talked about this quite a bit in our lessons on dynamic memory allocation and linked lists. We can also say that stack is used for recursion, because recursion is also a chain of function calls. ts just that, all the calls are to the same function. To know more about this application, you can check the description of this video, for a link to yodechool lesson on dynamic memory allocation. Another application of stack is we can use it to implement undo operation, in an editor. We can perform undo operation in any text editor or image editor. Right now, m pressing trl Z, and as you can see some of the text that have written, is getting cleared. You can implement this using a stack. tack is used in a number of important algorithms, like for example a compiler verifies whether parentheses in a source code are balanced or not using tack data structure. orresponding to each opening curly brace or opening parentheses in a source code, there must be a closing parentheses at appropriate position. And if parentheses in a source code are not put properly, if theyre not balanced, compiler should throw error and this check can be performed using a stack. We will discuss some of these problems in detail in coming lessons. This much is good for an introduction. n our next lesson we will discuss implementation of stack. This is it for this lesson. Thanks for watching!!","tack basically is a collection with this property, that an item in the stack must be inserted or removed, from the same end that we call the top of stack. A stack is a list or collection with the restriction that insertion and deletion can be performed only from one end, that we call the top of stack. Only the top of a stack is accessible and any item has to be inserted or removed from the top. o push is inserting an element on top of stack and pop is removing an element from top of stack. After the push, stack will look something like this: There is only one integer in the stack, so of course its on top. As you can see the element, the integer which is coming last, is going out first, Thats why we call stack last in first out data structure. And there can be an operation to check wheather a stack is empty or not. n the first example, you will always pick up a dinner plate from top of the stack and if you will have to put a plate back into the stack, you will always put it back on top of the stack. econd figure is of a mathematical puzzle, called tower of hanoi, where we have three rods or three pegs and multiple disks and the game is about moving a stack of discs, from one peg to another with this constraint that, a disc can not go on top of a smaller disc. will perform push and pop operations to insert and remove integers from the stack.",0.218707015130674
47,47,"DAVD ALAN: All right, welcome back. This is 50, and this is the start of week 10. o, for the past several weeks, weve been looking at a fairly low level how the internet works. TP/P, HTTP, the language which weve begun to build interesting things in languages like HTL, , PHP, and most recently Javacript. Today, though, were joined by Jonathan Zittrain, a professor at Harvard Law chool, at Harvard Kennedy chool, and the Harvard chool of Engineering and Applied ciences, who most recently taught a course called omputer cience 42, ontrolling yberspace. Today, we are poised to now look at a much bigger picture, and undoubtedly a bit at how life, the universe, and everything works, with our friend, professor Jonathan Zittrain. JONATHAN ZTTRAN: Thank you, David, and good afternoon. You are in an incredible course, as you know. You are apprenticing to a set of technologies that are really unusual, and want to talk today about what makes them so unusual, why we should care about it, and why they might be evolving in the future in directions that we dont like. And possibly even what to do about it, although realize we have about 49 minutes and 30 seconds yet, so some corners may be cut. aybe a framework to think about this stuff is technologies as owned and unowned. And want to explain what mean by owned and unowned. ost technology in the world is owned, and ll give a few examples of that. But the internet turns out to be an unowned technology, and that can make all the difference. o here are some owned technologies. This is the B ystem/360. This was a serious computer back in its day, and as you can see, everything about it radiates that you had better not go anywhere near it. This is the kind of computer that sat in the basement of a big company an actuary, an insurance company, a bank, or maybe a government tallying the census. And it was programmed, usually, by the vendor who operated the machine, in this case B. And thats because, while it was a general purpose computer, it could be programmed or reprogrammed, it was so precious and delicate, and part of the business model of B was to keep it apart from its consumer. You would instead tell B what you wanted, and then they would go ahead and program it for you. Not a bad deal, but a very owned technology in the sense that we know who is responsible for it, and whom to blame if something goes wrong with it, and it means that were not going to get that surprised by it, because everybody is so careful about what they use the computer for. Now, these are the sorts of things that went into it. Those are, of course, oldfashioned punch cards, and those represented, again, that you could program the machine in any way that you wanted, so long as you could get near it, which again, generally, you could not. This is another kind of technology that is also owned. This is the Friden Flexowriter. And the Friden Flexowriter was like a standard typewriter, and as you typed, the letters would appear on the paper, but it would also make for little indentations in this tape that ran through it. And the tape, if put back through the typewriter like a player piano, would type out whatever had previously been done. Which meant that with enough scissors and glue, you could actually cut and paste your way to a mail merge more easily than you can with icrosoft Word. o, the Flexowriter was very cool. t was very accessible. t doesnt threaten to electrocute you if you go near it. But theres no place to put a punch card to tell it what to do. The only punch technology is data. Type what you see, or it generates that strip as you type. Theres no code. Theres only content with the Flexowriter, and its successors are what the world of the 1980s and 1990s was shaping up to be for information technology. This is the Brother mart Word Processor. You turn this thing on, this is its home screen. Where would you like to go today? Word processing, spreadsheet, et cetera, et cetera. And the way this thing worked on Wednesday it was exactly the way it worked on Tuesday, and was the way it was going to work until you got rid of it. t was not programmable. Again, it only dealt with data, and its an owned technology, because how it would act was very much an artifact of what the Brother people put into it. Ditto, one might hope, for a AT scan machine. You want it to operate exactly as it was designed, although they are slightly reprogrammable in the B ystem/360 way. Again, we see some risks, some unpleasant surprises that could come if you have the wrong people programming your AT scanner, as actually has happened in the past few years. But now, part of this AT scanner is a little piece of what would describe as an unowned technology in the middle of it. And that is the personal computer. o lets look at the unowned technology that got things started. This is teve Jobs at the West oast omputer Faire in 1977, described as 10,000 walking, talking computer freaks. This was very marginal, but well attended, and this was, for the first time in consumer hands, a reprogrammable machine. You get your Apple . You turn it on after hooking it up to, yes, your television set, and you get a blinking cursor. And its up to you to figure out what to do. When you get the Apple out of the box, it is a door stop. ts only when you do such things as 10, print hello. 20, go to 10, that fun really begins. And you had lots and lots of people stepping forward to program their personal computers, intended as hobbyist machines. Within two years you had Dan Bricklin and Bob Frankston of the Boston area, programming the first digital spreadsheet ever, Visialc. And suddenly, businesses the nation over were like, oh my god, spreadsheets. And they started buying Apple computers. They are flying off the shelves, and Apple had to do market research to figure out why this thing was so popular. Thats what makes this an unowned technology. You didnt buy it from Apple, but Apple expects you or Bob Frankston or or somebody to program it later. And if your Apple doesnt spreadsheet the way you want, its not clear that its Apples fault. t is unowned in that it accepts contributions from anybody with the moxie and skill to program it, who can then share that program with anybody else. And that model became the model for all that followed. This is Bill Gates two years after he dropped out of Harvard, and he was pulled over for a traffic stop in Albuquerque, New exico. You can see the fashion was different then. And he does have a smile on his face, somehow knowing that he can buy and sell us all someday. And he was able to take what Jobs did and put it into, maybe, DO or later, Windows. But basically, this format, which is, you give this thing code, it may start with the blinking cursor, but then it will run the code. And that was true on many Ps until recently. And it was true then. This thing is probably around 1992. place it because of the 66 light here. t had a button that could alternate between 66 and 33, which was the speed at which the chip inside should run. You may wonder, why not leave it fast all the time? Thats because it would tire the hamsters out inside if you made it run too quickly. And Prince of Persia would be too fast as well. see, by the way, they now have hamster powered paper shredders. o you can put the paper in the top, and then the hamster runs on the wheel and shreds the paper, and then can live in the paper afterwards. o its all part of the cycle of life. Anyway, these things can run any code you give it, and that is a fundamental, but still contingent, piece of the technology. t didnt have to be that way. t could have been the Brother mart Word Processor, and as people at Brother or their competitors invent new stuff, they roll it out like any consumer product. ts not up to you to put a seventh blade into your safety razor. We wait for Gillette to say, if five is good, why not seven? And then we get it, and we buy it. This is different. With the modern P revolution, for the past 30 years, you hand a computer code that you have written or gotten from somebody else, it will run the code. And that changes everything. That is what gave rise to the off the shelf independent software movement. o you could buy a computer for any purpose, and then use it for any number of other purposes. Or your brother could, or your kid could, or anything else. t didnt have to be this way, but it turned out to be this way, once everybody discovered how many discoveries could come if you just released the technology blinking cursor style and figured that the world would build cool stuff. o thats, to me, the essence of unowned technologies, and just want to emphasize that you dont have to be this way. f you rewound time and played it back again, its not clear to me that we would end up with an unowned technology at the core of our consumer computing experience. Now, on the network side, there was a similar transformation. t began in the owned space. ATT ran the long distance system, and that was that, and it worked pretty well. And the prices were what they were, and the regulators came in to set the prices. And ATT purported to control the whole network. o back in the early 20th century, when an enterprising man invented this, the HushAPhone, it was something that would go over your telephone handset, so that your person you were talking to wouldnt hear extraneous noise. And no one could hear what you were saying to your interlocutor. ATT said no, we must a license that, because it could damage our network, and they purported to block people using it. This is an era, by the way, in which the telephones were leased from ATT. You could not go to the store and buy a telephone. You had to get it from your telephone services provider. This went up and down the Federal ommunications ommission. The F backed up ATT. t took a upreme ourt decision, ultimately, to reverse that. And the world was free to have their phone hushed, but not much else. That had its own successors. Back in the day and now mean probably the mid 80s into the early 90s there were services like ompuerve. That was going to be the future of networking. t had competitors, like Prodigy, and AOL, and the source, and ail. But you basically paid by the month, and then you got a rational menu of things you might want to do. Where you want to go today? But this menu was produced by ompuerve. f there was going to be something new on it, youd get it from ompuerve. And if somebody out there was like, think there should be a Visialc button, youd better persuade ompuerve of its worth, or it would never be accessible to somebody using the service. o thats an example of the B 360 or the Flexowriter model of technology for networking. That gets blown out of the water, unexpectedly to almost everybody in the field, by this academic research network known as the internet. Here are three of the founders, pictured here, of the internet classmates, it turns out, at Van Nuys High chool in alifornia. Theres Jon Postel and teve rocker and Vint erf, showing at their 25th anniversary retrospective picture for Newsweek that you can build a network out of pretty much anything. Although, as youll see, their network doesnt work. t goes from his ear to his ear, and mouth to mouth, which hope is an inside joke, rather than the founders of the internet dont know how to string tin cans together. But you can see that they built a network because they didnt have a lot of money and couldnt roll it out FedEx style, with lots of people working for them. And because they werent intending to make any money from it, they built a network that was unowned, whose points would be respectively owned or operated by who knows who, and maybe there would even be piggybacking. T would piggyback on BBN to get its packets going back and forth. But unowned as a total thing what they built were protocols to put the internet together in a way that there was no EO. There was no main menu. t just is. And its such an unusual way of doing it, both in methodology and in substance, that for many years, B was fond of saying you couldnt possibly build a corporate network using TP/P. And thats why internet engineers say that their mascot would be the bumblebee, because the fur to wingspan ratio of the bumblebee is far too large for it to be able to fly, and yet, miraculously, the bee flies. t turns out that we finally discovered how bees fly in 2006, thanks to massive government funding. t turns out they flap their wings very quickly. o the way the internet works is kind of like the way the beer finds its way around in Fenway Park. omebody asked for a beer, but theres no beer distribution limit network down to the last foot or so. For that, the person has to hand the beer to the toddler sitting on the end, who then passes it over. And at risk to each of our trousers, we do this because we stand together to let the fun flow. And thats basically the way packet networking works on the internet, where there are entities on the internet handling your packets, as you get them relayed from one point to another, who have no contractual relationship with you, nor with the ultimate destination. ts like nested matryoshka dolls, how it goes around. The basic format is this, and you may have learned a little bit about it. ts called hourglass architecture, and it says that you put not intellectual property, but internet protocol, in the middle of the hourglass here. Thats what the engineers work on, and its designed to be totally ecumenical, which is why its broad on the top and the bottom. At the top, we allow any number of applications. Who knows what somebodys going to build on it. ts just like a blinking cursor. The internet is just designed to take points of presence and route packets between them using best efforts. Thats it. Thats t. ts not a set of applications. ts just the network. And then anybody can build applications on top of it, and may the best apps win. And underneath, it was meant to be ecumenical about what hardware you would use. Whatever hardware you want to use, boom. You can bring it to the party, so long as it speaks internet protocol. And there is no copyright asserted in internet protocol, and as changes are made to it, youre free to adopt them or not. As an equipment manufacturer or as a network provider, it is, in fact, a collective hallucination, but a very sustained and powerful one. And the idea was, dont make the network smart. Dont keep adding features. Otherwise, we would be at the animated paper clip phase of the internet. We are not, because we never added many features to it. nstead, its just supposed to route and let the endpoints the smart things on the end like Ps deal with any features they want, such as encryption, such as return receipt, all of that stuff meant to be only if you need them. Now a different David that was David lark who contributed to last paper a different larke, Arthur . larke, came up with what he calls his Third Law. And this was, ""Any sufficiently advanced technology is indistinguishable from magic."" He was actually borrowing from a previous science fiction writer, Leigh Brackett, who put it a little less kindly. ""Witchcraft to the ignorant simple science to the learned."" And put it to you that part of 50 is to move you from one category to the other, so that you can start understanding stuff that seems like magic to others. turn on my Flexowriter, and magically, it pounds out a paper that had previously recorded. Youre learning about the guts of technology. And these are unowned technologies, both at the endpoint and in the network, that let you take what youve learned and just take it for a spin. Try it out. ee if you can change the world in some way. And it will look like magic to others, but you are learning the secrets to make it simple science. One such person who did this to the nth degree is ir Tim BernersLee. He wrote an app called the web, and that means he wrote protocols. ts like, hey, if you want to just ship a file to somebody but have it render into clickable links and pictures and stuff, heres how you would do that. And now ve programed a server and a client. o, OK, world, heres your web. Go to town. And unbelievably, the world did just that. Tim asserted no patent, no copyright in it. He gave it away. And the web is the second great unowned, collective hallucination that we have, which is also why if have a problem with a website, you cant go to the EO of the web and have it be taken down. There is no such person, and there is no main menu for the web, exactly the opposite of AT ts also, then, a moment, probably around 1995 or so, when Windows, which had no means of connecting to internet protocol, got finally hooked up to it, thanks to this guy, named Tattam. At niversity of Tasmania, in the psychology department, he wrote something called Trumpet Winsock. Thats because he likes trumpets. Thats him. And Trumpet Winsock was just a little shim that connected your Windows 95 or 3.1 into web and the internet. And suddenly everybody was like, holy crap, this is great. And Tattam just said, well, if you like it, you can send me some money, but otherwise its totally up to you. And thats how you start getting amazing contentlevel unowned technology taking off, such as catsthatlooklikehitler.com. ompuerve is unlikely to put this on its main menu, and yet its but a click away on this web, with enough people now connected that have cats and that know enough to upload a photo of them, that you can get this incredible number four Kitler, which is just like, dont know how you would come back home to that every day. t also lets such things that are crazy, like Wikipedia, is being created. an you imagine, in 2001, somebody named Jimbo saying to you, ve got a great idea. We start with 20 articles, and then anybody can edit anything, and before you know it, well have an incredibly useful source. How many people have used Wikipedia for medical advice in a direct way? Right. And the rest of you are not admitting it. That is amazing, given how it started and how it is sustained, by anybody able to edit any article at any time. n that sense, this is an unowned technology at the content layer of the hourglass, capable of incredible things. o popular, in fact, that it is now appearing on hinese restaurant menus. Not exactly sure why. have a theory, but we dont have time. o anyway, these unowned technologies can end up yielding new unowned technologies and content at other layers. And we end up with the amazing explosion weve seen in the past 30 years. ncredible, powerful, still contingent, especially because as more and more people use it, you start to see that its worth subverting. Whether to scam people or for your own purposes, these technologies that depend on some measure of trust and goodwill can themselves become very, very limited. And the fact that were not accrediting people who contribute anybody can write anything, even if youre from Tasmania that cool thing can become a problem. o we see , generally, a movement from owned stuff, with the introduction of the web and the internet, to an unowned zone. But then, you start to see certain apps themselves become foundational, and some of them may in turn be owned, and they start to look like the new ompuerve. o theres a kind of weird cycle going, as people shelter and look for stability and consistency and security and main menus. And once you start getting those, and some people and entities start to really get powerful in the space, they quite naturally might want to assert control. o things they could do to be open, they, in turn, can start to close, if it fits their business model to do it. And these are, of course, several instances of those sorts of things happening, as mere apps become foundational platforms and start pulling back. But this is really more of an entrepreneurial story. Theres a more fundamental problem going on, which is, in all the stuff running on this really cool P you might have, whose lineage traces back to 1977, what happens if just one piece of that code is bad? And it turns out, terrible things can happen, because any piece of code written by anybody running on your machine generally has had the keys to the kingdom. And thats kind of like The Princess and the Pea. Just one problem on the computer can spoil more than a good nights sleep. And this was something that the music industry discovered when they produced the compact disc before there were Ps. They produced this for an appliancesized world. The compact disc had digital quality music on it, read by D players. And the D players were in a very complicated arrangement with the music producers, so that it never even occurred to them to put any form of encryption on the disc. Because who would decrypt it? How would a consumer decrypt it? Well, it turns out, once you start putting DROs and D readers in Ps, anybody can decrypt it, and that leads to problems. n fact, the industry briefly tried to use the reprogrammability of the P as a feature rather than, to them, a bug, when they started putting code onto these Ds, so that, if it were loaded into a computer, the code would run and try to protect the computer from copying whats on the D, as against the wishes of the user, by just watching, at all times, to see if you were ever going to try to rip the D. This is called a rootkit, and it was not very popular when it was found out. And they started outing which Ds had this software that would ride on top, load itself on your computer, and refuse to leave, even if you no longer liked the music. This is the one where it was discovered, by the way, ironically called Get Right with the an. Here are my top other three rootkit Ds, The nvisible nvasion, uspicious Activity, and Healthy in Paranoid Times. Anyway, thats an example of a compromise of your machine from a trusted, or not so trusted, partner. But this stuff starts coming out of the wild, and you end up with things like the torm Worm in 2007. This is one of the biggies, and you see quotes like this. ""t can launch attacks against those who try to figure it out. t knows, and it punishes. Researchers are afraid. ve never seen this before."" And youre like, is this Network World, or Homeland? This is ridiculous. How is it so bad? And it has, in fact, gotten worse and worse. And as weve seen more and more sophisticated malware, we start to realize that just one bad move can end up ruining things for everybody. And we dont really have good defenses calculated for it yet, and that is a real problem. n fact, just today it was reported that the tuxnet virus has found its way onto the nternational pace tation because some Russians had a B key infected with it. And now the space station has come down with a problem. Thats pretty incredible. And its ironic, too, because it was a few years ago that somebody from icrosoft was pointing out that malware is becoming so bad that once youve got it, theres basically no way to perform an exorcism on your machine. Thats this wonderful quote that says, really what you have to do is nuke the systems from orbit, which starts to be a fairly serious thing to do to clean your machine of a virus. And if youre already in orbit on the space station, dont know whats going to happen. o, anyway, this is a real problem. And the fundamental problem is this, the apn runch bosuns whistle, a prize in a box of apn runch cereal in the early 1970s. After you have sugared up your child, why not have her run around the house and blow a whistles? ts the perfect prize. But it turns out that if you covered one hole of the whistle after extracting it from the box and blew, it emitted a tone of 2,600 hertz, which is exactly the tone used by ATT, monopoly telephone provider at the time, to indicate an idle line. Pick up the phone, blow the whistle, get free longdistance telephone calling. Boxes of apn runch cereal flying off the shelves. General ills has no idea why. And it turns out, theres a new thirdparty app for their cereal. T, but they have an owned network, which means they can fix it. Which they did. They turned to out of band signaling, so that data was distinct from code, and there was nothing you could put into the data channel the voice that could change the way the network worked. The internet is unowned and cannot be so readily fixed. The very channels that carry our music, our email, our dancing hamsters, are also the channels that carry executable code for the network itself, and for the endpoints. And we wouldnt want to have it any other way, except that now we are in a serious dilemma, because you click on the wrong thing, and now its all over. And we even start to see stuff being installed before you even take it out of the box. ts compromised in one way or another, and trying to figure out, as you peer into your box, whether its already compromised, is a hopeless, hopeless task. And try figuring out the same for anything you hook that box up to, the intermediate routers, et cetera. Which is one reason why think people are very puzzled at the prospect that their laptop webcam could simply be turned on by somebody who has compromised the machine, and viewing everything. dont know how many of you put a Postit note over your laptop webcam. dont see any hands up. Turns out its a cheap security feature, and recommend it, because you start to see that this is a socalled RAT a remote access technology. And here is a Danish family being viewed. This is from the point of view of the hacker, who has gotten into their machine and is watching them as they are computing, can completely control the machine, watch whats going on. Here is a police officer, whose machine in his squad car was compromised. o you can watch the police officer going around. guess you can see if hes coming to your house to arrest you for that. Theres the chat room where they are talking about this phenomenon, and amazed that they have managed to do this. This is the kind of stuff that makes it hard to be healthy in paranoid times. You add up this stuff anywhere you see a P, including that AT scan machine, you now start to worry about bad code getting near it. And this is becoming a somewhat dire threat. And think we have to recognize that threat, because it is already changing the nature of the unowned technologies that otherwise am extolling. o what do we do about it? Well, here is a quick tour of some potential solutions. Thinking through an axis between owned an unowned in a given environment, and then hierarchy and polyarchy. And hierarchy means theres only one choice for your solution. Everybody is bound by one entity that does something. And polyarchy means no, no, theres lots of choice and competition. And ll give you some examples to straighten this out. But lets first look at responses to the cybersecurity problem, quadrant by quadrant. o lets look at the unowned polyarchy one, which think is basically anarchy. t means youre on your own. Good luck. Theres lots of things you could do. Try to pick the right one, and just do the best you can to defend what you have. And that, think, is great if you happen to be a ninja. ts not so great if you are not. And its not even great for ninjas, because everybody asks them for help and they get bored. o we start to see things like this, designed to help you decide what to do. And if you see something like this dont know how often you see windows like this its a aturday night, youre clicking around. How many of you, when you see a window like this click, continue? Lots of hands. How many click cancel? A couple. Right. And then you click cancel, and youre back where you started. Youre like, but wanted to see the Hampster Dance. o then you click continue, and away you go. This is just not an effective way of securing things, and it reminds me of this email got several years ago warning Harvard Law chool faculty and staff of an insurgence of fraudulent emails at the law school, and all the things you have to do so that if you click on the wrong thing youre not totally screwed. And its just ridiculous how much you have to do every time you see an email. This is my favorite one, by the way. ""Be weary of emails that have misspelling, poor grammar or odd characters. They are a red flag for fraud."" wrote back, was like, got one. And they sent me to Oxford for three years, so never mess with your T department. And if youre going to end up in an T department, dont allow yourself to be messed with. But anyway, you see that user ignorance is something that is going to mean that its really hard to rely on that bottom right quadrant to help people. And ve got to say, m not even sure the answer to this question. Right? f its bad enough, suppose it could. f theres rain over Redmond. But anyway, lets look at another quadrant, upper left. When think of hierarchy and owned, m thinking government. And what might government do to try to help? Well, government has been trying to help for about 10 years now. This was the original strategy to secure cyberspace. t was huge. t basically said digital Pearl Harbor, be very afraid. And we dont know what to do about it. o theyve been trying to figure out what to do about it, like creating information sharing and analysis centers that look at the internet. Theyre like, its down, its down. ts like, OK, its down. We cant tell anybody, its down. o, one of the disadvantages of unowned technologies are theres no obvious place to send the arines, and they have no particular comparative advantage, even if you could send them, in securing this distributed network. Which means the government has had a hard time figuring out what to do. nstead they made calls like this, from former A director George Tenet, who said that, maybe we have to make it so that people the access to the web might need to be limited to those who show they can take security seriously. t would no longer be a world wide web. t would be like, three people being like, we are very secure. And thats one of the problems in trying to figure out what to do. And just a couple years ago, there was this big thing about, oh my god, they have a cybersecurity bill, and the presidents going to have emergency power to shut down the internet. dont think that amounted to much of anything. And in fact the lawmakers themselves were not pleased with these reports of the kill switch. Although it doesnt make me feel better that, as you can see at the bottom, the senator was like, the president already has the authority to shut down the internet under a littleknown provision of the ommunications Act passed one month after the 1941 attack on Pearl Harbor, which is a very forwardlooking law, to give the president the power to shut down the internet after the Japanese attacked in 1941. Anyway, we start to see other ways that government is trying to think of this like the government would think of any other form of intrusion into a space. And thats one of the negatives of calling it cyberspace, because its not really a space. But we think of perimeter defense. Of just like, well lets just put antivirus nodes all around the edges of the country, and they can shoot down incoming viruses like missile command. And it doesnt quite work that way. And it means, instead, that we may end up building a system to shoot down all sorts of other content that has nothing to do with network security. ts not clear. We want to take a page out of the books of countries that have already done that. n the meantime, weve seen proposals from some multinational multigovernment institutions, like the nternational Telecommunications nion, to completely redo the internet. As you can see, they are an extremely hierarchical organization, kind of the opposite of the three guys who started the internet off. And they have this idea of replacing the hourglass was what they call the next generation network. And they started a focus group on the next generation networks, also known as the FGNGN. And it came up with a new map for a better tomorrow. You ready for the new hourglass? Here it is. ts T next generation network, and it has everything but the animated paper clip. uddenly its feature laden in the network, because the idea should be, they figure, that you want to be able to make it, if packet says dont copy me, even if two users want to exchange it, the network should know not to do it. That could help with content control, and that could help with security. t doesnt, think, end up mattering that much, because trying to replace the network we have is really difficult to do. t has a lot of inertia to it. Just ask the internet engineers who are trying to upgrade it themselves. o a third quadrant here is the upper right. And its still owned, but its polyarchy. Theres lots of owners, and you get your pick. Thats basically the corporate sector. ts turning to the corporate world to say, ve got a problem. ell me a solution. m not looking for government. cant do it myself. But you could maybe come up with something. And sure enough, weve seen lots of efforts by private companies to secure the space, which in turn sometimes end up in trouble. Anyway, it turns out, then, that the corporate sector tries to offer some measure of security, but it has the feel, metaphorically, of securing the road from the Baghdad airport with your own bodyguard force. t has its element of inefficiency to it, and it means that different people will get different levels of security, which can end up not being all that fair. n a more subtle sense, weve seen the introduction of technologies made to be in the mold of the Flexowriter. n fact, the very company that in 1977 gave us the first great unowned technology, the P, gave us the first great owned technology exactly 30 years later, with the iPhone, where the iPhone says, look, we are going to define everything on it. You dont want to be like a P. Those crash all the time. nstead what you want is to be able to have it work, have it act just like that mart Word Processor. Now that was the original iPhone. There were no apps, no App tore on the original iPhone. t was more just like basically saying, look, were going to close it off, and this is going to look like something some of you have seen before. And we will define what goes on the phone. Now, that changed when, a couple years later, they introduced the software developers kit, and suddenly third parties could code for the iPhone. And that includes you. This is not a real Newsweek cover. n fact, its not clear to me Newsweek exists anymore. But anyway, its just a bad dream, the whole thing. t turns out, though, that they put an extra tweak in. its not like Visialc. f you invent something thats going to run on somebodys iPhone, and you want to give it to them, and they want to take it, it must go through the App tore, which in turn says, were not going to allow illegal, malicious, privacy invading, porn, bandwidth hog, or my favorite, unforeseen. We cant have anything unforeseen happening on the iPhone. And that App tore model is responding to a very real and pressing problem in the unowned universe. But its a solution that comes with its own worries. o, for example, when a guy created something called Freedom Time, counting down the end of George W. Bushs term, it was rejected from the App tore. And he actually wrote a note to teve Jobs asking why it had been rejected. teve Jobs wrote back and said, this is going to be offensive to roughly half our customers. Whats the point? And you realize that people are now walking around with their technology. They may want it. You may want to give it to them. But somebody in the middle has to be persuaded of its worth before allowing it. Thats a very different technological environment. And its one that teve Jobs accurately foresaw. ts not just about mobile phones. This is coming to all our technologies. And indeed, weve started to see hybrids and other ways in which our own P architecture is now App tore driven. This is now like, we take it for granted. Two years ago, it would have been, m not so sure thats going to happen. And years before that, it would have been insane to suggest such a thing. And, of course, the other day tried to load this dont even know what it does, this thing called Vuze. Anybody familiar with it? Anyway, tried to load it on my ac, and said, no, no, sorry. You can only allow things approved by the App tore. f youre totally nuts, you can change your settings to allow any old stuff to run on your machine. But why would you possibly want to do that? And it turns out that its not just Apple doing that now. Every major producer is building architectures that are both meant to secure things and that become vectors for control. And if you think Android is open, just wait until it gets a particularly bad set of malware, and youll see this is the Zombie dont know who would click on ""Animated Album Found When Fixed y Female oworkers omputer,"" but enough people did, and ended up then with Android malware. And you start to see the rate of uptake of malware happening. And you realize that its just a matter of time before we go to an App tore model for everything. o that which has become unowned is becoming owned, and that which is owned but open is becoming just owned, for all sorts of reasons. And were seeing it not just on endpoint devices, but in the cloud as well, as more and more platforms are starting to be intermediaries between you and ostensibly an independent party that you want to communicate with. Just ask the people who did something called ritter sland, a somewhat busy game. And it had 150 million users back in its heyday, until it did something that Facebook didnt like. Facebook simply pulled the plug, and there is its user graph right at the moment that Facebook pulled the plug. Thats very different from the zone where you get Napster out there, and Napster is out there. Theres no way that Bill Gates or anybody else could have pulled the plug on it, for better or for worse. And control over the code means control over the content. o, for example, when the Kindle came out perfect example of an owned Friden Flexowriter style device there was a third party that submitted through Amazon, 1984 for $0.99. And people bought it. And then the person submitting it was like, oops, its under copyright in the . thought it was in the public domain. y mistake. Amazon was like, oh my god, we could be in big trouble for allowing this to happen. And as a result, Amazon reached into every single Kindle that had downloaded 1984 and deleted 1984 from the Kindle from afar. ts like, you dont have 1984. You never had 1984. Theres no such book as 1984. Now, thats a problem. And its not as much of a problem when this happened, because there were still bookstores. Remember bookstores? Remember libraries? t was like, dont worry, theres a place that just has this already printed out and bound on paper in the unlikely event that somebody should walk in and be like, d like to give you $5 for a printed copy of 1984. How totally absurd is that as a business model? And as that fades, and you start to go to print on demand or read on demand, you realize that control over content is a serious thing. And just want to be sure, its not just Amazon here thats a baddy. ts Barnes and Noble as well. was reading talk about not having much of a life was reading War and Peace the other day when read this passage. ""A vivid glow Nookd in her face."" What the hell is that? ""The flame of the sulphur splinters Nookd"" Why is the work ""Nookd"" all over War and Peace? And then you realize that every place the word ""Kindle"" would appear, it has been replaced by the word ""Nook."" Yeah. Now that wasnt Barnes and Noble. That was a third party who had probably done this Amazon ebook and then just repurposed it to go on the Nook, and figured they would change their wrapper content on either end to say, oh, find us on our Nook store, and did a search and replace, and disaster happened. But you start to realize just how readily this could be repurposed. And believe me, if youre halfway through War and Peace, youre just like, whatevs, thats Tolstoy for you. What are you going to do? o this is an era in which our products are becoming services, and you think, ve got a toaster. Well, thats a product. magine your toaster as a webenabled service. What does that mean? t means you come down one day, its like, congratulations, youve gotten the Tuesday update. You now have three slots. Youre like, well thats pretty cool. And then the next day, theyre like , sorry we rolled it back. There was a problem. We apologize for any toast that was crushed. And then on Friday, you go down and its making orange juice. Youre like what do own? The answer is you own nothing. You have a long term service oriented relationship with a breakfast provider. And that is great, but its also something that we are still trying to wrap our arms around as we get used to this kind of thing. And the regulatory possibilities are only just beginning. o, for example, some of you may remember the old Ontar system. t was in your car. Youd be driving around, and you get lost or something, and you press the button in the rearview mirror. t has a little microphone so you can speak right into it, and speakers so you can hear what people say back. And this woman answers when you press the help button. And youre like, cant get up. Please help. And shes like, well, help is on the way. And then it turns out that the FB ends up going to an Ontar like company and says, want you, for this car, to simply turn on the microphone, and listen to everything going on in the car at all times. And the company was like, uhh. And theyre like, thats what youre going to do. Were the FB. The company said OK, and then sued anonymously, leading to this wonderful case, The ompany v. nited tates of America, in which, it turned out, then, that this was not permitted under the Wiretap Act for the thinnest of reasons. Which namely was the way the FB asked for it to happen, to be implemented. f the person asked for help because they were really in trouble, it would still only go to the FB, rather than Ontar, or that company, which presumably would not come and help. But if you could fix that glitch, you would be in a position to change the way this works. o, all sorts of ways in which malleable software, for which changing it is the prerogative and privilege of the vendor, overriding, or getting to permit, third party change, thats today. Thats the new environment, and it is the environment of the ystem/360. You dont own your stuff anymore, and that is a real problem. o what do we do about it? Well, m going to give you some ideas in the next 4 and 1/2 minutes. o, one possibility is to return to these very unowned technologies and look for new means of defense in this quadrant. oming in an unowned fashion, but so powerful, so persuasive, so widely adopted, that they end up being things that the criminals cant easily opt out of, which is what puts them on the left side of this graph. ts something that Wikipedia has discovered, in that any administrator of Wikipedia can be making changes to Wikipedia in a privileged way, but still in this kind of distributed, unowned fashion, in order to try to make for a better encyclopedia. And they just take through a list of people that complain about stuff all day long, and for no money, they just keep fixing it. That is an incredible story, and always a contingent one, that believe has lessons for how people can intervene usefully in ways to secure the internet. And ll just give you some examples of unowned but powerful so powerful they move to the left on that chart technologies like Tor, where, with enough computers together, you can end up laundering the source and destination of packets, so the something like the ilk Road could be up and unfindable, even though its a click away, for months at a time. t took the owner of the ilk Road trying to put out a hit on one of his vendors for the cops to be able to find this person. Thats pretty incredible. For better or worse, this is an example of a technology, then, that defies a certain kind of Ontar like surveillance. At the content layer, we see things like shahidi, which allow people to immediately throw up a map and make reports of things, so after an earthquake or with other problems, you have people coming together in a civic, unowned kind of way, to actually create a collective hallucination, that in this case is a map of trouble, that can become quite reliable. This is an idea that were pursuing over at the Berkman enter, in which currently, if you try to access a web page, it renders some links, and you click on one of those links and try to go where the link points. f you cant get there, thats it. Youre stuck. Well, what if we made it so that when you visit a page, it has already cached some of the links that point elsewhere? ts taken a copy from that server to itself, so that if you go to the server and you cant get there, you can go back to the place where you got the link, and it will send you what you missed. Thats an example of a distributed defensive system that could take some of the sting out of distributed denial of service attacks. And, it turns out, if the filtering, if the blocking is somewhere in the middle, maybe thanks to government filtering, this system would be a distributed means around it. Thats an example of an unowned civic technology coming back. Now if the entire plug is pulled on the internet, as now has been known to happen, although at the time it was like, wow, who knew that actually happened? t turns out theres mesh networking, in which each one of our devices could be programmed to be able to respond to nearby devices, and then onward onward, like that beer passing brigade at Fenway Park, so that we end up with a network among ourselves, possibly with cached Facebook and Twitter credentials. o you can find your Facebook friends in your mesh network without even having to get to facebook.com. Thats an example of a distributed, unowned collective hallucination that could greatly affect security. There was a time when there was a debate among state governments about slavery, and about returning slaves to the outh who were on the run in the North. And a political accommodation was reached, to try to prevent the ivil War at the time, that they would be returned. And it turns out it didnt work. And why didnt it work? Because there was not centralized law enforcement in any big way. f you needed to find somebody or do anything that was bigger than a single arrest, you needed a posse, you needed to recruit citizens to do it. And enough citizens were like, dont think so, that it didnt happen. Technologies that rely on the general public to work are also technologies that have some check valves against abuse in a way thats different from the check valves against abuse that we are familiar with from the more traditional centralized government scenarios. o end up with this question. You are now graduating from 50. Youve gotten the bug bit that gets you into this technology. And in this following puzzle from Game of Thrones, try to think about who you are. Three great men sit it a room, a king, a priest, and the rich man. Between them stands a common cell sword. Each great man bids the cell sword kill the other two. Who lives? Who dies? Does it? He has neither crown, nor gold, nor favor with the gods. But if its swordsmen who rule, why do we pretend kings hold all the power? JONATHAN ZTTRAN: OK, so, in this scenario, think youre either aesters in training, for those who read the books, or youve got the sword. Thats what youre sharpening. You have a tool that you can use in an environment still sufficiently generative that in a week you could flip things over. You could completely transform it with some good code deployed on this network. How you choose to use it will, in part, influence whether anybody sees reason to keep the network generative, or whether its time to just call it quits and go back to the Flexowriters we love. n this graph of people this is larkes Third Law coming back again basically there are Luddites on the left, so removed from technology that they dont care what happens in the world. Ontar isnt a problem, because they dont drive. OK, not that many left of those. They are all the Harvard bookstore. Then, on the right, you have the nerds, who are the ninjas, who can get around anything. n the middle, you have the public. And you have a chance to emerge from the herd, while still remembering what it was like, and influence the course of history in a way that is better for everybody. o, to me, that is the test. an we make systems that distribute power, rather than focus it, and still be secure? And am confident that the answer lies within this room, and over this webcast, and with those who are curious enough to undergo the phenomenon that is 50. And they are the kind of people that, clacking late at night, end up like this. Thats the instinct believe thats going to save us. Thank you very much. DAVD ALAN: Thank you so much to professor Zittrain. Do come on up if you have questions. Well see you on Wednesday.","n fact, the industry briefly tried to use the reprogrammability of the P as a feature rather than, to them, a bug, when they started putting code onto these Ds, so that, if it were loaded into a computer, the code would run and try to protect the computer from copying whats on the D, as against the wishes of the user, by just watching, at all times, to see if you were ever going to try to rip the D. This is called a rootkit, and it was not very popular when it was found out. Not a bad deal, but a very owned technology in the sense that we know who is responsible for it, and whom to blame if something goes wrong with it, and it means that were not going to get that surprised by it, because everybody is so careful about what they use the computer for. o thats, to me, the essence of unowned technologies, and just want to emphasize that you dont have to be this way. And the web is the second great unowned, collective hallucination that we have, which is also why if have a problem with a website, you cant go to the EO of the web and have it be taken down. ompuerve is unlikely to put this on its main menu, and yet its but a click away on this web, with enough people now connected that have cats and that know enough to upload a photo of them, that you can get this incredible number four Kitler, which is just like, dont know how you would come back home to that every day. And then it turns out that the FB ends up going to an Ontar like company and says, want you, for this car, to simply turn on the microphone, and listen to everything going on in the car at all times. ts taken a copy from that server to itself, so that if you go to the server and you cant get there, you can go back to the place where you got the link, and it will send you what you missed. And put it to you that part of 50 is to move you from one category to the other, so that you can start understanding stuff that seems like magic to others. This is just not an effective way of securing things, and it reminds me of this email got several years ago warning Harvard Law chool faculty and staff of an insurgence of fraudulent emails at the law school, and all the things you have to do so that if you click on the wrong thing youre not totally screwed. And it was programmed, usually, by the vendor who operated the machine, in this case B. And thats because, while it was a general purpose computer, it could be programmed or reprogrammed, it was so precious and delicate, and part of the business model of B was to keep it apart from its consumer. And ll just give you some examples of unowned but powerful so powerful they move to the left on that chart technologies like Tor, where, with enough computers together, you can end up laundering the source and destination of packets, so the something like the ilk Road could be up and unfindable, even though its a click away, for months at a time. And its up to you to figure out what to do. And the way this thing worked on Wednesday it was exactly the way it worked on Tuesday, and was the way it was going to work until you got rid of it. Try to pick the right one, and just do the best you can to defend what you have. uddenly its feature laden in the network, because the idea should be, they figure, that you want to be able to make it, if packet says dont copy me, even if two users want to exchange it, the network should know not to do it. And you have a chance to emerge from the herd, while still remembering what it was like, and influence the course of history in a way that is better for everybody. And thats one of the problems in trying to figure out what to do. And we wouldnt want to have it any other way, except that now we are in a serious dilemma, because you click on the wrong thing, and now its all over. t didnt have to be this way, but it turned out to be this way, once everybody discovered how many discoveries could come if you just released the technology blinking cursor style and figured that the world would build cool stuff. Theres only content with the Flexowriter, and its successors are what the world of the 1980s and 1990s was shaping up to be for information technology. But the internet turns out to be an unowned technology, and that can make all the difference. This is the one where it was discovered, by the way, ironically called Get Right with the an. Although it doesnt make me feel better that, as you can see at the bottom, the senator was like, the president already has the authority to shut down the internet under a littleknown provision of the ommunications Act passed one month after the 1941 attack on Pearl Harbor, which is a very forwardlooking law, to give the president the power to shut down the internet after the Japanese attacked in 1941. You are apprenticing to a set of technologies that are really unusual, and want to talk today about what makes them so unusual, why we should care about it, and why they might be evolving in the future in directions that we dont like. f you invent something thats going to run on somebodys iPhone, and you want to give it to them, and they want to take it, it must go through the App tore, which in turn says, were not going to allow illegal, malicious, privacy invading, porn, bandwidth hog, or my favorite, unforeseen. And the Friden Flexowriter was like a standard typewriter, and as you typed, the letters would appear on the paper, but it would also make for little indentations in this tape that ran through it. And were seeing it not just on endpoint devices, but in the cloud as well, as more and more platforms are starting to be intermediaries between you and ostensibly an independent party that you want to communicate with. This is an idea that were pursuing over at the Berkman enter, in which currently, if you try to access a web page, it renders some links, and you click on one of those links and try to go where the link points. t was more just like basically saying, look, were going to close it off, and this is going to look like something some of you have seen before. This was a serious computer back in its day, and as you can see, everything about it radiates that you had better not go anywhere near it. t was like, dont worry, theres a place that just has this already printed out and bound on paper in the unlikely event that somebody should walk in and be like, d like to give you $5 for a printed copy of 1984. Thats what the engineers work on, and its designed to be totally ecumenical, which is why its broad on the top and the bottom. And thats basically the way packet networking works on the internet, where there are entities on the internet handling your packets, as you get them relayed from one point to another, who have no contractual relationship with you, nor with the ultimate destination. At the content layer, we see things like shahidi, which allow people to immediately throw up a map and make reports of things, so after an earthquake or with other problems, you have people coming together in a civic, unowned kind of way, to actually create a collective hallucination, that in this case is a map of trouble, that can become quite reliable. This is one of the biggies, and you see quotes like this. But anyway, you see that user ignorance is something that is going to mean that its really hard to rely on that bottom right quadrant to help people. And these are unowned technologies, both at the endpoint and in the network, that let you take what youve learned and just take it for a spin. They turned to out of band signaling, so that data was distinct from code, and there was nothing you could put into the data channel the voice that could change the way the network worked. There is no such person, and there is no main menu for the web, exactly the opposite of AT ts also, then, a moment, probably around 1995 or so, when Windows, which had no means of connecting to internet protocol, got finally hooked up to it, thanks to this guy, named Tattam. nstead what you want is to be able to have it work, have it act just like that mart Word Processor. And that is great, but its also something that we are still trying to wrap our arms around as we get used to this kind of thing. But then, you start to see certain apps themselves become foundational, and some of them may in turn be owned, and they start to look like the new ompuerve. But now, part of this AT scanner is a little piece of what would describe as an unowned technology in the middle of it. nstead, its just supposed to route and let the endpoints the smart things on the end like Ps deal with any features they want, such as encryption, such as return receipt, all of that stuff meant to be only if you need them. Theres a more fundamental problem going on, which is, in all the stuff running on this really cool P you might have, whose lineage traces back to 1977, what happens if just one piece of that code is bad? For that, the person has to hand the beer to the toddler sitting on the end, who then passes it over. Thats this wonderful quote that says, really what you have to do is nuke the systems from orbit, which starts to be a fairly serious thing to do to clean your machine of a virus. f you rewound time and played it back again, its not clear to me that we would end up with an unowned technology at the core of our consumer computing experience. Again, it only dealt with data, and its an owned technology, because how it would act was very much an artifact of what the Brother people put into it. o we start to see things like this, designed to help you decide what to do. nstead they made calls like this, from former A director George Tenet, who said that, maybe we have to make it so that people the access to the web might need to be limited to those who show they can take security seriously. And we dont know what to do about it. ts like, hey, if you want to just ship a file to somebody but have it render into clickable links and pictures and stuff, heres how you would do that. And Tattam just said, well, if you like it, you can send me some money, but otherwise its totally up to you. Now if the entire plug is pulled on the internet, as now has been known to happen, although at the time it was like, wow, who knew that actually happened? But it turns out that if you covered one hole of the whistle after extracting it from the box and blew, it emitted a tone of 2,600 hertz, which is exactly the tone used by ATT, monopoly telephone provider at the time, to indicate an idle line. o we see , generally, a movement from owned stuff, with the introduction of the web and the internet, to an unowned zone. And its ironic, too, because it was a few years ago that somebody from icrosoft was pointing out that malware is becoming so bad that once youve got it, theres basically no way to perform an exorcism on your machine. This is from the point of view of the hacker, who has gotten into their machine and is watching them as they are computing, can completely control the machine, watch whats going on. And thats why internet engineers say that their mascot would be the bumblebee, because the fur to wingspan ratio of the bumblebee is far too large for it to be able to fly, and yet, miraculously, the bee flies. And the D players were in a very complicated arrangement with the music producers, so that it never even occurred to them to put any form of encryption on the disc. But basically, this format, which is, you give this thing code, it may start with the blinking cursor, but then it will run the code. But if you could fix that glitch, you would be in a position to change the way this works. The company said OK, and then sued anonymously, leading to this wonderful case, The ompany v. nited tates of America, in which, it turned out, then, that this was not permitted under the Wiretap Act for the thinnest of reasons. And think we have to recognize that threat, because it is already changing the nature of the unowned technologies that otherwise am extolling. Those are, of course, oldfashioned punch cards, and those represented, again, that you could program the machine in any way that you wanted, so long as you could get near it, which again, generally, you could not. Then, on the right, you have the nerds, who are the ninjas, who can get around anything. Anyway, these things can run any code you give it, and that is a fundamental, but still contingent, piece of the technology. And the rest of you are not admitting it. o you can put the paper in the top, and then the hamster runs on the wheel and shreds the paper, and then can live in the paper afterwards. And just a couple years ago, there was this big thing about, oh my god, they have a cybersecurity bill, and the presidents going to have emergency power to shut down the internet. And there is no copyright asserted in internet protocol, and as changes are made to it, youre free to adopt them or not. Anyway, we start to see other ways that government is trying to think of this like the government would think of any other form of intrusion into a space. How you choose to use it will, in part, influence whether anybody sees reason to keep the network generative, or whether its time to just call it quits and go back to the Flexowriters we love. t has its element of inefficiency to it, and it means that different people will get different levels of security, which can end up not being all that fair. And thats kind of like The Princess and the Pea. And, it turns out, if the filtering, if the blocking is somewhere in the middle, maybe thanks to government filtering, this system would be a distributed means around it. f the person asked for help because they were really in trouble, it would still only go to the FB, rather than Ontar, or that company, which presumably would not come and help. n this graph of people this is larkes Third Law coming back again basically there are Luddites on the left, so removed from technology that they dont care what happens in the world. And it turns out that its not just Apple doing that now. But you basically paid by the month, and then you got a rational menu of things you might want to do. That was a third party who had probably done this Amazon ebook and then just repurposed it to go on the Nook, and figured they would change their wrapper content on either end to say, oh, find us on our Nook store, and did a search and replace, and disaster happened. And it means, instead, that we may end up building a system to shoot down all sorts of other content that has nothing to do with network security. o, one of the disadvantages of unowned technologies are theres no obvious place to send the arines, and they have no particular comparative advantage, even if you could send them, in securing this distributed network. Anyway, it turns out, then, that the corporate sector tries to offer some measure of security, but it has the feel, metaphorically, of securing the road from the Baghdad airport with your own bodyguard force. What are you going to do?",0.0952765442067016
48,48,"ANNONER: The following program is brought to you by altech. YAER ABOTAFA: Welcome back. Last time, we discussed the feasibility of learning. And we realized that learning is indeed feasible, but only in a probabilistic sense. And we modeled that probabilistic sense in terms of a bin that has an outofsample performance. We already mapped that to the outofsample performance. The performance we dont know. And in order to be able to tell what E_out of h is h is the hypothesis that corresponds to that particular bin we look at the insample. And we realize that the insample tracks the outofsample well through the mathematical relationship which is the Hoeffding nequality, that tells us that the probability that E_in deviates from E_out by more than our specified tolerance is a small number. And that small number is a negative exponential in N. o the bigger the sample, the more reliable that E_in will track E_out well. That was the basic building block. But then we realized that this applies to a single bin. And a single bin corresponds to a single hypothesis. o now we go for a case where we have a full model, h_1 up to h_. And we take the simple case of a finite hypothesis set. And we ask ourselves, what would apply in this case? We realized that the problem with having multiple hypotheses is that the probability of something bad happening could accumulate. Because if there is a 0.5% chance that the first hypothesis is bad, in the sense of bad generalization, and 0.5% for the second one, we could be so unlucky as to have these 0.5% accumulate, and end up with a significant probability that one of the hypotheses will be bad. And when one of the hypotheses will be bad, if we are further unlucky, and this is the hypothesis we pick as our final hypothesis, then E_in will not track E_out for the hypothesis we pick. o we need to accommodate the case where we have multiple hypotheses. And the argument was extremely simple. g is our notation for the final hypothesis. t is one of these guys that the algorithm will choose. Well, the probability that E_in doesnt track E_out will obviously be included in the fact that E_in for h_1 doesnt track the outofsample for that one, or E_in for h_2 doesnt track, or E_in of h_ doesnt track. The reason is very simple. g is one of the guys. f something bad happens with g, it must happen with one of these guys at least, the one that was picked. o we can always say that this implies these things, which is this or this or this or this or this. And after that, we apply a very simple mathematical rule, which is the union bound. The probability of an event or another event or another event is at most the sum of the probabilities. That rule applies regardless of the correlation between these events, because it takes the worstcase scenario. f all the bad events happen disjointly, then you add up the probabilities. f there is some correlation, and they overlap, you will get a smaller number. n all of the cases, the probability of this big event will be less than or equal to the sum of the individual probabilities. And this is useful because in the coin flipping case, which started this argument, the events are independent. n the case of the hypotheses of a model, the events may not be independent, because we have the same sample. And we are only changing the hypotheses. o it could be that the deviation here is related to the deviation here. But the union bound doesnt care. Regardless of such correlations, you will be able to get a bound on the probability of this event. And therefore, you will be able to bound the probability that you care about, which has to do with the generalization, to the individual Hoeffding applied to each of those. And since you have of them, you have an added factor. o the final answer is that the probability of something bad happening after learning is less than or equal to this quantity, which is a helpful small quantity, times . And we realize that now we have a problem because if you use a bigger hypothesis set, will be bigger. And therefore, the righthand side here will become bigger and bigger when you add the . And therefore, at some point, it will even become meaningless. And we are not even worried yet about being infinity, which will be true for many hypothesis sets, in which case, this is totally meaningless. However, we werent establishing the final result in learning. We were establishing the principle that, through learning, you can generalize. And we have established that. t will take us a couple of weeks to get from that to the ability to say that a general learning model, an infinite one, will generalize. And we will get the bound on generalization. Thats what the theory of generalization will address. o today the subject is linear models. And as mentioned at the beginning, this is out of sequence. f was following the logical sequence, would go immediately to the theory and take , which takes care of the finite case, and then generalize it to the more general case. However, as mentioned, decided to give you something concrete and practical to work with early on. And then we will go back to the theory after that. The linear model is one of the most important models in machine learning. And what we are going to do in this lecture, were going to start with a practical data set that we are going to use over and over in this class. And then, if you remember the perceptron that we introduced in the first lecture, the perceptron is a linear model. o here is the sequence of the lecture. We are going to take the perceptron and generalize it to nonseparable data. Thats a relief, because we already admitted that separable data is very rare. And we would like to see what will happen when we have nonseparable data. Then, we are going to generalize this further to the case where the target function is not a binary classification function, but a realvalued function. That also is a very important generalization. And linear regression, as you will see, is one of the most important techniques that is applied mostly in statistics and economics, and also in machine learning. Finally, as if we didnt do enough generalization already, we are going to take this and generalize it to a nonlinear case. All in a days work, all in one lecture. ts a pretty simple model. And at the end of the lecture, you will be able to actually deal with very general situations. And you may ask yourself, why am calling the lecture Linear odel when m going to talk about nonlinear transformation? Well, youll realize that nonlinear transformation remains within the realm of linear models. Thats not obvious. We will see how that materializes. o thats the plan. Now, lets look at a real data set that we are going to use, and will be available to you to try different ideas on. And its very important to try your ideas on real data. Regardless of how sure you are when you have a toy data set that you generate, you should always go for real data sets and see how the system that you thought of performs in reality. o here is the data set. t comes from ZP codes in the postal office. o people write the ZP code. And you extract individual characters, individual digits. And you would like to take the image, which happens to be 16 by 16 gray level pixels, and be able to decipher what is the number in it. Well, that looks easy except that people write digits in so many different ways. And if you look at it, there will be some cases like this fellow. s this a 1 or a 7? s this a 0 or an 8? o you can see that there is a problem. And indeed, if you get a human operator to actually read these things and classify them, they will probably be making an error of about 2.5%. And we would like to see if machine learning can at least equal that, which means that we can automate the process, or maybe beat that. o this is a data set that we are going to work with. Lets look at it a little bit more closely to see how we input it to our algorithm. We have one algorithm so far, which is the perceptron learning algorithm. We are going to try on this. And then we are going to generalize it a little bit. The first item is the question of input representation. What do mean? This is your input, the raw input, if you will. Now this is 16 pixels by 16 pixels. o there are 256 real numbers in that input. f you look at the raw input x, this would be x_1, x_2, x_3, dot, dot, dot, dot, and x_256. Thats a very long input to encode such a simple object. And we add our mandatory x_0. Remember, in linear models, we have this constant coordinate, x_0 equals 1, we add in order to take care of the threshold. o this will always be in the background whether we mention it or not. f you take this raw input and try the perceptron directly on it, you realize that the linear model in this case, which has a bunch of parameters, has really just too many parameters. t has 257 parameters. f you are working in a 257thdimensional space, that is a huge space. And the poor algorithm is trying to simultaneously determine the values of all of these ws based on your set. o the idea of input representation is to simplify the algorithms life. We know something about the problem. We know that its not really the individual pixels that matter. You can probably extract some features from the input, and then give those to the learning algorithm and let the learning algorithm figure out the pattern. o this gives us the idea of features. What are features? Well, you extract the useful information. And as a suggestion, very simple one, lets say that in this particular case, instead of giving the raw input with all of the pixel values, you extract some descriptors of what the image is like. For instance, you look at this. Depending on whether this is the digital 8 or the digit 1, et cetera, there is a question of the intensity, average intensity. 1 doesnt have too many black pixels. 8 has a lot. 5 has some. o if you simply add up the intensity of all the pixels, you probably will get a number that is related to the identity. t doesnt uniquely determine it, but its related. ts a higherlevel representation of the raw information there. ame as symmetry if you think of the digit 1, 1 will be symmetric. f you flip it upside down, or you flip it right and left, you will get something that overlaps significantly with it. o you can also define a symmetry measure, which means that you take the symmetric difference between something and its flipped versions, and you see what you get. f something is symmetric, things will cancel because its symmetric. Youll get a very small value. And if something is not symmetric, lets say like the 5, you will get lots of values in the symmetric difference. And you will get a high value for that. o what you are measuring is the antisymmetry. You take the negative of that, and you get the symmetry. o you get another guy, which is the symmetry. o now, x_1 is the intensity variable, x_2 is the symmetry variable. Now admittedly, you have lost information in that process. But the chances are you lost as much irrelevant information as relevant information. o this is a pretty good representation of the input, as far as the learning algorithm is concerned. And you went from 257 dimensional to 3 dimensional. Thats a pretty good situation. And you probably realize that having 257 parameters is bad news for generalization, if you extrapolate from what we said. Having 3 is a much better situation. o this is what we are going to work with. When you take the linear model in this case, you just have w_0, w_1, and w_2. And thats what the perceptron algorithm, for example, needs to use to determine. Now lets look at the illustration of these features. You have these as your inputs. And x_1 is the intensity, x_2 is the symmetry. What do they look like? They look like this. This is a scatter diagram. Every point here is a data point. ts one of the digits, one of the images you have. And m taking the simple case of just distinguishing the 1s from the 5s. o m only taking digits that are 1s or 5s. And you can always take other digits versus each other, and then combine the decision. f you can solve this unit problem, you can generalize it to the other problem. o when you put all the 1s and all the 5s in a scatter diagram, you realize for example that the intensity on the 5s is usually more than the intensity on the 1s. There are more pixels occupied by the 5s than the 1s. This is the coordinate which is the intensity. And indeed, the red guys, which happen to be the 5s, are tilted a little bit more to the right, corresponding to the intensity. f you look at the other coordinate, which is symmetry, the 1 is often more symmetric than the 5. Therefore, the guys that happen to be the 1s, that are the blue, tend to be higher on the vertical coordinate. And just by these two coordinates, you already see that this is almost linearly separable. Not quite, but its separable enough that if you pass a boundary here, you will be getting most of them right. Now you realize that its impossible really to ask to get all of them right because, believe it or not, this fellow is a 5, at least meant to be a 5 by the guy who wrote it. o we have to accept the fact that there will be stuff that is completely undoable. And we will accept an error. ts not a zero error. But hopefully, its a small error. o this is what the features look like. Now what does the perceptron learning algorithm do? What it does is this complicated figure, which takes the evolution of E_in and E_out as a function of iteration. When you apply the perceptron learning algorithm, you apply it only to E_in. E_in is the only value you have. E_out is sitting out there. We dont know what it is. We just hope that E_in tracks it well. Lets look at the figure. These are the iteration numbers. o this is the first misclassified example. You go and apply the perceptron learning algorithm again, again, again for 1000 times. As you do that, E_in, which is the green curve, will go down and sometimes will go up. We realize that the perceptron learning algorithm takes care of one point at a time, and therefore may mess up other points while its taking care of a point. o in general, it can go up or down. But the bad news here is that the data is not linearly separable. And we made the remark that the perceptron learning algorithm behaves very badly when the data is not linearly separable. t can go from something pretty good to something pretty bad, in just one iteration. o this is a very typical behavior of the perceptron learning algorithm. Because the data is not linearly separable, the perceptron learning algorithm will never converge. o what do we do? We force it to terminate at iteration 1000. That is, we stop at 1000 and take whatever weight vector we have. And we call this g, the final hypothesis of the perceptron learning algorithm. Now we obviously look at this, and we say, if only took this guy. This is a better guy than the other. But you know, youre just applying the algorithm and cutting it off. Now, one of the things you observe from here, plotted E_out. Youre not going to be able to plot E_out in a real problem that you deal with, if E_out is really an unknown function. You may be able to estimate it using some test examples. But all you need to know here is that E_out is drawn here for illustration, just to tell you what is happening in reality as you work on the insample error. And in this case, you find that E_out actually tracks the E_in pretty well. There is a difference. o if we go from here to here, thats our epsilon. ts a big epsilon. But the good news is that it tracks it. When this goes down, this goes down. When this goes up, this goes up. o if you make your decision based on E_in, the decision based on E_out will also be good. Thats good for generalization. And that is one of the advantages of something as simple as the perceptron learning algorithm. t doesnt have too many parameters. And because of our efforts in getting only three features, it has even three parameters now. o the chances are that it will generalize well, which it does. Now what does the final boundary look like? This is only the illustration here, its just this is the evolution. Eventually, you end up with a hypothesis. The hypothesis would separate the points in the scatter diagram you saw. o what does it look like? Well, it looks like this. This is your boundary. This is the final hypothesis, that corresponds to the hypothesis you got at the final iteration. Well, its OK, but definitely not good. ts too deep into the blue region. You would have been better off doing this. And the chances are maybe earlier guys that had better insample error will do that. But thats what you have to live with, if you apply the perceptron learning algorithm. o now we go and try to modify the perceptron learning algorithm in a very simple way, that is the simplest modification you can ever imagine. o lets see what happens. This is what that PLA did, right? And when we looked at it, we said: if we only could keep this value. Well, this value is not a mystery. t happened in your algorithm. You can measure it explicitly. ts an insample error. And you know that its better than the value you ended up with. o in spite of the fact that youre doing these iterations according to the prescribed perceptron learning algorithm rule modify the weights according to one misclassified point you can keep track of the total insample error of the intermediate hypothesis you got. Right? And only keep the guy that happens to be the best throughout. o youre going to continue as if its really the perceptron learning algorithm. But when you are at the end, you keep this guy and report it as the final hypothesis. What an ingenious idea! Now the reason the algorithm is called the pocket algorithm is because the whole idea is to put the best solution so far in your pocket. And when you get a better one, you take the better one, put it in your pocket, and throw the old one. And when you are done, report the guy in your pocket. We can do that. What does this diagram look like, when you are looking at the pocket algorithm? uch better. You can look at these values, and it is the best value so far. Here, we went down. And here, we indeed went down. Here, we went up. You see this green thing? Here, we didnt, because the good guy is in our pocket and thats what were reporting the value for. And we continued with it until we dropped again. And we dropped again. And we never changed that, because there was never a better guy than this guy. o when we come to iteration 1000, we have this fellow. Now when you do that, you can use perceptron learning algorithm with nonseparable data, terminate it by force at some iteration, and report the pocket value. And that will be your pocket algorithm. And if you look at the classification boundary, PLA versus pocket, this is what we had with the perceptron learning algorithm. We complained a little bit that its too deep in the blue region. And when you look at the other guy, which is the pocket algorithm, it looks better. t actually does what we thought it would do. t separates them better. till, obviously, it cannot separate them perfectly. Nothing can, because they are not linearly separable. On the other hand, this is a good hypothesis to report. o with this very simple algorithm, you can actually deal with general inseparable data, but inseparable data in the sense that its basically separable. However, it really is this guy is bad, and this guy is bad. Theres nothing we can do about them. But there are few, so we will just settle for this. Well see that there are other cases of inseparable data that is truly inseparable, in which we have to do something a little bit more drastic. o thats as far as the classification is concerned. Now we go to linear regression. The word regression simply means realvalued output. There is absolutely no other connotation to it. ts a glorified way of saying my output is realvalued. And it comes from earlier work in statistics. And theres so much work on it that people could not get rid of that term. And it is now the standard term. Whenever you have a realvalued function, you call it a regression problem. o thats, with that, out of the way. Now, linear regression is used incredibly often in statistics and economics. Every time you say: are these variables related to that variable, the first thing that comes to mind is linear regression. Let me give an example. Lets say that you would like to relate your performance in different types of courses, to your future earnings. This is what you do. You look at here are the courses took. Here is the math, science, engineering, humanities, physical education, other. And you get your GPA in each of them. o here, got 3.5 Here, got 3.8 Here, got 3.2 Here, got 2.8 2.8? No, no. That doesnt happen at altech! You go for the other one, et cetera. o you just have the GPAs for the different groups of courses. Now, you say someone graduates. m going to look 10 years after graduation, and see their annual income. o the inputs are the GPAs in the courses at the time they graduated. The output is how much money they make per year 10 years away from graduation. Now you ask yourself: how do these things affect the output? o apply linear regression, as you will see it in detail, and you finally find maybe the math and sciences are more important. Or maybe all of that is an illusion. t was actually the humanities that are important. You dont know. You will see the data, and the data will tell you what affects what. And any other situation like that, people simply resort to linear regression. o in order to build it up, we are going to use the credit example again, in order to be able to contrast it with the classification problem we have seen before. What do we have? We have in the classification we have the credit approval, yes or no. Thats a classification function, binary function, which says the output is +1 or 1. n the case of regression, we will have realvalued function. And the interpretation in this case is that youre trying to predict the proper credit line for a customer. The customer applies. And its not a question of approving the credit or not. Do you give them credit limit of $800 or $1,200 or $30,000 or what, depending on their input? o this is a realvalued function. And we are going to apply regression. Now you take the input. This is the same input as we had before, data from the applicant that are related to the credit behavior, so the age, the salary. suspect that the salary will figure very significantly now when youre trying to tell the credit line, because if someone is making 30,000 a year, you probably are not going to give them a credit line of 200,000. o you can see that this will probably be affected. And there are other guys that merely have to do with the stability of the person. Years in residence. f the person has been in the same residence for 10 years, they are unlikely to skip town. On the other hand, if they have been there for only one month, well, you dont know that type of thing. o you have these variables. You encode them as the input x. And then your output in this case, which is the linear regression output, is a hypothesis form which takes this particular form. Lets spend some time with it to understand it. First, its regression because the output is real. ts linear regression because the form, in terms of the input, is linear. Now, we have seen this before. We sum up from basically 1 to d. These are the genuine inputs, the weighted version of the input variables. And then we add the mandatory x_0, which is 1, which takes care of the threshold, which is w_0. This is the form we have seen before, except that when we saw it before, we took this as a signal that we only care about its sign. f its plus, we approve credit. f its minus, we dont approve credit. And we treated it as a credit score, per se, when you take out the threshold. Now in this case, this is the output. We dont threshold it. We dont say its +1 or 1. There is w_0 in. But we dont take it as +1 or 1. We take it as a real number. And this is the dollar amount we are going to give you as a credit line. Now the signal here will play a very important role in all the linear algorithms. This is what makes the algorithm linear. And whether you leave it alone as in linear regression, you take a hard threshold as in classification or, as we will see later, you can take a soft threshold, and you get a probability and all of that All of these are considered linear models. And the algorithm depends on this particular part, which is the signal being linear. We also took the trouble to put it in vector form. And the vector form will simplify the calculus that we do in this lecture in order to derive the linear regression algorithm. But you can always if you hate the vector form, you can always go back to this. There is nothing mysterious about this. This simply has a bunch of parameters, w_0, w_1, up to w_d. And if m trying to minimize something, you can minimize it with respect to scalar variables, which applies very primitive calculus. But we obviously will do it in the shorthand version, which is the vector or the matrix form, in order to be able to get the derivation in an easier way. o thats the problem. What is the data set in this case? Well, its historical data, but its a different set of historical data. The credit line is decided by different officers. omeone sits down and evaluates your application and decides that this person gets 1000 limit, this person gets 5000 limit, and whatnot. All we are trying to do in this particular example is to replicate what theyre doing. We dont want the credit officer to do that. The credit officers sometimes are inconsistent from one another. They may have a good day or a bad day. o wed like to figure out what pattern they collectively have in deciding the credit, and have an automated system decide that. Thats what the linear regression system will do for us. The historical data here are again examples from previous customers. And the previous customers this is x_1, and this is y_1. o this is the application that the customer gave. And this is the credit line that was given to them. No tracking of credit behavior, were just trying to replicate what the experts do in this case. And then you realize that each of these ys is actually a real number, which is the credit line that is given to customer x_n. And that real number will likely be a positive integer. ts a credit line. ts a dollar amount. And what we are doing is trying to replicate that. Thats the statement of the problem. o what does linear regression do? First, we have to measure the error. We didnt talk about that in the case of classification, because it was so simple. Here, its a little bit less simple. And then, well be able to discuss the error function for classification as well. What do we mean by that? You will have an algorithm that tries to find the optimal weights. These are the weights youre going to have. These weights are going to determine what hypothesis you get. ome hypotheses will approximate f well. ome hypotheses will not. We would like to quantify that, to give a guidance to the algorithm in order to move from one hypothesis to another. o we will define an error measure. And the algorithm will try to minimize the error measure by moving from one hypothesis to the next. f you take linear regression, the standard error function used there is the squared error. Let me write it down. Well, if you had a classification, there is only a simple agreement on a particular example. You either got it right or got it wrong. There is nothing else. Therefore, in that case, we just defined binary error. Did you get it right or wrong? And we found the frequency of getting it right. And we got the E_in and E_out. Here, you are estimating a credit line. o if the guy gets 1000, and you tell them 900, thats not too bad. f the guy gets 1000, and you tell them 5000, thats bad. o you need to measure how bad the situation is. And you define an error measure, and you define it by the simple squared error. Now, squared error doesnt have an inherent merit here. t just happens to be the standard error function used with linear regression. And its merit really is the simplicity in the analytic solution that we are going to get. But when we discuss error measures in the next lecture, we will go back to the principle, does error measure matter? Why? How do we choose it? Et cetera. This will be answered in a principled way next time. But for this time, lets take this as a standard error measure we are going to use. When you look at the insample error, you use the error measure. On the particular example n, n from 1 to N. For each example, this is the contribution of the error. Each of these is affected by the same w, because h depends on w. o as you change w, this value will change for every example. And this is the error in that example. And if you want to get all the insample error, you simply take the average of those. That will give me a snapshot of how my hypothesis is doing on the data set. And now, we are going to ask our algorithm to take this error and minimize it. Lets actually just look at what happens as an illustration. This is the simplest case for linear regression. The input is onedimensional. have only one relevant variable. want to relate your overall GPA to your earnings 10 years from now. Your overall GPA is x. Your earnings 10 years from now is y. Thats it. OK? would have properly called this x_1 according to our notation. And then there would be an x_0, which is the constant 1. But didnt bother, because have only one variable. But this is what we have. o you look at this. And you see that, for different xs, you have these guys. Wow. Your earnings are going down with Well, that may not have been the example that is drawn here. What linear regression does is it tries to produce a line, which is what you have here, that tries to fit this data according to the squarederror rule. o it may look like this. And in this case, the threshold here depends on w_0. The slope depends on w_1, which is the weight for x. And that is the solution you have. Now you didnt get it right, but what you got is some errors. And you realize that this is the error on the first example. This is the error on the second example. And if you sum up the squares of the lengths of these bars, that is what we called the insample error that we defined in the previous viewgraph. Well, linear regression can apply to more than one dimension. And can plot 2 dimensions here just to illustrate it. ts the same principle. What you have here is you have x_1. f can get the pointer OK, well leave it to rest. We have x_1 and x_2. And in this case, the linear thing is really a plane. And youre again not separating, but trying to estimate these guys. And youre making errors. And in general, when you go to a higherdimensional space, the line which is the reason why we call it linear is not really a line. ts a hyperplane, one dimension short of the space you are working with. And thats what you are trying to use to approximate the guys. Now lets look at the expression for E_in. And that is the analytic expression we are going to try to minimize. And that will make us derive the linear regression algorithm. We wrote this before. And you have the value of the hypothesis minus y_n squared. That is because its a squared error. And because its linear regression, this value, h of x_n, happens to be w transposed x_n. ts a linear function of x_n. Now let us try to write this down in a vector form. will explain this in detail. But lets look at this. nstead of the summation, all of a sudden, have a norm squared of something that is apital X, havent seen capital X before. havent seen vector y before. Well, its basically a consolidation of the different x_ns here. x_n is a vector. o you put the vectors in a matrix. You call it X. And you put the scalars, the y_n, in a vector. And you call it y. The definition of capital X and the vector y is as follows. For the matrix X, what you do you put your first example here. o this would be the constant coordinate 1, the first coordinate, second coordinate, up to the dth coordinate, the last coordinate. And then you go for the second example, and do the same and construct this matrix. And for y, you put the corresponding output. This is the output for the first example, output for the second example, output for the last example. Now one thing to realize about the matrix X is that its pretty tall. The typical situation is that you have few parameters. We reduced them to three, for example, in the case of the classification of the digits. But you usually have many, many examples, in the 1000s. o this will be a very, very long matrix. Now the way you take this well, the norm squared will be simply this vector transposed times itself. And when you do it, you realize that what you are doing is summing up contributions from the different components. And each component happens to be exactly what you are having here. o this becomes a shorthand for writing this expression. Now, lets look at minimizing E_in. When you look at minimizing, you realize that the matrix X, which has the inputs of the data, and y, which has the outputs of the data, are, as far as we are concerned, constants. This is the data set someone gave me. The parameter m actually playing with in order to get a good hypothesis is w. o E_in is of w. And w appears here. And the rest are constants. f do any calculus of minimization, it is with respect to w. o try to minimize this. And what you do you get the derivative and equate it with 0, except here, its a glorified derivative. You get the gradient, which is the derivative on a bunch of them all at once. And there is a formula for it, which is pretty simple in this case. will explain it. By the way, if you hate this, and you want to make sure, because linear regression is so important. And you want to verify that its true, you can always go for the scalar form, get partial E by partial every w: partial w_0, partial w_1, partial w_d, get a formula that is a pretty hairy one, and then try to reduce it. And surprise, surprise you will get the solution here that we have in matrix form in two steps. Now if you look at this, deal with it in terms of calculus as if it was just a simple square. f this was a simple square, and w was the variable, what would the derivative be? You will get 2 sitting outside. Well, youve got it here. And then you will get the same thing in a linear form. You got it here. And then you will get whatever constant was multiplied by w to sit outside, which you got here. You just got here with a transpose, because this is really not a square. This is the transpose of this times itself. Thats where you get the transpose. Pretty straightforward and standard matrix calculus. o thats what you have. And then you equate this to 0, but its a fat 0. ts a vector of 0s. You want all the derivatives to be 0 all at once. And that will define a point where this achieves a minimum. Now, you would suspect that the solution will be simple, because this is a very simple quadratic form. And indeed, the solution is simple. And if you look at it, you realize that if want this to be 0, then want this to cancel out. o when multiply X transposed X w, get the same thing as X transposed y. o they cancel out, and get my 0. o you write this down, and you find that this is the situation. want this term to be equal to this term. And that will give me the 0. The interesting thing is that in spite of the fact that the matrix X is a very tall matrix, definitely not square, hence not invertible, X transposed X is actually a square matrix, because X transposed is this way and X is this way. You multiply them, and you get a pretty small square matrix. And as we will see, the chances are overwhelming that it will be invertible. o you can actually solve this very simply, by inverting this. You multiply by the inverse in this direction. You multiply by this. This will disappear, and you will get an explicit formula for w, which you were trying to solve for. And when you do that, you will get w equals this funny symbol, X dagger. What is X dagger? This is simply a shorthand for writing this. o got the inverse of that, and then multiplied it by here. o this is really what get to be multiplied by y. call it X dagger. And indeed, it gets multiplied by y to give me my w. Now the X dagger is a pretty interesting notion. ts called the pseudoinverse of X. X, being a noninvertible matrix, does not have an inverse. But it does have a pseudoinverse. And the pseudoinverse has interesting properties. For example, if you take this, the X dagger, and multiply it by X so X dagger times X what do you get? You add X here. You get X transposed X. Oh, have X transposed X to the 1 here. o they cancel out, and get an identity. o when multiply X dagger by X, get the identity. o its OK to call it an inverse of sorts. t doesnt work the other way around. The other way around gives us an interesting matrix, which well talk about later. But basically, this is the essence of it. f we were in a trivial situation where X was a square have 3 parameters, and have 3 examples to determine them that can be solved perfectly. can actually get this to be 0. And how would you get it to be 0? You would just multiply by the proper inverse of X in this case, and you will get X inverse y. o this is pretty much similar, when X is a tall one. And we are not going to get a 0. Were just going to get a minimum using the pseudoinverse. Now would like you to appreciate the pseudoinverse from a computational point of view. This is the formula for the pseudoinverse that you will need to compute, in order to get the solution for linear regression. o lets look at it. omething is inverted. And when you see inversion in matrix, you say, oh, computation, computation. f this was a million by a million, m in trouble. f this is 5 by 5, m in good shape. o wed like to know, what kind of matrix do we have here? Well, nothing mysterious about whats inside this. You have this fellow, which is X transposed. ts d plus 1, d is the length of your input, 1 is the added constant variable. o these are the number of parameters. This would be 3 in the digit classification guy. We have only x_1 and x_2, so d equals 2. d plus 1 equals 3, which corresponds to x_0, x_1, x_2, or to w_0, w_1, w_2. o this is 3 times N. N is the scary one. Thats the number of examples. That could be in the thousands. Now you multiply this by X, and thats what you have. The multiplication will be multiplication is not that difficult. Even if this is 10,000, can multiply this by 10,000. But the good news is that when go to this guy, will have to be dealing with a simpler guy. Lets just complete the formula first. This is what you have. This is what you are computationally doing. And if you look at whats inside here, it completely shrinks. That is what the matrix inside is. ts just 3 by 3 in our case. You can invert that. Just accumulating it is the one that you have to go through all of the examples. And theres a very simple way of doing it. ts not that difficult to get this fellow. And you can see now that, oh, good thing that we had 3 parameters. f we had the 257 parameters to begin with, this would have been 257 by 257. Not that this will discourage us. But if you go for some raw inputs, you can get something really in the thousands or sometimes even more than that. o the computational aspect of this is very simple. And there are so many packages for computing the pseudoinverse, or outright getting the solution for linear regression, that you will never have to do that yourself, except if youre doing something very specialized. f you do have something very specialized, its not that bad. o that is the final matrix. And the final matrix will have the same dimensions as this guy. And if you look at it, this will be multiplied by what? ultiplied by y, which is y_1, y_2, y_3, y_N, corresponding to different outputs. And then, as a result of that, you will get the ws w_0, w_1, up to w_d. ndeed, if you multiply this by an N tall vector, you will get a d plus 1 tall vector, and thats what we expect. Lets now flash the full linear regression algorithm here. Thats a crowded slide. That is what you do. The first thing is you take the data that is given to you, and put them in the proper form. What is the proper form? You construct the matrix X and the vector y. And these are what we introduced before. This will be the input data matrix, and this will be the target vector. And once you construct them, you are basically done, because all you are going to do you plug this into a formula, which is the pseudoinverse. And then you will return the value w, that is the multiplication of that pseudoinverse with y. And you are done. Now you can call this onestep learning if you want. With the perceptron learning algorithm, it looked more like learning, because have an initial hypothesis. And then take one example at a time, and try to figure out what is going on, move this around, et cetera. And after 1000 iterations, get something. t looks more like what we learn. We learn in steps. This looks like cheating. You give me the thing, and . And you have the answer. Well, as far as we are concerned, we dont care how you got it. f its correct and gives you a correct E_out, you have learned. And because this is so simple, this is a very popular algorithm that is used often, and used often as a building block for other guys. We can afford to use it as a building block, because the step here will be so simple that we can become more sophisticated in using it. Just one remark about the inversion this has to be invertible in order for this formula to hold. Now the chances are, that this will be invertible in a real application you have, is close to 1. The reason is the following. sually, you use very few parameters and tons of examples. You will be very, very, very unlucky to have these so dependent on each other that you cannot even capture the dimensionality which is the number of columns. The number of columns is 3, 5, 10, and you have 10,000 of those. o the chances are overwhelming in a real problem that this will be invertible. Nonetheless, if it is not invertible, you can still define the pseudoinverse. t will not be unique and has some elaborate features, but its not a big deal. That is not a situation you will encounter in practice. o now we have linear regression. m going to tell you that you can use linear regression not only for a realvalued function, for regression problems. But youre also going to be able to use it for classification. aybe the perceptron is now going out of business. t has a competitor now. And the competitor has a very simple algorithm. o lets see how this works. The idea is incredibly simple. Linear regression learns a realvalued function. Yeah, we know that. That is the realvalued function. The value belongs to the real numbers. Fine. Now the main observation, the ingenious observation, is that binaryvalued functions, which are the classification functions, are also realvalued. +1 and 1, among other things, happen to be real numbers. o linear regression is not going to refuse to learn them as real numbers. Right? o what do we do? You use linear regression in order to get a solution, such that the solution is approximately y_n in the mean squared sense. For every example, the actual value of the signal is close to the numerical +1 and the numerical 1. Thats what linear regression does. Now, having done that with y_n equals +1 or 1, you realize that in this case, if you take the classification version of it you take the sign of that signal in order to be able to classify as +1 or 1. f the value is genuinely close to +1 or 1 numerically, then the chances are when its +1, this would be positive. And when its 1, its negative. The chances are youre getting close to a number, youll probably cross the zero in doing that. And if you cross the zero, the classification will be correct. o if you take this, and then plug it in as weights for classification, you will likely get something that will give you likely to agree with +1 or 1. Thats a pretty simple trick, because its almost free. All you need to do have a classification problem. Lets run linear regression. ts almost for free. Do this onestep learning, get a solution, and use it for classification. Now, lets see if this is as good as it sounds. Well, the weights are good for classification, so to speak, just by conjecture. But they also may serve as good initial weights for classification. Remember that the perceptron algorithm, or the pocket algorithm, are really very slow to get there. You start with a random guy. Half the guys are misclassified. And it just goes around, tries to correct one, messes up the others, until it gets to the region of interest. And then it converges. Why not give it a jump start? Why not run linear regression first, get the ws. We know that the ws are OK, but they are not really tailored toward classification. But theyre good initial condition. Feed those to the pocket algorithm, and let it run to the solution, which is a classification solution. Thats a pretty nice idea. o lets actually look at the linear regression boundary. Now, take an example here. Again, have the +1 class and the 1 class. And applied were trying to find, what is the linear regression solution? Now, we remember, the blue region and the pink region belong to classification. When you talk about linear regression, you have the value here. And the signal is 0 here. The signal is positive, more positive, more positive, more positive. And here, the signal is negative, more negative, more negative, more negative. There is a realvalued function that we are trying to interpret as a classification by taking the sign. Now, if you look at what the linear regression is trying to do when you use it for classification, all of these guys have a target value 1. t is actually trying to make the numerical value equal 1 to all of them. o the chances are, these will be 1. This will be 2, 3. And the linear regression algorithm is very sad about that. t considers it an error, in spite of the fact that, when we plug it into the classification, it just has the correct sign. And thats all we care about. But we are applying linear regression. t is actually trying very hard to make all of them 1 at the same time, which obviously it cannot. And you can see now the problem with linear regression. n its attempt to make this 8, 1, it moved the boundary to the level where its in the middle of the red region. And now, its very happy because it minimized its error function. But thats not really the classification. Nonetheless, its a good starting point. And then you take the classification now, that forgets about the values and tries to adjust it according to the classification. And you will get a good boundary. Thats the contrast between applying linear regression for classification and linear classification outright. Now we are done. m going to start on nonlinear transformation. And m going to give you a very interesting tool to play with. Here is the deal. You probably realized that, even when dealing with nonseparable data, we are dealing with nonseparable data that are really basically separable with few exceptions. But in reality, when you take a real problem, a reallife problem, you will find that the data you are going to get could be anything. t could be, for example, something that looks like this. o you want to classify these as +1s and these as 1s. Lets take the classification paradigm here. Now can put the line anywhere. And obviously, m in trouble because this is not linearly separable, even by a long shot. You can look at this and say: can see the pattern here. loser to the center, you have blues. loser to the peripherals, you have reds. o it would be very nice if could apply a hypothesis that looks like this. Yes. The only problem is that thats not linear. We dont have the tools to deal with that, yet. Wouldnt it be nice if in two viewgraphs, you can use linear regression and linear classification, the perceptron or the pocket, to apply it to this guy? Thats what will happen. told you this is a practical lecture. o we take another example of nonlinearity. We take the credit line. Now if you look at the credit line, the credit line is affected by years in residence. We argued that if someone has been in the same residence for a long time, there is stability and trustworthiness. And someone has been a short time, theres a question mark. Now one thing is to say that this is a variable that affects the output. Another thing to say is that this is a variable that affects the output linearly. t would be strange if m trying to determine a credit line, to decide that the credit line will be proportional to the time you have lived in residence. f you have 10 years, 20 years, will give you twice the credit line. t doesnt make sense. Because stability is established probably by the time you get to 5 years. After that, its diminishing returns. o it would be very nice if can instead of using the linear one, define nonlinear features, which is the following. Lets take the condition, the logical condition, that the years in residence are less than 1. And in my mind, m considering that this is not very stable. You havent been there for very long. And another guy, which is x_i greater than 5, you have been there for more than 5 years. o you are stable. The notation here, when put something between these brackets, means that this returns 1 if the condition is true, and returns 0 if the condition is false. o this is 1, 0, and this is 1, 0. Now if had those as variables in my linear regression, they would be much more friendly to the linear formula in deciding the credit line, rather than the crude input. But these are nonlinear functions of x_i. And again, we have the nonlinearity. And we wonder if we can apply the same techniques to a nonlinear case. This is the question. an we use linear models? The key question to ask is, linear in what? What do mean? Look at linear regression. What does it implement? t implements this. This is indeed a linear formula. And when you look at the linear classification counterpart, it implements this. This is a linear formula, and the algorithm being simple depends on this part being linear. And then you just make a decision based on that signal. Now, these you would think are called linear because they are linear in the xs, which they are. Yeah, get these inputs. And combine them linearly. And get my surface. Thats why m calling it linear. However, you will realize that, more importantly, these guys are linear in w. Now when you go from the definition of a function to learning, the roles are reversed. The inputs, which are supposed to be the variable when you evaluate a function, are now constants. They are dictated by the training set. Theyre just a bunch of numbers someone gave me. The real variables, as far as learning is concerned, are the parameters. The fact that its linear in the parameters is what matters in deriving the perceptron learning algorithm, and the linear regression algorithm. f you go back to the derivation, it didnt matter what the xs were. The xs were sitting there as constants. And their linearity in w is what enabled the derivation. That results in the algorithm working, because of linearity in the weights. Now that opens a fantastic possibility, because now can take the inputs, which are just constants. omeone gives me data. And can do incredible nonlinear transformations to that data. And it will just remain more elaborate data, but constant. When get to learn using the nonlinearly transformed data, m still in the realm of linear models, because the weight that will be given to the nonlinear feature will have a linear dependency. Lets look at an example. Lets say that you take x_1 and x_2. omitted the constant x_0 here, for simplicity. And these are the guys that gave us trouble. These are the coordinates. This is x_1. This is x_2. These guys should map to +1. These guys should map to 1. dont have a linear separator. OK, fine. These are data, right? o everything that appears within this box is just a bunch of constant xs and corresponding constants y. Now m going to take a transformation. m going to call it phi. Every point in that space, m going to transform to another space. And my formula for transformation will be this. m assuming here that the origin of the coordinate system is here. o m taking x_1 squared and x_2 squared. And you can see where m leading, because now m measuring distances from the origin. And that seems to be a helpful guy here. Now in doing this, all did was take constants and produce other constants. Now, you can look at this and say: this is my training data. take your original training data, do the transformation, and forget about the original one. an you solve the problem in the new space? Oh, yes you can, because thats what they look like in the new space. All of a sudden, the red guys, which happen to be far away, will have bigger values for x_1 squared and x_2 squared. They will sit here. And the guys that are closer to the origin, by the time they transform them, they will have smaller values here. o this is now your new data set. an you separate this using a perceptron? Yes, can. can put a line going through here. Great. When you get a new point to classify, transform it the same way, classify it here, and then report that. Thats the game. And there is really no limit, at least computationally, in terms of what you can do here. You can dream up really elaborate nonlinear transformations, transform the data, and then do the classification. There is a catch. And its a big catch. will stop here. And well continue with the nonlinear transformation at the beginning of the next lecture. And well take a short break now, before we go to the QA session. We have from the online audience. ODERATOR: A popular question is how to figure out in a systematic way the nonlinear transformations, instead of from the data. PROFEOR: said that the nonlinear transformation is a loaded question. And there will be two steps in dealing with it. will talk about it a little bit more elaborately at the beginning of next lecture. And then we are going to talk about the guidelines for choice, and what you can do and what you cannot do, after we develop the theory of generalization because it is very sensitive to the generalization issue. And that should not come as a surprise, because can see that can take the input, which is, lets say, two variables corresponding to two parameters. And want the transformation to be as elaborate as possible, in order to stand a good chance of being able to separate them linearly. o m going to go all out. m just going to keep getting nonlinear coordinates x_1, x_1 squared, x_1 cubed, x_1 squared x_2, e to the x, just go on. Now at some point, you should smell a rat, because you realize that have this very, very long vector and corresponding number of parameters. And generalization may become an issue, which it will become an issue. o there are guidelines for how far you can go. And also, there are guidelines for how you can choose them. Do look at the data and figure out what is a good nonlinear transformation? s this allowed? s this not allowed? What the ramifications are? All of these will become clear only after you look at the theory part. ODERATOR: OK. Theres a question about slide 15. o regarding the expression of E_in. How does the insample error here, or the outofsample error, relate to the probabilistic definition of last time? PROFEOR: OK. Here we dealt only with the insample error. o we decided on E_in. And in general in learning, you only have the insample error to deal with. You have on the side a guarantee that when you do well insample, you will do well outofsample. o you never handle the outofsample explicitly. You just handle the insample, and have the theoretical guarantee that what you are doing will help you outofsample. Now, the error measure here was a squared error. Therefore, when you define the insample error, you get the squared error and average it. And when you define the outofsample error, its really the expected value of the squared error. Now in the case of the binary classification, the error was binary. Youre either right or wrong. o you can always define the insample error as also the average of the question. Am right or wrong on every point? o if you are right, theres no error and you get 0. f you are wrong, you get 1. o you ask yourself: what is the frequency of 1s insample? And that would give you the insample error. The expected value of that error happens to be the probability of error. Thats why we simply, without going into expectation and insample average versus outofsample expected value in the case of classification, we simply talked about frequency of error and probability of error, not because they are different, but just because they are simple to state. But in reality, the aspect of them that made them qualify as insample and outofsample is that the probability is the expected value of an error measure that happens to be a binary error measure. And the frequency of error happens to be the average value of that error measure. TDENT: o you showed us a very nice graph with negative slope about dependence of future income and PROFEOR: This is unintentional. didnt think of the income at the time drew the graph. o any implication that you should really do worse in school in order to gain more money is disown any such conclusion! TDENT: OK. But you mentioned the example of determining future income from grade point average, or at least finding some correlation. o the question m interested in is, where can we get data? PROFEOR: You can get obviously, the alumni association of every school keeps track of the alumni. And they send them questionnaires. And they have some of the inputs, and how much money they make. There are a number of parameters. o there will be a number of schools that have that. And actually, this is actually used. f you realize that something is related to success or something, you can go back and revise your curriculum or revise your criteria. o the data is indeed available, if thats the question. TDENT: mean, its available in principle. But can we get it? PROFEOR: Oh, we get it. thought it was generic we. dont obviously, the data will be anonymous after a while. Youll just get the GPA and the income, without knowing who the person is. You are dependent on the kindness of the alumni associations at different schools, guess. Or maybe there are some available in public domain. have not looked. o my understanding is that you want to run linear regression, see what happens, and then focus your time on the courses that matter. Thats the idea now? Thats your feedback? ODERATOR: A technical question. Why is the w_0 included in the linear regression. o theres a confusion about this. And also in that point, what do you do specifically in the binary case? How do you incorporate the +1s or 1? Theres some people asking about this. PROFEOR: Let me answer one at a time. ll talk about the threshold first. Why the threshold is there, right? Lets look here. f you look at the line here, the linear regression line. The linear regression line is not a homogeneous line. t doesnt pass by the origin. f told you that you cannot use a threshold, then the constant part of the equation goes away, and the line you have will have to pass through the origin. an you imagine if you were trying to fit this with a line? Obviously, it would be down there if you have the negative slope, or if you want to pass through the points up there. o obviously, need the constant in order to get a proper model. And in general, there is an offset depending on the values of these variables. And the offset is compensated for by the threshold. Thats why we need the threshold for linear regression. What is the second question? ODERATOR: n the binary case, when you use y as +1 or 1, why does that just work? PROFEOR: Well, if you apply linear regression, you have the following guarantee at the end. The hypothesis you have has the least squared error from the targets on the examples. Thats what has been achieved by the linear regression algorithm. Now the outputs of the examples being +1 or 1, we can put that together with the first statement. And then we realize that the output of my hypothesis is closest to the value +1 or 1 with a mean squared error. The leap of faith is that, if you are close to +1 versus 1, then the chances are when you are close to +1, you are at least positive. And when you are close to 1, you are at least negative. f you accept that leap of faith, then the conclusion is that, when you take the threshold of the value of the signal from linear regression, you will get the classification right because positive will give you +1. Negative will give you 1. This is not quite the case, because in the attempt to numerically replicate all the points, the signal for linear regression can become lets say as mentioned, +7 for some points and 7 for another point. And the linear regression is trying to push the w, which is what will end up being the boundary, in order to capture that numerical value. o in attempting to fit stuff that is irrelevant to the classification, it may mess up the classification. And thats why the suggestion is, dont use it as a final thing for classification. Just use it as an initial weight, and then use a proper classification, something as simple as the pocket algorithm, in order to finetune it further in order to get the classification part, without having to suffer from the numerical angle. ODERATOR: o also on that, does it make a difference what you use? +1, 1, or something else? PROFEOR: OK. f its plus something and minus the same thing, its a matter of scale. f its plus and minus, and not symmetric, it will be absorbed in the threshold. o it really doesnt matter. t will just make things look different. ODERATOR: Regarding the first part of the lecture, how do you usually come up with features? PROFEOR: OK. The best approach is to look at the raw input, and look at the problem statement, and then try to infer what would be a meaningful feature for this problem? For example, the case where talked about the years in residence. t does make sense to derive some features that are closer to the linear dependency. There is no general algorithm for getting features. This is the part where you work with the problem, and you try to represent the input in a better way. And the only catch is, if you look at the data in order to try to derive the features, there is a problem there that will become apparent when we come to the theory. But the bottom line is that, if you dont look at the data, and you study the problem and derive features based on that, that will almost always be helpful if you dont have too many of them. f you have too many of them, it starts becoming a problem. But something first order, usually when get a problem, look at the data. And probably can think of less than a dozen variables that will be helpful. And put all of them. And usually, a dozen variables in this case doesnt increase the input space by much. These are big problems. o dont suffer much from the generalization issue. ODERATOR: o added to that, a short clarification so the nonlinear transformations they become features? PROFEOR: Yeah. The word feature, we are going to use. Theres a feature space which is called Z. And anything that you take the input and transform it into something else, this will be called feature. And features of features will also be features. o if you take for example the classification of the digits, we had the pixel values. Thats the raw input. And then we had the symmetry and the intensity. These were features. f you go further and find nonlinear transformations of those, these will also be called features. A feature is any higherlevel representation of a raw input. ODERATOR: Another question is: how does this analysis change if we cannot assume that the data if theyre not independent. PROFEOR: Not clear about the question. o there is really think get it. Probably when we get the inputs, the question is independence versus dependence. And the independence was used in getting the generalization bound. Thats probably the direction of the question. The independence was from one data point to another. o have N inputs. And want these guys to be generated independently, according to a probability distribution. f they were originally independent, and transformed one of them and transformed the other, the independence is inherited. There is no question of independence between coordinates of the same input. The independence was a question of the independence between the different inputs. ODERATOR: o the different inputs. PROFEOR: Different input points. ODERATOR: o another question is, are there methods that use different hyperplanes and intersections of them to separate data? PROFEOR: orrect. The linear model that we have described is the building block of so many models in machine learning. You will find that if you take a linear model with a soft threshold, not the hardthreshold version, and you put a bunch of them together, you will get a neural network. f you take the linear model, and you try to pick the separating boundary in a principled way, you get support vector machines. f you take the nonlinear transformation, and you try to find a computationally efficient way of doing it, you get kernel methods. o there are lots of methods within machine learning that build on the linear model. The linear model is somewhat underutilized. ts not glorious. ts not glorious, but it does the job. The interesting thing is that if you have a problem, there is a very good chance that if you take a simple linear model, you will be able to achieve what you want. You may not be able to brag about it. But you are going to do the job. And obviously, the other models will give you incremental performance in some cases. ODERATOR: o a question, getting a little bit ahead how do you assess the quality of E_in and E_out systematically? PROFEOR: This is a theoretical question. E_in is very simple. have the value of E_in. can assess its value by just looking at its value. can evaluate it at any given point. And this is what makes the algorithm able to pick the best insample hypothesis, by picking the one that has the smallest insample error. The outofsample error, dont have access to. There will be some methods described after the theory that will give us an explicit estimate of the outofsample error. But in general, rely on the theory that guarantees that the insample error tracks the outofsample error, in order to go all out for the insample error, and hope that the outofsample error follows, which we have seen in the graph when we were looking at the evolution of the perceptron. And the insample error was going down and up. And the outofsample error was also going down and up, albeit with a discrepancy between the two. But they were tracking each other. ODERATOR: o heres a question thats kind of a confusion. f you want to fit a polynomial, is this still a linear regression case? PROFEOR: orrect. Because right now, lets say we have a single input variable, x, like the case gave. o you have x and y. Now you have a line. f you use the nonlinear transformation, you can transform this x to x, x squared, x cubed, x to the fourth, x to the fifth, and then fit a line to the new space. And a line in the new space will be a polynomial in the old space. o this is covered through the nonlinear transformation. ODERATOR: What is the relation between linear regression least squares with maximum likelihood estimation. PROFEOR: OK. When you look at linear regression in the statistics literature, there are many more assumptions about the probabilities and what the noise is. And you can get actually more results about it. nder certain conditions, you can relate it to the maximum likelihood. You can say, Gaussian goes with the squared error. And in this case, minimizing it will correspond to maximum likelihood. o there is a relationship. On the other hand, prefer to give the linear regression in the context of machine learning, without making too many assumptions about distributions and whatnot, because want it to be applied to a general situation rather than applied to a particular situation. As a result of that, will be able to say less in terms of what is the probability of being right or wrong. just have the generalization from insample and outofsample. But that suffices for most of the machine learning situation. o there is a relationship. And its studied fairly well in other disciplines. But it is not of particular interest to the line of logic that m following. ODERATOR: o a popular question is: can you give at least a set of usual nonlinear transformations used? PROFEOR: There will be many. When we get to support vector machines, we will be dealing with a number of transformations, some of them polynomials like the ones that were mentioned. One of the useful ones is referred to as radial basis functions. We will talk about that as well. o there will be transformations. And the main point is to be able to understand what you can and what you cannot do, in terms of jeopardizing the generalization performance by taking a nonlinear transformation. o after we are done with that theory, we will have a significant level of freedom of choosing what nonlinear transform we use. And well have some guidelines of some of the famous nonlinear transforms. o this is coming up. ODERATOR: think you already answered this question last time. But again, someone asks, is it impossible for machine learning to find a pattern of a pseudorandom number generator? PROFEOR: Well, if its pseudo random, then in principle, if you get the seed, you can produce it. But the way its usually used is you use a pseudorandom number, and then you take a few bits and have them as an output for different inputs. o just looking at the inputs and trying to decipher it its next to impossible. o its a practical question. Philosophically, yes you can. Practically, it looks random for all intents and purposes. ODERATOR: o what are the different treatments for continuous responses versus discrete responses in guess PROFEOR: Yeah. Obviously, this is dictated by the problem. f someone comes, and they want to approve credit, etc, m going to use the classification hypothesis set. f someone wants to get a credit line or something else, then will have to use regression. o it really is dependent on the problem. And the funny part is that real numbers look more sophisticated. Yet the algorithm that goes with them, which is linear regression, is much easier than the other one. The reason is that the other one is combinatorial. And combinatorial optimization is pretty difficult in general. o the answer to the question is that it depends on the target function that the person is coming up with. And when there is cross fertilization between the techniques, its just a way to use an analytic advantage from one method to give the other one a jump start, or to give it a reasonable solution. But its a computational question. The distinction is really in the problem statement itself. ODERATOR: an you say what makes a nonlinear transformation good? PROFEOR: OK. will be able to talk about this a little bit more intelligently after the theory. would like to emphasize that the theory part will be very important in giving us all the tools to talk, with authority, about all the issues that are being raised. o there is a reason for including the theory before we go into more details. This lecture was meant to give you just a little bit of standard tools that you use, and if you look at it now, you can use for many applications and many data sets, because now you can deal with nonseparable data. You can deal with realvalued data. And you can even deal with some nonlinear situations. o its just a toolbox for you to get your hands wet. And then things will become more principled when we develop more material. ODERATOR: Yeah, think thats it. PROFEOR: OK, thats it. We will see you on Thursday.","Now, having done that with y_n equals +1 or 1, you realize that in this case, if you take the classification version of it you take the sign of that signal in order to be able to classify as +1 or 1. f the value is genuinely close to +1 or 1 numerically, then the chances are when its +1, this would be positive. f you accept that leap of faith, then the conclusion is that, when you take the threshold of the value of the signal from linear regression, you will get the classification right because positive will give you +1. And the only catch is, if you look at the data in order to try to derive the features, there is a problem there that will become apparent when we come to the theory. And this is the error in that example. Now, if you look at what the linear regression is trying to do when you use it for classification, all of these guys have a target value 1. t is actually trying to make the numerical value equal 1 to all of them. And then we are going to talk about the guidelines for choice, and what you can do and what you cannot do, after we develop the theory of generalization because it is very sensitive to the generalization issue. And in order to be able to tell what E_out of h is h is the hypothesis that corresponds to that particular bin we look at the insample. This is what you have. The first thing is you take the data that is given to you, and put them in the proper form. And you realize that this is the error on the first example. This is the formula for the pseudoinverse that you will need to compute, in order to get the solution for linear regression. The interesting thing is that if you have a problem, there is a very good chance that if you take a simple linear model, you will be able to achieve what you want. When you look at minimizing, you realize that the matrix X, which has the inputs of the data, and y, which has the outputs of the data, are, as far as we are concerned, constants. But the bottom line is that, if you dont look at the data, and you study the problem and derive features based on that, that will almost always be helpful if you dont have too many of them. But in general, rely on the theory that guarantees that the insample error tracks the outofsample error, in order to go all out for the insample error, and hope that the outofsample error follows, which we have seen in the graph when we were looking at the evolution of the perceptron. Because if there is a 0.5% chance that the first hypothesis is bad, in the sense of bad generalization, and 0.5% for the second one, we could be so unlucky as to have these 0.5% accumulate, and end up with a significant probability that one of the hypotheses will be bad. And then you will return the value w, that is the multiplication of that pseudoinverse with y. And you are done. But we obviously will do it in the shorthand version, which is the vector or the matrix form, in order to be able to get the derivation in an easier way. o this is 1, 0, and this is 1, 0. You take the negative of that, and you get the symmetry. And the vector form will simplify the calculus that we do in this lecture in order to derive the linear regression algorithm. This is what you do. But all you need to know here is that E_out is drawn here for illustration, just to tell you what is happening in reality as you work on the insample error. But this is what we have. And if you sum up the squares of the lengths of these bars, that is what we called the insample error that we defined in the previous viewgraph. Wouldnt it be nice if in two viewgraphs, you can use linear regression and linear classification, the perceptron or the pocket, to apply it to this guy? This is the part where you work with the problem, and you try to represent the input in a better way. Now the chances are, that this will be invertible in a real application you have, is close to 1. And therefore, you will be able to bound the probability that you care about, which has to do with the generalization, to the individual Hoeffding applied to each of those. And whether you leave it alone as in linear regression, you take a hard threshold as in classification or, as we will see later, you can take a soft threshold, and you get a probability and all of that All of these are considered linear models. That is what you do. And the linear regression is trying to push the w, which is what will end up being the boundary, in order to capture that numerical value. The fact that its linear in the parameters is what matters in deriving the perceptron learning algorithm, and the linear regression algorithm. Now you multiply this by X, and thats what you have. Now you realize that its impossible really to ask to get all of them right because, believe it or not, this fellow is a 5, at least meant to be a 5 by the guy who wrote it. Just accumulating it is the one that you have to go through all of the examples. And the main point is to be able to understand what you can and what you cannot do, in terms of jeopardizing the generalization performance by taking a nonlinear transformation. And in general, when you go to a higherdimensional space, the line which is the reason why we call it linear is not really a line. o in order to build it up, we are going to use the credit example again, in order to be able to contrast it with the classification problem we have seen before. And if you look at it, this will be multiplied by what? Now, lets look at a real data set that we are going to use, and will be available to you to try different ideas on. o if you are right, theres no error and you get 0. f you are wrong, you get 1. o you ask yourself: what is the frequency of 1s insample? And this is the dollar amount we are going to give you as a credit line. And if you look at the classification boundary, PLA versus pocket, this is what we had with the perceptron learning algorithm. This lecture was meant to give you just a little bit of standard tools that you use, and if you look at it now, you can use for many applications and many data sets, because now you can deal with nonseparable data. The slope depends on w_1, which is the weight for x. And that is the solution you have. But when you are at the end, you keep this guy and report it as the final hypothesis. o if you simply add up the intensity of all the pixels, you probably will get a number that is related to the identity. And now, we are going to ask our algorithm to take this error and minimize it. And when one of the hypotheses will be bad, if we are further unlucky, and this is the hypothesis we pick as our final hypothesis, then E_in will not track E_out for the hypothesis we pick. f you take this raw input and try the perceptron directly on it, you realize that the linear model in this case, which has a bunch of parameters, has really just too many parameters. And its merit really is the simplicity in the analytic solution that we are going to get. And if you look at it, you realize that if want this to be 0, then want this to cancel out. f you use the nonlinear transformation, you can transform this x to x, x squared, x cubed, x to the fourth, x to the fifth, and then fit a line to the new space. And when you look at the other guy, which is the pocket algorithm, it looks better. f told you that you cannot use a threshold, then the constant part of the equation goes away, and the line you have will have to pass through the origin. What linear regression does is it tries to produce a line, which is what you have here, that tries to fit this data according to the squarederror rule. And as a suggestion, very simple one, lets say that in this particular case, instead of giving the raw input with all of the pixel values, you extract some descriptors of what the image is like. o the answer to the question is that it depends on the target function that the person is coming up with. o this is what we are going to work with. And then, if you remember the perceptron that we introduced in the first lecture, the perceptron is a linear model. But in reality, the aspect of them that made them qualify as insample and outofsample is that the probability is the expected value of an error measure that happens to be a binary error measure. And if you want to get all the insample error, you simply take the average of those. Now in this case, this is the output. This is the question. When you look at linear regression in the statistics literature, there are many more assumptions about the probabilities and what the noise is. This is the same input as we had before, data from the applicant that are related to the credit behavior, so the age, the salary. And then you take the classification now, that forgets about the values and tries to adjust it according to the classification. And there is a formula for it, which is pretty simple in this case. o if you take this, and then plug it in as weights for classification, you will likely get something that will give you likely to agree with +1 or 1. However, you will realize that, more importantly, these guys are linear in w. Now when you go from the definition of a function to learning, the roles are reversed. And you can see now the problem with linear regression. And once you construct them, you are basically done, because all you are going to do you plug this into a formula, which is the pseudoinverse. And then we realize that the output of my hypothesis is closest to the value +1 or 1 with a mean squared error. o this is a data set that we are going to work with. And we are not going to get a 0. What is the data set in this case? And linear regression, as you will see, is one of the most important techniques that is applied mostly in statistics and economics, and also in machine learning. You would just multiply by the proper inverse of X in this case, and you will get X inverse y. o this is pretty much similar, when X is a tall one. This will be the input data matrix, and this will be the target vector. And this is the credit line that was given to them. And then you go for the second example, and do the same and construct this matrix. suspect that the salary will figure very significantly now when youre trying to tell the credit line, because if someone is making 30,000 a year, you probably are not going to give them a credit line of 200,000. o you can see that this will probably be affected. You will see the data, and the data will tell you what affects what. And that is one of the advantages of something as simple as the perceptron learning algorithm. The best approach is to look at the raw input, and look at the problem statement, and then try to infer what would be a meaningful feature for this problem? And this is what makes the algorithm able to pick the best insample hypothesis, by picking the one that has the smallest insample error. We are going to take the perceptron and generalize it to nonseparable data. And what we are going to do in this lecture, were going to start with a practical data set that we are going to use over and over in this class. And when you do it, you realize that what you are doing is summing up contributions from the different components. The leap of faith is that, if you are close to +1 versus 1, then the chances are when you are close to +1, you are at least positive. When get to learn using the nonlinearly transformed data, m still in the realm of linear models, because the weight that will be given to the nonlinear feature will have a linear dependency. You encode them as the input x. And then your output in this case, which is the linear regression output, is a hypothesis form which takes this particular form. And in general in learning, you only have the insample error to deal with. This is only the illustration here, its just this is the evolution. What you have here is you have x_1. And we call this g, the final hypothesis of the perceptron learning algorithm. You just handle the insample, and have the theoretical guarantee that what you are doing will help you outofsample. But thats what you have to live with, if you apply the perceptron learning algorithm. But in reality, when you take a real problem, a reallife problem, you will find that the data you are going to get could be anything. And then you realize that each of these ys is actually a real number, which is the credit line that is given to customer x_n. And thats what you are trying to use to approximate the guys. This is not quite the case, because in the attempt to numerically replicate all the points, the signal for linear regression can become lets say as mentioned, +7 for some points and 7 for another point. We reduced them to three, for example, in the case of the classification of the digits. o when multiply X transposed X w, get the same thing as X transposed y. o they cancel out, and get my 0. o you write this down, and you find that this is the situation. And there are other guys that merely have to do with the stability of the person. And how would you get it to be 0? Now, you can look at this and say: this is my training data. o now we go and try to modify the perceptron learning algorithm in a very simple way, that is the simplest modification you can ever imagine. Here, we didnt, because the good guy is in our pocket and thats what were reporting the value for. o in spite of the fact that youre doing these iterations according to the prescribed perceptron learning algorithm rule modify the weights according to one misclassified point you can keep track of the total insample error of the intermediate hypothesis you got. When you take the linear model in this case, you just have w_0, w_1, and w_2. By the way, if you hate this, and you want to make sure, because linear regression is so important. On the particular example n, n from 1 to N. For each example, this is the contribution of the error. And there are so many packages for computing the pseudoinverse, or outright getting the solution for linear regression, that you will never have to do that yourself, except if youre doing something very specialized. You construct the matrix X and the vector y. And these are what we introduced before. Now one thing is to say that this is a variable that affects the output. When you talk about linear regression, you have the value here. And you would like to take the image, which happens to be 16 by 16 gray level pixels, and be able to decipher what is the number in it. And as we will see, the chances are overwhelming that it will be invertible. Now the reason the algorithm is called the pocket algorithm is because the whole idea is to put the best solution so far in your pocket. o when you put all the 1s and all the 5s in a scatter diagram, you realize for example that the intensity on the 5s is usually more than the intensity on the 1s. Thats why we simply, without going into expectation and insample average versus outofsample expected value in the case of classification, we simply talked about frequency of error and probability of error, not because they are different, but just because they are simple to state. As a result of that, will be able to say less in terms of what is the probability of being right or wrong. And that is the analytic expression we are going to try to minimize. And when you define the outofsample error, its really the expected value of the squared error. ts linear regression because the form, in terms of the input, is linear. Now when you do that, you can use perceptron learning algorithm with nonseparable data, terminate it by force at some iteration, and report the pocket value. o you have x and y. Now you have a line. o this is what the features look like. And at the end of the lecture, you will be able to actually deal with very general situations. You can probably extract some features from the input, and then give those to the learning algorithm and let the learning algorithm figure out the pattern. And then you will get the same thing in a linear form. You can look at this and say: can see the pattern here. This is the coordinate which is the intensity. For example, if you take this, the X dagger, and multiply it by X so X dagger times X what do you get? This is the form we have seen before, except that when we saw it before, we took this as a signal that we only care about its sign. The only problem is that thats not linear. Therefore, when you define the insample error, you get the squared error and average it. This is the error on the second example. And you call it y. The definition of capital X and the vector y is as follows. And what we are doing is trying to replicate that. When you look at the insample error, you use the error measure. And the frequency of error happens to be the average value of that error measure. f you look at the line here, the linear regression line. But the good news is that when go to this guy, will have to be dealing with a simpler guy. Regardless of how sure you are when you have a toy data set that you generate, you should always go for real data sets and see how the system that you thought of performs in reality. And if you look at it, there will be some cases like this fellow. The interesting thing is that in spite of the fact that the matrix X is a very tall matrix, definitely not square, hence not invertible, X transposed X is actually a square matrix, because X transposed is this way and X is this way. And the algorithm will try to minimize the error measure by moving from one hypothesis to the next. But for this time, lets take this as a standard error measure we are going to use. n the case of the hypotheses of a model, the events may not be independent, because we have the same sample. And the guys that are closer to the origin, by the time they transform them, they will have smaller values here. And we realize that the insample tracks the outofsample well through the mathematical relationship which is the Hoeffding nequality, that tells us that the probability that E_in deviates from E_out by more than our specified tolerance is a small number. And that would give you the insample error. PROFEOR: Well, if you apply linear regression, you have the following guarantee at the end. o if you take for example the classification of the digits, we had the pixel values. But the way its usually used is you use a pseudorandom number, and then you take a few bits and have them as an output for different inputs. And when you look at the linear classification counterpart, it implements this. o you can also define a symmetry measure, which means that you take the symmetric difference between something and its flipped versions, and you see what you get. This is the final hypothesis, that corresponds to the hypothesis you got at the final iteration. f you look at the other coordinate, which is symmetry, the 1 is often more symmetric than the 5. You will find that if you take a linear model with a soft threshold, not the hardthreshold version, and you put a bunch of them together, you will get a neural network. t considers it an error, in spite of the fact that, when we plug it into the classification, it just has the correct sign. And the interpretation in this case is that youre trying to predict the proper credit line for a customer. But the bad news here is that the data is not linearly separable. f was following the logical sequence, would go immediately to the theory and take , which takes care of the finite case, and then generalize it to the more general case.",0.13789141051422
49,49,"PEAKER: OK. o, as far as clarifications, and hints and comments, and so on on the homework, these are kind of general comments. One is dont lose your common sense. That doesnt mean you can rely only on common sense, because were going to see over and over again in this course a lot of counterintuitive results that may seem to defy common sense. But that doesnt mean just because were doing some counterintuitive stuff sometimes doesnt mean you should abandon common sense. And also mean, mean this not only in terms of do your answers makes sense, but also just in terms of being reasonable. Like a couple of you asked about calculators and things like that. On the homework you can use calculators if you want, but it shouldnt be necessary for the most part. Once in a while it will be obvious that you should use a calculator for that part. On the exams, theres no calculators allowed but also no calculators needed. o, either on the homework or on the exam, you should use some common sense. For example, if you have 52 choose 5, its perfectly fine to leave it as 52 choose 5. ertainly on the exam, common sense, dont expect you to compute 52 choose 5 by hand. On the homework, if youre curious what the number is, you can do this very easily using a calculator or a computer. But you can also leave it as 52 choose 5, which has the virtue of being what call call this selfannotating. Now, 52 choose 5, its some big number, right? f just gave you that big number, it would be hard to know what it was, but 52 choose 5, its already making you think, oh, this has something to do with choosing 5 out of 52. o this is selfannotating, which is good. You could leave it that way. On the other hand so this you would leave. But common sense would be, well, if you have 4 divided by 2 times 1, would prefer, either on the homework or on the exam, would prefer that you simplify it to 2. Occasionally, you may need to add fractions, so 1/2 plus 1/3 m assuming you can do without a calculator. But youre never going to have ugly, tedious stuff on the exam, certainly. And if its tedious on the homework, then you can use a calculator for that. And thats also a hint. f, on the midterm or final, you find yourself doing all these massive calculations with huge fractions and things like, that then theres probably something wrong. o thats just a clarification about that. econd, useful throughout the course, do check answers, and ll talk more about checking answers at different points. But just briefly, checking answers does not mean go through and look what you did and say that looks OK. Like do the same thing twice, right? Because if you made a mistake the first time, youre probably not going to detect it when you look through it again. hecking an answer means trying special cases, things like that, thinking of another approach. A lot of problems can be solved in more than one method, and if you get the same answer, youre happy. f you get a different answer, then you have to think harder and you learn something from that. o the best advice for this is, especially, by doing simple and extreme cases. Like you might have a problem that you have some answer in terms of n, and k, and q, and w, and if you plug in n equals 1 and its completely obviously wrong, then thats useful information, right? But most students dont bother to just plug in n equals 1 and see if it makes sense. o simple and extreme cases, The third thing, ve never seen it given a name or any emphasis at all, but its very, very useful to label. And let me tell you what mean by label people, objects, whatever. Let me tell you what mean by that. o on a lot of the homework problems and a lot of problems in this course, lets say, well, you have a group of n people, or a group of n elk, or gummy bears, or something, and youre just asking about this collection of objects, or people, or animals. And all mean by this is just, if you have n people so the problem will just say we have n people, blah, blah, blah then assume that theyre labeled with the numbers 1 up to n. Very, very simple, but thats extremely useful. o basically, you have this collection of people. Now, in the problem m not going to give names to all of these n people, and n could be a billion or anything, right? But assume that they each have just think of this as an D number, or social security number, or whatever. o you definitely want to do that with the people. Thats going to be helpful with the elk. And you have a homework problem about balls in a jar. Theres a certain number of red balls and a certain number of green balls. ts very useful if you, for example you can choose your own notation. ts very useful, for example though, you could say lets number or label the red balls 1 through r, and the green balls r plus 1 through r plus g, for example. You can do something like that, and youre referring to specific balls. Now, this might seem very obvious, but m stressing it just because ve seen students very often get confused because theyre not thinking of this labeling. And for people its pretty clear. Even if you have identical twins, you can still tell them apart in some way, right? o perfect sense with people. People start to get more confused when we talk about this for, like, balls and the question is are they indistinguishable or distinguishable, and whatever. o suppose you have 10 green balls in a jar and they all look completely identical to you. m not actually saying that the numbers are actually written on them. They may look completely identical to you, but the point is that, as far as probability is concerned, as far as nature is concerned, it behaves as if they are distinguishable and labeled. And that thats going to give you the correct answers. Whereas, if you just say, well, theyre there, theyre completely identical, so theyre indistinguishable, youll run into trouble. o it helps to think about the labeling. And think that is also relevant for the robberies and districts. There are six districts, six robberies. think the best way to think about it is to imagine numbering the districts 1 to 6, number the robberies 1 to 6, and think about it that way. omeone asked me whether robberies are distinguishable or indistinguishable. Well, the problem doesnt designate this robbery was this type and this was this type. They may look very similar, but theyre different robberies. They didnt occur at exactly the same time, exactly the same circumstances. Theres something that distinguishes them. You may as well give each one an D number, as if youll file a report on each one. You have robbery 1 through 6, and thinking about how it will work better. Well come back to this issue of indistinguishability later because its actually quite subtle. And just a quick clarification about the problem about the teams youre splitting up. think its clearly worded, but just to make sure everyone understands, if say you have 10 people and you want to split them into a team of 4 and a team of 6 let me just do as a quick example because theres actually some interesting things here. ts a bit of a hint but its also useful. o suppose we have 10 people and want to split into a team of 6 and a team of 4. And want to know how many ways are there to do that. Well, then its just 10 choose 4. Because pick the team of 4, whoevers left is the team of 6. Thats it. Of course, could have said pick the team of 6, and whoevers left is the team of 4. o could have said 10 choose 6. We actually just proved that 10 choose 4 equals 10 choose 6. Because just counted the same thing in two different ways, it must be the same. mean, you can check this. This is 10 factorial over 6 factorial 4 factorial, and so is this, so its true. But we proved it just by thinking about it. On the other hand, if we wanted two teams of 5, now if we just do 10 choose 5, were going to be off, because say pick 5 for one team and the remaining 5 are the other team. But if had pick the remaining 5 first, that would have been the same. Because its not like said that theres a team A and a team B that have some difference between them. ts just two teams. o picking play, again it helps to label them. o assume the people are numbered 1 through 5. f the teams are 1 through 5 and 6 through 10, theres only one way to do that. m not designating some distinction between the two teams. Therefore, in this case it would be 10 choose 5 divided by 2 because weve doublecounted. Does everyone understand the difference between these two things? Here youre dividing by 2, here youre not. Because there is a clear difference between a team of 4 and a team of 6. While 2 teams of 5, unless you said one team is supposed to wear this jersey and the other one wears this jersey, its equivalent. o thats a key, sometimes subtle, distinction that sometimes gets missed, so want to emphasize that a little bit. o theres a difference there. o you should think carefully about issues like that. ts a little too simplistic to say order matters or order doesnt matter. Well, what matters is thinking in a way that makes sense. And for this homework, the naive definition of probability is enough for all the probability questions, because thats the probability weve done so far. But that doesnt mean you can just naively apply the naive definition of probability. The naive definition assumes youve broken up your problem into equally likely outcomes. o if you break up the problem in a way where theyre clearly not equally likely, then the naive definition will not work. o what m saying is, for every probability question on the homework, if you frame it in the right way, you can apply the naive definition. But you have to think hard about making sure that it makes sense to assume equally likely outcomes. Does that make sense? o coming back to this sampling table. o drew this 2 by 2 table last time. wont draw the whole table again. But you can look it up in your notes to refresh you. Three of the four entries so the table had sampling with and without replacement, and order matters or doesnt matter. And last time we talked about the fact that three of the four entries of the table are obvious just from the multiplication rule. o three of the four entries you could just fill in right away once youve mastered the multiplication rule. The fourth entry is the tricky one. o thats the one that want to talk about now. o for the fourth one, stated the result. wrote it down, but its mysterious where it comes from, so want to show you where does that come from. o the problem is, we want to pick k times from a set of n objects could be objects, people, whatever. And were looking at the case where order doesnt matter, and its with replacement. Thats the case were interested in. o we pick one of the n objects, put it back, pick another one, put it back. Do that k times, but we also dont care if we got that same list in a different order, that counts as the same. o we want to count how many ways are there to do this. And stated last time that the answer is n plus k minus 1 choose k ways. o want to prove this result and also give you a little more intuition for it. The other three results should be easy, but this one is tricky. But first, should follow my own advice. Lets see, is that plausible? Lets check a couple of simple and extreme cases. Well, the most extreme cases so extreme cases. The most extreme example can think of is k equals 0. That is, k equals 0 says you dont do anything. o k equals 0. But lets just see if it makes sense. Assuming that this is correct, k equals 0, this is n minus 1 choose 0. n minus 1 choose 0 is 1. For the same reasoning, why is 0 factorial 1? Well, always think of factorial in terms of if you have n people and theyre lined up for ice cream, how many ways can you order them? Well, if there is no one there, theres one way. ts the way when theres no one there, theres one way. 0 factorial is 1. n choose 0 is 1 for any n, because if you have a group of n people and you choose none of them, theres one way to do that. You just dont choose any one. o its 1, not 0, is the key point. o if you had memorized and then got confused whether this was k or n, if you put n here, then it would be but m changing k to n in my mind and let that k be 0. n minus 1 choose n is zero, because you cant pick n people out of n minus 1. Theres not enough people. o the answer should be 1, not 0, so that makes sense. Lets do a slightly less extreme case than that. Well, what if k equals 1? f k equals 1, then this would just be n choose 1, which equals n. Well, that makes sense. Youre just picking once. Notice, if you only pick once, it makes no difference if its with replacement or without replacement. t makes no difference if its ordered or unordered. t has to be n. o there would be something really wrong if we didnt get that. And lets do one other case that is pretty simple. Lets do n equals 2. Thats an interesting case for us. You can try n equals 1 is another easy case, but lets do n equals 2. n equals 2 is what would call the simplest nontrivial example. One of the best just general pieces of research advice in general is to look at the simplest nontrivial example. o this one is special in some sense, simplest nontrivial. These ones are pretty trivial but theyre still worth checking, because if they are wrong then we know theres something wrong. This is the simplest nontrivial example, f n equals 2, according to this formula, this is going to be k plus 1 choose k. Notice k plus 1 choose k is the same thing as k plus 1 choose 1. ame argument as why is 10 choose 4 the same as 10 choose 6. You could choose the 4 or you could choose the 6. Theyre the same thing. This is exactly the same reason. o thats the same as that. k plus 1 choose 1 is obviously k plus 1, because youre choosing one thing out of k plus 1. All right, now lets see if thats correct. We have n, we have 2 objects and we are picking k times. o lets just draw two buckets to represent the two objects. And were picking k times. o lets just put a you can put a check mark, but for simplicity ll put a dot every time put this object 1, this is object 2. Every time select object 1 m going to put a dot here. Every time select object 2 m going to put a dot here. You can make up some numbers if you want. aybe it looks like that. chose object 1 three times, object 2 four times. ince its with replacement, theres no restriction on how many, as long as the total equals whatever the total is supposed to be. And since order doesnt matter, dont actually care which dot came before which dot. All care about is theres 3 dots here and 4 dots here. Now, it follows from that that, in order to specify this result, all we need to do is say how many dots are there in this box? Because if know how many dots are in this box, then however many are left are in this one. o the number of dots here, number of dots, is in the set 0, 1, up to k, because have k dots. n my example, k is 7, but k could be whatever, k dots. o however many are in here it could be that theyre all in here, or none of them, or anything in between. o there are k plus 1 possibilities. o just by thinking of this two boxes, that gives a direct proof that this is correct in this case. o that gives us some comfort. And ll come back a little bit to this case, n equals 2. But now lets prove that this formula is correct in general. Well, this dot picture is already giving us a hint at the problem. One of the most difficult and most important things in this course is to try to get in the habit of trying to recognize pattern and structure, that is, recognizing when two problems are equivalent even if they sound different. o thats not a calculus thing, thats a thinking thing. You have to think hard. o the way to think about this problem, the best way to think about this problem is not actually to think of it this way. ts to notice that this is equivalent to having indistinguishable particles in distinguishable boxes. o ll just say this is equivalent, and you can think through for yourself why is this the same thing. This is already giving you a hint as to why this is equivalent, but you should think about why this is true in general. o equivalently, how many ways are there to put k indistinguishable particles into n distinguishable boxes? o m thinking of these dots as indistinguishable particles. The boxes are distinguishable because this is box 1 and this is box 2. k ndistinguishable particles into n distinguishable boxes. And the answer is n plus k minus 1 choose k, but lets see why. Another piece of advice that could have added there is draw a diagram and try some simple examples. mentioned the simple examples but didnt mention draw a diagram. m just going draw a picture that should make everything nice. o suppose we had that was a really easy case where we only had 2 boxes. Well, lets boldly do 4 boxes. can do as many boxes as have space for. ts not any harder. Once you see the idea, it could be any number of boxes. But ll draw a picture with 4 boxes. uppose that this box has 3 particles, this box is empty, this box has 2 particles, and this box has 1 particle. o just for this picture, we had n equals 4, and we had k equals 6. Just the number of particles, number of boxes. And you could try to draw out. wouldnt suggest doing it for this case. Theres already too many possibilities. tll be very tedious to list them all out. But you could do one thats somewhere in between this one and this one and just try listing out the cases. Do an example thats simple enough thats not too tedious just as a check. But here its already getting complicated, because can have empty boxes, can have all 4 over here, anything. Now, this idea of indistinguishability, this is in contrast to the labeling problem. o for most real, like, reallife marbles and things like that, it behaves more like that, where you can label them. But for certain counting problems and for certain problems in physics, they are so completely indistinguishable that you cant even think of them as labeled. o, in other words, if swapped this one and this one when you werent looking, you would never know the difference. But not only would you never know the difference, god could not tell the difference. Thats this problem here. Now, usually thats not the case. o usually we can think of labeling, where even if you cant tell the difference god can tell the difference. Thats the most common scenario. For some physics problems it behaves more like this, and also for some counting problems. m just emphasizing that this is important in physics, but most of you are not physicists. m not a physicist. The reason m talking about it is not because of the physics applications but because it is important for counting. But for probability, usually with the naive definition, youre not going to be able to apply this because its going to function more like the labeled case, not, like this case. o now, it doesnt look like weve done anything except draw a picture, but actually were basically done deriving the result. Almost done. All we need to do is convert this. This picture will be kind of hard to type up, even though you can draw this. Lets convert this into something simpler that you could actually write down easier. o m just going to make up a little code. This is a very simple code. m going to represent these dots by dots, so dot, dot, dot. And then m not going to draw these rectangles. m just going to draw separators. o m going to draw a vertical line segment to denote the separators between boxes, like that. o the second box was empty, so therefore m drawing two vertical lines without any dots in between. The third box had 2 dots, and then there is a separator, and then theres another dot. o does everyone see what did? o its a very simple encoding. o notice mean, just did this as an example, but obviously, no matter what configuration we had drawn here, that can be encoded in this way, and you can go from here to here, you can go from here to here. ts just a different way to represent the same situation. And once we have this we actually are done, because in this picture there must be k dots, because there were k dots here, there must be k dots here. And there must be n minus 1 separators. That looks like a 1, but thats a separator symbol. Because if have n boxes, then theres n minus 1 separators in between. Now, to specify this, how many ways are there to do this? Well, you could think of it as like the factorial of all these things, except that thats overcounting. ts similar to the problem, like on the strategic practice, there is a problem like how many ways are there to rearrange the letters of the word pepper? o its not just the factorial of the number of letters in pepper, because theres multiple Ps and multiple Es and you have to adjust for that overcounting. ame as that. But an even easier way to think of it is, to specify this we have n plus k minus 1 positions here. And in order to specify our code, all we need to do is specify where are the dots. hoose the positions for the dots. The remaining positions are the positions for the separators. o n plus k minus 1 choose k. There isnt even anything else can write. This is selfannotating again. have n plus k minus 1 positions here, and m going to pick k of them to put the dots. f you want, you could also say this is n plus k minus 1 choose n minus 1. hoose where the dots are and the separators are determined, or choose where the separators are and then the dots are determined. ame thing. o that completes the proof of that result. o just coming back very briefly to this n equals 2 case, a case where you could think of that would be if you imagine flipping two coins. sually, we think of that as having four assume theyre fair coins, so heads and tails are equally likely. sually we think of four equally likely outcomes heads heads, heads tails, tails heads, tails tails. o theres four of them. Now, suppose the coins look completely indistinguishable to you. till, we could imagine that theres coin number 1 and coin number 2. Or, rather than thinking of flipping two coins, we could think of flipping the same coin twice. o we have the first toss and the second toss. o even if the coins look the same to you, we would think of four outcomes. Does that make sense? Heads heads, heads tails, tails heads, tails tails. A massive controversy in physics arose in the 1920s when a young ndian physicist named Bose proposed, not for coins he was doing a particle physics problem but what he was saying was equivalent for coins as saying that there are only three outcomes, not four, and all are equally likely. Because, in terms of coins, well be saying, well, either its heads heads, tails tails, or one head and one tail. And if the coins are completely indistinguishable, so you cant distinguish between heads tails and tails heads. o he was proposing a model in physics, not for coins, where there were three equally likely outcomes, not four. And he basically got laughed at for that. But he wrote a letter to a guy called Einstein, and Einstein really liked the idea. And then they were able to finally start convincing people. That was in the 1920s. o they actually used something and this just looks like a simple little counting thing but ideas along these lines in physics, they used these tips to predict a new state of matter called the BoseEinstein condensate. They predicted that theoretically. t was only empirically observed 70 years later, so they predicted it 70 years in advance. This is not a physics class, but you can look that up if youre curious. t has all kinds of bizarre properties. The point of this, though, is that, for coins, the thinking of it as labeled, whether you can tell them apart or not, is normally the right way to go. o this is useful for counting and for physics, but you have to be careful about using this with the naive definition of probability. o lets talk a little more about counting and what call story proofs. o lets start with a simple example. o a story proof is still a proof, otherwise wouldnt call it a proof. omeone asked whether that means an example. What it means is an application or an interpretation. o proof by interpretation, would say, rather than proof by algebra or calculus. o there are some examples on the strategic practice, and just want to do a couple more quick examples. Then were ready to go beyond the naive definition of probability. o proof by interpretation, what do mean by that? Well, we already saw an example today. We proved that 10 choose 4 equals 10 choose 6. o, in general, n choose k equals n choose n minus k. Thats a very useful fact. And m not going to write the proof again. This one is so this is example 1. This one is easy to do using algebra as well. But its even easier to just say, well, if pick k out of n, or could pick the other n minus k, and you could write one sentence explaining that. But already talked about that, so m not going write that again. ts the same idea. o this is obvious by the story. The story is just the interpretation that we are picking k people out of n, rather than just thinking of this as a formal symbol involving factorials that you manipulate. Thats actually thinking about what this means rather than manipulating factorials. Thats what mean by that. OK, lets do a slightly harder one, and then one that would be a nightmare using algebra. o heres kind of an intermediate example. This is a very handy identity. That n minus 1 choose k minus 1 equals k times n choose k. o didnt memorize. ve used this identity many, many times in my life. But dont like memorizing things, so wrote this kind of slowly because didnt remember it, just derived it. Again, you can check this by algebra pretty easily, but thats not going to help you remember it or understand it. Thats just like you could write out the algebra and it will just look like a curiosity. Oh, it cancels out and so you know. t doesnt give you any intuition for why thats true. o the story proof for this would be to imagine that were going to pick k people out of n. And, of course, it doesnt have to be people. t can be whatever example you want, but the point is youre not losing any generality. m not saying 3 people or something. ts still a general interpretation. t doesnt matter that said its people. Pick k people out of n, with one of them designated as the president, say, or whatever you want to call the title. That is, you have a committee and theres a chair of a committee, or a president, or whatever, the president of the club. o want to know how many ways are there to do that. Well, theres two different approaches could take. Either could first select whos in the club. There are k people in my club. There are n people in the population, there are k people in my club. o could pick whos in the club, n choose k. And then one of those k must be elected as president, so we multiply by k. ultiplication rule. hoose whos in the club, then choose the president. Thats this. But could also just say, first choose the president. And then once have the president, then need k minus 1 more people in my club, and those could be any of the remaining n minus 1. That is again the motivation rule. They are the same thing. Thats a proof. Thats a completely rigorous mathematical proof, but it also gives you some interpretation. o thats the kind of thing that mean, that were counting the same thing in two different ways. o if both ways are correct, they must agree, right? o thats the idea. o count the same thing in two ways. And one more example that will be useful several times in this course. ts a handy identity. uppose we had m plus n choose k, and want to write this as a sum. sually, it goes the other way around, that were going to see a sum that looks like this. o were going to sum j equals 0 to k of m choose j, n choose k minus j. o suppose we have this sum, and we want to prove that this sum just collapses just to this one binomial coefficient. This is a famous identity in math called Vandermondes identity. t comes up actually a lot in different areas of math, and especially in probability, but it also comes up outside of probability. o if you try to derive this one using algebra, its pretty horrible. mean, you write all this in terms of factorials, you can try to cancel stuff, but you still have to deal with this sum. You can try to apply the binomial theorem. You can do a lot of stuff but its not easy at all. o lets prove this using a story. Well, here again, its very helpful that this is selfannotating. You dont have to be that smart to think of, well, this is m plus n choose k, so m going to think about picking k people out of m plus n. Thats what it says to do, right? o thats what m going to do. m going to pick k out of m plus n, thats the story. Now have to say, well, of course clearly thats this, but how does that relate to the sum here? How does that relate to picking k out of m plus n? Well, m plus n is kind of selfannotating also. t means have an m and have an n added them together. o m imagining two groups a group of size m and a group of size n, and together its m plus n. o that seems like a pretty natural thing to do. mean, thats what m plus n means. o we have a group of m. Heres m people. And m not going to try to draw stick figures or anything, but these dots represent people, not indistinguishable particles. o you should think of these people as labeled. f you want, you can call this person 1, 2, 3. o 1 through m are in this group. And then n. aybe m is 3 and n is 5. Heres n people. o maybe ll number these 1 through 3, and these ones are 4, 5, 6, 7, 8, or however you want to label them. Now need to select k people from, total, from these two groups. o maybe picked ll just circle the ones picked. aybe picked this one, this one, this one, that one. picked 4 of them, for example. Well, lets pick 5 of them. There. o how many ways are there to do that? Well, obviously need to pick some number of people from this group and some number from this group such that the total is k. o suppose that picked j from here, picked j. And suppose picked well. o in this case j is 2, picked 2 from this group. dont know why the projector is going on. picked j from this group, j is 2 in this example. f need 5 total and picked 2 from here, there must be 3 from here. Obviously, right? o this is just k minus j from over here. f theres j here, there must be k minus j here. How many ways are there to pick j here, k minus j here? Well, its just this, right? Thats the multiplication rule. Then we add those up. Were not double counting anything, were just adding up disjoint cases. o we just add them up and thats that. And thats the proof. Thats it. Just writing up a few sentences about that in words, thats a proof, and its a lot better to do it that way than to try to do that using algebra. o last thing for today is to start, just very briefly start, on the nonnaive definition of probability, and then obviously well be continuing that for the rest of the semester. But just want to get the basic structure in place. o heres the general definition of probability. p until now, we were assuming equally likely outcomes. And we want to go beyond that. We dont want to assume everything is equally likely, and we dont want to have to assume that there are only finitely many possible outcomes. We want to go beyond that. o this is the nonnaive definition. For the nonnaive definition, we need the notion of a probability space. And already introduced the concept of a sample space. o a probability space consists of two ingredients, which ll call and P. And just have to tell you what and P are and what rules they obey. o the same notation we used before is a sample space. Remember, thats just the set of all possible outcomes of some experiment, and well interpret the word experiment very broadly. o is a sample space. already talked about what that means. p until now, we had to assume that this was finite and that all of the outcomes were equally likely. And now were going to go way, way beyond that and not have to assume that any more. o P is a function. But its not the kind of function that you usually see, like f of x equals x squared. The domain of P is all subsets of . o P is a function. ts a mapping that takes as an input, it takes an event. Remember, we talked before and theres also a handout that you should look at, if you havent already an event is a subset of . o P is a function which takes an event, any event A ll just call it A an event A, which is a subset of , as input. o an event is a subset of thats the subset symbol input, and gives P of A, which is a number between 0 and 1. Because we want probabilities, just by convention, standard convention is that we want probabilities to be numbers between 0 and 1. o the input is an event, the output is a number between 0 and 1. o thats the output. Now, the only thing left is have to tell you what axioms does this have to satisfy? n other words, what properties do we need P to satisfy, aside from the fact that its between 0 and 1? o you might think that a probability is a very complicated thing, like understanding uncertainty is a very complicated thing. But actually we only need two axioms, two rules. o such that, rule number 1, the probability of the empty set equals 0, and the probability of the full space equals 1. And you might say cheated by including two things in one. Actually, you can try to simplify the axioms and derive, and you could try to derive this, but m not trying to make this completely minimal. like to write it this way because these are the two extremes. The probability of the empty set equals 0. That makes sense intuitively. What would it mean for the empty set to occur? We say that an event occurs if so our set up is we have this Venn diagrams are useful here. is like the universe of all possible outcomes. And we get one specific lets say this is A is this oval. And lets suppose that s thats a lowercase s but did it big so that its visible. That lowercase s0 is an example of an outcome. o our picture is, before we do the experiment, we just have this general sample space. After we do the experiment, then we get to observe the outcome. And suppose that the actual outcome of the experiment was s0. Now, if that actual outcome is an element of A, then we say that A occurred. And if this s0 is outside of A, we say it doesnt occur. Now, what would it mean for the empty set to occur? t would mean that this s0 is in the empty set. But nothing is in the empty set. ts empty. o thats why want this to be 0. What thats saying is we want impossible events to have probability 0. This is impossible. This, on, the other hand, itself happens with certainty. f somehow s0 fell outside of that rectangle, that would mean you had the wrong rectangle. This is the whole universe here. o it cant be outside. o thats 1. Thats basically a convention. The most important axiom is the second one, and luckily theres only two. o youre not going to have to memorize 10 axioms. ts just two simple rules. o, just very quickly, the second one says that the probability of the union so m assuming that you know unions now. This is called a countable union. That is, m taking a union of infinitely many, but its called a countably infinitely many. o the probability of the union equals the sum of the probabilities. And theres an important condition there. Thats true if the A1, A2, et cetera are disjoint, which just means that no two of them overlap. o ll just say nonoverlapping. o those are the two axioms of probability. And find it pretty amazing that, from these two simple rules, we can derive every single theorem and result in probability eventually follows from these two rules.","You dont have to be that smart to think of, well, this is m plus n choose k, so m going to think about picking k people out of m plus n. Thats what it says to do, right? 0 factorial is 1. n choose 0 is 1 for any n, because if you have a group of n people and you choose none of them, theres one way to do that. o the story proof for this would be to imagine that were going to pick k people out of n. And, of course, it doesnt have to be people. The point of this, though, is that, for coins, the thinking of it as labeled, whether you can tell them apart or not, is normally the right way to go. Now, it follows from that that, in order to specify this result, all we need to do is say how many dots are there in this box? o this is useful for counting and for physics, but you have to be careful about using this with the naive definition of probability. o if you had memorized and then got confused whether this was k or n, if you put n here, then it would be but m changing k to n in my mind and let that k be 0. n minus 1 choose n is zero, because you cant pick n people out of n minus 1. On the other hand, if we wanted two teams of 5, now if we just do 10 choose 5, were going to be off, because say pick 5 for one team and the remaining 5 are the other team. Like you might have a problem that you have some answer in terms of n, and k, and q, and w, and if you plug in n equals 1 and its completely obviously wrong, then thats useful information, right? f just gave you that big number, it would be hard to know what it was, but 52 choose 5, its already making you think, oh, this has something to do with choosing 5 out of 52. o this is selfannotating, which is good. o ll just say this is equivalent, and you can think through for yourself why is this the same thing. But you could do one thats somewhere in between this one and this one and just try listing out the cases. o were going to sum j equals 0 to k of m choose j, n choose k minus j. o suppose we have this sum, and we want to prove that this sum just collapses just to this one binomial coefficient. But for probability, usually with the naive definition, youre not going to be able to apply this because its going to function more like the labeled case, not, like this case. Now have to say, well, of course clearly thats this, but how does that relate to the sum here? think its clearly worded, but just to make sure everyone understands, if say you have 10 people and you want to split them into a team of 4 and a team of 6 let me just do as a quick example because theres actually some interesting things here. This is the simplest nontrivial example, f n equals 2, according to this formula, this is going to be k plus 1 choose k. Notice k plus 1 choose k is the same thing as k plus 1 choose 1. ame argument as why is 10 choose 4 the same as 10 choose 6. And all mean by this is just, if you have n people so the problem will just say we have n people, blah, blah, blah then assume that theyre labeled with the numbers 1 up to n. Very, very simple, but thats extremely useful. o the way to think about this problem, the best way to think about this problem is not actually to think of it this way. But an even easier way to think of it is, to specify this we have n plus k minus 1 positions here. One of the most difficult and most important things in this course is to try to get in the habit of trying to recognize pattern and structure, that is, recognizing when two problems are equivalent even if they sound different. The story is just the interpretation that we are picking k people out of n, rather than just thinking of this as a formal symbol involving factorials that you manipulate. o just by thinking of this two boxes, that gives a direct proof that this is correct in this case. think the best way to think about it is to imagine numbering the districts 1 to 6, number the robberies 1 to 6, and think about it that way. f you want, you could also say this is n plus k minus 1 choose n minus 1. hoose where the dots are and the separators are determined, or choose where the separators are and then the dots are determined. o thats the kind of thing that mean, that were counting the same thing in two different ways. have n plus k minus 1 positions here, and m going to pick k of them to put the dots. But its even easier to just say, well, if pick k out of n, or could pick the other n minus k, and you could write one sentence explaining that. Well, you could think of it as like the factorial of all these things, except that thats overcounting. This is already giving you a hint as to why this is equivalent, but you should think about why this is true in general. o even if the coins look the same to you, we would think of four outcomes. o what m saying is, for every probability question on the homework, if you frame it in the right way, you can apply the naive definition. o notice mean, just did this as an example, but obviously, no matter what configuration we had drawn here, that can be encoded in this way, and you can go from here to here, you can go from here to here. Now, in the problem m not going to give names to all of these n people, and n could be a billion or anything, right? o last thing for today is to start, just very briefly start, on the nonnaive definition of probability, and then obviously well be continuing that for the rest of the semester. Just writing up a few sentences about that in words, thats a proof, and its a lot better to do it that way than to try to do that using algebra. And want to know how many ways are there to do that. This one is so this is example 1. And last time we talked about the fact that three of the four entries of the table are obvious just from the multiplication rule. Pick k people out of n, with one of them designated as the president, say, or whatever you want to call the title. o thats the one that want to talk about now. And that thats going to give you the correct answers. o thats the same as that. o its not just the factorial of the number of letters in pepper, because theres multiple Ps and multiple Es and you have to adjust for that overcounting. mean, you write all this in terms of factorials, you can try to cancel stuff, but you still have to deal with this sum. p until now, we had to assume that this was finite and that all of the outcomes were equally likely. m just emphasizing that this is important in physics, but most of you are not physicists. o such that, rule number 1, the probability of the empty set equals 0, and the probability of the full space equals 1. People start to get more confused when we talk about this for, like, balls and the question is are they indistinguishable or distinguishable, and whatever. Because we want probabilities, just by convention, standard convention is that we want probabilities to be numbers between 0 and 1. o the input is an event, the output is a number between 0 and 1. o thats the output. o just coming back very briefly to this n equals 2 case, a case where you could think of that would be if you imagine flipping two coins. ts similar to the problem, like on the strategic practice, there is a problem like how many ways are there to rearrange the letters of the word pepper? And for this homework, the naive definition of probability is enough for all the probability questions, because thats the probability weve done so far. Now, the only thing left is have to tell you what axioms does this have to satisfy? Now, to specify this, how many ways are there to do this? And now were going to go way, way beyond that and not have to assume that any more. o the number of dots here, number of dots, is in the set 0, 1, up to k, because have k dots. Actually, you can try to simplify the axioms and derive, and you could try to derive this, but m not trying to make this completely minimal. o on a lot of the homework problems and a lot of problems in this course, lets say, well, you have a group of n people, or a group of n elk, or gummy bears, or something, and youre just asking about this collection of objects, or people, or animals. o, just very quickly, the second one says that the probability of the union so m assuming that you know unions now. o you definitely want to do that with the people. But its not the kind of function that you usually see, like f of x equals x squared. o we want to count how many ways are there to do this. t would mean that this s0 is in the empty set. Well, obviously need to pick some number of people from this group and some number from this group such that the total is k. o suppose that picked j from here, picked j. And suppose picked well. Assuming that this is correct, k equals 0, this is n minus 1 choose 0. n minus 1 choose 0 is 1. And then once have the president, then need k minus 1 more people in my club, and those could be any of the remaining n minus 1. The reason m talking about it is not because of the physics applications but because it is important for counting. m going to pick k out of m plus n, thats the story. o want to know how many ways are there to do that. And once we have this we actually are done, because in this picture there must be k dots, because there were k dots here, there must be k dots here. Again, you can check this by algebra pretty easily, but thats not going to help you remember it or understand it. And thats the proof. Now, this idea of indistinguishability, this is in contrast to the labeling problem.",0.1274642182014582
50,50,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer highquality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. ANA BELL: All right, so lets see. o first we have L1 is equal to bacon, eggs. m going to short form shorthand this. L2 is equal to toast and jam. Brunch should actually make these arrows brunch is equal to L1. o this is just aliasing, which means brunch is going to point to whatever L1 object points to. And if do L1.append juice, L1 is now going to be bacon, eggs, and juice. OK? This L1 has been mutated to be that. And since brunch still points to the same object that L1 points to, brunch is now going to point to there. OK? o when do brunch.extend L2, m going to take whatever brunch is, which is this part here, and m going to extend it by L2, which is toast and jam. OK? o its just going to contain a large list of those five elements because of this side effect issue, where brunch was pointing to the same thing that L1 was pointing to, OK? o its close, but think perfect. Yes, thats the right answer.","And since brunch still points to the same object that L1 points to, brunch is now going to point to there. o this is just aliasing, which means brunch is going to point to whatever L1 object points to. o when do brunch.extend L2, m going to take whatever brunch is, which is this part here, and m going to extend it by L2, which is toast and jam. o its just going to contain a large list of those five elements because of this side effect issue, where brunch was pointing to the same thing that L1 was pointing to, OK?",0.365079365079365
51,51,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer highquality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: ontinuing in the theme of sorting in general, but in particular, binary search trees, which are a kind of way of doing dynamic sorting, if you will, where the elements are coming and going. And at all times, you want to know the sorted order of your elements by storing them in a nice binary search tree. Remember, in general, a binary search tree is a tree. ts binary, and it has the search property. Those three things. This is a rooted binary tree. t has a root. ts binary, so theres a left child and a right child. ome nodes lack a right or left child. ome nodes lack both. Every node has a key. This is the search part. You store key in every node, and you have this BT property, or also called the search property, that every node if you have a node the stores key x, everybody in the left subtree stores a key thats less than or equal to x, and everyone thats in the right subtree stores a key thats greater than or equal to x. o not just the left and right children, but every descendant way down there is smaller than x. Every descendent way down there is greater than x. o when you have a binary search tree like this, if you want to know the sorted order, you do whats called an inorder traversal. You look at a node. You recursively visit the left child. Then you print out the root. Then you recursively visit the right child. o in this case, wed go left, left, print 11. Print 20. Go right. Go left. Print 26. Print 29. Go up. Print 41. Go right. Print 50. Print 65. Then check thats in sorted order. f youre not familiar with inorder traversal, look at the textbook. ts a very simple operation. m not going to talk about it more here, except were going to use it. All right, well get to the topic of todays lecture in a moment, which is balance. What we saw in last lecture and recitation is that these basic binary search trees, where when you insert a node you just walk down the tree to find where that item fits like if youre trying to insert 30, you go left here, go right here, go right here, and say, oh 30 fits here. Lets put 30 there. f you keep doing that, you can do insert. You can do delete. You can do these kinds of searches, which we saw, finding the next larger element or finding the next smaller element, also known as successor and predecessor. These are actually the typical names for those operations. You can solve them in order h time. Anyone remember what h was? The height. Yeah, good. The height of the tree. o h is the height of the BT. What is the height of the tree? ADENE: . PROFEOR: Log n? Log n would be great, but not always. o this is the issue of being balance. o in an ideal world, your trees going to look something like this. ve drawn this picture probably the most in my academic career. This is a nice, perfectly balanced binary search tree. The height is log n. This would be the balance case. mean, roughly log n. Lets just put theta to be approximate. But as we saw at the end of last class, you can have a very unbalanced tree, which is just a path. And there the height is n. Whats the definition of height? Thats actually what was looking for. hould be 6.042 material. Yeah? ADENE: s it the length of the longest path always going down? PROFEOR: Yeah, length of the longest path always going down. o length of the longest path from the root to some leaf. Thats right. OK, so this is highlight this because were going to be working a lot with height today. All thats happening here, all of the paths are length log n. Here, there is a path of length n. ome of them are shorter, but in fact, the average path is n over 2. ts really bad. o this is very unbalanced. ll put ""very."" ts not a very formal term, but thats like the worst case for BTs. This is good. This does have a formal definition. We call a tree balanced if the height is order log n. o youre storing n keys. f your height is always order log n, we get a constant factor here. Here, its basically exactly log n, 1 times log n. ts always going to be at least log n, because if youre storing n things in a binary tree, you need to have height at least log n. o in fact, it will be theta log n if your tree is balanced. And todays goal is to always maintain that your trees are balanced. And were going to do that using the structure called AVL trees, which ll define in a moment. Theyre the original way people found to keep trees balanced back in the 60s, but theyre still kind of the simplest. There are lots of ways to keep a tree balanced, so ll mention some other balance trees later on. n particular, your textbook covers two other ways to do it. t does not cover AVL trees, so pay attention. One more thing wanted to define. We talked about the height of the tree, but d also like to talk about the height of a node in a tree. an anyone define this for me? Yeah? ADENE: ts the level that the node is at. PROFEOR: The level that the node is at. That is roughly right. mean, that is right. ts all about, what is the level of a node? ADENE: Like how many levels of children it has. PROFEOR: How many levels of children it has. Thats basically right, yeah. ADENE: The distance from it to the root. PROFEOR: Distance from it to the root. That would be the depth. o depth is counting from above. Height is ADENE: . PROFEOR: Yes, longest path from that node to the leaf. Note thats why wrote this definition actually, to give you a hint. Here should probably say down to be precise. Youre not allowed to go up in these paths. All right. orry. ve got to learn how to throw. All right. o for example, over here m going to write depths in red. f youre taking notes its OK. Dont worry. o length off the longest path from it down to a leaf. Well, this is a leaf, so its height is 0. OK. Yeah, ll just leave it at that. t takes 0 steps to get from a leaf to a leaf. This guys not a leaf. t has a child, but it has a path of length one to a leaf. o its one. This guy has a choice. You could go left and you get a path of length 1, or you could go right and get a path of length 2. We take the max, so this guy has height 2. This node has height 1. This node has height 3. How do you compute the height of a node? Anyone? Yeah. ADENE: ax of the height of the children plus 1. PROFEOR: Right. You take the max of the height of the children. Here, 2 and 1. ax is 2. Add 1. You get 3. o its going to always be this is just a formula. The height of the left child maxed with the height of the right child plus 1. This is obviously useful for computing. And in particular, in lecture and recitation last time, we saw how to maintain the size of every tree using data structure augmentation. Data structure augmentation. And then we started with a regular vanilla binary search tree, and then we maintained every time we did an operation on the tree, we also updated the size of the subtree rooted at that node, the size field. Here, want to store a height field, and because have this nice local rule that tells me how to compute the height of a node using just local information the height of its left child, the height of its right child. Do a constant amount of work here. Theres a general theorem. Whenever you have a nice local formula like this for updating your information in terms of your children, then you can maintain it using constant overhead. o we can store the height of every node for free. Why do care? Because AVL trees are going to use the heights of the nodes. Our goal is to keep the heights small. We dont want this. We want this. o a natural thing to do is store the heights. When they get too big, fix it. o thats what were going to do. aybe one more thing to mention over here for convenience. Leaves, for example, have children that are mean, they have null pointers to their left and right children. You could draw them explicitly like this. Also some nodes just lack a single child. m going to define the depths of these things to be negative 1. This will be convenient later on. Why negative 1? Because then this formula works. You can just think about it. Like leaves, for example, have two children, which are negative 1. You take the max. You add 1. You get 0. o that just makes things work out. We dont normally draw these in the pictures, but its convenient that dont have to do special cases when the left child doesnt exist and the right child doesnt exist. You could either do special cases or you could make this definition. p to you. OK. AVL trees. o the idea with an AVL tree is the following. Wed like to keep the height order log n. ts a little harder to think about keeping the height order log n than it is to think about keeping the tree balance, meaning the left and right sides are more or less equal. n this case, were going to think about them as being more or less equal in height. You could also think about them being more or less equal in subtree size. That would also work. ts a different balanced search tree. Height is kind of the easiest thing to work with. o if we have a node, it has a left subtree. t has a right subtree, which we traditionally draw as triangles. This subtree has a height. Well call it HL for left. By the height of the subtree, mean the height of its root. And the right subtree has some height, r. ve drawn them as the same, but in general they might be different. And what we would like is that h sub l and h sub r are more or less the same. They differ by at most an additive 1. o if look at h sub l minus h sub r in absolute value, this is at most 1, for every node. o have some node x. For every node x, want the left and right subtrees to be almost balanced. Now, could say differ by at most 0, that the left and right have exactly the same heights. Thats difficult, because that really forces you to have exactly the perfect tree. And in fact, its not even possible for odd n or even n or something. Because at the very end youre going to have one missing child, and then youre unbalanced there. o 0s just not possible to maintain, but 1 is almost as good, hopefully. Were going to prove that in a second. And it turns out to be easy to maintain in log n time. o lets prove some stuff. o first claim is that AVL trees are balanced. Balanced, remember, means that the height of them is always order log n. o were just going to assume for now that we can somehow achieve this property. We want to prove that it implies that the height is at most some constant times log n. We know its at least log n, but also like it to be not much bigger. o what do you think is the worst case? ay have n nodes. How could make the tree as high as possible? Or conversely, if have a particular height, how could make it have as few nodes as possible? Thatd be like the sparsest, the least balanced situation for AVL trees. Yeah? ADENE: You could have one node on the last level. PROFEOR: One node on the last level, yeah, in particular. Little more. What do the other levels look like? That is correct, but want to know the whole tree. ts hard to explain the tree, but you can explain the core property of the tree. Yeah? ADENE: . PROFEOR: For every node, lets make the right side have a height of one larger than the left side. think thats worth a cushion. ee if can throw better. Good catch. Better than hitting your eye. o m going to not prove this formally, but think if you stare at this long enough its pretty obvious. Worst case is when there are multiple worst cases, because right and left are symmetric. We dont really care. But lets say that the right subtree has height one more than the left for every node. OK, this is a little tricky to draw. Not even sure want to try to draw it. But you basically draw it recursively. o, OK, somehow ve figured out this where the height difference here is 1. Then take two copies of it. ts like a fractal. You should know all about fractals by now. Problem set two. And then you just well, thats not quite right. n fact, need to somehow make this one a little bit taller and then glue these together. Little tricky. Lets not even try to draw the tree. Lets just imagine this is possible. t is possible. And instead, m going to use mathematics to understand how high that tree is. Or actually, its a little easier to think about let me get this right. ts so easy that have to look at my notes to remember what to write. Really, no problem. All right, so m going to define n sub h is the minimum number of nodes thats possible in an AVL tree of height h. This is sort of the inverse of what we care about, but if we can solve the inverse, we can solve the thing. What we really care about is, for n nodes, how large can the height be? We want to prove thats order log n. But itll be a lot easier to think about the reverse, which is, if fix the height to be h, whats the fewest nodes that can pack in? Because for the very unbalanced tree, have a height of n, and only need to put n nodes. That would be really bad. What prefer is a situation like this, where with height h, have to put in 2 to the h nodes. That would be perfect balance. Any constant to the h will do. o when you take the inverse, you get a log. OK, well get to that in a moment. How should we analyze n sub h? hear something. Yeah? ADENE: 2 to the h minus 1 . PROFEOR: aybe, but dont think that will quite work out. Any yeah? ADENE: o you have only 1 node in the last level, so it would be 1/2 to the h plus 1. PROFEOR: That turns out to be approximately correct, but dont know where you got 1/2 to the h plus 1. ts not exactly correct. ll tell you that, so that your analysis isnt right. ts a lot easier. You guys are worried about the last level and actually what the tree looks like, but in fact, all you need is this. All you need is love, yeah. ADENE: . PROFEOR: No, its not a half. ts a different constant. Yeah? ADENE: tart with base cases and write a recursive formula. PROFEOR: Ah, recursive formula. Good. You said start with base cases. always forget that part, so its good that you remember. You should start with the base case, but m not going to worry about the base case because it wont matter. Because know the base case is always going to be n order 1 is order 1. o for algorithms, thats usually all you need for base case, but its good that you think about it. What was looking for is recursive formula, aka, recurrence. o can someone tell me maybe even you could tell me a recurrence for n sub h, in terms of n sub smaller h? Yeah? ADENE: 1 plus . PROFEOR: 1 plus n sub h minus 1. Not quite. Yeah? ADENE: N sub h minus 1 plus n sub h minus 2. PROFEOR: N plus do you want the 1 plus? ADENE: dont think so. PROFEOR: You do. ts a collaboration. To combine your two answers, this should be the correct formula. Let me double check. Yes, whew. Good. OK, why? Because the one thing we know is that our tree looks like this. The total height here is h. Thats what were trying to figure out. How many nodes are in this tree of height h? Well, the height is the max of the two directions. o that means that the larger has height h minus 1, because the longest path to a leaf is going to be down this way. Whats the height of this? Well, its one less than the height of this. o its going to be h minus 2. This is where the n sub h minus 1 plus n sub h minus 2 come in. But theres also this node. t doesnt actually make a big difference in this recurrence. This is the exponential part. This is like itty bitty thing. But it matters for the base case is pretty much where it matters. Back to your base case. Theres one guy here, plus all the nodes on the left, plus all the nodes on the right. And for whatever reason, put the left over here and the right over here. And of course, you could reverse this picture. t doesnt really matter. You get the same formula. Thats the point. o this is the recurrence. Now we need to solve it. What we would like is for it to be exponential, because that means theres a lot of nodes in a height h AVL tree. o any suggestions on how we could figure out this recurrence? Does it look like anything youve seen before? ADENE: Fibonacci. PROFEOR: Fibonacci. ts almost Fibonacci. f hid this plus 1, which you wanted to do, then it would be exactly Fibonacci. Well, thats actually good, because in particular, n sub h is bigger than Fibonacci. f you add one at every single level, the certainly you get something bigger than the base Fibonacci sequence. Now, hopefully you know Fibonacci is exponential. have an exact formula. f you take the golden ratio to the power h, divide by root 5, and round to the nearest integer, you get exactly the Fibonacci number. razy stuff. We dont need to know why thats true. Just take it as fact. And conveniently phi is bigger than 1. You dont need to remember what phi is, except it is bigger than 1. And so this is an exponential bound. This is good news. o ll tell you its about 1.618. And so we get is that if we invert this, this says n sub h is bigger than some phi to the h. This is our n, basically. What we really want to know is how h relates to n, which is just inverting this formula. o we have, on the other hand, the phi to the h divided by root 5 is less than n. o got a log base phi on both sides. eems like a good thing to do. This is actually quite annoying. ve got h minus a tiny little thing. ts less than log base phi of n. And will tell you that is about 1.440 times log base 2 of n, because after all, log base 2 is what computer scientists care about. o just to put it into perspective. We want it to be theta log base 2 of n. And heres the bound. The height is always less than 1.44 times log n. All we care about is some constant, but this is a pretty good constant. Wed like one. There are binary search tress that achieve 1, plus very, very tiny thing, arbitrarily tiny, but this is pretty good. Now, if you dont know Fibonacci numbers, pull a rabbit out of a hat and ve got this phi to the h. ts kind of magical. Theres a much easier way to analyze this recurrence. ll just tell you because its good to know but not super critical. o we have this recurrence, n sub h. This is the computer scientist way to solve the recurrence. We dont care about the constants. This is the theoretical computer scientist way to solve this recurrence. We dont care about constants. And so we say, aw, this is hard. ve got n sub h minus 1 and n sub h minus 2. o asymmetric. Lets symmetrify. ould make them both n sub h minus 1. Or could make them both n sub h minus 2? uggestions? ADENE: . PROFEOR: inus 2 is the right way to go because want to know n sub h is greater than something in order to get a less than down here. By the way, use that log is monatomic here, but it is, so were good. o this is going to be greater than 1 plus 2 times n sub h minus 2. Because if have a larger height m going to have more nodes. Thats an easy proof by induction. o can combine these into one term. ts simpler. can get rid of this 1 because that only makes things bigger. o just have this. OK, now need a base case, but this looks like 2 the something. Whats the something? H over 2. o ll just write theta to avoid the base case. 2 to the h over 2. Every two steps of h, get another factor of 2. o when you invert and do the log, this means that h is also less than log base 2 of n. Log base 2 because of that. Factor 2 out here because of that factor 2 when you take the log. And so the real answer is 1.44. This is the correct this is the worst case. But its really easy to prove that its, at most, 2 log n. o keep this in mind in case we ask you to analyze variance of AVL trees, like in problem set three. This is the easy way to do it and just get some constant times log n. lear? All right, so thats AVL trees, why theyre balanced. And so if we can achieve this property, that the left and right subtrees have about the same height, well be done. o how the heck do we maintain that property? Lets go over here. obius trees are supposed to support a whole bunch of operations, but in particular, insert and delete. m just going to worry about insert today. Delete is almost identical. And its in the code that corresponds to this lecture, so you can take a look at it. Very, very similar. Lets start with insert. Well, its pretty straightforward. Our algorithm is as follows. We do the simple BT insertion, which we already saw, which is you walk down the tree to find where that key fits. You search for that key. And wherever it isnt, you insert a node there, insert a new leaf, and add it in. Now, this will not preserve the AVL property. o the second step is fix the AVL property. And theres a nice concise description of AVL insertion. Of course, how do you do step two is the interesting part. All right, maybe lets start with an example. That could be fun. Hey, look, heres an example. And to match the notes, m going to do insert 23 as a first example. OK, m also going to annotate this tree a little bit. o said we store the heights, but what care about is which height is larger, the left or the right. n fact, you could just store that, just store whether its plus 1, minus 1, or 0, the difference between left and right sides. o m going to draw that with a little icon, which is a left arrow, a descending left arrow if this is the bigger side. And this is a right arrow. This is even. Left and right are the same. Here, the left is heavier, or higher, guess. Here its even. Here its left. This is AVL, because its only one heavier wherever have an arrow. OK, now insert 23. 23 belongs its less than 41, greater than 20, less than 29, less than 26. o it belongs here. Heres 23, a brandnew node. OK, now all the heights change. And its annoying to draw what the heights are, but ll do it. This one changes to 1. This is 0. This changes to 2. This changes to 3. This changes to 4. Anyway, never mind what the heights are. Whats bad is, well, this guys even. This guys left heavy. This guys now doubly left heavy. Bad news. OK, lets not worry about above that. Lets just start. The algorithm is going to walk up the tree and say, oh, when do get something bad? o now have 23, 26, 29 in a path. d like to fix it. Hmm, how to fix it? dont think we know how to fix it, so will tell you how. Actually, wasnt here last week. o did we cover rotations? ADENE: No. PROFEOR: OK, good. Then you dont know. Let me tell you about rotations. uper cool. ts just a tool. Thats x and y. always get these mixed up. o this is called left rotate of x. OK, so heres the thing we can do with binary search trees. ts like the only thing you need to know. Because youve got search in binary search trees and youve got rotations. o when have a tree like this, ve highlighted two nodes, and then theres the children hanging off of them. ome of these might be empty, but theyre trees, so we draw them as triangles. f just do this, which is like changing which is higher, x or y, and whatever the parent of x was becomes the parent of y. And vice versa, in fact. The parent of y was x, and now the parent of x is y. OK, the parent of a is still x. The parent of b changes. t used to be y. Now its x. The parent of c was y. ts still y. o in a constant number of pointer changes, can go from this to this. This is constant time. And more importantly, it satisfies the BT order property. f you do an inorder traversal of this, you will get a, x, b, y, c. f do an inorder traversal over here, get a, x, b, y, c. o theyre the same. o it still has BT ordering. You can check more formally. b has all the nodes between x and y. till all the nodes between x and y, and so on. You can check it at home, but this works. We call it a left rotate because the root moves to the left. You can go straight back where you came from. This would be a right rotate of y. OK, its a reversible operation. t lets you manipulate the tree. o when we have this picture and were really sad because this looks like a mess, what wed like to do is fix it. This is a path of three nodes. Wed really prefer it to look like this. f we could make that transformation, wed be happy. And we can. t is a right rotate of 29. o thats what were going to do. o let me quickly copy. want to rotate 29 to the right, which means 29 and 26 this is x. This is y. turn them, and so get 26 here now, and 29 is the new right child. And then whatever was the left child of x becomes the left child of x in the picture. You can check it. o this used to be the triangle a. And in this case, its just the node 23. And we are happy. Except didnt draw the whole tree. Now were happy because we have an AVL tree again. Good news. o just check. This is even. This is right heavy. This is even. This is left heavy still. This is left heavy, even, even, even. OK, so now we have an AVL tree and our beauty is restored. ll do one more example. nsert 55. We want to insert 55 here. And what changes is now this is even. This is right heavy. This is doubly left heavy. Were super sad. And then we dont look above that until later. This is more annoying, because you look at this thing, this little path. ts a zigzag path, if you will. f do a right rotation where this is x and this is y, what ll get is x, y, and then this is b. This is whats in between x and y. And so itll go here. And now its a zag zig path, which is no better. The heights the same. And were sad. told you, though, that somehow rotations are all we need to do. What can do? How could fix this little zigzag? Just need to think about those three nodes, but all give you are rotations. ADENE: Perhaps rotate 50. PROFEOR: aybe rotate 50. That seems like a good idea. Lets try it. f you dont mind, m just going to write 41, and then theres all the stuff on the left. Now we rotate 50. o 65 remains where it is. And we rotate 50 to the left. o 50 and its child. This is x. This is y. And so get 55 and get 50. Now, this is bad from an AVL perspective. This is still doubly left heavy, this is left heavy, and this is even. But it looks like this case. And so now can do a right rotation on 65, and will get so let me order the diagrams here. do a right rotate on 65, and will get 41. And to the right get 55. And to the right get 65. To the left get 50. And then get the left subtree. And so now this is even, even, even. Wow. How high was left subtree? think its still left heavy. ool. This is what some people call double rotation, but like to call it two rotations. ts whatever you prefer. ts not really a new operation. ts just doing two rotations. o thats an example. Lets do the general case. ts no harder. You might say, oh, gosh, why do you do two examples? Well, because they were different. And theyre are two cases on the algorithm. You need to know both of them. OK, so AVL insert. Here we go. Fix AVL property. m just going to call this from the changed node up. o the one thing thats missing from these examples is that you might have to do more than two rotations. What we did was look at the lowest violation of the AVL property and we fixed it. When we do that, theres still may be violations higher up, because when you add a node, you change the height of this subtree, the height of this subtree, the height of this subtree, and the height of this subtree, potentially. What happened in these cases when was done, what did fixed one violation. They were all fixed. But in general, there might be several violations up the tree. o thats what we do. Yeah, ll leave it at that. o suppose x is the lowest node that is not AVL. The way we find that node is we start at the node that we changed. We check if thats OK. We update the heights as we go up using our simple rule. And thats actually not our simple rule, but its erased. We update the height based on the heights of its children. And you keep walking up until you see, oh, the left is twice, two times or not two times, but plus 2 larger than the left, or vice versa. Then you say, oh, thats bad. And so we fix it. Yeah, question. ADENE: o here we continue to . add n to the level . PROFEOR: AVL propertys not about levels. ts about left subtrees and right subtrees. o the trouble is that 65 you have a left subtree, which has height 2 or sorry, height 1, guess because the longest path from here to a leaf is 1. The right subtree has height negative 1 because it doesnt exist. o its one versus negative 1. o thats why theres a double arrow. Yeah, good to ask. ts weird with the negative 1s. Thats also why wanted to define those negative 1s to be there, so the AVL property is easier to state. Other questions? All right. Good. think want a symmetry assumption here. dont know why wrote right of x. guess in modern days we write x dot right. ame thing. OK, m going to assume that the right child is the heavier one like we did before. ould be the left. ts symmetric. t doesnt matter. o now there are two cases, like said. m going to use this term right heavy because its super convenient. OK, right heavy is what ve been drawing by a descending right arrow. Balance is what ve been drawing by a horizontal line. OK, so were just distinguishing between these two cases. This turns out to be the easy case. o we have x, y, a, b, c. Why are we looking at the right child? Because we assumed that the right one is higher, so that x was right heavy. o this subtree as ve drawn it is higher than the left one by 2, in fact. And what we do in this case is right rotate of x. And so we get x, y, a, b, c. could have drawn this no matter what case were in, so we need to check this actually works. Thats the interesting part. And thats over here. OK, so said x is right heavy, in fact doubly so. y is either right heavy or balanced. Lets start with right heavy. o when we do this rotation, what happens to the heights? Well, its hard to tell. ts a lot easier to think about what the actual heights are than just these arrows. o lets suppose x has height k. Thats pretty generic. And its right heavy, so that means the y has height k minus 1. And then this is right heavy, so this has height k minus 2. And this is something smaller then k minus 2. n fact, because this is AVL, we assume that x was the lowest that is not AVL. o y is AVL. And so this is going to be k minus 3, and this is going to be k minus 3 because these differ by 2. You can prove by a simple induction you never get more than 2 out of whack because were just adding 1, off by 1. o we got off by 2. o this is the bad situation. Now we can just update the heights over here. o k minus 3 for a, k minus 3 for b, k minus 2 for c. Those dont change because we didnt touch those trees, and height is about going down, not up. And so this becomes k minus 2, and this becomes k minus 1. And so we changed the height of the root, but now you can see that life is good. This is now balanced between k minus 3 and k minus 3. This is now balanced between k minus 2 and k minus 2. And now the parent of y may be messed up, and thats why after this we go to the parent of y, see if its messed up, but keep working our way up. But it worked. And in the interest of time, will not check the case where y is balanced, but it works out, too. And see the notes. o the other case is where we do two rotations. And in general, so here x was doubly right heavy. And the else case is when the right child of x, which m going to call z here, is left heavy. Thats the one remaining situation. You do the same thing, and you check that right rotating and left rotating, which makes the nice picture, which is x, y, z, actually balances everything and you restore the AVL property. o again, check the notes on that. have a couple minutes left, and instead d like to tell you a little bit about how this fits into bigpicture land. Two things want to talk about. One is you could use this, of course, to sort, which is, if you want to sort n numbers, you insert them and you do inorder traversal. How long does this take? norder traversal takes linear time. Thats the sense in which were storing things in sorted order. nserting n items well, each insert takes h time, but now were guaranteed that h is order log n. o all the insertions take log n time each, n log n total. o this is yet another way to sort n items in n log n time, in some ways the most powerful way. Weve seen heaps, and weve seen merge sort. They all sort. Heaps let you do two operations, insert and delete min, which a lot of times is all you care about, like in p set two. But these guys, AVL trees, let you do insert, delete, and delete min. o theyre the same in those senses, but we have the new operation, which is that we can do find next larger and next smaller, aka successor and predecessor. o you can think about what we call an abstract data type. These are the operations that you support, or that youre supposed to support. f youre into Java, you call this an interface. But this is an algorithmic specification of what your data structure is supposed to do. o we have operations like insert and delete. We have operations like find the min and things like successor and predecessor, or next larger, next smaller. You can take any subset of these and its an abstract data type. nsert, delete, and min is called a priority queue. o if you just take these first two, its called a priority queue. And there are many priority queues. This is a generic thing that you might want to do. And then the data structure on the other side is how you actually do it. This is the analog of the algorithm. OK, this is the specification. You want a priority queue. One way to do it is a heap. Another way to do it is an AVL tree. You could do it with a sorted array. You could do lots of suboptimal things, too, but in particular, heaps get these two operations. f you want all three, you basically need a balanced binary search tree. There are probably a dozen balanced binary search trees out there, at least a dozen balanced search trees, not all binary. They all achieve log n. o it doesnt really matter. There are various practical issues, constant factors, things like that. The main reason you prefer a heap is that its in place. t doesnt use any extra space. Here, youve got pointers all over the place. You lose a constant factor in space. But from a theoretical standpoint, if you dont care about constant factors, AVL trees are really good because they get everything that weve seen so far and log n. And ll stop there.","All right, so m going to define n sub h is the minimum number of nodes thats possible in an AVL tree of height h. This is sort of the inverse of what we care about, but if we can solve the inverse, we can solve the thing. f do a right rotation where this is x and this is y, what ll get is x, y, and then this is b. This is whats in between x and y. And so itll go here. And what we do in this case is right rotate of x. And so we get x, y, a, b, c. could have drawn this no matter what case were in, so we need to check this actually works. What is the height of the tree? You store key in every node, and you have this BT property, or also called the search property, that every node if you have a node the stores key x, everybody in the left subtree stores a key thats less than or equal to x, and everyone thats in the right subtree stores a key thats greater than or equal to x. o not just the left and right children, but every descendant way down there is smaller than x. Every descendent way down there is greater than x. o when you have a binary search tree like this, if you want to know the sorted order, you do whats called an inorder traversal. And the else case is when the right child of x, which m going to call z here, is left heavy. And so we get is that if we invert this, this says n sub h is bigger than some phi to the h. This is our n, basically. And so we changed the height of the root, but now you can see that life is good. When we do that, theres still may be violations higher up, because when you add a node, you change the height of this subtree, the height of this subtree, the height of this subtree, and the height of this subtree, potentially. And so if we can achieve this property, that the left and right subtrees have about the same height, well be done. And then this is right heavy, so this has height k minus 2. o that means that the larger has height h minus 1, because the longest path to a leaf is going to be down this way. o said we store the heights, but what care about is which height is larger, the left or the right. We want to prove thats order log n. But itll be a lot easier to think about the reverse, which is, if fix the height to be h, whats the fewest nodes that can pack in? And this is something smaller then k minus 2. n fact, because this is AVL, we assume that x was the lowest that is not AVL. And so this is going to be k minus 3, and this is going to be k minus 3 because these differ by 2. You guys are worried about the last level and actually what the tree looks like, but in fact, all you need is this. Here, want to store a height field, and because have this nice local rule that tells me how to compute the height of a node using just local information the height of its left child, the height of its right child. Wed like to keep the height order log n. ts a little harder to think about keeping the height order log n than it is to think about keeping the tree balance, meaning the left and right sides are more or less equal. This is 0. We talked about the height of the tree, but d also like to talk about the height of a node in a tree. want to rotate 29 to the right, which means 29 and 26 this is x. This is y. turn them, and so get 26 here now, and 29 is the new right child. Because know the base case is always going to be n order 1 is order 1. o for algorithms, thats usually all you need for base case, but its good that you think about it. o this used to be the triangle a. And in this case, its just the node 23. What we would like is for it to be exponential, because that means theres a lot of nodes in a height h AVL tree. o the trouble is that 65 you have a left subtree, which has height 2 or sorry, height 1, guess because the longest path from here to a leaf is 1. ADENE: o you have only 1 node in the last level, so it would be 1/2 to the h plus 1. ADENE: ts the level that the node is at. You get 3. o its going to always be this is just a formula. We want to prove that it implies that the height is at most some constant times log n. We know its at least log n, but also like it to be not much bigger. Balanced, remember, means that the height of them is always order log n. o were just going to assume for now that we can somehow achieve this property. The height of the tree. This is the correct this is the worst case. What prefer is a situation like this, where with height h, have to put in 2 to the h nodes. The height is log n. This would be the balance case. What we saw in last lecture and recitation is that these basic binary search trees, where when you insert a node you just walk down the tree to find where that item fits like if youre trying to insert 30, you go left here, go right here, go right here, and say, oh 30 fits here. o this is called left rotate of x. OK, so heres the thing we can do with binary search trees. But lets say that the right subtree has height one more than the left for every node. PROFEOR: inus 2 is the right way to go because want to know n sub h is greater than something in order to get a less than down here. And its in the code that corresponds to this lecture, so you can take a look at it. One is you could use this, of course, to sort, which is, if you want to sort n numbers, you insert them and you do inorder traversal. o h is the height of the BT. This is still doubly left heavy, this is left heavy, and this is even. t used to be y. Now its x. The parent of c was y. ts still y. o in a constant number of pointer changes, can go from this to this. OK, m going to assume that the right child is the heavier one like we did before. The height of the left child maxed with the height of the right child plus 1. o when we have this picture and were really sad because this looks like a mess, what wed like to do is fix it. And its right heavy, so that means the y has height k minus 1. Because the one thing we know is that our tree looks like this. This is the easy way to do it and just get some constant times log n. lear? This is x. This is y. And so get 55 and get 50. PROFEOR: The level that the node is at. You take the max of the height of the children. The height is always less than 1.44 times log n. All we care about is some constant, but this is a pretty good constant. You do the same thing, and you check that right rotating and left rotating, which makes the nice picture, which is x, y, z, actually balances everything and you restore the AVL property. f you dont mind, m just going to write 41, and then theres all the stuff on the left. ts all about, what is the level of a node? t is a right rotate of 29. o thats what were going to do. And then the data structure on the other side is how you actually do it. And there the height is n. Whats the definition of height? By the way, use that log is monatomic here, but it is, so were good. And we rotate 50 to the left. o this is the recurrence. And to the right get 65. And what changes is now this is even. Because AVL trees are going to use the heights of the nodes. And what we would like is that h sub l and h sub r are more or less the same. What we really want to know is how h relates to n, which is just inverting this formula. And its annoying to draw what the heights are, but ll do it. Every two steps of h, get another factor of 2. o when you invert and do the log, this means that h is also less than log base 2 of n. Log base 2 because of that. Well, this is a leaf, so its height is 0. This is right heavy. This is right heavy. p to you. o the one thing thats missing from these examples is that you might have to do more than two rotations. But its really easy to prove that its, at most, 2 log n. o keep this in mind in case we ask you to analyze variance of AVL trees, like in problem set three. And to the right get 55. This is good. By the height of the subtree, mean the height of its root. Because we assumed that the right one is higher, so that x was right heavy. All thats happening here, all of the paths are length log n. Here, there is a path of length n. ome of them are shorter, but in fact, the average path is n over 2. ts really bad. This is a generic thing that you might want to do. Well, the height is the max of the two directions. f just do this, which is like changing which is higher, x or y, and whatever the parent of x was becomes the parent of y. And vice versa, in fact. We dont normally draw these in the pictures, but its convenient that dont have to do special cases when the left child doesnt exist and the right child doesnt exist. Well, its one less than the height of this. Here, its basically exactly log n, 1 times log n. ts always going to be at least log n, because if youre storing n things in a binary tree, you need to have height at least log n. o in fact, it will be theta log n if your tree is balanced. And this is a right arrow. ADENE: 2 to the h minus 1 . And in the interest of time, will not check the case where y is balanced, but it works out, too. And then we started with a regular vanilla binary search tree, and then we maintained every time we did an operation on the tree, we also updated the size of the subtree rooted at that node, the size field. Whats the height of this? What we really care about is, for n nodes, how large can the height be? The way we find that node is we start at the node that we changed. o we have this recurrence, n sub h. This is the computer scientist way to solve the recurrence. This is more annoying, because you look at this thing, this little path. This is the analog of the algorithm. The algorithm is going to walk up the tree and say, oh, when do get something bad? Because for the very unbalanced tree, have a height of n, and only need to put n nodes. And then whatever was the left child of x becomes the left child of x in the picture. o what do you think is the worst case? We call it a left rotate because the root moves to the left. We want it to be theta log base 2 of n. And heres the bound. What we did was look at the lowest violation of the AVL property and we fixed it. o have some node x. For every node x, want the left and right subtrees to be almost balanced. Theres one guy here, plus all the nodes on the left, plus all the nodes on the right. OK, so this is highlight this because were going to be working a lot with height today. But as we saw at the end of last class, you can have a very unbalanced tree, which is just a path. How do you compute the height of a node? Left and right are the same. And the right subtree has some height, r. ve drawn them as the same, but in general they might be different. PROFEOR: For every node, lets make the right side have a height of one larger than the left side. The height. Now, could say differ by at most 0, that the left and right have exactly the same heights. You could go left and you get a path of length 1, or you could go right and get a path of length 2. ADENE: The distance from it to the root. OK, this is the specification. o this subtree as ve drawn it is higher than the left one by 2, in fact. And at all times, you want to know the sorted order of your elements by storing them in a nice binary search tree. ts like the only thing you need to know. That is correct, but want to know the whole tree.",0.158203125
52,52,"ve been multiplying matrices already, but certainly time for me to discuss the rules for matrix multiplication. And the interesting part is the many ways you can do it, and they all give the same answer. And theyre all important. o matrix multiplication, and then, come inverses. o we mentioned the inverse of a matrix. Thats a big deal. Lots to do about inverses and how to find them. Okay, so ll begin with how to multiply two matrices. First way, okay, so suppose have a matrix A multiplying a matrix B and giving me a result well, could call it . A times B. Okay. o, let me just review the rule for this entry. Thats the entry in row i and column j. o thats the i j entry. Right there is i j. We always write the row number and then the column number. o might might maybe take it 3 4, just to make it specific. o instead of i j, let me use numbers. 3 4. o where does that come from, the three four entry? t comes from row three, here, row three and column four, as you know. olumn four. And can just write down, or can we write down the formula for it? f we look at the whole row and the whole column, the quick way for me to say it is row three of A could use a dot for dot product. wont often use that, actually. Dot column four of B. But this gives us a chance to just, like, use a little matrix notation. What are the entries? Whats this first entry in row three? That number thats sitting right there is... A, so its got two indices and what are they? 3 1. o theres an a 3 1 there. Now whats the first guy at the top of column four? o whats sitting up there? B 1 4, right. o that this dot product starts with A 3 1 times B 1 4. And then whats the next so this is like m accumulating this sum, then comes the next guy, A 3 2, second column, times B 2 4, second row. o its b A 3 2, B 2 4 and so on. Just practice with indices. Oh, let me even practice with a summation formula. o this is most of the course, use whole vectors. very seldom, get down to the details of these particular entries, but here wed better do it. o its some kind of a sum, right? Of things in row three, column K shall say? Times things in row K, column four. Do you see that thats what were seeing here? This is K is one, here K is two, on along so the sum goes all the way along the row and down the column, say, one to N. o thats what the three four entry looks like. A sum of a three K b K four. Just takes a little practice to do that. Okay. And well, maybe should say when are we allowed to multiply these matrices? What are the shapes of these things? The shapes are if we allow them to be not necessarily square matrices. f theyre square, theyve got to be the same size. f theyre rectangular, theyre not the same size. f theyre rectangular, this might be well, always think of A as m by n. m rows, n columns. o that sum goes to n. Now whats the point how many rows does B have to have? n. The number of rows in B, the number of guys that we meet coming down has to match the number of ones across. o B will have to be n by something. Whatever. P. o the number of columns here has to match the number of rows there, and then whats the result? Whats the shape of the result? Whats the shape of , the output? Well, its got these same m rows its got m rows. And how many columns? P. m by P. Okay. o there are m times P little numbers in there, entries, and each one, looks like that. Okay. o thats the standard rule. Thats the way people think of multiplying matrices. do it too. But want to talk about other ways to look at that same calculation, looking at whole columns and whole rows. Okay. o can do A B again? A B equaling again? But now, tell me about... ll put it up here. o here goes A, again, times B producing . And again, this is m by n. This is n by P and this is m by P. Okay. Now want to look at whole columns. want to look at the columns of heres the second way to multiply matrices. Because m going to build on what know already. How do multiply a matrix by a column? know how to multiply this matrix by that column. hall call that column one? That tells me column one of the answer. The matrix times the first column is that first column. Because none of this stuff entered that part of the answer. The matrix times the second column is the second column of the answer. Do you see what m saying? That could think of multiplying a matrix by a vector, which already knew how to do, and can think of just P columns sitting side by side, just like resting next to each other. And multiply A times each one of those. And get the P columns of the answer. Do you see this as this is quite nice, to be able to think, okay, matrix multiplication works so that can just think of having several columns, multiplying by A and getting the columns of the answer. o, like, heres column one shall call that column one? And whats going in there is A times column one. Okay. o thats the picture a column at a time. o what does that tell me? What does that tell me about these columns? These columns of are combinations, because weve seen that before, of columns of A. Every one of these comes from A times this, and A times a vector is a combination of the columns of A. And it makes sense, because the columns of A have length m and the columns of have length m. And every column of is some combination of the columns of A. And its these numbers in here that tell me what combination it is. Do you see that? That in that answer, , m seeing stuff thats combinations of these columns. Now, suppose look at it thats two ways now. The third way is look at it by rows. o now let me change to rows. Okay. o now can think of a row of A a row of A multiplying all these rows here and producing a row of the product. o this row takes a combination of these rows and thats the answer. o these rows of are combinations of what? Tell me how to finish that. The rows of , when have a matrix B, its got its rows and multiply by A, and what does that do? t mixes the rows up. t creates combinations of the rows of B, thanks. Rows of B. Thats what wanted to see, that this answer can see where the pieces are coming from. The rows in the answer are coming as combinations of these rows. The columns in the answer are coming as combinations of those columns. And so thats three ways. Now you can say, okay, whats the fourth way? The fourth way so thats now weve got, like, the regular way, the column way, the row way and whats left? The one that can well, one way is columns times rows. What happens if multiply o this was row times column, it gave a number. Okay. Now want to ask you about column times row. f multiply a column of A times a row of B, what shape ending up with? o if take a column times a row, thats definitely different from taking a row times a column. o a column of A was whats the shape of a column of A? m by one. A column of A is a column. ts got m entries and one column. And whats a row of B? ts got one row and P columns. o whats the shape what do get if multiply a column by a row? get a big matrix. get a fullsized matrix. f multiply a column by a row should we just do one? Let me take the column two three four times the row one six. That product there mean, when m just following the rules of matrix multiplication, those rules are just looking like kind of petite, kind of small, because the rows here are so short and the columns there are so short, but theyre the same length, one entry. o whats the answer? Whats the answer if do two three four times one six, just for practice? Well, whats the first row of the answer? Two twelve. And the second row of the answer is three eighteen. And the third row of the answer is four twenty four. Thats a very special matrix, there. Very special matrix. What can you tell me about its columns, the columns of that matrix? Theyre multiples of this guy, right? Theyre multiples of that one. Which follows our rule. We said that the columns of the answer were combinations, but theres only to take a combination of one guy, its just a multiple. The rows of the answer, what can you tell me about those three rows? Theyre all multiples of this row. Theyre all multiples of one six, as we expected. But m getting a fullsized matrix. And now, just to complete this thought, if have let me write down the fourth way. A B is a sum of columns of A times rows of B. o that, for example, if my matrix was two three four and then had another column, say, seven eight nine, and my matrix here has say, started with one six and then had another column like zero zero, then heres the fourth way, okay? ve got two columns there, ve got two rows there. o the beautiful rule is see, the whole thing by columns and rows is that can take the first column times the first row and add the second column times the second row. o thats the fourth way that can take columns times rows, first column times first row, second column times second row and add. Actually, what will get? What will the answer be for that matrix multiplication? Well, this one its just going to give us zero, so in fact m back to this thats the answer, for that matrix multiplication. m happy to put up here these facts about matrix multiplication, because it gives me a chance to write down special matrices like this. This is a special matrix. All those rows lie on the same line. All those rows lie on the line through one six. f draw a picture of all these row vectors, theyre all the same direction. f draw a picture of these two column vectors, theyre in the same direction. Later, would use this language. Not too much later, either. would say the row space, which is like all the combinations of the rows, is just a line for this matrix. The row space is the line through the vector one six. All the rows lie on that line. And the column space is also a line. All the columns lie on the line through the vector two three four. o this is like a really minimal matrix. And its because of these ones. Okay. o thats a third way. Now want to say one more thing about matrix multiplication while were on the subject. And its this. You could also multiply You could also cut the matrix into blocks and do the multiplication by blocks. Yet thats actually so, useful that want to mention it. Block multiplication. o could take my matrix A and could chop it up, like, maybe just for simplicity, let me chop it into two into four square blocks. uppose its square. Lets just take a nice case. And B, suppose its square also, same size. o these sizes dont have to be the same. What they have to do is match properly. Here they certainly will match. o heres the rule for block multiplication, that if this has blocks like, A so maybe A1, A2, A3, A4 are the blocks here, and these blocks are B1, B2,3 and B4? Then the answer can find block. And if you tell me whats in that block, then m going to be quiet about matrix multiplication for the rest of the day. What goes into that block? You see, these might be this matrix might be these matrices might be, like, twenty by twenty with blocks that are ten by ten, to take the easy case where all the blocks are the same shape. And the point is that could multiply those by blocks. And what goes in here? Whats that block in the answer? A1 B1, thats a matrix times a matrix, its the right size, ten by ten. Any more? Plus, what else goes in there? A2 B3, right? ts just like block rows times block columns. Nobody, think, not even Gauss could see instantly that it works. But somehow, if we check it through, all five ways were doing the same multiplications. o this familiar multiplication is what were really doing when we do it by columns, by rows by columns times rows and by blocks. Okay. just have to, like, get the rules straight for matrix multiplication. Okay. All right, m ready for the second topic, which is inverses. Okay. Ready for inverses. And let me do it for square matrices first. Okay. o ve got a square matrix A. And it may or may not have an inverse, right? Not all matrices have inverses. n fact, thats the most important question you can ask about the matrix, is if its if you know its square, is it invertible or not? f it is invertible, then there is some other matrix, shall call it A inverse? And whats the if A inverse exists theres a big ""if"" here. f this matrix exists, and itll be really central to figure out when does it exist? And then if it does exist, how would you find it? But whats the equation here that havent that have to finish now? This matrix, if it exists multiplies A and produces, think, the identity. But a real an inverse for a square matrix could be on the right as well this is true, too, that its if have a yeah in fact, this is not this is probably the this is something thats not easy to prove, but it works. That a left square matrices, a left inverse is also a right inverse. f can find a matrix on the left that gets the identity, then also that matrix on the right will produce that identity. For rectangular matrices, well see a left inverse that isnt a right inverse. n fact, the shapes wouldnt allow it. But for square matrices, the shapes allow it and it happens, if A has an inverse. Okay, so give me some cases lets see. hate to be negative here, but lets talk about the case with no inverse. o these matrices are called invertible or nonsingular those are the good ones. And we want to be able to identify how if were given a matrix, has it got an inverse? an talk about the singular case? No inverse. All right. Best to start with an example. Tell me an example lets get an example up here. Lets make it two by two of a matrix that has not got an inverse. And lets see why. Let me write one up. No inverse. Lets see why. Let me write up one three two six. Why does that matrix have no inverse? You could answer that various ways. Give me one reason. Well, you could if you know about determinants, which youre not supposed to, you could take its determinant and you would get Zero. Okay. Now all right. Let me ask you other reasons. mean, as for other reasons that that matrix isnt invertible. Here, could use what m saying here. uppose A times other matrix gave the identity. Why is that not possible? Because oh, yeah m thinking about columns here. f multiply this matrix A by some other matrix, then the the result what can you tell me about the columns? Theyre all multiples of those columns, right? f multiply A by another matrix that the product has columns that come from those columns. o can get the identity matrix? No way. The columns of the identity matrix, like one zero its not a combination of those columns, because those two columns lie on the both lie on the same line. Every combination is just going to be on that line and cant get one zero. o, do you see that sort of column picture of the matrix not being invertible. n fact, heres another reason. This is even a more important reason. Well, how can say more important? All those are important. This is another way to see it. A matrix has no inverse yeah here now this is important. A matrix has no a square matrix wont have an inverse if theres no inverse because can solve can find an X of a vector X with A times this A times X giving zero. This is the reason like best. That matrix wont have an inverse. an you well, let me change to . o tell me a vector X that, solves A X equals zero. mean, this is, like, the key equation. n mathematics, all the key equations have zero on the righthand side. o whats the X? Tell me an X here so now m going to put slip in the X that you tell me and m going to get zero. What X would do that job? Three and negative one? s that the one you picked, or yeah. Or another well, if you picked zero with zero, m not so excited, right? Because that would always work. o its really the fact that this vector isnt zero thats important. ts a nonzero vector and three negative one would do it. That just says three of this column minus one of that column is the zero column. Okay. o now know that A couldnt be invertible. But whats the reasoning? f A X is zero, suppose multiplied by A inverse. Yeah, well heres the reason. Here this is why this spells disaster for an inverse. The matrix cant have an inverse if some combination of the columns gives z it gives nothing. Because, could take A X equals zero, could multiply by A inverse and what would discover? uppose take that equation and multiply by if A inverse existed, which of course m going to come to the conclusion it cant because if it existed, if there was an A inverse to this dopey matrix, would multiply that equation by that inverse and would discover X is zero. f multiply A by A inverse on the left, get X. f multiply by A inverse on the right, get zero. o would discover X was zero. But it X is not zero. X this guy wasnt zero. There it is. ts three minus one. o, conclusion only, it takes us some time to really work with that conclusion our conclusion will be that noninvertible matrices, singular matrices, some combinations of their columns gives the zero column. They they take some vector X into zero. And theres no way A inverse can recover, right? Thats what this equation says. This equation says take this vector X and multiplying by A gives zero. But then when multiply by A inverse, can never escape from zero. o there couldnt be an A inverse. Where here okay, now fix all right. Now let me take all right, back to the positive side. Lets take a matrix that does have an inverse. And why not invert it? Okay. an so let me take on this third board a matrix shall fix that up a little? Tell me a matrix that has got an inverse. Well, let me say one three two what shall put there? Well, dont put six, guess is right? Do any favorites here? One? Or eight? dont care. What, seven? even. Okay. even is a lucky number. All right, seven, okay. Okay. o now whats our idea? We believe that this matrix is invertible. Those who like determinants have quickly taken its determinant and found it wasnt zero. Those who like columns, and probably that that department is not totally popular yet but those who like columns will look at those two columns and say, hey, they point in different directions. o can get anything. Now, let me see, what do mean? How going to computer A inverse? o A inverse heres A inverse, now, and have to find it. And what do get when do this multiplication? The identity. You know, forgive me for taking two by twos, but lts good to keep the computations manageable and let the ideas come out. Okay, now whats the idea want? m looking for this matrix A inverse, how going to find it? Right now, ve got four numbers to find. m going to look at the first column. Let me take this first column, A B. Whats up there? What tell me this. What equation does the first column satisfy? The first column satisfies A times that column is one zero. The first column of the answer. And the second column, D, satisfies A times that second column is zero one. You see that finding the inverse is like solving two systems. One system, when the righthand side is one zero m just going to split it into two pieces. dont even need to rewrite it. can take A times so let me put it here. A times column j of A inverse is column j of the identity. ve got n equations. ve got, well, two in this case. And they have the same matrix, A, but they have different righthand sides. The righthand sides are just the columns of the identity, this guy and this guy. And these are the two solutions. Do you see what m going m looking at that equation by columns. m looking at A times this column, giving that guy, and A times that column giving that guy. o Essentially so this is like the Gauss were back to Gauss. Were back to solving systems of equations, but were solving weve got two righthand sides instead of one. Thats where Jordan comes in. o at the very beginning of the lecture, mentioned GaussJordan, let me write it up again. Okay. Heres the GaussJordan idea. GaussJordan solve two equations at once. Okay. Let me show you how the mechanics go. How do solve a single equation? o the two equations are one three two seven, multiplying A B gives one zero. And the other equation is the same one three two seven multiplying D gives zero one. Okay. Thatll tell me the two columns of the inverse. ll have inverse. n other words, if can solve with this matrix A, if can solve with that righthand side and that righthand side, m invertible. ve got it. Okay. And Jordan sort of said to Gauss, solve them together, look at the matrix if we just solve this one, would look at one three two seven, and how do deal with the righthand side? stick it on as an extra column, right? Thats this augmented matrix. Thats the matrix when m watching the righthand side at the same time, doing the same thing to the right side that do to the left? o just carry it along as an extra column. Now m going to carry along two extra columns. And m going to do whatever Gauss wants, right? m going to do elimination. m going to get this to be simple and this thing will turn into the inverse. This is whats coming. m going to do elimination steps to make this into the identity, and lo and behold, the inverse will show up here. K lets do it. Okay. o what are the elimination steps? o you see heres my matrix A and heres the identity, like, stuck on, augmented on. TDENT: m sorry... TRANG: Yeah? TDENT: is the two and the three supposed to be switched? TRANG: Did oh, no, they werent supposed to be switched. orry. Thanks. Okay. Thank you very much. And there ve got them right. Okay, thanks. Okay. o lets do elimination. All right, its going to be simple, right? o take two of this row away from this row. o this row stays the same and two of those come away from this. That leaves me with a zero and a one and two of these away from this is that what youre getting after one elimination step Let me sort of separate the the left half from the right half. o two of that first row got subtracted from the second row. Now this is an upper triangular form. Gauss would quit, but Jordan says keeps going. se elimination upwards. ubtract a multiple of equation two from equation one to get rid of the three. o lets go the whole way. o now m going to this guy is fine, but m going to what do do now? Whats my final step that produces the inverse? multiply this by the right number to get up to ther to remove that three. o guess, since this is a one, theres the pivot sitting there. multiply it by three and subtract from that, so what do get? ll have one zero oh, yeah that was my whole point. ll multiply this by three and subtract from that, which will give me seven. And multiply this by three and subtract from that, which gives me a minus three. And whats my hope, belief? Here started with A and the identity, and ended up with the identity and who? That better be A inverse. Thats the Gauss Jordan idea. tart with this long matrix, doublelength A , eliminate, eliminate until this part is down to , then this one will must be for some reason, and weve got to find the reason must be A inverse. hall just check that it works? Let me just check that can multiply this matrix this part times A, ll carry A over here and just do that multiplication. Youll see ll do it the old fashioned way. even minus six is a one. Twenty one minus twenty one is a zero, minus two plus two is a zero, minus six plus seven is a one. heck. o that is the inverse. Thats the GaussJordan idea. o, youll one of the homework problems or more than one for Wednesday will ask you to go through those steps. think you just got to go through GaussJordan a couple of times, but yeah just to see the mechanics. But the, important thing is, why is, like, what happened? Why did we why did we get A inverse there? Let me ask you that. We got so we take We do row reduction, we do elimination on this long matrix A until the first half Then a second half is A inverse. is up. Well, how do see that? Let me put up here how see that. o heres my GaussJordan thing, and m doing stuff to it. o m well, whole lot of Es. Remember those are those elimination matrices. Those are the those are the things that we figured out last time. Yes, thats what an elimination step is its in matrix form, m multiplying by some Es. And the result well, so m multiplying by a whole bunch of Es. o, get a can call the overall matrix E? Thats the elimination matrix, the product of all those little pieces. What do mean by little pieces? Well, there was an elimination matrix that subtracted two of that away from that. Then there was an elimination matrix that subtracted three of that away from that. guess in this case, that was all. o there were just two Es in this case, one that did this step and one that did this step and together they gave me an E that does both steps. And the net result was to get an here. And you can tell me what that has to be. This is, like, the picture of what happened. f E multiplied A, whatever that E is we never figured it out in this way. But whatever that E times that E is, E times A is Whats E times A? ts . That E, whatever the heck it was, multiplied A and produced o E must be E A equaling tells us what E is, . namely it is TDENT: ts the inverse of A. TRANG: ts the inverse of A. Great. And therefore, when the second half, when E multiplies , its E Put this A inverse. You see the picture looking that way? E times A is the identity. t tells us what E has to be. t has to be the inverse, and therefore, on the righthand side, where E where we just smartly tucked on the identity, its turning in, step by step ts turning into A inverse. There is the statement of GaussJordan elimination. Thats how you find the inverse. Where we can look at it as elimination, as solving n equations at the same time and tacking on n columns, solving those equations and up goes the n columns of A inverse Okay, thanks. ee you on Wednesday.","This is K is one, here K is two, on along so the sum goes all the way along the row and down the column, say, one to N. o thats what the three four entry looks like. o the beautiful rule is see, the whole thing by columns and rows is that can take the first column times the first row and add the second column times the second row. These columns of are combinations, because weve seen that before, of columns of A. Every one of these comes from A times this, and A times a vector is a combination of the columns of A. And it makes sense, because the columns of A have length m and the columns of have length m. And every column of is some combination of the columns of A. And its these numbers in here that tell me what combination it is. Do you see this as this is quite nice, to be able to think, okay, matrix multiplication works so that can just think of having several columns, multiplying by A and getting the columns of the answer. That just says three of this column minus one of that column is the zero column. o that is the inverse. But a real an inverse for a square matrix could be on the right as well this is true, too, that its if have a yeah in fact, this is not this is probably the this is something thats not easy to prove, but it works. f multiply this matrix A by some other matrix, then the the result what can you tell me about the columns? would say the row space, which is like all the combinations of the rows, is just a line for this matrix. The rows of , when have a matrix B, its got its rows and multiply by A, and what does that do? The matrix times the second column is the second column of the answer. And get the P columns of the answer. What can you tell me about its columns, the columns of that matrix? That leaves me with a zero and a one and two of these away from this is that what youre getting after one elimination step Let me sort of separate the the left half from the right half. uppose take that equation and multiply by if A inverse existed, which of course m going to come to the conclusion it cant because if it existed, if there was an A inverse to this dopey matrix, would multiply that equation by that inverse and would discover X is zero. Well, this one its just going to give us zero, so in fact m back to this thats the answer, for that matrix multiplication. A B is a sum of columns of A times rows of B. o that, for example, if my matrix was two three four and then had another column, say, seven eight nine, and my matrix here has say, started with one six and then had another column like zero zero, then heres the fourth way, okay? Let me take the column two three four times the row one six. A times column j of A inverse is column j of the identity. The one that can well, one way is columns times rows. And if you tell me whats in that block, then m going to be quiet about matrix multiplication for the rest of the day. The rows of the answer, what can you tell me about those three rows? P. o the number of columns here has to match the number of rows there, and then whats the result? The matrix times the first column is that first column. That tells me column one of the answer. The columns of the identity matrix, like one zero its not a combination of those columns, because those two columns lie on the both lie on the same line. Thats the matrix when m watching the righthand side at the same time, doing the same thing to the right side that do to the left? Tell me an X here so now m going to put slip in the X that you tell me and m going to get zero. The first column of the answer. And the interesting part is the many ways you can do it, and they all give the same answer. And again, this is m by n. This is n by P and this is m by P. Okay. f we look at the whole row and the whole column, the quick way for me to say it is row three of A could use a dot for dot product. Well, whats the first row of the answer? And whats going in there is A times column one. And the second row of the answer is three eighteen. And you can tell me what that has to be. And Jordan sort of said to Gauss, solve them together, look at the matrix if we just solve this one, would look at one three two seven, and how do deal with the righthand side? o whats the shape what do get if multiply a column by a row? And the other equation is the same one three two seven multiplying D gives zero one. The first column satisfies A times that column is one zero. The righthand sides are just the columns of the identity, this guy and this guy. know how to multiply this matrix by that column. Thatll tell me the two columns of the inverse. Let me just check that can multiply this matrix this part times A, ll carry A over here and just do that multiplication. Rows of B. Thats what wanted to see, that this answer can see where the pieces are coming from. That product there mean, when m just following the rules of matrix multiplication, those rules are just looking like kind of petite, kind of small, because the rows here are so short and the columns there are so short, but theyre the same length, one entry. We said that the columns of the answer were combinations, but theres only to take a combination of one guy, its just a multiple. want to look at the columns of heres the second way to multiply matrices. o now can think of a row of A a row of A multiplying all these rows here and producing a row of the product. m going to do elimination steps to make this into the identity, and lo and behold, the inverse will show up here. Lets make it two by two of a matrix that has not got an inverse. o, do you see that sort of column picture of the matrix not being invertible. Whats the answer if do two three four times one six, just for practice? multiply this by the right number to get up to ther to remove that three. multiply it by three and subtract from that, so what do get? Where we can look at it as elimination, as solving n equations at the same time and tacking on n columns, solving those equations and up goes the n columns of A inverse Okay, thanks. f can find a matrix on the left that gets the identity, then also that matrix on the right will produce that identity. And the second column, D, satisfies A times that second column is zero one. o this row takes a combination of these rows and thats the answer. o thats the fourth way that can take columns times rows, first column times first row, second column times second row and add. That number thats sitting right there is... A, so its got two indices and what are they? What will the answer be for that matrix multiplication? n fact, thats the most important question you can ask about the matrix, is if its if you know its square, is it invertible or not? The fourth way so thats now weve got, like, the regular way, the column way, the row way and whats left? TDENT: is the two and the three supposed to be switched? A matrix has no a square matrix wont have an inverse if theres no inverse because can solve can find an X of a vector X with A times this A times X giving zero. o a column of A was whats the shape of a column of A?",0.1568552981390049
53,53,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer highquality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. ANA BELL: All right, everyone, lets get started. o today is going to be the second lecture on objectoriented programming. o just a quick recap of last time on onday, we saw we were introduced to this idea of objectoriented programming, and we saw these things called abstract data types. And these abstract data types we implemented through Python classes. And they allowed us to create our own data types that sort of abstracted a general object of our choosing, right? o weve used lists before, for example. But with abstract data types, we were able to create objects that were of our own types. We saw the coordinate example. And then at the end of the class, we saw the fraction example. o today were going to talk a little bit more about objectoriented programming and classes. Were going to see a few more examples. And were going to talk about a few other nuances of classes, talk about information hiding and class variables. And in the second half of the lecture, were going to talk about the idea of inheritance. o were going to use objectoriented programming to simulate how real life works. o in real life, you have inheritance. And in objectoriented programming, you can also simulate that. OK, so the first few slides are going to be a little bit of recap just to make sure that everyones on the same page before introduce a couple of new concepts related to classes. o recall that when in the last lecture, we talked about writing code from two different perspectives, right? The first was from someone who wanted to implement a class. o implementing the class meant defining your own object type. o you defined the object type when you defined the class. And then you decided what data attributes you wanted to define in your object. o what data makes up the object? What is the object, OK? n addition to data attributes, we also saw these things called methods. And methods were ways to tell someone how to use your data type. o what are ways that someone can interact with the data type, OK? o thats from the point of view of someone who wants to write their own object type. o youre implementing a class. And the other perspective was to write code from the point of view of someone who wanted to use a class that was already written, OK? o this involved creating instances of objects. o youre using the object type. Once you created instances of objects, you were able to do operations on them. o you were able to see what methods whoever implemented the class added. And then, you can use those methods in order to do operations with your instances. o just looking at the coordinate example we saw last time, a little bit more in detail about what that meant so we had a class definition of an object type, which included deciding what the class name was. And the class name basically told Python what type of an object this was, OK? n this case, we decided we wanted to name a coordinate we wanted to create a oordinate object. And the type of this object was therefore going to be a coordinate. We defined the class in the sort of general way, OK? o we needed a way to be able to access data attributes of any instance. o we use this self variable, OK? And the self variable we used to refer to any instance to the data attributes of any instance in a general way without actually having a particular instance in mind, OK? o whenever we access data attributes, we would say something like self dot to access a data attribute. Youd access the attribute directly with self.x. Or if you wanted to access a method, you would say self, dot, and then the method name for example, distance. And really, the bottom line of the class definition is that your class defines all of the data so data attributes and all of the methods that are going to be common across all of the instances. o any instance that you create of a particular object type, that instance is going to have this exact same structure, OK? The difference is that every instances values are going to be different. o when youre creating instances of classes, you can create more than one instance of the same class. o we can create a oordinate object here using this syntax right here. o you say the type, and then, whatever values it takes in. And you can create more than one oordinate object. Each oordinate object is going to have different data attributes. orry, its going to have different data attribute values, OK? Every oordinate object is going to have an x value and a y value. But the x and y values among different instances are going to vary, OK? o thats the difference between defining a class and looking at a particular instance of a class. o instances have the structure of the class. o for a coordinate, all instances have an x value and a y value. But the actual values are going to vary between the different instances. OK, so ultimately, why do we want to use objectoriented programming? o, so far, the examples that weve seen were numerical, right a coordinate, a fraction. But using objectoriented programming, you can create objects that mimic real life. o if wanted to create objects of an object that defined a cat and an object that defined a rabbit, could do that with objectoriented programming. would just have to decide, as a programmer, what data and what methods d want to assign to these groups of objects, OK? o using objectoriented programming, each one of these is considered a different object. And as a different object, can decide that a cat is going to have a name, an age, and maybe a color associated with it. And these three here, on the right, each one of these rabbits is also an object. And m going to decide that m going to represent a rabbit by just an age and a color, OK? And with objectoriented programming, using these attributes, can group these three objects together and these three objects together, OK? o m grouping sets of objects that are going to have the same attributes together. And attributes this is also a recap of last time come in two forms, right, data attributes and procedural attributes. o the data attributes are basically things that define what the object is. o how do you represent a cat as an object? And its up to you, as the programmer, to decide how you want to do that. For a coordinate, it was pretty straightforward. You had an x and a y value. f were representing something more abstract like an animal, then maybe would say, well, m going to represent an animal by an age and a name, OK? o its really up to you to decide how you want to represent what data attributes you want to represent your object with. Procedural attributes were also known as methods. And the methods are essentially asking, what can your object do, OK? o how can someone who wants to use your object how can someone interact with it? o for a coordinate, we saw that you could find the distance between two coordinates. aybe for our abstract Animal object, you might have it make a sound, OK, by maybe printing to the screen or something like that. OK, this slides also a recap of how to create a class just to make sure everyones on the same page before we go on. o we defined a class using this class keyword. And we said, class, the name of the class. o now were going to create a more abstract Animal class. Were going to see, in the second half of the lecture, what it means to put something else in the parentheses. But for now, we say that an animal is an object in Python. o that means its going to have all of the properties that any other object in Python has. And as were creating this animal, were going to define how to create an instance of this class. o we say def. And this __init__ was the special method that told Python how to create an object. nside the parentheses, remember, we have the self, which is a variable that we use to refer to any instance of the class, OK? We dont have a particular instance in mind, we just want to be able to refer to any instance, OK? o we use this self variable. And then, the second parameter here is going to represent what other data we use to initialize our object with. o in this case, m going to say, m going to initialize an Animal object with an age, OK? o when create an animal, need to give it an age. nside the __init__ are any initializations that want to make. o the first thing is, m going to assign an instance variable, age so this is going to be the data attribute age to be whatever is passed in. And then, m also making another assignment here, where m assigning the data attribute name to be None originally. Later on in the code, when want to create an Animal object, say the class name. And then pass it in whatever parameters it takes in this case, the age. And m assigning it to this instance here, OK? All right, so now we have this class, Animal. Weve done the first part here, which is to initialize the class, right? o weve told Python how to create an object of this type. Theres a few other methods here that ve implemented. Next two we call getters, and the two after that we call setters, OK? And getters and setters are very commonly used when implementing a class. o getters essentially return the values of any of the data attributes, OK? o if you look carefully, get_age() is just returning self.age, and get_name() just returns self.name. o theyre very simple methods. imilarly, set_age() and set_name() were going to see what this funny equal sign is doing here in the next couple of slides. But setters do a very similar thing where theyre going to set the data attributes to whatever is passed in, OK? o those are getters and setters. And then, the last thing down here is this __str__ method. And this __str__ method is used to tell Python how to print an object of this type Animal. o if you didnt have this __str__ method, if you remember from last lecture, what ends up happening is youre going to get some message when you print your object that says, this is an object of type Animal at this memory location, which is very uninformative, right? o you implement this method here, which tells Python how to print an object of this type, OK? o the big point of this slide is that you should be using getters and setters you should be implementing getters and setters for your classes. And were going to see, in the next couple of slides, why exactly. But basically, theyre going to prevent bugs from coming into play later on if someone decides to change implementation. o we saw how to so the previous slide, this slide here, shows the implementation of the Animal class. And here we can see how we can create an instance of this object. o we can say a = Animal(3). o this is going to create a new Animal object with an age of 3. And we can access the object through the variable a. Dot notation, recall, is a way for you to access data attributes and methods of a class, OK? o you can say a.age later on in your program, and that is allowed. tll try to access the age data attribute of this particular instance of the class, a. o this is going to give you 3. However, its actually not recommended to access data attributes directly. o this is the reason so youre going to see in the next slide, the reason why were going to use getters and setters. nstead, you should use the get_age() getter method to get the age of the animal. o this is going to return, also, 3. o these are going to do the same thing. And the reason why youd want to use getters and setters is this idea of information hiding, OK? o the whole reason why were using classes in objectoriented programming is so that you can abstract certain data from the user, OK? One of the things you should be abstracting is these data attributes. o users shouldnt really need to know how a class is implemented. They should just know how to use the class, OK? o consider the following case. Lets say whoever wrote the Animal class wants to change the implementation. And theyve decided they dont want to call the data attribute ""age"" anymore, they want to call it ""years,"" OK? o when they initialize an animal they say self.years = age. o an animal still gets initialized by its age. And the age gets passed into a data attribute named ""years,"" OK? ince m implementing this class, want to have a getter, which is going to return self.years. o m not returning self.age anymore, because age is no longer the data attribute m using. o with this new implementation, if someone was using this implementation and was accessing age directly as was accessing the data attribute age directly with this new implementation, theyd actually get an error, right? Because this animal that they created using my old implementation no longer has an attribute named ""age."" And so Pythons going to spit out an error saying no attribute found or something like that, OK? f they were using the getter a.get_age() the person who implemented the class reimplemented get_age() to work correctly, right, with their new data attribute, years, as opposed to age so if was using the getter get_age(), wouldnt have run into the bug, OK? o things to remember write getters and setters for your classes. And later on in your code, use getters and setters to prevent bugs and to promote easy to maintain code. OK, so information hiding is great. But having said that, Pythons actually not very great at information hiding, OK? Python allows you to do certain things that you should never be doing. OK. o the first, weve just seen. The first is to access data attributes from outside of the class, OK? o if were to say a.age, Python allows me to do that without using a getter and setter. Python also allows you to write to data attributes from outside the class. o if implemented the class Animal assuming that age was a number, an integer, and all of my methods work as long as age is an integer, but someone decided to be smart and, outside of the class, set age to be infinite as a string, that might cause the code to crash, OK? Python allows you to do that. But now youre breaking the fact that age has to be an integer, right? o now the methods should probably be checking the fact that age is an integer all the time. The other thing that youre allowed to do is to create data attributes outside of the class definition, OK? o if wanted to create a new data attribute called ""size"" for this particular instance, Python also allows me to do that. And can set it to whatever want, OK? o Python allows you to do all these things, but its actually not good style to do any of them. o just dont do it. All right. o the last thing want to mention the last thing about classes before we go on to inheritance is this idea called default arguments. And default arguments are passed into methods. And since methods are functions, you can also pass in different arguments to functions. o for example, this set_name() method had self. And then, this new name is equal to this empty string here, OK? We havent seen this before. But this is called a default argument. And you can use the function in one of two ways. The first way is so we can create a new instance of an Animal type object with this line here, a = Animal(3). And then we can say a.set_name(). o this calls the setter method to set the name. And notice, weve always said that you have to put in parameters for everything other than self, OK? But here we have no parameters passed in. But thats OK, because newname actually has a default argument, OK? o that tells Python, if no parameter is passed in for this particular formal parameter, then use whatever is up here by default. o if havent passed in the parameter a.get_na a.set_name(), sorry a.sett_name() is going to be setting the name to the empty string, because thats what the default parameter is. o in the next line, when print a.get_name(), this is just going to print the empty string, OK? f you do want to pass in a parameter, you can do so as normal. o you can say a = Animal(3), a.set_name(), and then pass in a parameter here. And then, newname is going to be assigned to whatever parameter is passed in like that. Whatever you pass in overrides the default argument, and everything is good. o when print a.get_name(), this is going to print out the name that youve passed in. Questions about default? Yeah. ADENE: ANA BELL: What if you dont provide a default value for ADENE: For newname? ANA BELL: For newname? f you dont provide a default argument for newname and you do this case here, then thats going to give you an error. o Pythons going to say something like, expected one argument, got zero, or something like that. Great question. OK. All right, so lets move on to this idea of hierarchies, OK? o the great thing about objectoriented programming is that it allows us to add layers of abstraction to our code, all right? o we dont need to know how very, very lowlevel things are implemented in order to use them. And we can build up our code to be more and more complex as we use up these different abstractions. o consider every one of these things on this slide as being a separate object, all right? Every one of these things can be considered to be an animal, OK? According to our implementation of an animal, the one thing that an animal has is an age, OK? And thats probably true, right? Every one of these things has an age. But now want to build up on this and create separate groups, right? And each one of these separate groups that create on top of Animal is going to have its own functionality, right? Theyre going to be a little bit more specific, a little more specialized. o can create these three groups now, a cat, a rabbit, and a person group. And for example so theyre all animals, right? They all have an age. But for example, maybe a persons going to have a list of friends whereas a cat and a rabbit do not. aybe a cat has a data attribute for the number of lives they have left, right, whereas a person and a rabbit do not, OK? o you can think of adding these more specialized adding functionality to each one of these subgroups, OK? o theyre going to be more and more specialized, but all of them retaining the fact that they are animals. o they all have an age, for example. o on top of these, we can add another layer and say that a student is a person and is an animal, OK? But in addition to having an age and maybe also having a list of friends, a student might also have a major or theyre pretty, so maybe their favorite subject in school. o thats the general idea of hierarchies, OK? o we can sort of abstract the previous slide into this one and say that we have parent classes and child classes, OK? The Animal class is like our parent class. ts the highestlevel class. nheriting from the Animal class, we have these child classes or subclasses, OK? Whatever an animal can do, a person can do. Whatever an animal can do, a cat can do. And whatever an animal can do, a rabbit can do, OK that is, have an age and maybe some really basic functionality, OK? But between person, cat, and rabbit, theyre going to be varying wildly as to the kinds of things that they can do, right? But they can all do whatever Animal can do. o child classes inherit all of the data attributes and all of the methods, or behaviors, that their parents classes have, OK? But child classes can add more information. Like for example, a person can have a list of friends whereas a general animal will not. t can add more behavior. Like, maybe a cat can climb trees whereas people and rabbits cannot. Or you can also override behavior. o in the previous one, we had animal, person, student. o maybe we have, an animal doesnt speak at all, but a person can speak. o thats added functionality to the person. And maybe a person can only say hello. But then, when we talk to a student, we can override the fact override the speak() method of a person and say that a student can say, you know, have homework, or need sleep, or something like that, OK? o we have the same speak() method for both person and student, because they can both speak. But student will override the fact that they say hello with something else. OK, so lets look at some code to put this into perspective. o we have this Animal class, which weve seen before. This is the parent class, OK? t inherits from object, which means that everything that a basic object can do in Python, an animal can do, which is things like binding variables, you know, very lowlevel things, OK? Weve seen the __init__. Weve seen the two getters, the setters, and the string method to print an object of type Animal. All right, now, lets create a subclass of Animal. Well call it at, OK? We create a class named at. n parentheses, instead of putting ""object,"" we now put ""Animal."" And this tells Python that ats parent class is Animal. o everything that an animal can do, a cat can do. o that includes all of the attributes, which was age and name, and all of the methods. o all the getters, the setters, the __str__, the __init__, everything that the animal had, now the cat has the at class has. n the at class, were going to add two more methods though. The first is speak(). o speak() is going to be a method thats going to just take in the self, OK no other parameters. And all its doing is printing ""meow"" to the screen very simple, OK? o through this speak(), weve added new functionality to the class. o an animal couldnt speak, whereas a cat says ""meow."" Additionally, through this __str__ method here, were overriding the animal __str__, OK? o if we go back to the previous slide, we can see that the animals __str__ had animal, plus the name, plus the age here whereas the cats __str__ now says ""cat,"" name, and the age, OK? o this is just how chose to implement this, OK? o here ve overridden the __str__ method of the Animal class. Notice that this class doesnt have an __init__, and thats OK. Because Pythons actually going to say, well, if theres no __init__ in this particular method sorry, in this particular class then look to my parents and say, do my parents have an __init__, OK? And if so, use that __init__. o thats actually true for any other methods. o the idea here is, when you have hierarchies, you have a parent class, you have a child class, you could have a child class to that child class, and so on and so on. o you can have multiple levels of inheritance. What happens when you create an object that is of type something thats been of a type thats the child class of a child class of a child class, right? What happens when you call a method on that object? Well, Pythons are going to say, does a method with that name exist in my current class definition? And if so, use that. But if not, then, look to my parents. Do my parents know how to do that, right? Do my parents have a method for whatever want to do? f so, use that. f not, look to their parents, and so on and so on. o youre sort of tracing back up your ancestry to figure out if you can do this method or not. o lets look at a slightly more complicated example. We have a class named Person. ts going to inherit from Animal. nside this person, m going to create my own m going to create an __init__ method. And the __init__ method is going to do something different than what the animals __init__ method is doing. ts going to take in self, as usual. And its going to take in two parameters as opposed to one, a name and an age. First thing the __init__ methods doing is its calling the animals __init__ method. Why am doing that? Well, could theoretically initialize the name and the age data attributes that Animal initializes in this method. But m using the fact that ve already written code that initializes those two data attributes. o why not just use it, OK? o here, this says, m going to call the class Animal. m going to call its __init__ method. And m going to leave it up to you to not you as the class, but m talking as the programs is running m going to leave it up to you to figure out how to initialize an animal with this particular age and what to name it. o Python says, yep, know how to do this, so m going to go ahead and do that for you. o now it says person is an animal. And ve initialized the age and the name for you. The next thing m doing in the __init__ is m going to set the name to whatever name was passed in, OK? o in the __init__, notice, can do whatever want, including calling methods. And then, the last thing m doing here is m going to create a new data attribute for Person, which is a list of friends, OK? o an animal didnt have a list of friends, but a person is going to. The next four methods here are this ones a getter, so its going to return the list of friends. This is going to append a friend to the end of my list. want to make a note that actually didnt write a method to remove friends. o once you get a friend, theyre friends for life. But thats OK. The next method here is speak(), which is going to print ""hello"" to the screen. And the last method here is going to get the age difference between two people. o that just basically subtracts their age and says its a fiveyear age difference, or whatever it is. And down here, have an __str__ method, which ve overridden from the Animal, which, instead of ""animal: name,"" its going to say ""person: name : age,"" OK? o we can run this code. o thats down here. have an animal person here. o m going to run this code. And what did do? created a new person. gave it a name and an age. created another person, a name and an age. And here ve just run some methods on it, which was get_name(), get_age(), get_name(), and get_age() for each of the two people. o that printed, Jack is 30, Jill is 25. f print p1, this is going to use the __str__ method of Person. o its to print ""person:"", their name, and then, their age. p1.speak() just says ""hello."" And then, the age difference between p1 and p2 is just 5. o thats just subtracting and then printing that out to the screen. OK, so thats my person. Lets add another class. This class is going to be a student, and its going to be a subclass of Person. ince its a subclass of Person, its going to a student is going inherit all the attributes of a person, and therefore, all the attributes of an animal. The __init__ method of a student is going to be a little different from the one of Person. Were going to give it a name, an age, and a major. Notice were using default arguments here. o if create a student without giving it a major, the major is going to be set to None originally. Once again, this line here, Person. init (self, name, age), tells Python, hey, you already know how to initialize a person for me with this name and this age. o can you just do that? And Python says, yes, can do that for you. And so that saves you, maybe, like five lines of code just by calling the __init__ method that youve already written through Person, OK? o tudent has been initialized to be a person. And additionally, were going to set another data attribute for the student to be the major. And were going to set the major to be None. The student is going to get this setter here, this setter method, which is going to change the major to whatever else they want if they want to change it. And then, m going to override the speak() method. o the speak method for the person, recall, just said ""hello."" A student is going to be a little bit more complex. m going to use the fact that someone created this random class, OK? o this is where we can write more interesting code by reusing code that other people have written. o someone wrote a random class that can do cool things with random numbers. o if want to use random numbers in my code, m going to put this ""import random"" at the top of my code, which essentially brings in all of the methods from the Random class, one of the methods being this random() method. o random() is a random() method from the Random class. And this essentially gives me a number between 0 and 1, including 0 but not including 1, OK? o this random number get here is going to help me write my method for speak(), where its going to with 25% probability, its either going to say, "" have homework,"" "" need sleep,"" "" should eat,"" or ""m watching TV,"" OK? o a student is going to say one of those four things. And the last thing m doing down here is overwriting the __str__ method. o lets look at the code. m going to comment this part out, and uncomment the student, and see what we get. OK, so here, am creating the student. m creating one student whose major is , name is Alice, and age is 20. s2 is going to be another student name Beth, age 18. And the major is going to be None, because didnt pass in any major here. o by default, using the default argument, its going to be None. f print s1, s2, thats going to print out these two things over here just by whatever __str__ method does. And then m going to get the students to speak. And if run it multiple times, you can see that its going to print different things each time. o "" need sleep,"" "" have homework,"" "" need sleep,"" "" have homework,"" yeah. o every time, its going to print something different. OK, questions about inheritance in this example? OK. Last thing were going to talk about in this class is an idea of or in this lecture, is the idea of a class variable, OK? o to illustrate this, m going to create yet another subclass of my animal called a rabbit. o class variables so so far, weve seen sorry, let me back up. o so far, weve seen instance variables, right? o things like self.name, self.age, those are all instance variables. o theyre variables that are specif they are common across all of the instances of the class, right? Every instance of the class has this particular variable. But the value of the variable is going to be different between all of the different instances. o class variables are going to be variables whose values are shared between all of the instances in the class. o if one instance of the class modifies this class variable, then, any other instance of the class is going to see the modified value. o its sort of shared among all of the different instances. o were going to use class variables to keep track of rabbits. OK, so were creating this class, Rabbit. tag = 1. We havent seen something like this before. o tag is our class variable. lass variables are typically defined inside the class definition but outside of the __init__. o tag is going to be a class variable, and m initializing it to 1. nside the __init__, this tells us how to create a Rabbit object. o m going to give it self as usual, an age, and then two parents. Dont worry about the two parents for now. nside the __init__ sorry, inside the __init__ m going to call the __init__ of the animal just to do less work. Python already knows how to initialize an animal for me, so lets do that. o thats going to set the two data attributes, name and age. m going to set the data attributes for parent1, parent2 for a rabbit to be whatevers passed in. And then, this is where m going to use this class variable. o m creating this data attribute instance variable particular to a specific instance called rid, OK? And m assigning this instance variable to the class variable. And access class variables using not self, but the class name so in this case, rabbit.tag. o initially, tag is going to be 1. And then, the __init__ is going to increment the tag by 1 here, OK? o that means that, from now on, if create any other instances, the other instances are going to be accessing the updated value of tag instead of being 1. o lets do a quick drawing to show you what mean. o lets say have Rabbit.tag here, OK? o initially, tag is going to be 1, OK? And then m going to create a new Rabbit object. o this is as m calling the code, OK? o lets say this is a rabbit object oh boy, OK r1. You know, actually googled how to draw a rabbit, but that didnt help at all. OK, so r1 is going to be a new rabbit that we create. nitially, what happens is, when first create this new rabbit, its going to access the class variable, which, its current value is 1. o when create the rabbit D the rabbit D, r1.rid this is going to get the value 1. And according to the code, after set the rabbit D to whatever tag is, m going to increment the tag. o this is going to say, OK, now that ve said it, m going to go back up here and increment the tag to be 2. OK. o lets say create another Rabbit object, OK? All right, there thats a sad rabbit, r2. The D of r2 is going to be what? Well, according to the way we create a new Rabbit object is its going to access whatever the value of tag is, which is a class variable. t was changed by the previous creation of my rabbit, so now m going to access that, right? o the value is going to be 2. And according to the code, the next thing do after create the instance rid is m going to increment tag. o m incrementing the class variable to be 3, OK? o notice that all of my instances are accessing this shared resource, this shared variable called tag. o as m creating more and more rabbits, theyre all going to be incrementing the value of tag, because its shared among all of the instances. And so this value, this tag class variable, keeps track of how many different instances of a rab of how many different instances of rabbits ve created throughout my entire program, OK? o the big idea here is that class variables are shared across all the instances. o they can all modify them. But these rids, right, these instance variables, are only for that particular instance. o r2 cant have access to r1s D value, nor could change it. But it wont change it across all of the different instances, OK? o thats how the __init__ method works of Rabbit, OK? o we have these tags that keep track of how many rabbits weve created. We have a couple of getter we have some getters here to get all the parents. o now lets add a somewhat more interesting function. Oh, just want to mention, when m getting the rid, m actually using this cool zfill() function here, or method, which actually pads the beginning of any number with however many zeros in order to get to that number here. o the number 1 becomes 001 and so on. o it ensures that have this nicelooking D type thing thats always three digits long. o lets try to work with this Rabbit object. Lets define what happens when you add two rabbits together, OK in this class, not in the real world. OK. o if want to use the plus operator between two rabbit instances, have to implement this __add__ method, OK? o all m doing here is m returning a new Rabbit object, OK? Whoops, sorry about that. And lets recall the __init__ method of the rabbit, OK? o when m returning a new Rabbit object, m returning a new Rabbit object thats going to have an age of 0. elf so the Rabbit object m calling this method on is going to be the parent of the new rabbit. And other is going to be the other parent of the new rabbit, OK? o if we look at the code, and run it, this part here, m creating three rabbits, r1, r2, and r3. Notice this class variable is working as expected, because the Ds of each of my rabbits increments as create more rabbits. o we have 001, 002, 003. f print r1, and r2, and r3 that was these three lines over here the parents of r1 and r2 are None, because thats just the default yes, the default arguments for creating a rabbit. To add two rabbits together, use the plus operator between two Rabbit objects. And on the right here, m testing rabbit addition. And can print out the Ds of all my rabbits. And notice that, when ve created this new rabbit, r4, the D of it still kept incrementing. o now, the D of the fourth rabbit is 004. And then, when get r4s parents, they are as we want them to be, so r1 and r2. The other thing want to do is to compare two rabbits. o if want to compare two rabbits, want to make sure that their parents are the same. o can compare the first parent of the first rabbit with the first parent of the second rabbit and the second parent of the first rabbit to the second parent of second rabbit or getting the combinations of those two. o thats what these two Booleans are doing. o these are going to tell me these are going to be Boolean values, either True or False. And m going to return either they have the same parents of that type or the same parents crisscrossed, OK? o here, notice that m actually comparing the Ds of the rabbits as opposed to the Rabbit objects directly, OK? o if, instead of comparing the Ds in here, was comparing the parents themselves, directly, what would end up happening is this function, this method, eq(), would get called over and over again. Because here, we have parents that are rabbits. And at some point, the parents of the very, very first rabbits ever created by this program are None. And so when try to call when try to call the parent one of None, thats going to give me an error, OK, something like an attribute error where None doesnt have this parent attribute, OK? o thats why m comparing Ds here, OK? And the code in the lecture here shows you some tests about whether rabbits have the same parents. And ve created new rabbits here, r3 and r4, the addition of those two. And r5 and r6 are going to have the same parents down here True but r4 and r6 dont, OK? o just to wrap it up, objectoriented programming is the idea of creating your own collections of data where you can organize the information in a very consistent manner. o every single type of object that you create of this particular type that you create sorry, every object instance of a particular type is going to have the exact same data attributes and the exact same methods, OK? o this really comes back to the idea of decomposition and abstraction in programming. All right, thanks, everyone.","o the first thing is, m going to assign an instance variable, age so this is going to be the data attribute age to be whatever is passed in. And really, the bottom line of the class definition is that your class defines all of the data so data attributes and all of the methods that are going to be common across all of the instances. o this is going to create a new Animal object with an age of 3. Well, according to the way we create a new Rabbit object is its going to access whatever the value of tag is, which is a class variable. And m going to leave it up to you to not you as the class, but m talking as the programs is running m going to leave it up to you to figure out how to initialize an animal with this particular age and what to name it. tll try to access the age data attribute of this particular instance of the class, a. o this is going to give you 3. nside the parentheses, remember, we have the self, which is a variable that we use to refer to any instance of the class, OK? o this is going to say, OK, now that ve said it, m going to go back up here and increment the tag to be 2. This class is going to be a student, and its going to be a subclass of Person. Last thing were going to talk about in this class is an idea of or in this lecture, is the idea of a class variable, OK? And then, the last thing m doing here is m going to create a new data attribute for Person, which is a list of friends, OK? o in this case, m going to say, m going to initialize an Animal object with an age, OK? o tag is going to be a class variable, and m initializing it to 1. nside the __init__, this tells us how to create a Rabbit object. And as were creating this animal, were going to define how to create an instance of this class. And then, the __init__ is going to increment the tag by 1 here, OK? And we can access the object through the variable a. Dot notation, recall, is a way for you to access data attributes and methods of a class, OK? The other thing that youre allowed to do is to create data attributes outside of the class definition, OK? o if implemented the class Animal assuming that age was a number, an integer, and all of my methods work as long as age is an integer, but someone decided to be smart and, outside of the class, set age to be infinite as a string, that might cause the code to crash, OK? But the value of the variable is going to be different between all of the different instances. The __init__ method of a student is going to be a little different from the one of Person. And other is going to be the other parent of the new rabbit, OK? o when m returning a new Rabbit object, m returning a new Rabbit object thats going to have an age of 0. elf so the Rabbit object m calling this method on is going to be the parent of the new rabbit. nitially, what happens is, when first create this new rabbit, its going to access the class variable, which, its current value is 1. o when create the rabbit D the rabbit D, r1.rid this is going to get the value 1. The first way is so we can create a new instance of an Animal type object with this line here, a = Animal(3). The next thing m doing in the __init__ is m going to set the name to whatever name was passed in, OK? Later on in the code, when want to create an Animal object, say the class name. o if one instance of the class modifies this class variable, then, any other instance of the class is going to see the modified value. OK, so r1 is going to be a new rabbit that we create. Weve seen the two getters, the setters, and the string method to print an object of type Animal. And here we can see how we can create an instance of this object. The first is to access data attributes from outside of the class, OK? o on top of these, we can add another layer and say that a student is a person and is an animal, OK? ince its a subclass of Person, its going to a student is going inherit all the attributes of a person, and therefore, all the attributes of an animal. And we said, class, the name of the class. And this __str__ method is used to tell Python how to print an object of this type Animal. And whatever an animal can do, a rabbit can do, OK that is, have an age and maybe some really basic functionality, OK? And then, this is where m going to use this class variable. o every single type of object that you create of this particular type that you create sorry, every object instance of a particular type is going to have the exact same data attributes and the exact same methods, OK? And down here, have an __str__ method, which ve overridden from the Animal, which, instead of ""animal: name,"" its going to say ""person: name : age,"" OK? But then, when we talk to a student, we can override the fact override the speak() method of a person and say that a student can say, you know, have homework, or need sleep, or something like that, OK? o this is going to return, also, 3. o these are going to do the same thing. o speak() is going to be a method thats going to just take in the self, OK no other parameters. o that includes all of the attributes, which was age and name, and all of the methods. And then, the second parameter here is going to represent what other data we use to initialize our object with. The D of r2 is going to be what? o here, this says, m going to call the class Animal. o the value is going to be 2. This is the parent class, OK? And m going to decide that m going to represent a rabbit by just an age and a color, OK? But between person, cat, and rabbit, theyre going to be varying wildly as to the kinds of things that they can do, right? o if we go back to the previous slide, we can see that the animals __str__ had animal, plus the name, plus the age here whereas the cats __str__ now says ""cat,"" name, and the age, OK? o if you didnt have this __str__ method, if you remember from last lecture, what ends up happening is youre going to get some message when you print your object that says, this is an object of type Animal at this memory location, which is very uninformative, right? But for now, we say that an animal is an object in Python. o any instance that you create of a particular object type, that instance is going to have this exact same structure, OK? And according to the code, after set the rabbit D to whatever tag is, m going to increment the tag. And the class name basically told Python what type of an object this was, OK? o this is the reason so youre going to see in the next slide, the reason why were going to use getters and setters. And its up to you, as the programmer, to decide how you want to do that. And lets recall the __init__ method of the rabbit, OK? o thats going to set the two data attributes, name and age. And according to the code, the next thing do after create the instance rid is m going to increment tag. o you implement this method here, which tells Python how to print an object of this type, OK? And as a different object, can decide that a cat is going to have a name, an age, and maybe a color associated with it. According to our implementation of an animal, the one thing that an animal has is an age, OK? The student is going to get this setter here, this setter method, which is going to change the major to whatever else they want if they want to change it. What is the object, OK? And the type of this object was therefore going to be a coordinate. Because Pythons actually going to say, well, if theres no __init__ in this particular method sorry, in this particular class then look to my parents and say, do my parents have an __init__, OK? o all the getters, the setters, the __str__, the __init__, everything that the animal had, now the cat has the at class has. And additionally, were going to set another data attribute for the student to be the major. The next method here is speak(), which is going to print ""hello"" to the screen. m going to use the fact that someone created this random class, OK? And the __init__ method is going to do something different than what the animals __init__ method is doing. And the last method here is going to get the age difference between two people. o as m creating more and more rabbits, theyre all going to be incrementing the value of tag, because its shared among all of the instances. All right, so now we have this class, Animal. And access class variables using not self, but the class name so in this case, rabbit.tag. o the idea here is, when you have hierarchies, you have a parent class, you have a child class, you could have a child class to that child class, and so on and so on. And this __init__ was the special method that told Python how to create an object. o that means its going to have all of the properties that any other object in Python has. But setters do a very similar thing where theyre going to set the data attributes to whatever is passed in, OK? And in the second half of the lecture, were going to talk about the idea of inheritance. o if want to use random numbers in my code, m going to put this ""import random"" at the top of my code, which essentially brings in all of the methods from the Random class, one of the methods being this random() method. o we can sort of abstract the previous slide into this one and say that we have parent classes and child classes, OK?",0.1736814064997336
54,54,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: We looked at the macro state, where the volume of the box, the temperature were given, as well as the chemical potential. o rather than looking at a fixed number of particles, we were looking at the grand canonical prescription where the chemical potential was specified. We said that when we have a system of noninteracting particles, we can construct a many bodied state based on occupation of single particle states such that the action of the Hamiltonian on this will give me a sum over all single particle states, the occupation number of that single particle state times the energy of that single particle state, and then the state back. We said that in quantum mechanics, we have to distinguish between bosons and fermions, and in the occupation number prescription. For the case of fermions, corresponding theta v minus 1 and k was 01. For the case of bosons, corresponding theta plus 1 and k equals . We said that within this planned canonical prescription, these occupation numbers were independently given. The probabilities were independently distributed according to exponential rules such that when we calculated the average, nk. We commented on the statistics what are the very simple forms, which was 1 over into the beta epsilon k minus eta, where z was constructed from the temperature and the temporal potential as to the beta mu. Now, for the case that we are interested, for this gas that is in a box of volume v, the one particle states were characterized by waves and their energies were f h bar squared k squared over 2m, basically just the kinetic energy. And then at various stages, we have to perform sums over ks, and for a box of volume v, the sum over k was replaced in the limit of a large box with an integral over k times the density of states, which was v divided by 2 pi cubed. And we said that again, we recognized that quantum particles have another characteristic, which is their spin, and there is a degeneracy factor associated with that that we can just stick over here as if we had copies corresponding to the different components of the spin. Now, once we put this form of epsilon k over here, then we can calculate what the mean number of particles is in the grand canonical ensemble as sum over k, the average nk, given the appropriate statistics. Of course, this is a quantity that is extensive. We then construct a density, which is intensive, simply by dividing by v, which gets rid of that factor of v over here, and we found that the answer could be written as g over lambda cubed, again, coming from this g, lambda cubed from the change of variables, where we defined lambda and h over root 2 pi mkp. And then a function that we indicated by s 3/2, depending on the statistics of z. And this was a member of our class of functions we defined in general, f m h of z, to be 1 over m minus 1 factorial, integral 0 to infinity, ex, x to the m minus 1, z inverse, e to the x minus eta. o for each m, we have a function of z. n fact, we have two functions, depending on whether we are dealing with the fermionic or the bosonic variety. We saw that these functions we could expand for small z as z plus eta z squared over 2 to the m. ts an alternating series for fermions. All terms are positive for bosons, zq, 3 to the m, eta z to the fourth, 4 to the m, and so forth. And we also noted that these functions have a nice property in that if you were to take a derivative with respect to z and multiply by z, you get the function with one lower index. o this is, in fact, the same thing as z divided by zz of f m plus 1 . This just made the connection between the number of particles and the chemical potential that we need to use when we go to the grand canonical prescription, but we are interested in calculating various properties of this gas, such as the pressure. We found that the formula for the pressure was simply g over lambda cubed, very similar to what we had over here, except that, rather than having f 3/2, we had f 5/2 of z to deal with, and if we were interested in energy, we could simply use 3/2. ndeed, this was correct for both fermions and bosons, irrespective of the difficulties or the variations in pressure that you would have, depending on your bosons or fermions due to gases. o what is the task here, if you are interested in getting a formula for pressure or the energy or the heat capacity? What should we do if we are interested in a gas that has a fixed number of particles or a fixed density? learly, the first stage is to calculate z as a function of density. What we can do is to solve the first equation graphically. We have to solve the equation f 3/2 8 of z equals the combination m lambda cubed over g, which we call the degeneracy factor. o if tell you what the temperature is, you know lambda. f tell you what the density is, you know the combination on the right hand side. There is this function. We have to plot this function and find the value of the argument of the function. That gives the value of m lambda cubed over g as the value of the function. o graphically, what we have to do is to plot as a function of z these functions, f 3/2. will generically plot f m 8 of z, but for the case that we are interested, we really need to look at m equals 3/2. As we will see in problem set, if you were to solve the gas in d dimensions, this m rather than the 3/2 would be g over 2. o pictorially, the same thing would work with slightly different forms of these functions in different dimensions. How do these functions look like? Well, we see that initially, they start to be linear, but then, the case of the bosons and fermions, the next order term goes in different directions. And in particular, for the case of fermions, the quadratic term starts to bring you down, whereas for the case of bosons, the quadratic term is the opposite direction and will tend to make you large. This is what these functions look. For the case of fermions, have additional terms in the series. ts an alternating series. And then the question is, what happens to this at large values of z? And what we found was that at large values, this function satisfies the asymptotic form that is provided by ommerfeld formula, which states that f m of z, for the case of fermions, so this is eta equals minus 1, is log z to the power of m divided by m factorial. Of course, we are interested in m goes to 3/2. 1 plus pi squared over 6 m, m minus 1, log z squared, and higher order inverse powers of log z, which one can compute, but for our purposes, these two terms are sufficient. And if m interested in doing the inversion for fermions, what need to do is to plot a line that corresponds to n lambda cubed over g and find the intersection of that line, and that would give me the value of z. When m very close to the origin, start with the linear behavior, and then can systematically calculate the corrections to the linear behavior, which would have been z equals n lambda cubed over g in higher powers of n lambda cubed over g. And we saw that by doing that, can gradually construct a visual expansion that is appropriate for these gases. can do the same thing, of course, for the case of bosons also, but now m interested in the limiting form where the density goes higher and higher or the temperature goes lower and lower so that this horizontal line that have drawn will go to larger and larger values, and m interested in the intersections that have when z is large and this asymptotic formula is being satisfied. o what do need to do for the case of degenerate fermions? have this quantity, n lambda cubed over g, which claim is much larger than 1, so have to look for values of the function, f 3/2 minus of z that are large, and those values are achieved when the argument is large, and then it takes the form log z to the 3/2 divided by 3/2 factorial, 1 plus pi squared over 6. m in this case is 3/2, so have 3/2 times m minus 1, which is 1/2, divided by log z over 2. And then will have potentially higher order terms. o can invert this formula to find log z as a function of density. And to lowest order, what have is 3/2 factorial n lambda cubed over g raised to the 2/3 power. f were to ignore everything here, just arrange things this way, thats the formula that would get. But then have this correction, so divide by this. have 1 plus this combination is equivalent to pi squared over 8, 1 over log z squared. have taken it to the other side, so put the minus sign and raise again to the 2/3 power. Now, the way that have defined z up there, clearly log z is proportional to beta nu. We can see that lambda is proportional to beta to the 1/2, so lambda cubed is beta to the 3/2, which, when raised to the 2/3, gives me a factor of beta. o basically, the quantity that we have over here is of the order of beta, and then we saw that the remainder, which depends on properties such as the mass of the gas, gh, et cetera, which is independent of temperature, is a constant. That is the Fermi energy that we can compute by usual route of filling up a Fermi c. And the value of epsilon f is h bar squared over 2m times kf squared, and the kf was 6 pi squared n over g to the 1/3 power, so kf squared would be something like this. o once we know the density of the fermion, its spin and its mass, we can figure out what this quantity is. The zeroth order solution for the chemical potential is clearly this epsilon f. What do have to do if want to calculate the next correction? Well, what can do is can put the zeroth order solution for log z in this expression, and then that will give me the first order correction. have to raise this whole thing to the minus 2/3 power, so pi squared over 8 becomes 1 minus 2/3 of pi squared over h, which is minus pi squared over 12. 1 over log of z squared log z is again mu over kt, so its inverse is kt over the zeroth order value of mu, which is epsilon f raised to second power, and we expect that there will be higher order terms. o what have stated? f really take this curve and try to solve for mu as a function of temperature for a given density, find that at very low temperatures, the value that get is epsilon f, and then the value starts to get reduced as go to higher temperatures and the initial fall is quadratic. Of course, the function will have higher and higher order corrections. t will not stay quadratic. And also know that at very large values, it essentially converges to a form that is minus kt log of n lambda cubed, because down here, know that this solution is z is simply n lambda cubed over g. o at low densities and high temperatures, basically, will have something that is almost linear with logarithmic corrections and negative, and so presumably, the function looks something like this. Of course, can always convert. We can say that what see here is the combination of kt over epsilon f. can define epsilon f over kv to be something that ll call tf, and this correction can write as t over tf squared. o basically, have defined epsilon f to be some kv times a Fermi temperature. Just on the basis of dimensional argument, you would expect that the place where this zero is occurring is of the order of this tf. dont claim that it is exactly tf. t will have some multiplicative factor, but it will be of the order of tf, which can be related to the density, mass, and other properties of the gas by this formula. And if you look at a metal, such as copper, where the electrons are approximately described by this Fermi gas, this tf is of the order of, say, 10 to the 4 degrees Kelvin. Yes? ADENE: o you were saying that at a high temperature, the chemical potential should go as minus kvt times ln of n lambda cubed, yeah? PROFEOR: Yes. ADENE: Now, the lambda itself has temperature dependence, so how can you claim that it is behaving linearly? PROFEOR: said almost linearly. As you say, the correct behavior is something like minus 3/2 t log t, so log t changes the exponent to be slightly different from 1. This is not entirely linear. t has some curvature due to this logarithm. n fact, something that people sometimes get confused at is when you go to high temperatures, beta goes to 0. o why doesnt z go to 1? Of course, beta goes to 0 but mu goes to infinity such that the product of 0 and infinity is actually still pretty large, partly because of this logarithm. ADENE: ould you just explain again why its linear? PROFEOR: Down here it is linear, so know that z is n lambda cubed over g. z is e to the mu over kt. o mu is kt log of n lambda cubed over g. Remember lambda depends on temperature, so it is almost linear, except that really what is inside here has a temperature dependence. o its really more like t log t. ADENE: When z is small? PROFEOR: When z is small and only when z is small, and z small corresponds to being down here. Again, thats what was saying. Down here, beta goes to zero but mu goes to minus infinity such that the product of 0 and minus infinity is still something that is large and negative because of this. ADENE: f the product is large, then isnt z large? PROFEOR: f the product is large and negative, then z is exponentially small. n fact, if were to plot this function just without the higher order corrections, if plot the function, kt log of n lambda cubed over g, you can see that it goes to 0 when the combination n lambda cubed over g is 1 because log of 1 is 0. o that function by itself, asymptotically this is what have, it will come and do something like this, but it really is not something that we are dealing with. ts in some sense the classical version. o classically, your mu is given by this formula, except that classically, you dont know what the volume factor is that you have to put there because you dont know what h is. o thats the chemical potential, but the chemical potential is really, for our intents and purposes, a device that need to put here in order to calculate the pressure, to calculate the energy, et cetera. o lets go to the next step. We are again in this limit. Beta p is g over lambda cubed f 5/2 plus z. Thats going to give me g over lambda cubed in the limit where z is large we established that z is large log z to the 5/2 divided by 5/2 factorial using the ommerfeld expansion, 1 plus pi over 6. Now my m is 5/2. m minus 1 would be 3/2 divided by log z squared in higher order terms. ADENE: Question. PROFEOR: Yes? ADENE: Why is that plus? PROFEOR: Because made a mistake. Just to make my life and manipulations easier, ll do the following. write under this the formula for n being g over lambda cubed, f 3/2 minus z, which is g over lambda cubed log z to the 3/2 divided by 3/2 factorial, 1 plus calculated the correction pi squared over 6 divided by various combination, which is pi squared over 8, 1 over log z squared. Now what will do is to divide these two equations. f were to divide the numerator by the denominator, what do get? The left hand side becomes beta p over n. The right hand side, can get rid of the ratio of log z 5/2 to log z. 3/2 is just one factor of log z. What do have when divide 5/2 factorial by 3/2 factorial? have 5/2. This division allows me to get rid of some unwanted terms. And then what do have here? This combination here is 5 pi squared over 8 minus 1 pi squared over 8. Because of the division, will put a minus sign here, so that becomes 4 pi squared over 8, which is pi squared over 2. o will get 1 plus pi squared over 2, and at the order that we are dealing, we can replace 1 over log z squared with kt over epsilon f squared. Now, for log z over here, can write beta mu, and for mu, can write the formula that have up here. o you can see that once do that, the betas cancel from the two sides of the equation, and what get is that p is 2/5, the inverse of 5/2. The n will take to the other side of the equation. To the lowest order mu is epsilon f, so will put epsilon f, but mu is not exactly epsilon f. t is 1 minus pi squared over 12, kt over epsilon f squared. And actually, there was this additional factor of 1 plus pi squared over 2, kt over epsilon f squared. o what do we find? We find that to the zeroth order, pressure is related to the density but multiplied by epsilon f, so that even at zero temperature, there is a finite value of pressure left. o we are used to ideal gases that are classical, and when we go to zero temperature, they start to stop moving around. ince they are not moving around, there is no pressure that is exerted, but for the Fermi gas, you cannot say that all of the particles are not moving around because that violates this exclusion of n being zero or one. You have to give particles more and more momentum so that they dont violate the condition that they should have different values of the occupation numbers, the Pauli exclusion. And therefore, even at zero temperature, you will have particles that in the ground state are zipping around, and because theyre moving around, they can hit the wall and exert pressure on it, and this is the value of the pressure. As you go to higher temperature, that pressure gets modified. We can see that balancing these two terms, there is an increase in pressure, which is 1 plus 5 pi squared over 6, kt over epsilon f squared. o you expect the pressure, if plot it as a function of temperature, at zero temperature, it will have this constant value, pf. As put a higher temperature, there will be even more energy and more kinetic energy, and the pressure will rise. Eventually, at very high temperatures, will regain the classical, rather pressure is proportional to temperature. Thats the pressure of this ideal Fermi gas. The energy. Well, the energy is simply what we said over here, always 3/2 pv. o it is 3/2, the pressure over there multiplied by v. When multiply the density by v, will get the number of particles. 3/2 times 5/2 will give me 3/5, so will 3/5 and epsilon f at the lowest order. Again, youve probably seen this already where you draw diagrams for filling up a Fermi c. p to some particular kf, you would say that all states are occupied. When epsilon f is the energy of this state that is sitting right at the edge of the Fermi c, and clearly, the energies of the particles vary all the way from zero up to epsilon f, so the average energy is going to be less than epsilon f. Dimensionally, it works out to be 3/5 of that. Thats what that says. But the more important part is how it changes as a function of temperature. What you find is that it goes to 1 plus 5 pi squared over 6, kt over epsilon f squared in high order terms. ADENE: Question. hould the 6 be 12? PROFEOR: 6 should be 12? Lets see. This is 6/12 minus 1/12 is indeed 5/12. Thank you. o if ask whats the heat capacity at constant volume, this is dE by dT, so just taking the derivative. The zeroth order term, of course, is irrelevant. t doesnt vary the temperature. ts this t squared that will give you the variations. o what does it give me? will have n epsilon f. have 3/5 times 5 pi squared over 12. The derivative of this combination will give me 2kb squared t divided by epsilon f squared. There will be, of course, higher order, but this combination you can see can write as follows. t is extensive. t is proportional to n. There is one kb that can take out, which is nice, because natural units of heat capacity, as we have emphasized, are kb. This combination of numbers, believe the 5 cancels. 3 times 2 divided by 12 gives me 1/2, so have pi squared over 2. And then have the combination, kt over epsilon f, which can also write as t over tf. o the heat capacity of the Fermi gas goes to 0 as go to zero temperature, in accord with the third law of thermodynamics, which we said is a consequence of quantum mechanics. o we have this result, that the proportionality is given by the inverse of tf. f were to plot the heat capacity in units of nkb as a function of t, at very high temperatures, will get the classical result, which is 3/2, and then will start to get corrections to that. Those corrections will gradually reduce this, so eventually, the function that will get starts linearly and then gets matched to that. o the linear behavior is of the order of t over tf, and presumably at some temperature that is of the order of tf, you will switch to some classical behavior. And as said, if you look at metals, this tf is very large. o when you look at the heat capacity of something like copper or some other metal, you find that there is a contribution from the electrons to the heat capacity that is linear. Now, the reason for this linear behavior is also good to understand. This calculation that did is necessary in order to establish what this precise factor of pi squared over 2 or appropriate coefficient is, but the physical reason for the linearity you should be able to know. Basically, we saw that the occupation numbers for the case of fermions have this form that is 1 over z inverse, e to the epsilon plus 1. f plot the occupation numbers as a function of energy, for a case that is at zero temperature, you can see that the occupation number is 1 or 0, so you have a picture that is like this. You switch from 1 to 0 at epsilon f. Oops, there was a beta here that forgot, beta epsilon. When go to finite temperature, this becomes fuzzy because this expectation value for the number, rather than being a one zero step function, becomes smoothed out, becomes a function such as this, and the width over which this smoothing takes place is of the order of kt. o what happens is that rather than having this short Fermi c, you will start having less particles here and more occupied particles over here. To do that, you have created energies going from here to here that you have stored that is of the order of this kt. What fraction of the total number have you given this energy? Well, the fraction is given from here. t is the ratio of this to the entirety, and that ratio is t, or kt, divided by epsilon f is the fraction that has been given energy this. o the excitation energy that you have is this number, the total times this fraction times the energy that you have, which is kt. And if take the derivative of this, will get precisely this formula over here up to this factor of pi squared over 2, which was very cavalier, but the scaling everything comes from this. And to all intents and purposes, this is the characteristic of all Fermi systems, that as you go to low temperatures, there is a small fraction of electrons, if you like, that can be excited. That fraction goes to 0 at low temperatures as kt over epsilon f, and the typical energies that they have is of the order of kt. o essentially, many of the results for the degenerate Fermi gas you can obtain by taking classical results and substituting for the number of particles nkt over epsilon f. You will see in the problem set that this kind of argument tells you something also about the magnetic response of a system of electrons, what is the susceptibility and why does the susceptibility saturate at low temperatures. ts just substituting for the number of particles this fraction, nkt over epsilon f, will give you lots of things. This picture is also valid in all dimensions, whereas next, well be discussing the case of bosons, where the dependence is on temperature. There is an exponent here that determines which dimension you are or depends on dimensions. For the case of fermions, this linearity exists independently. o now lets think about bosons and whats going to happen for bosons. For bosons, we saw that the series, rather than being alternating, all of the terms are adding up together so that the parabolic correction, rather than reducing the function for the case of bosons, will increase it. o for a particular value of density, we find that the z of fermions is larger than the z of bosons. o for a given density, what we will find is that the chemical potential, whereas for the case of fermions was deviating from the classical form by going up, for the case of bosons, well start to deviate from the classical form and go down. Now, there is something, however, that the chemical potential cannot do for the case of bosons which it did it for the case of fermions, and that was to change sign. Why is that? Well, we have that over there, the occupation number for a boson is 1 over z inverse, which is e to the beta epsilon k minus mu. z inverse is e to the minus meta mu minus 1. Now, this result was obtained from summing a geometric series, and the condition certainly is that this object, that is, the inverse of what was multiplying different terms in the series, has to be larger than 1. Of course, it has to be larger than 1 so that get a positive occupation number. f it was less than 1, the geometric series was never convergent. learly, the negativity of the occupation number has no meaning. o this being positive strictly immediately implies that mu has to be less than epsilon k for all k. o it has to be certainly less than the minimum of epsilon k with respect to all k. And for the case that m looking at where my epsilon ks are h bar squared k squared over 2m, the lowest one corresponds to k equals 0, so mu has to be less than 0. t can never go to the other side of this. Or alternatively, z has to be less than or approaches maybe 1. o there is certainly a barrier here that we are going to encounter for the case of bosons at z equals 1. We should not go beyond that point. o lets see whats happening for z equals 1 to this function. You see, my task remains the same. n order to find z, have to solve graphically for the intersection of the curve that corresponds to f plus 3/2 and the curve that corresponds to the density n lambda cubed over g. o what happens as go to higher and higher values of n lambda cubed over g? You can see one possibility is that this curve just diverges as z approaches 1. Then for every value of n lambda cubed over g, you will find some value of z that will gradually become closer and closer to 1. But is that the scenario? For that, we need to know what the value of this f function is at z equals 1, so the limit f m plus of z as z goes to 1. How do you obtain that? Well, that is 1 over m minus 1 factorial, integral 0 to infinity, dx, x to the m minus 1, z inverse, which is 1, e to the x minus 1. have to integrate this function. The integrand, how does it look like? x to the m minus 1, e to the x minus 1. Well, at large x, there is no problem. t goes to 0 exponentially. At small x, it goes to 0 as x to the m minus 2. o basically, its a curve such as this that have to integrate. And if the curve is like have drawn it, there is no problem. can find the integral underneath it and there actually is a finite value that has a name. ts called a zeta function, so this is some tabulated function that you can look at. But you can see that if m, lets say, is 0, then it is dx over x squared. o the other possibility is that this is a function that diverges at the origin, which then may or may not be integrable. And we can see that this is finite and exists only for m that is larger then 1. o in particular, we are interested in the case of m equals 3/2. o then at z equals 1, have a finite value. t is zeta of 3/2. o basically, the function will come up to a finite value at z equals 1, which is this zeta of 3/2, which you can look up in tables. ts 2.612. Now, tried to draw this curve as if it comes and hugs the vertical line tangentially, that is, with infinite slope, and that is the case. Why do know that? Because the derivative of the function will be related to the function at one lower index. o the derivative of f 3/2 is really an f 1/2. f 1/2 does not exist. ts a function that goes to infinity. o essentially, this curve comes with an infinite slope. Now, it will turn out that this is the scenario that we have in three dimensions. f you are in two dimensions, then what you need to do is look at f that corresponds to m equals 2 over 2 or 1, and in that case, the function diverges. o then, you have no problem in finding some intersection point for any combination of n lambda cubed over g. But currently in three dimensions, we have a problem because for n lambda cubed over g that falls higher than zeta of 3/2, it doesnt hit the curve at any point, and we have to interpret what that means. o for d equals 3, encounter singularity when n lambda cubed over g is greater than or equal to zeta of 3/2. This corresponds at a fixed density to temperatures that are less than some critical temperature that depends on n, and that can read off as being 1 over kb. This is going to give me a combination. Lambda cubed is proportional inversely to 3/2, so this will give me nz 3/2 over g to the 2/3, and then have h squared 2 pi m. put the kb over here. o the more dense your system is, the lower guess thats why have it the opposite way. t should be the more dense it is, the higher the temperature. Thats fine. But what does that mean? The point is that if we go and look at the structure that we have developed, for any temperature that is high enough, or any density that is low enough, so that hit the curve on its continuous part, it means that can find the value of z that is strictly less than 1, and for that value of z, can occupy states according to that probability, and when calculate the net mean occupation, will get the actual density that m interested in. As go to temperatures that are lower, this combination goes up and up. The value of z that have to get gets pushed more towards 1. As it gets pushed more towards 1, see that mu going to 0, the state that corresponds to the lowest energy, k equals 0, gets more and more occupied. But still, nothing special about that occupation except what happens when am at higher values. The most natural thing is that when am at higher values, should pick z equals 1 minus a little bit. And if choose z to be 1 minus a very small quantity, then when do the integration over here, didnt get a value for the density, which is g over lambda cubed, the limiting value of this f function, which is zeta of 3/2. will call this n star because this is strictly less than the total density that have. That was the problem. f could make up the total density with z equals 1, would be satisfied. would be making it here, but m not making it up with the spectrum that have written over here. But on the other hand, if epsilon is incredibly small, what find is that the occupation number of the k equals to 0 state lets write it n of k equals 0. What is that? t is 1 over z inverse, which is the inverse of this quantity, which is 1 plus epsilon, which is approximately e to the epsilon minus 1. Actually, maybe what should do is to write it in the form e to the beta mu minus 1 and realize that this beta mu is a quantity that is very small. t is the same thing that was calling epsilon before. And if it is very small, can make it to be 1 over beta mu. o by making mu arbitrarily close to the origin or epsilon mu arbitrarily close to 1, could in principle pump a lot of particles in the k equals 0 state, and that does not violate anything. Bosons, you can put as many particles as you like in the k equals 0 state. o essentially, what can do is can make this, but you say, well, isnt this already covered by this curve that you have over here? Are you doing something different? Well, lets follow this line of thought a little bit more. How much do have to put here? This n is the total number of particles divided by volume, and so this is going to be this quantity, n star, which is gv over lambda cubed divided by volume. Lets write it in this fashion, z over lambda cubed zeta of 3/2 and then whatever is left over, and the leftover can write as n0. o what we know is if we take z equals 1, this function will tell me how many things have put in everything except k equals 0 that will calculate separately. That amount is here. And will put some more in k equals 0, and you can see that the amount have to put there is going to be n minus g over lambda cubed zeta of 3/2. Actually, will have to put a volume here. Why is that? The reason have to put the volume is because the volume that had here, that ultimately divided the number of particles by volume to get the density, came from replacing the sum with an integration. o what have envisioned now is that there are all of these points that correspond to different ks. ll replace the sum with a integration, but then there was one point, k equals 0, that am treating separately. When m treating that separately, it means that is really is the same as the total number expectation value without having to divide by the volume, which comes from this density of state, and have something like this. Now, the problem is that you can see that suddenly, you have to pick a value of mu that is inversely proportional to the volume of the system. This is a problem in which the thermodynamic limit is taken in this strange sense. You have all of these potential values of the energy, epsilon of k, that correspond to h bar squared, k squared over 2m. There is one that is at 0, and then choosing k equals 2 to pi over l, you will have one other state, another state, all of these states. All of these states are actually very finely spaced. The difference between the ground state and the first excited state is h bar squared over 2m, 2 pi over l squared. o this distance over here is of the order of 1 over l squared. But see that in order to occupy this with the appropriate number, have to choose my chemical potential to be as close as 1 over the volume. o in the limit where take the size of the system go to infinity, this approaches this. t never touches it, but the distance that have here is much, much less than the distance that have in the spacings that made this replacement. o can indeed treat these separately. can give a particular weight to this state where the mu has come this close to it as 1 over v, treat all of the other ones as part of this replacement of the summations with the integral. You can also see that this trick is going to be problematic if were to perform it in two dimensions, because rather than 1 over l to the third power, that is, the volume in three dimensions, would have had 1 over l squared. And indeed, thats another reason why two dimensions is special. And as said, in two dimensions, the curve actually does go all the way to infinity. You wont have this problem. o this is essentially what happens with this BoseEinstein condensation, that the occupation number of the excited state for bosons is such that you encounter a singularity in three dimensions beyond a particular density or temperatures lower than a certain amount or highly degenerate case, you have to separately treat all the huge number of particles that is proportional to the volume that have now piled up in the single state, k equals 0, the ground state in this case, but whatever the ground state may be for your appropriate system, and all the other particles go in the corresponding excited states. Now, as you go towards zero temperature, you can see that this combination is proportional to temperature goes to zero. o at 0 temperature, essentially all of the particles will need to be placed in this k equals 0 state. o if you like, a rough picture is that the net density. This is the density, n, at high temperature is entirely made up as being part of the excited state. Lets draw it this way. When you hit Tc, you find that the excited state can no longer accommodate the entire density. This is a coordination that, as we said, goes to zero at zero temperature as T to the 3/2. o theres a curve that goes like T to the 3/2, hits 1 at exactly Tc, which is a function of the density that you choose. This is the fraction that corresponds to the excited states. And on top of that, there is a macroscopic fraction that is occupying the ground state, which is the compliment to this curve. Basically, it behaves something like this. One thing that should emphasize is that what see here makes it look like at high temperatures, there is no occupation of k equals 0 state. That is certainly not correct because if you look at this function at any finite z as a function of epsilon, you can see that the largest value of this function is still at epsilon equals 0. t is much more likely that there is occupation of the ground state than any other state, except that the fraction that is occupying here, when divide by the total number, goes to zero. t becomes macroscopic when m below the BE transition. Now, the properties of what is happening in this system below Tc is actually very simple because below Tc, the chemical potential in the macroscopic sense is stuck at 1. Essentially, what m saying is that in this system, the chemical potential comes down. t hits Tc of n, and then it is 0. Now of course, 0, as we have discussed here, if put a magnifier here and multiply by a factor of n or a factor of volume, then see that there is a distinction between that and 0. o it doesnt quite go to 0, but the distance is of the order of 1 over volume or 1 over the number of particles. Effectively, from the thermodynamic perspective, it is 0. But again, for the purposes of thermodynamics, pressure, everything comes from the particles that are moving around. The particles in the ground state are frozen out. They dont do anything. o the pressure is very simple. Beta p is g over lambda cubed, f 5/2, eta of z equals 1, which is some number. t is g over lambda cubed, the zeta function at 5/2, which again has some value that you can read of in tables, 1.34, something like that. o you can see that this lambda cubed is inversely proportional to t to the 3/2, so pressure is proportional to t to the 5/2. f were to plot the pressure as a function of temperature, what find is that at low temperatures, the pressure is simply given as t to the 5/2. And you can see that this pressure knows nothing about the overall density of the system because this is a pure number. t is only a function of temperature, mass, et cetera. There is no factor of density here. o what is happening? Because we certainly know that when am at very high temperatures, ideal gas behavior dictates that the pressure should be proportional to temperature times density. o if have two different gases where the density here is greater than the density here, these will be the form of the isotherms that have at high temperatures. o what happens is that you will start with the ideal gas behavior. We saw that for the case of bosons, there is some kind of an effective attraction that reduces the pressure, so the pressure starts to go down. Eventually, it will join this universal curve at the value that corresponds to Tc of n2. Whereas if see whats happening with the lower density, at high temperatures, again, will have the linear behavior of the ideal gas with a lower slope because have a lower density. As go to lower temperatures, quantum corrections will reduce the pressure. Eventually, find that this curve will join my universal curve at the point that would correspond to Tc of n1, which in this case would be lower. But beyond that, it will forget what the density was, the pressure would be the same for all of these curves. And have kind of indicated, or tried to indicate, that the curves join here in a manner that there is no discontinuity in slope. o an interesting exercise to do is to calculate derivatives of this pressure as a function of temperature and see whether they match coming from the two sides. And you will find that if you do the algebra correctly, there is a matching of the two. Now, if have the pressure as a function of temperature, then also have the energy as a function of temperature, right? Because we know that the energy is 3/2 PV. o if know P, can immediately know what the energy is. know, therefore, that in the condensate, the energy is proportional to T to the 5/2. Take a derivative, know that the heat capacity will be proportional to T to the 3/2. That T to the 3/2 is a signature of this condensate that we have. Again, heat capacity will go to 0 as T goes to 0, as we expect. And as opposed to the case of the fermions, where the vanishing of the heat capacity was always linear, the vanishing of the heat capacity for bosons will depend on dimensions, and this vanishing as T to the 3/2 is in general T to the d over 2, as can be shown very easily by following this algebra. wanted to do a little bit of work on calculating the heat capacity because its shape is interesting. Lets write the formula a little bit more accurately. have 3/2 v. Pressure is kTg over lambda cubed, f 5/2 plus of z. This formula is valid, both at high temperatures and at low temperatures. At high temperatures, z will be varying as a function of temperature. Let me also write the formula for the number of particles. The number of particles is V times g over lambda cubed f 3/2 plus of z. Once more, can get rid of a number of things by dividing these two. o the energy per particle is 3/2 kT, f 5/2 of z divided by f 3/2 of z. Lets make sure didnt make any mistake. Now, want to calculate the heat capacity, lets say, per particle, which is d by dT of the energy per particle. Now, have one factor out here, which is easy to evaluate. Theres an explicit temperature dependence here, so will get 3/2 kb. will have f 5/2 plus of z divided by f 3/2 plus of z. omething here that dont like. Everything that have written here is clearly valid only for T greater than Tc because for T less than Tc, cannot write n in this fashion. o what m writing for you is correct for T greater than Tc. Point is that there is also an implicit dependence on temperature because z is a function of temperature. o what can do is can do 3/2 kbT, and this is a function of z, and z is a function of temperature. o what have it is dz by dT done at constant volume or number of particles times the derivative of this function with respect to z. Now, when take the derivative with respect to z, know that introduce a function with one lower derivative up to a factor of z. o divide by the 1 over z, and then the derivative of the numerator is f 3/2 plus divided by f 3/2 plus in the denominator minus the derivative of the denominator, which is f 1/2 plus of z times the numerator divided by f 3/2 plus of z squared. o what you would need to evaluate in order to get an expression that is meaningful and we can eliminate zs as much as possible is what dz by dT is. Well, if this is our formula for the number of particles and the number of particles or density is fixed, then we take a derivative with respect to temperature, and will get that dN by dT, which is 0, is v. And then we have to take the temperature derivative of this combination, g over lambda cubed, f 3/2 plus of z. Now, lambda cubed scales like 1 over t to the 3/2, so when take a derivative of this, will get 3/2 T to the 1/2, which can again combine and write in this fashion, 1 over T, f 3/2 plus of z. Or then take the implicit derivative that have here, so just like there, will get dz by dT at constant density times 1 over z. The derivative of f 3/2 will give me f 1/2 plus of z. etting it to zero, we can immediately see that this combination, T over z dz by dT at constant density over the number of particles is minus 3/2, f 3/2 plus divided by f 1/2 plus of z. o what need to do is to substitute this over here. And then when want to calculate the various limiting behaviors as start with the high temperature and approach Tc, all need to know is that z will go to 1. f 1/2 of 1 is divergent, so this factor will go to 0. This factor will disappear, and so then these things will take nice, simple forms. Once you do that we will do this more correctly next time around well find that the heat capacity of the bosons in units of kb as a function of temperature has a discontinuity at Tc of n. t approaches the classical result, which is 3/2 at high temperatures. At low temperatures, we said it is simply proportional to T to the 3/2, so have this kind of behavior just a simple T to the 3/2 curve. And one can show through arguments, such as the one that showed you above, that the heat capacity is continuous but its derivative is discontinuous at Tc. o the behavior overall of the heat capacity of this Bose gas is something like this. o next time, well elucidate this a little bit better and go on and talk about experimental realizations of BoseEinstein condensation.","Well, if this is our formula for the number of particles and the number of particles or density is fixed, then we take a derivative with respect to temperature, and will get that dN by dT, which is 0, is v. And then we have to take the temperature derivative of this combination, g over lambda cubed, f 3/2 plus of z. Now, lambda cubed scales like 1 over t to the 3/2, so when take a derivative of this, will get 3/2 T to the 1/2, which can again combine and write in this fashion, 1 over T, f 3/2 plus of z. Or then take the implicit derivative that have here, so just like there, will get dz by dT at constant density times 1 over z. The derivative of f 3/2 will give me f 1/2 plus of z. etting it to zero, we can immediately see that this combination, T over z dz by dT at constant density over the number of particles is minus 3/2, f 3/2 plus divided by f 1/2 plus of z. o what need to do is to substitute this over here. Basically, we saw that the occupation numbers for the case of fermions have this form that is 1 over z inverse, e to the epsilon plus 1. f plot the occupation numbers as a function of energy, for a case that is at zero temperature, you can see that the occupation number is 1 or 0, so you have a picture that is like this. n fact, if were to plot this function just without the higher order corrections, if plot the function, kt log of n lambda cubed over g, you can see that it goes to 0 when the combination n lambda cubed over g is 1 because log of 1 is 0. o that function by itself, asymptotically this is what have, it will come and do something like this, but it really is not something that we are dealing with. For that, we need to know what the value of this f function is at z equals 1, so the limit f m plus of z as z goes to 1. have this quantity, n lambda cubed over g, which claim is much larger than 1, so have to look for values of the function, f 3/2 minus of z that are large, and those values are achieved when the argument is large, and then it takes the form log z to the 3/2 divided by 3/2 factorial, 1 plus pi squared over 6. m in this case is 3/2, so have 3/2 times m minus 1, which is 1/2, divided by log z over 2. And if m interested in doing the inversion for fermions, what need to do is to plot a line that corresponds to n lambda cubed over g and find the intersection of that line, and that would give me the value of z. When m very close to the origin, start with the linear behavior, and then can systematically calculate the corrections to the linear behavior, which would have been z equals n lambda cubed over g in higher powers of n lambda cubed over g. And we saw that by doing that, can gradually construct a visual expansion that is appropriate for these gases. That is certainly not correct because if you look at this function at any finite z as a function of epsilon, you can see that the largest value of this function is still at epsilon equals 0. t is much more likely that there is occupation of the ground state than any other state, except that the fraction that is occupying here, when divide by the total number, goes to zero. The point is that if we go and look at the structure that we have developed, for any temperature that is high enough, or any density that is low enough, so that hit the curve on its continuous part, it means that can find the value of z that is strictly less than 1, and for that value of z, can occupy states according to that probability, and when calculate the net mean occupation, will get the actual density that m interested in. And what we found was that at large values, this function satisfies the asymptotic form that is provided by ommerfeld formula, which states that f m of z, for the case of fermions, so this is eta equals minus 1, is log z to the power of m divided by m factorial. When epsilon f is the energy of this state that is sitting right at the edge of the Fermi c, and clearly, the energies of the particles vary all the way from zero up to epsilon f, so the average energy is going to be less than epsilon f. Dimensionally, it works out to be 3/5 of that. o this is essentially what happens with this BoseEinstein condensation, that the occupation number of the excited state for bosons is such that you encounter a singularity in three dimensions beyond a particular density or temperatures lower than a certain amount or highly degenerate case, you have to separately treat all the huge number of particles that is proportional to the volume that have now piled up in the single state, k equals 0, the ground state in this case, but whatever the ground state may be for your appropriate system, and all the other particles go in the corresponding excited states. can do the same thing, of course, for the case of bosons also, but now m interested in the limiting form where the density goes higher and higher or the temperature goes lower and lower so that this horizontal line that have drawn will go to larger and larger values, and m interested in the intersections that have when z is large and this asymptotic formula is being satisfied. Now of course, 0, as we have discussed here, if put a magnifier here and multiply by a factor of n or a factor of volume, then see that there is a distinction between that and 0. o it doesnt quite go to 0, but the distance is of the order of 1 over volume or 1 over the number of particles. Now, the problem is that you can see that suddenly, you have to pick a value of mu that is inversely proportional to the volume of the system. n order to find z, have to solve graphically for the intersection of the curve that corresponds to f plus 3/2 and the curve that corresponds to the density n lambda cubed over g. o what happens as go to higher and higher values of n lambda cubed over g? And if choose z to be 1 minus a very small quantity, then when do the integration over here, didnt get a value for the density, which is g over lambda cubed, the limiting value of this f function, which is zeta of 3/2. And will put some more in k equals 0, and you can see that the amount have to put there is going to be n minus g over lambda cubed zeta of 3/2. o basically, the quantity that we have over here is of the order of beta, and then we saw that the remainder, which depends on properties such as the mass of the gas, gh, et cetera, which is independent of temperature, is a constant. o what have it is dz by dT done at constant volume or number of particles times the derivative of this function with respect to z. Now, when take the derivative with respect to z, know that introduce a function with one lower derivative up to a factor of z. o divide by the 1 over z, and then the derivative of the numerator is f 3/2 plus divided by f 3/2 plus in the denominator minus the derivative of the denominator, which is f 1/2 plus of z times the numerator divided by f 3/2 plus of z squared. f you are in two dimensions, then what you need to do is look at f that corresponds to m equals 2 over 2 or 1, and in that case, the function diverges. f were to plot the pressure as a function of temperature, what find is that at low temperatures, the pressure is simply given as t to the 5/2. And also know that at very large values, it essentially converges to a form that is minus kt log of n lambda cubed, because down here, know that this solution is z is simply n lambda cubed over g. o at low densities and high temperatures, basically, will have something that is almost linear with logarithmic corrections and negative, and so presumably, the function looks something like this. f really take this curve and try to solve for mu as a function of temperature for a given density, find that at very low temperatures, the value that get is epsilon f, and then the value starts to get reduced as go to higher temperatures and the initial fall is quadratic. We have to plot this function and find the value of the argument of the function. o you can see that this lambda cubed is inversely proportional to t to the 3/2, so pressure is proportional to t to the 5/2. To do that, you have created energies going from here to here that you have stored that is of the order of this kt. o you can see that once do that, the betas cancel from the two sides of the equation, and what get is that p is 2/5, the inverse of 5/2. Well, we have that over there, the occupation number for a boson is 1 over z inverse, which is e to the beta epsilon k minus mu. Once you do that we will do this more correctly next time around well find that the heat capacity of the bosons in units of kb as a function of temperature has a discontinuity at Tc of n. t approaches the classical result, which is 3/2 at high temperatures. PROFEOR: Down here it is linear, so know that z is n lambda cubed over g. z is e to the mu over kt. t will have some multiplicative factor, but it will be of the order of tf, which can be related to the density, mass, and other properties of the gas by this formula. Lambda cubed is proportional inversely to 3/2, so this will give me nz 3/2 over g to the 2/3, and then have h squared 2 pi m. put the kb over here. And as opposed to the case of the fermions, where the vanishing of the heat capacity was always linear, the vanishing of the heat capacity for bosons will depend on dimensions, and this vanishing as T to the 3/2 is in general T to the d over 2, as can be shown very easily by following this algebra. When m treating that separately, it means that is really is the same as the total number expectation value without having to divide by the volume, which comes from this density of state, and have something like this. And we can see that this is finite and exists only for m that is larger then 1. o in particular, we are interested in the case of m equals 3/2. But on the other hand, if epsilon is incredibly small, what find is that the occupation number of the k equals to 0 state lets write it n of k equals 0. This just made the connection between the number of particles and the chemical potential that we need to use when we go to the grand canonical prescription, but we are interested in calculating various properties of this gas, such as the pressure. That fraction goes to 0 at low temperatures as kt over epsilon f, and the typical energies that they have is of the order of kt. You can also see that this trick is going to be problematic if were to perform it in two dimensions, because rather than 1 over l to the third power, that is, the volume in three dimensions, would have had 1 over l squared. o essentially, many of the results for the degenerate Fermi gas you can obtain by taking classical results and substituting for the number of particles nkt over epsilon f. You will see in the problem set that this kind of argument tells you something also about the magnetic response of a system of electrons, what is the susceptibility and why does the susceptibility saturate at low temperatures. o this being positive strictly immediately implies that mu has to be less than epsilon k for all k. o it has to be certainly less than the minimum of epsilon k with respect to all k. And for the case that m looking at where my epsilon ks are h bar squared k squared over 2m, the lowest one corresponds to k equals 0, so mu has to be less than 0. t can never go to the other side of this. We find that to the zeroth order, pressure is related to the density but multiplied by epsilon f, so that even at zero temperature, there is a finite value of pressure left. And then a function that we indicated by s 3/2, depending on the statistics of z. And this was a member of our class of functions we defined in general, f m h of z, to be 1 over m minus 1 factorial, integral 0 to infinity, ex, x to the m minus 1, z inverse, e to the x minus eta. f were to plot the heat capacity in units of nkb as a function of t, at very high temperatures, will get the classical result, which is 3/2, and then will start to get corrections to that. We can see that lambda is proportional to beta to the 1/2, so lambda cubed is beta to the 3/2, which, when raised to the 2/3, gives me a factor of beta. And to all intents and purposes, this is the characteristic of all Fermi systems, that as you go to low temperatures, there is a small fraction of electrons, if you like, that can be excited. o the excitation energy that you have is this number, the total times this fraction times the energy that you have, which is kt. 1 over log of z squared log z is again mu over kt, so its inverse is kt over the zeroth order value of mu, which is epsilon f raised to second power, and we expect that there will be higher order terms. And then when want to calculate the various limiting behaviors as start with the high temperature and approach Tc, all need to know is that z will go to 1. f 1/2 of 1 is divergent, so this factor will go to 0. Because of the division, will put a minus sign here, so that becomes 4 pi squared over 8, which is pi squared over 2. o will get 1 plus pi squared over 2, and at the order that we are dealing, we can replace 1 over log z squared with kt over epsilon f squared. o the linear behavior is of the order of t over tf, and presumably at some temperature that is of the order of tf, you will switch to some classical behavior. o what can do is can do 3/2 kbT, and this is a function of z, and z is a function of temperature. Actually, maybe what should do is to write it in the form e to the beta mu minus 1 and realize that this beta mu is a quantity that is very small. We can say that what see here is the combination of kt over epsilon f. can define epsilon f over kv to be something that ll call tf, and this correction can write as t over tf squared. That T to the 3/2 is a signature of this condensate that we have. o this distance over here is of the order of 1 over l squared. That gives the value of m lambda cubed over g as the value of the function. o if have two different gases where the density here is greater than the density here, these will be the form of the isotherms that have at high temperatures. At low temperatures, we said it is simply proportional to T to the 3/2, so have this kind of behavior just a simple T to the 3/2 curve. o for a particular value of density, we find that the z of fermions is larger than the z of bosons. And if take the derivative of this, will get precisely this formula over here up to this factor of pi squared over 2, which was very cavalier, but the scaling everything comes from this. Now, there is something, however, that the chemical potential cannot do for the case of bosons which it did it for the case of fermions, and that was to change sign.",0.1049761417859577
55,55,"NARRATOR: The following content is provided by T OpenourseWare under a reative ommons license. Additional information about our license and T OpenourseWare in general is available at ocw.mit.edu. PROFEOR: o, you see that thought maybe weve got enough good examples in theory where we need some practice. We need some experiments with some of the finite difference methods that weve spoken about. o created this homework in which you would like choose one of these that weve done. Well, actually, number two, the chrodinger equation, which has that imaginary i in there, we havent spoken about yet, but you could do the kind of Fourier analysis that weve done for the heat equation. But, of course, its going to be different because of that i. Youll get an e to the minus i k squared t, guess. And that means thats a number of absolute value 1 instead of a rapidly decaying coefficient. OK, so my idea is to choose one of those groups, or a number five, if you want. And say, by next Friday, just give me some experiment with a code. n this case, you could use well, we have four methods and three of them are already in code, and youre welcome to use those or your own. And raised questions last time and ll put this on the website with a little more detail. o just to give you an idea of whats coming. o this is the problem that were in the middle of speaking about, near the end, the mixture of convection and diffusion. And this is chrodinger, which is new. This is the advection equation, or the oneway wave equation, which weve started with, and this is whats thisll be the subject of todays lecture and next time. And the best reference can give, as well as the textbook, the applied math book has significant discussion of this conservation law. But also its very well presented in the 16.920 lecture notes that are on OpenourseWare, lectures 11 and 12. o m going to use those as a guide in this lecture. Anyway, thought d just put that up as, you know, time to see what various difference methods will do, And again, itll be on the website. OK. Now to say another word about this one, this convectiondiffusion equation. o, last time wrote down a difference equation very much like that except what wrote down last time was upwinded so that was just j. Now its j minus 1. And the choice there is not clear. This gives us higher accuracy, of course, for that x difference, because its centered at j. And we still have this centered, of course. The second difference is centered. o wrote down here what the coefficients are of the three old values at j plus 1, at j, and at j minus 1. But there is one important point here. The price for that extra accuracy may possibly be a negative coefficient. This could be negative, or it could be positive. t depends which is more important: the small r, the convection part, or the big R. And let me follow up. o you remember the price. f we see a negative coefficient, then we expect some oscillations to appear. And actually, they were supposed to appear in Figure 5.12, but it hasnt been done yet. And if you would choose this one and do Figure 5.12, ll be incredibly grateful. o Figure 5.12 was to show the evolution from a u naught for the convectiondiffusion. OK, so can just do you see that in comparing R with r over 2, those are dimensionless quantities, right? an take another minute to try? m a total amateur at dimensional analysis. did always think, however, that Einstein could have proved well, not maybe proved, but could have conjectured that E was m c squared, the most important equation in relativity, just by checking the dimensions. dont know if that would mean, right? That would like might have occurred to him to realize that m c squared has the dimensions, if we wrote those out, of energy. dont know if that put an idea into the system which produced, of course, the fact that and, of course, there would be a dimensionless constant, which amazingly happens to be 1. OK, so dimensional analysis could, like, change the world. This wont quite change the world, but a little touch of something. OK, so can just you remember that this Peclet number, and ll write his name down again. PELET. The Peclet number. n the differential equation, the Peclet number was a ratio, essentially a ratio, of c to d. But to get the think we needed was that what we needed? We needed a length to make it dimensionless. c is the coefficient for c over d sort of tells us roughly the importance of convection and diffusion, but theres a length scale to be included. Now the cell Peclet number, which ll just call P, m just going to take to be r over R because those are dimensionless. And you see that its going to play a part here. Let me see, forgot. think want r over R. aybe want a 2 just because would like that to d like it to come out simply there. OK, so may put in a 2. o that r would be 2R*p. Where did that 2 come from, by the way? Oh, guess it came from there, right? t came from that minus 1/2. o probably want a 2 there just so that have a very simple decision of whether this coefficient is positive or negative, yeah. o let me put in a 2. OK, so r over 2 is R*P then. This quantity is R times P, so this is R minus R times P, or its R, which we know is OK, times 1 minus P. n other words, P smaller than 1, that coefficient all the three coefficients are positive. ts certainly stable. Not only stable, it cant oscillate. We cant have were taking a combination of the three old values and oscillations cant show up. OK, so just to see, what is this? And you remember that r is c delta t over delta x, and capital R is d delta t over delta x squared. o just to complete this, the delta ts cancel. One of the delta x cancels. The other one comes up. o its c over d, and then we have a delta x and a 2. o thats why its called the cell Peclet number, because its the cell size, or in my convention, half the cell size that gives the length scale and decides whether P is so m just going to write down the conclusion that the figure should show. P smaller than 1. That means that this coefficient is also positive. o thats positive coefficients, no oscillation. And think youll see from the output that P bigger than 1 produces oscillation we dont like. n other words, we really dont want P larger than 1. And of course you could say, well, look at P. ts got a delta x in it. Thats not going to be a big number. And the method will work fine as delta x and delta t go to zero, but were going to do a finite calculation. And so were seeing for the first time, like, because we have two terms that are not dimensionally identical, c and d, that in that competition between convection and diffusion, the length scale is crucial, and the mesh size is crucial. o guess the answer is we can resolve it by taking delta x small enough, and hopefully, thats not a problem. But, of course, you know that there are many physical cases in which the diffusion coefficient or the viscosity coefficient is small, is quite small. And so getting that below 1 might be hard, because this d could be a very small number. OK, so maybe can also mention on the as something that we havent done here, was to move the second derivative, this guy, to time n plus 1, make it implicit. n the diffusion part, one often does that. The diffusion is the term, is the one thats forcing us to take this coefficient R below a half, delta t of order delta x squared, and the way to avoid that is to move the viscosity term, at least half of it, up to the new time. OK, so thats a third method. o weve done upwind last time, centered, explicit this time, but then thats a third one that has to be looked at. OK, so thats where we stand on linear problems. m sort of ready to jump into nonlinear ones, recognizing that well, theres always more to say about linear ones. What are some of the things that we have not properly discussed? Boundary conditions, above all. Boundary conditions, and we could do something with those, and the notes will say more about boundary conditions. o there are several types of boundary conditions. guess m going to say something about them now, but thisll be a board that kind of gets covered up. o boundary conditions: o what are the different types? Well, at a real physical boundary, we might have u equals 0. ay, in heat flow, we might hold the boundary it might be the edge of a freezer or something we might hold the temperature at zero, or at some given value. We might prescribe u at a boundary. Or we might prescribe the derivative. And just in case were in several dimensions, d better write it as the outgoing derivative. t could be prescribed, say, 0. o what are those two types of boundary conditions called? Ones called absorbing, an absorbing boundary, and the other is an insulated boundary where no heat goes through. To maintain this, heat probably does go through. f our body is warm and were holding it at zero on its boundary, then heat is going to travel out. And in fact, as time goes to infinity, were going to approach cold a uniformly cold. Here, heats not allowed to travel out, so what will happen as time goes on is it will approach a constant. The temperature will approach a constant because it cant leave and it diffuses. And now wanted to mention a third type of boundary, which is a purely computational boundary. We talked last time about axwells equations in free space, and the big application of axwells equations, or one of the big ones historically, has been to, like, the exterior of an airplane, to radar, or other electromagnetic signals. o what do we do if we have an exterior problem? ee, here, was always thinking about an interior problem. You know, were inside 0 to L or something. But now, what do we do if a region is the outside, its free space all the way? Well, we have to create a computational box of some sort. o we create a box in which we compute, but the boundaries of this box, the xboundaries of it, are purely artificial. Theyre just because we cant compute to infinity. o waves travel in this box, and they travel to the so you see what mean? Were solving it in all this space, except thats too much, so thats meant to be a very large box, not a small one, as large as we can afford. But waves are still going to hit the boundary of that box. OK, what do we choose as boundary condition there? We dont want it to reflect back because its not a real boundary. o we have to create ll put AB for absorbing boundary condition. Absorbing. That A is for an absorbing boundary condition. And the creation of these boundary conditions is an important topic within finite differences. For axwells equations, a good idea was produced and keeps being developed, called perfectly matched layer. The idea in that perfectly matched layer let me just write PL and havent seen this so much in other application areas. The idea of this perfectly matched layer is to create an artificial dielectric, guess, a dielectric medium it would be, in the electromagnetic case. An artificial thin layer, which has just the right properties to stop the wave and swallow it. OK, ll put on the website references to that. ts a bright idea, but not totally simple, and when we change difference methods, then that perfectly matched layer has to go with it. OK, thats the end of my thoughts about linear problems for now. Were coming back, of course, in a few weeks to solving large systems, large linear systems, the kind that we would meet if we use an implicit method, or the kind that we meet if were solving a steadystate problem. But for now, let me go to that model equation. And you see, right away, the difference between that equation and the advection equation that had a constant velocity. Here we have of a velocity that so c is minus u, guess. c is minus think of c this u as being comparable to minus c. o, in other words, if u is positive, m now going to have waves going to the right. o let me just put this down. u greater than 0 will mean waves going to increasing x, to the right. And the idea will be so we have to discuss that problem. And as always, we would like to solve it analytically. Find whatever formulas we can, understand whats happening in terms of these characteristic lines, because when we have one equation and one space variable, we really can catch the essence of the solution by understanding the characteristic lines. And then how do we do it numerically? OK, before class, wrote just a few equivalent forms of it. This is an equivalent form, and this is, so to speak, the right form. This is the form where we see a time derivative of u, as always, and we see a space derivative of u squared over 2, which, of course, produces that. But somehow this is the quantity its called the flux thats physically meaningful, f of u. o ve chosen a simple flux function, u squared over 2. o thats the differential equation. But youll see that the differential equation can produce two solutions at the same point, and we have to choose. n other words, the differential the solution can become discontinuous. A perfectly smooth starting function, after a while this is the essence of the problem now. might have a smooth starting function, at least continuous. The solution is constant along characteristic lines. But what happens if two characteristic lines run into each other? ve drawn that possibility here. And itll take a little time to see exactly whats happening, but the characteristics we can see here. o the starting values were 1 up to that point. o when the starting value is 1 and u is constant along characteristic lines, then u is 1, and those lines, its just like having c equal to minus 1, guess, in the oneway wave equation. We have a oneway wave going to the right now along characteristic lines. o u is 1 in this part of the xtplane, right? We have to come back to the formulas to confirm what just said. And then, over here on the far right, the starting value is 0, and the characteristics in that case, the c associated with that is 0, so its a standing its not moving. The characteristics are just straight lines in the time direction. Nothings happening, right? When you think of a characteristic thats going straight up, its just carrying the value of u without moving left or right, so u stays 0. o u will be 1 here, u will be 0 here, and now somethings happening in here, and we have to figure out what it is. But maybe can anticipate the main point before any algebra. The characteristic lines in this region have a slope somewhere between the slope 1 and the slope 0. You see, were starting let me graph u_0 of x. o let me graph. u_0 of x is the key points are 0 and 1, 0 and 1. o u_0 of x, start with a constant initial function, a constant profile to the left of 0, and a constant profile to the right of 1, and linear between. OK, so its that starting values that wrote here: 1, 1 minus x, and 0 are these three pieces. And then here m in the xtplane where here m in the xuplane. o drew characteristics, and we still have to track we have to say more what characteristics are and understand that these have slopes in between, but youre not surprised because the values of u are in between 1 and 0. o they go that way and they meet here. o up to time t equal 1, characteristics tell everything. p to t equal 1, know that u will be 1 along these characteristics, and when find these characteristics, u is whatever it starts at. And u is 0 up all of these characteristics, so up to that time, the algebra the differential equation is going to be totally OK. But after that time, the differential equation is in difficulty. Because at that moment t so whats happening as t increases from 0 to 1? This is the picture at t equals 0. The picture at t equal to 1 t equal to half, lets say. What would be the picture at t equal to half? Well, at t equal to half, this state 1, has penetrated this far, halfway. Then, in here is this converging characteristic that come from a linear profile. And over here, its all zeros. o that zero is still there, but you see whats happening? This guy is the profile is steepening. This is at t equal to half. teeper. We still have the algebra to do, but think the first thing is to see roughly whats happening. Then at t equal to 1, this is totally steep. o this is the situation at t equal to 1. And couldve given you that as the initial condition, but and the 16.920 notes, thought, OK, lets have a little peace for awhile, see what those characteristics are doing when theyre converging, but theyre not crossing. The trouble is, after t equal to 1, they are crossing. And now, whats the solution in this crosshatched region? Do take the value of 0 thats coming up along these characteristics? Do take the value 1 thats coming along these characteristics? Whats happening in here? o theres a crosshatched region there, which is a big question mark because my rule of following characteristics has led me to two answers. And we want a physically relevant answer. Were sort of expecting, and well see, that this step function is a step function. mean, the answer will be a step function. t will be 1 and then drop to 0 along some path in the xtplane, which is not a characteristic. The characteristics are going at 45 degrees and at 90 degrees. And actually, so this is the region m interested in. o can blow up that region? o if blow up that region, thats where the shocks the shocks are meeting everywhere in this region. This was the point what was that point? guess that was the point x equals 0 and t equal to 1, there. OK, and the question is and know that have u equals 0 over here, and know oops, u equal 1 over here, and know that have u equal 0 here, but the question is where, and well, what happens in between? You can think of at least two possible scenarios, and one of those is that theres a shock line that goes, lets say, somewhere in the middle. And so u stays 1 all the way to that line and drops to 0 after it. n other words, this wall of water travels to the right. The shock, the discontinuity, moves along, not necessarily at the speed that this one this would be moving along the speed of 1, with this pushing. Not stationary either, which is what this right side would like, but somewhere between. o thats a shock line. ll just, for short, call it a shock. Then theres another possibility. And that will happen in this case, actually. That will happen in this case. But there is another possibility that will happen in other cases, and that would be a fan of characteristics. Let me see that when it comes, but m just mentioning it to say that this business of the shock line, which is what well pursue now, which because its what happens with this problem is not the only is not always the way to match, the way to deal with a problem of lack of information coming from the characteristics. OK, so guess our question is, what is that shock line? Whats the path? Whats the slope, and what controls it? And the answer will be that what controls we run into problems with the differential equations. And the way to see what continues to happen is to go to the integrated form, the integral form of the equation, where, of course, by taking an integral, make everything smoother, and also preserve the physical law, the conservation law. o this is really the conservation law. What is this? Well, lets first of all see where it comes from. t just comes from integrating this equation with respect to x. o have a time derivative that keep out here, but my space derivative is going to disappear because m integrating with respect to x. o can call d by dt or partial d by dt the integral of u with respect to x, and m integrating from somewhere from some point to some other point. nstead of a and b, ll call them really should m sorry, should say x on the left and x on the right. ome point x at the left and some point x at the right. And now ve integrated this one. ve called this is f of u so m now thinking of any flux function. And the integral, of course, gives me f of u at the upper end minus f of u at the lower end. OK, so thats the conservation law. Let me write that word down. This is the conservation law. What is it saying? ts saying that the change in the quantity in the integral is given by if theres any change, any time derivative, the rate of change in the interval its the conservation of this integral, meaning that any change in that integral happens because there is flow going out the righthand side of the interval or theres flow coming in the left side of the interval. o thats why we have a minus sign between them. o thats a statement of conservation, and oh, theres one more example that ll mention now. Let me give this particular equation a name, just because you see it. You often see it written called Burgers equation. But actually, the full name is the inviscid, no viscosity, Burgers equation because its a 0. o just one word about Burgers equation. o Burger allowed a u_xx term here. o he had one of our nonlinear convection and a diffusion term, and his convection was this simple expression u*u_x. ts the kind of thing you see in the Naviertokes equations, right? The nonlinearity in Naviertokes is sort of in this term, of this general sort, but this is a major simplification. uch a simplification that Burger could solve, even though the equation was nonlinear. Well, two people, at the same time, discovered the change of variable, a simple little trick. Hopf and ole. ll put their names down because they had nice, short last names. They both discovered in the case of a diffusion term, they both discovered a little a change of variables that made the equation linear. Then they could solve the equation. Thats in the textbook in the section on conservation laws. And then the important thing that Burger could do was let the diffusion coefficient go to zero. o then in the limit, he got the inviscid Burgers equation, the one were looking at. And he could look at the solution and let the limit go to zero. And thats another way to figure out whats the right solution. Thats the viscosity method, a fundamental method, for finding out when we run into a problem, shocks run into each other. We dont know what to choose. One way to find out, in this 1D problem and also in much tougher nonlinear multidimensional problems, is put a little viscosity in, and let it be small. Let it even get smaller. Then with a little viscosity, officially this term in some sense, this term, because its a second derivative, is officially, mathematically speaking, sort of dominates this one as far as mean, this will produce a smooth solution. This term, as we know doesnt smooth the solution. t just advects it, just carries it along. But this term will smooth it. o this is the dominant term, the second derivative. No surprise. o when we introduce that, that smooths everything. We have no problem to say there is a solution, at least for simpler models, and then we can see what happens when that term approaches zero. OK, so thats another method. And, of course, thats kind of what were doing with finite differences. Finite differences, if we put in a little viscosity, a little secondspace difference, and take delta t small enough for stability, then we can solve it. And if we take it very small, we should get something close to the inviscid limit. OK, was just giving Burgers name to that particular example. o this isnt the only example, but its the simplest example. OK, another example of conservation laws that everybody likes, think, is the traffic flow, traffic flow in one direction, say, on the ass. Pike. o whats the equation of traffic flow? Let me put that on a board here, and then have to come back to this. Well, ll put it here. o Example 2 well, dont know if you can call that Example 1. We didnt give it a physical meaning. Example 2 will be traffic flow in 1D, and whats the unknown? Well, its the thing ve been calling u, but for traffic flow, the natural unknown is the density of the traffic, the density of the cars. And everybody uses rho for density. o can use rho rather than u? And then, in the equation will be a car a velocity that ll call v. Thats not a new unknown. n fact, the velocity experimentally depends oh, theres a relation. The density and velocity are, by experiments, if you run an experiment on the ass. Pike in heavy traffic that could be a homework problem, too to find the relation between v and rho. o m going to keep this rho as the unknown and imagine that we have some experimental velocity v of rho. And what would we expect? guess we expect that as rho increases as the density increases, what will happen? As the density increases, the traffic is going to slow down, ultimately come to a stop, where if the density is very small, so the density 0 mean, we just have a single driver, the police are not out, so hes going v_max or 2*v_max. But as the road gets crowded, the velocity drops and maybe drops linearly. That would be a possible and not that terrible relation between v and rho. o what would be the flux function in this traffic flow problem? The conservation law is like conservation of cars, right? f we look at a unit, a piece of the turnpike, then the conservation law says that the total number of cars inside, which is this, changes by cars leaving and by cars entering. And so the flux function think will be so we have to know like sort of how many cars are leaving, and think the flux function would be probably v times rho. think the flux function of rho, now, is my unknown think would be v of rho times rho, the velocity times the density. That tells me how many cars are going out. OK, so guess what want to say is that we will meet that we meet, in this traffic flow example, exactly these possibilities that we meet them mathematically just the way we meet them in actual driving of a shock. o this shock corresponds to so what thats? Like stopgo this corresponds to, suppose, like meeting a red light? oming up to a red light, the traffic youre behind normally stops. t has to slow way down and stop. Then the light turns green. The first car takes off. The cars begin to spread out. Thats the solution that dont see with this starting function, a sort of fan of cars. o maybe could write those words down: shock or a fan. The shock is what happens when characteristics come together, cars come together. The fan is when the characteristics fan out. Actually, could illustrate a fan here right away. uppose reverse 0 and 1 just to see what OK, so heres my this is my x again. o here m going to have u_0 to be 0 now, u_0 to be well, can even its called a Riemann problem when there are just two values here. u_0 is 1. o the socalled Riemann problem is the cleanest example of the whole conservation law theory when there are just two starting values, 0 and 1, or 1 and 0. And it makes a big difference. 1 and 0, we saw when we reached time 1, we had a 1 and a 0, and after that, a shock formed. Here 0 and 1, the characteristics here are going as always. o u is 1 here. The characteristics here, this is staying at 0. Theres no reason to change. But here we have now a region, just like the other one, but with a major difference. That now, its 0 on the left and 1 on the right, where before, it was 1 on the left and 0 on the right. And the question is, what happens in here? Well, you might think a shock, a step up from 0 to 1 at some point. And would have to agree that you could do that in such a way that you maintained the conservation law. But thats not what happens. This is going to be a fan in here. Were going to have what well have is characteristics in here. This will let me draw the profile. o the profile is 0 there to 0 at some time. Then over here, where this characteristic is met, its 1, and this is what happens: nstead of getting worse, less smooth, the solution gets better, more smooth, when the direction is reversed. o we see that our problem had to be nonlinear. A linear problem couldnt do these two opposite things in the same thing. o here we would have a fan and the characteristics go there, and its reflected there. OK, so theres a picture of the phenomena that were looking at and we would like to see numerically, too. o next time want to find so can say now what have to do next time. One is to locate the shock, the shock line, when there is one, and two, to decide how do we decide between shock or fan when from the point of view of pure conservation, we could choose. But nature makes a choice. And one way to decide, make that decision, is Burgers way of putting in a little diffusion and letting it disappear, but we need a direct rule. o we need a direct entropy condition. o this is going to be a shock speed a formula for shock speed s s equals shock speed well find, if there is a shock. And number two is going to be this decision is going to be based on a quantity called entropy, which appears only in nonlinear problems, really. OK, so thats a beginning picture, which these 16.920 notes are an excellent reference for, the book also. And ondays lecture will pick up there. o we can see the two situations here, and the question is what do we do at this point. OK, see you onday. Have a good weekend. And any email questions about the homework, just you could send to me or and/or to maybe and to r. ho, who will be looking at the homeworks that come in. Good, thanks.","Let me see that when it comes, but m just mentioning it to say that this business of the shock line, which is what well pursue now, which because its what happens with this problem is not the only is not always the way to match, the way to deal with a problem of lack of information coming from the characteristics. The diffusion is the term, is the one thats forcing us to take this coefficient R below a half, delta t of order delta x squared, and the way to avoid that is to move the viscosity term, at least half of it, up to the new time. When you think of a characteristic thats going straight up, its just carrying the value of u without moving left or right, so u stays 0. o u will be 1 here, u will be 0 here, and now somethings happening in here, and we have to figure out what it is. And u is 0 up all of these characteristics, so up to that time, the algebra the differential equation is going to be totally OK. And then, over here on the far right, the starting value is 0, and the characteristics in that case, the c associated with that is 0, so its a standing its not moving. OK, so just to see, what is this? o we can see the two situations here, and the question is what do we do at this point. OK, and the question is and know that have u equals 0 over here, and know oops, u equal 1 over here, and know that have u equal 0 here, but the question is where, and well, what happens in between? o drew characteristics, and we still have to track we have to say more what characteristics are and understand that these have slopes in between, but youre not surprised because the values of u are in between 1 and 0. o they go that way and they meet here. And the way to see what continues to happen is to go to the integrated form, the integral form of the equation, where, of course, by taking an integral, make everything smoother, and also preserve the physical law, the conservation law. This is going to be a fan in here. And the idea will be so we have to discuss that problem. One is to locate the shock, the shock line, when there is one, and two, to decide how do we decide between shock or fan when from the point of view of pure conservation, we could choose. o this is the problem that were in the middle of speaking about, near the end, the mixture of convection and diffusion. c is minus think of c this u as being comparable to minus c. o, in other words, if u is positive, m now going to have waves going to the right. o its c over d, and then we have a delta x and a 2. o thats why its called the cell Peclet number, because its the cell size, or in my convention, half the cell size that gives the length scale and decides whether P is so m just going to write down the conclusion that the figure should show. And so were seeing for the first time, like, because we have two terms that are not dimensionally identical, c and d, that in that competition between convection and diffusion, the length scale is crucial, and the mesh size is crucial. o when the starting value is 1 and u is constant along characteristic lines, then u is 1, and those lines, its just like having c equal to minus 1, guess, in the oneway wave equation. ts saying that the change in the quantity in the integral is given by if theres any change, any time derivative, the rate of change in the interval its the conservation of this integral, meaning that any change in that integral happens because there is flow going out the righthand side of the interval or theres flow coming in the left side of the interval. And any email questions about the homework, just you could send to me or and/or to maybe and to r. ho, who will be looking at the homeworks that come in. This is the form where we see a time derivative of u, as always, and we see a space derivative of u squared over 2, which, of course, produces that. Were going to have what well have is characteristics in here. And the question is, what happens in here? We have no problem to say there is a solution, at least for simpler models, and then we can see what happens when that term approaches zero. But youll see that the differential equation can produce two solutions at the same point, and we have to choose. This is the advection equation, or the oneway wave equation, which weve started with, and this is whats thisll be the subject of todays lecture and next time. But there is another possibility that will happen in other cases, and that would be a fan of characteristics. Here we have of a velocity that so c is minus u, guess. Let me put that on a board here, and then have to come back to this. OK, so theres a picture of the phenomena that were looking at and we would like to see numerically, too. We still have the algebra to do, but think the first thing is to see roughly whats happening. OK, so maybe can also mention on the as something that we havent done here, was to move the second derivative, this guy, to time n plus 1, make it implicit. And would have to agree that you could do that in such a way that you maintained the conservation law. And so the flux function think will be so we have to know like sort of how many cars are leaving, and think the flux function would be probably v times rho. Well, actually, number two, the chrodinger equation, which has that imaginary i in there, we havent spoken about yet, but you could do the kind of Fourier analysis that weve done for the heat equation. u_0 is 1. o the socalled Riemann problem is the cleanest example of the whole conservation law theory when there are just two starting values, 0 and 1, or 1 and 0. But, of course, its going to be different because of that i. Youll get an e to the minus i k squared t, guess. As the density increases, the traffic is going to slow down, ultimately come to a stop, where if the density is very small, so the density 0 mean, we just have a single driver, the police are not out, so hes going v_max or 2*v_max. u_0 of x is the key points are 0 and 1, 0 and 1. o u_0 of x, start with a constant initial function, a constant profile to the left of 0, and a constant profile to the right of 1, and linear between. dont know if that put an idea into the system which produced, of course, the fact that and, of course, there would be a dimensionless constant, which amazingly happens to be 1. You can think of at least two possible scenarios, and one of those is that theres a shock line that goes, lets say, somewhere in the middle. What is this? And so u stays 1 all the way to that line and drops to 0 after it. And you see, right away, the difference between that equation and the advection equation that had a constant velocity. OK, so guess what want to say is that we will meet that we meet, in this traffic flow example, exactly these possibilities that we meet them mathematically just the way we meet them in actual driving of a shock. o this is the situation at t equal to 1.",0.1111873713109128
56,56,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. ERK DEANE: All right, today were going to do some crossover between two kinds of data structures, memory hierarchy data structures and geometric data structures. And this will be the final lecture in the memory hierarchy series, so the end of cache oblivious. o were going to look at twodimensional geometric data structure problems, both offline and online. o our good friend, orthogonal 2D range searching, which we spent a lot of time in a few years ago, we will come back to, and try to get our bounds good, even cache obliviously. o instead of log n, we want log base b of n to make things interesting. And the batch version is where youre given a whole bunch of rectangles, and a whole bunch of points up front, and you want to find all the points that live in all the rectangles. o thats an easier version of the problem. Well start with that and then well go to the usual online version, where you have queries coming one at a time, rectangles coming one at a time. The points are preprocessed, it will be static. And to do the batched, were going to introduce a new technique called distribution sweep, which is a combination of the sweep line technique we saw back as we used persistence to make sweep line thing into a data structure thing. But were just going to use the algorithmic version of that plus a cache oblivious sorting algorithm. o well finally do cache oblivious sorting and optimal N/B log base /B / of N/B using a particular algorithm called lazy funnel sort, which you can actually also use to make another kind of cache oblivious priority queue, but we wont get into that. And so by combining those two things, well get a divide and conquer technique for geometric problems that lets us solve the batched thing, and then well use completely different techniques for the online thing. o for starters, lets finally do cache oblivious optimal sorting. m not going to analyze this algorithm because its just an algorithm, not a data structure, and also because the analysis is pretty close to the analysis for priority queues we did last class. o funnel sort is basically a merge sort. mentioned last time that in external memory, the right way to do, or a right way to do optimal external memory sorting is an m over Bway merge sort. n cache obliviously, you dont know what m and b are, so its hard to do m over Bway merge. o instead, you basically do a Nway merge. Not quite Nway, cant afford that, but its going to be n to the 1/3 way merge sort. And the big question then becomes, how do you do emerge? And the answer is with a funnel. And so the heart of the algorithm is a funnel. o if you have Ksorted lists that are big, sized K cubed, then you can merge them in, basically, the optimal bound. o Kfunnel, Ksorted lists, total size K cubed. Number of memory transfers to merge them is K cubed over B times log base /B of K cubed over B. Theres a plus K term and when you plug this into an actual sorting algorithm, you need to think about that, but thats not a big deal. sually this term will dominate. OK, so let me show you how funnel works. Were just going to go through the algorithmic part and wont analyze the number of memory transfers. aybe ll draw this here. o were going to have the inputs down at the bottom of this funnel. ts going to have some data in them. Those k inputs down here, total size, all these is theta K cubed. And then at the top here, we have our output buffer. This is where were going to put the results and this will have size K cubed. aybe weve already done some work and weve filled some of it. OK, the question is what do you put in this triangle to do the merge? And the obvious thing is recursive triangles. Recursion is like the one technique we know in cache oblivious data structures. o were going to take square root of Kfunnels and just join them together in the obvious way. o just like layout, except didnt quite leave enough room here in between the levels are buffers. Theres a buffer here two between the nodes of this funnel and the nodes of this funnel. OK, these buffers may have some stuff in them at any moment. OK, and the big question is how do you set the buffer size? This is the key step. And the claim is each buffer, we set to a size of K to the 3/2 because the number of buffers is about square root of K because theres one per leaf of this funnel. And a Kfunnel has K inputs, so a root K funnel is going to have root K inputs here. And so the total size of all the buffers is K squared, which is not too big. m not going to go through the recurrence, but if you add up the total size of this thing, it is linear size in the output, K cubed. think also if you dont count the output buffer, its linear and K squared. f recall correctly. Were not too concerned with that here, just overall. Once we have Kfunnels, funnel sort is just going to be N to the 1/3 way merge sort with an N to the 1/3 funnel as the merger. We can only up to n the 1/3 because of this cubic thing. We can only merge if we want the sorting bound N/B log base /B of N/B we can only afford K being up to n to the 1/3. o thats the biggest we can do. o its a recursive algorithm where each of the merging steps is this recursive data structure. Now, this is really just about layout. havent told you what the actual algorithm is yet, but its a recursive layout. You store the entire upper triangle, then each of the triangles, somewhere you put the buffers. t doesnt really matter where the buffers are as long as each triangle is stored. As a consecutive array of memory, well be OK. And now let me tell you about the actual algorithm to do this. ts a very simple lazy algorithm. o theres a whole bunch of buffers. f you want to do this merge, really what youd like to do is fill this output buffer. o you call this subroutine called fill on the output buffer and say, would like to fill this entire buffer with elements. Precondition, if youre going to do a fill, right now the buffer is empty, and then at the end of the fill youd like this to be completely full. And how do you do it? Well, if you look at any buffer partially filled, whatever and you look right below it, theres a node in this tree. You recurse all the way down. n the end, this is just a binary tree with buffers in it. o its going to be theres a buffer, then theres a node, then theres two children, each of which is a buffer, and then theres a node below that. OK, so how do fill this thing? just read the first item, the beginning, the smallest item for each of these, compare them. Whichever smaller, stick at here. ts just a regular binary merge which is kind of cool. Youve got two arrays. You want to merge them. tick the results here. o thats how we do fill. Binary merge of the two children buffers until were full. But theres one thing that can happen, which is that one of the child buffers might empty. What do we do then? Recursively fill it. Thats the algorithm. Very simple. The obvious lazy thing to do. Do a binary merge. This is going to be nice because its like two scans, until one of these guys empties, and then you pause this merge, and then say OK, m going to fill this entire buffer, which will recursively do stuff until its completely full or run out of input elements, whichever comes first, and then resume this merge. Question? ADENE: Arent there more than two child buffers? ERK DEANE: hould only be two children buffers. The question is, are there more than two? This recursion of the root k and root k child triangles of size root k is exactly the recursion we did on a binary tree. didnt say, but underlying this is a binary tree. The only difference between this and a layout is were adding these buffers. intended to draw this as binary. ts a little hard to tell because didnt draw the base case, but it is indeed a binary tree in the end. OK, other questions? o thats the algorithm and as said, m not going to analyze it, but its the same kind of analysis. You look at the threshold where things fit in cache or dont and argue accordingly. ts pretty handwavy. What want to get to is how we use this to solve more interesting problems than sorting. orting is a little bit boring. o lets go to batched orthogonal range searching. And in general, this technique called distribution sweep. The idea with distribution sweep is that not only can we use this cool funnel sort algorithm to sort, but we can think of it as doing a divide and conquer on the key value. And in this case, we have two coordinates. Were going to use the divide and conquer on one of the coordinates. And where we have some flexibility is in this binary merge step. Were doing this binary merge, and normally its just you take the min, you spit it out here, you take the min, you spit it out here. Thats the min of one particular coordinate. Now youve got to deal with some auxiliary information about the other coordinates. o in general, youre merging two sorted things. f theres other geometric information, you can try to preserve it during the merge. As long as you can do that, this is the conqueror part or that combine step of divide and conquer. You can do a lot. Theres a powerful technique, it turns out. ts by Brodal and Fagerberg. ts in their early days of cache oblivious. t was the first geometric paper. Fine, so replace or say augment the binary merge, which is, in the end, the only part of the algorithm other than the recursion. o its the only thing you need to do to maintain auxiliary information. Thats the generic idea of distribution sweep. And distribution sweep has been applied to solve lots of different problems. Batched orthogonal range queries is one of them. Generally, youve got a bunch of orthogonal segments, rectangles, points, and you want to compute how they intersect. Those sorts of problems that can be solved here. Also weird things like give you a bunch of points and want to know for every point whats its nearest neighbor. n Euclidean sense, that can be solved. But like orthogonal range searching because its the closest to our data structure problem and thats a problem weve seen. o the actual batched orthogonal range searching is your given N points, and N rectangles, and you want to know which points are in which rectangles. Thats the general problem. o normally, were given the points first, and then were given the rectangles one at a time. Thats what weve solved in the past. Thats what we will solve later. Thats the online version. The batched version is give you a whole bunch of queries want to simultaneously and were going to achieve the sorting bound N/B log base /B of N/B plus the size of the output over B. And this is generally the optimal bound you could hope for. ts not obvious you need the log, but think for most problems in external memory you need this log. ts hard to beat the sorting bound, and then once you pay the sorting bound, this is the optimal linear time to just write down the output. Now, this problem can be solved. Give me all the point rectangle pairs that result. m not going to solve it here exactly. Were going to solve a slightly different version, or in general whatever. Let me tell you about another version of this problem, which is a little bit easier. Then ll sketch how you solve that problem. o remember, weve talked about range reporting and also range counting, which is you just want to know the number of answers. Heres something in between. You want to know for every point, how many rectangles contain it? And particularly, this will tell you for each point, does it appear in any of the rectangles in the set? t will tell you how many and this is actually necessary as a first step because one of the hard parts in solving these kinds of problems or reporting problems, is that the output could be big. We know thats always an issue, but with cache oblivious, its a big issue, literally, because space is important. You cant afford to put space anywhere. f these buffers have to get much bigger in order to store those answers, then life is kind of tough because then this data structure gets too big, and then my analysis goes out the window because things that used to fit in cache, no longer fit in cache. The analysis didnt show you. o its an issue. o the first step of this algorithm is to first figure out how big those buffers have to be so that we dont have to allocate them too large. And to do that, we need to basically count how many answers there are, and this is what well do. To compute these values, the answers arent very big. These answers are just single numbers per point, so its no big deal. OK, so heres what we do. ort the points and the corners of the rectangles by xcoordinate using lazy final sort. Nothing fancy here. No augmentation, regular old sort. Then this will be useful later then were going to divide and conquer on y via a distribution sweep. And here, our binary merger is going to be an upward sweep line algorithm. o lets talk about that sweep line algorithm. We presorted our points by x. f you think about the merging step, what this means its confusing. Were trying to sort by y, we were in a certain sense, but were always going to be sorted by x because we did that up front. o the picture is going to be something like this. Were in a slab. Theres going to be the left slab. o heres the binary merger. Heres the L points and the R points. The L points are going to be in a particular x interval. The R points are going to be in an adjacent x interval corresponding to this tree picture. And then we have these points, which they overlap and why? Because the whole point is were trying to merge by y. OK, we also have some rectangles, and their corners are what we have represented. probably should have used colors here. omething like this. o were given, essentially we have whatever we want on the points and corners in here. We have whatever we want in the points and corners in this slab. Let me add a little bit of color. These lines. And now we want to merge these two things and merging here is all about counting how many rectangles contain each point. Now, we already know how many points over here are contained in rectangles that are over here. o weve presumably already found that this point lies in this rectangle. Weve already found guess theres no points here. Weve already found that this point is contained in this rectangle. OK, because these corners were in this slab, and so lets say every corner knows the entire rectangle. o when you were processing R, you saw these corners, you saw this point. omehow you figured that out. What were missing are things like this rectangle, where none of the corners are inside R. o R knew nothing about this rectangle, and yet it has points that are contained in it. imilarly, there are these rectangles that completely span L, and so therefore none of the corners are inside L. But we need to know that these points are in there. Those are the only things that will be missing at this level. There might be other rectangles that completely span L and R. Those will be discovered at higher levels, now here. ts a little bit awkward to check if this will actually find everything, but it will. o to figure this out, when were merging L and R see, L knows about this rectangle because it sees these points. We want to keep track as we sweep upwards. We want to realize that these points are in a big rectangle here, whereas they werent discovered in L, and they werent discovered in R. To do that, we maintain a number as we have a horizontal line, were sweeping up. We want to maintain the number of active rectangles. Active means that its currently being sliced by the sweep line. That have left corners in L and completely span R. o thats these guys. o thats easy to do. Were merging these points. o that each of them has been sorted by y. Now were doing a merge, so were considering all the corners, and all the points, and increasing the ycoordinate as we do that binary merge. o whenever we visit a left corner of a rectangle a lower left corner we say oh, does this rectangle go all the way across? This one does not. By the time we get to here, this one goes all the way cross R, and so we increment L. And when we get to the upper left corner, we decrement L. ay oh, that rectangles over. o its very easy to do constant time, but its only going to be 1/B memory transfers per one of these because its a nice, cheap merge. And then symmetrically, we do R. ts the number of active rectangles with the right corners in R that span L. o thats this guy, R, guess, this guy is L. n general, there might be a lot of them, so you count them. And then the only thing we need to do is whenever we encounter a point as opposed to a corner, because were storing them all together, we add got this right R to its counter. We want to know how many rectangles contain that point. And so for example, when we see this point, and R is currently one, then we know that this point appeared in some rectangle that spanned L. o we increment this points counter. imilarly, when we see these points, L is positive, so we increment these guys counters by whatever L is. o this is a symmetric version in R when we add L. Probably should have called them the other names, but anyway, L, R, doesnt matter. LR. Question? ADENE: The bottom is the xaxis, right? ERK DEANE: This is the xaxis, yeah. ADENE: o are we dividing and conquering on x? ERK DEANE: t does look like were dividing and conquering on x, think youre right. orry. For some reason thought it was y. Youre right. o its a funny thing. Were presorting by x, which is whats getting us thank you. Thats much clearer now. n my mind was like theres something weird here. Were presorting on x and then were just sticking these guys down here. o evenly dividing them into lists. Or, guess actually, were doing our funnel sort, the merge sort. Things have already been sorted by x, but now were merge sorting again, and this time when we merge, we carry along this information. o theyre both in terms of x, which is kind of funny. s there another question? ADENE: orry, is it important that we do the upward sweep ? ERK DEANE: The upward sweep. Yeah, we have to do the points in order by y. ADENE: o do we want to just sort by y, and then . ERK DEANE: Ah, so confused now. ADENE: Because in the notes, it said x and then y. ERK DEANE: Yeah, know in the notes it says y. t used to say x. believe, were dividing and conquering on x, but were sorting by y, and thats the confusion. ll double check this, but in order for this sweep to work so its like you first sort by x. You We are in some sense doing divide and conquer by x because we did this sort by x. But the merge short is on y. t makes more sense. f youre already in x order, sorting isnt going to learn you much. t isnt going to teach you much. o first you sort by x. Things are nicely ordered by x. o we get these nice horizontal slabs in the decomposition, but now when we merge Now were going to sort by y. o were going to reorder the points and thats what lets us do the sweep. And we are, in the end, merging all these points together in y order. And as we do it, then we get the information we want about rectangles and points. OK, this is why wanted this to be both x and y. But really, the divide and conquer is happening on x, but we are doing a merge sort on y. Finally clear. Thanks for helping me. This is a new lecturers, as you may have guessed, so still working out some kinks. really wanted to introduce this lecture because the next thing were going to cover, which is a way to do orthogonal 2D range search and cache obviously, is super cool. ts like one of the craziest things there is. At least in the cache oblivious world. Any other questions before Oh, should say a little bit more about this. Weve now solved this first step, which is figuring out the output size. ounting for each point how many rectangles contain it, which is an interesting problem by itself. Thats the range counting problem. You can also use it to figure out, at this level, at this merging step, how many things will be output here? How many new outputs are there? How many points in rectangles are there? ts essentially just the sum of all those things. o you can count the number of outputs per merge and so then theres a natural strategy, which is you build a new funnel structure where these buffers have the right size. Youve precomputed what all sizes need to be. At every merge you know how many things are going to get spit out here. o you could allocate that much space and that will be a kind of decent merge sort. Because havent done the analysis, its hard to get into detail about this. But it will not be optimal, unfortunately. To actually make it work, you end up having to take this tree, carving it into subtrees of linear size. o normally, the whole thing is linear size. Everythings fine. And where the analysis breaks, essentially, is if you have a giant buffer because one of the outputs potentially, the output size here is quadratic. And so the overall thing might be super linear. And so when you have a super linear buffer or a bunch of very large buffers that sum to linear size, you essentially need to carve that tree, which you do by recursive carving of the tree. o that each of the trees has linear size. Then you apply the analysis to each of the trees separately. You store them consecutively, separately. Each of them has good optimal running time and then the combination does. Thats the handwavy version of how to do actual range reporting with end points and end rectangles. f youre interested in the details, read the paper. ts just a little bit messy and especially when you dont know the analysis. want to move on to online orthogonal 2D range searching because its the hardest and coolest of them all. nless there are more questions. All right. ADENE: o you do the range counting to the .. ERK DEANE: Exactly. At this point, if you believe in funnel sort, you should believe that range counting is easy to do, and ve just hand waved the range reporting part. Are you scribing? s that why you ask? Thats where we stand. The next thing were going to do is regular range reporting, regular online stuff. o this is orthogonal 2D range search. And we spent a couple of lectures on 2D and 3D range search. All this crazy stuff with fractional cascading, and so on, and the layered range trees. Were going to use some of those techniques that we built there, and in particular, you may recall there was this idea that if we have a bunch of points, regular 2D range searching is give you a rectangle, give me all the points in the rectangle. Fine. Our goal is to achieve log base B of N plus output size over B. Thats the new optimal bound. This is how long it takes to do a regular search in one dimension. o if you have output size whatever and well probably be able to do range counting, but wont worry about it here. Well just think about range reporting. f theres this many points, well output them all in that much over B. This is what we call a regular range search, but m going to distinguish it and call it a four sided range search because a rectangle has four sides. But you could think of the other versions and we actually did this when we were doing the 3D problem. o if these are two rays and an edge, this you might call a three sided rectangle, and you can go all the way down to two sides. Hard to go down to one side. Heres a two sided rectangle, it just has two rays. OK, as you might expect, this is easier than that. And if recall, in 3D we ended up doing this thing in linear space with this fancy first you do a search on the left coordinate and then you just walk. Wed subdivided with fractional cascading so that every face had constant size, and so you could just walk, and each step youd report a new point. f you may recall for this kind of two sided thing. First, you would search for this, and then you would basically just follow this line until you found this point, this corner. This we could achieve in a linear space, logarithmic time. This one we needed N log N space. Actually, the best known is N log N divided by log log N. But we could N log N using range trees. And we got down to log N time using log N query time and log N space using layered range trees. That was the internal memory regular algorithms. ADENE: Arent you missing an /B though? ERK DEANE: Am missing an /B? No, this is log base B of N, not log base /B of N. Yeah, its good to ask. When were sorting this kind of thing, we get log base /B, but when youre searching, the best you can do is log base B. We actually proved a lower bound about this in the first memory hierarchy lecture. Because this is online, you read it in a block. You can only learn where you fit among B items. And so the best you can hope to achieve is log base B of N for search in one dimension. o this is a lower bound for search. When youre doing batch operations, then you can hope to achieve this stuff, which is a lot faster. Then its like 1/B times log base /B of /B. OK, so in a certain sense, this is slower than the batched operations, but its more online. o its a tradeoff. o for all these problems we can achieve log base B of N plus over B. The issue is with space. aybe ll do sort of regular RA algorithms versus cache oblivious. o weve got two sided, three sided, four sided. And for two sided, believe these are the right answers. Log N over log log N. But we havent actually seen this one. And cache oblivious, heres what we can do. This is with optimal query times and this is all static. OK, and if theres time, ll cover all of these. o theyre not perfect. These two were off by a log factor, but not bad. Pretty good orthogonal 2D range queries. And really, the coolest one is this one. This one blows my mind every time see it. o lets do it. Well start with two sided and then we have existing techniques once you have two sided to add on more sides, you may recall from the 3D range searching lecture. o were going to use those techniques and refine them a little bit to get that log log factor. But you may recall way back when, at lecture six or so, that we had a technique. Once it was two sided, every time we added a log factor in space, we could add another side. The hard part was getting up the number of dimensions. Then the easy part was turning half infinite intervals into regular intervals. o once we have this, its easy to add a log, add another log. With a bit of sophistication, we can save a log log factor. OK, but lets do two sided. This will be the bulk of the lecture. This is a paper by and in 2006. All right, so we want to do m going to assume that they are this kind of quarter plain query. o less than or equal to x, less than or equal to some ycoordinate. We want to know all the points in that quarter plane. o heres what were going to do. ts all static. Were going to have a Van Emde Boas layout. o a binary tree on the ycoordinate. o this just stores all the points sorted by y. o if you want to do this query, use search for that value of y, then each of these positions in between two keys in here has a pointer to an array. The array is not sorted by x or y, its a very weird thing. And then heres the algorithm you follow. You follow this pointer, you go here, you walk to the right until you find a point whose xcoordinate is too big. ts bigger than x. should probably call this x2, y2. o first you search for a y2 here, in this thing keyed by y. Follow the pointer. You look at all the points that have xcoordinate less than or equal to x2. Those are the ones you want. Once you find a point whose xcoordinate is bigger than x2, you stop, and then you report these points. ts not quite so simple because some of these points might be duplicates. You have to remove duplicates. That is your answer. To me, this is an insane idea. would never imagine this to work. But the claim is you can make this array have linear size. Thats the hard part. ake this, the amount of stuff that you have to traverse here, be linear in out in the number of points that are actually in this range. You are going to do a little bit more work because there are duplicates in here, but only a constant factor of more work. And yet somehow, youve taken this two dimensional problem and squashed it onto a line. You did one search at the beginning, which costs you log base B of N, then you do this linear scan, and you get the right answer, magically. dont know how they thought this would be possible, but magically, it turns out it is possible. t was kind of a breakthrough in cache oblivious range searching. t was known how to do this for external memory a lot easier. For example, you can do it with persistence, but this is a much cooler way to do two sided range queries. All right, so ve explained the query algorithm. The big thing havent explained is how to build this array. aybe ll write down the things we need to prove as well before we get there, so you can think about them as were writing down the algorithm. First claim is that this algorithm, which just decides to stop whenever it gets an xcoordinate that is too big, actually finds the right answer. t Finds all points in the range that we care about. The second thing is that the number of scanned points, the length of that step here, is order the size of the output. The number of actual output points. We dont waste time doing the scan. And the other thing is that the array has size order N. Thats the biggest surprise to me. o those are the three things we need to prove about the algorithm, which will now tell you. OK, before can define how this array works, need to define a concept called density. f we look at a query, theres two things that could happen. The good thing for us would be if get this right. The number of points in lesser or equal to x star is at most, alpha times the number of points in the answer. OK, star means no restriction on y. inus infinity to infinity. This would be good for us because it says ultimately what were trying to do here is do a scan in x. ts the right thing to do here. Then for this particular ycoordinate, we could just basically start at the beginning of the array, start scanning, and just report all the points that are actually in our range. orry, need to also potentially throw away points that are not low enough. o the answer is contained in here. should say to throw away duplicates, you have to throw away points that are not in the range lesser or equal to x, comma lesser or equal to y. till, we claim the number of scan points is proportional to the output size. Thats what we need. o if this held for every query, wed be happy. Just start at the beginning, scan, and as long as this alpha is some constant its going to be a constant bigger than 1, then the number of points in the answer is proportional sorry, the number of points we had to scan through is proportional to the number of points in the answer, and so were done. o this is the easy case. We need to distinguish it, otherwise we call this range query sparse, and those are the interesting cases. o nothing deep here, but were going to use this concept a lot. OK, so were going to actually try to solve this problem twice. The first try isnt going to be quite successful, but it gets a lot of the right ideas. o m going to let 0 be all the points sorted by x. ts going to be sorted by x. put things down here. And just to give you an idea of where were going, the array were imagining here is first we write down all the points, then well write down some subset of the points, 1, then some subset of that subset, and so on until we get down to a constant size structure. OK, first we write down all the points. Why? Because for dense queries, thats what we want. We want all the points just sitting there. o then you can just read through all the points and dense queries will be happy. o if we detect a ycoordinate where the queries going to be dense dont know how we detect that. Lets not worry about it right now then you could just look through 0. Thats fine. But some queries are going to be sparse, and for that were going to use 1, 2, and so on. The intuition is the following. f in your query, the ycoordinate is very large, like say infinity, then your query is guaranteed to be dense. t doesnt matter what x is. And in general, if y is near the top, like its at the top most point, or maybe the next of top most point, or maybe a little bit farther down, it depends on the point set, then a lot of queries are going to be dense. o thats good news. Lets consider the first time when theres a sparse query. o were going to let yi be the largest ycoordinate where some query, some xcoordinate that ycoordinate. This is going to be less than or equal to x, comma less than or equal to yi is sparse in i minus 1. OK, so initially we have 0, all points. y1 is the largest y coordinate where theres so we work our way down until theres some sparse query in 0. Thats yi. o then we just filter, based on that. o throw away all the points above yi. o were going to say take i minus 1, intersect it with the range query, star less than or equal to yi. OK, so the picture is we have some point set. p here, every possible query along this line is going to be dense because everything to the left of the xcoordinate will be in the output. At some point, were going to decide this is too scary. Theres a query here, maybe this one, or maybe its this query thats sparse. And so we say OK, throw away these points. Redo the data structure from here down, ignoring all these points, repeat, and write down these things. o the idea is that if you look at a particular query, it will be dense in one of these is. And you can tell that just according to your ycoordinate. Because you said oh, well, if youre up here in ycoordinate, youre guaranteed safe. o just do that search and youre OK. n general, we continue this process until we get to some i that has constant size. At that point, were done, and then we can afford to look through all the points. nfortunately, this is not a very good strategy, but its the first cut, and its close to what works. Heres a problem with it. uppose you have this point set. OK, what happens is you start at the top, everything looks fine. At some point you decide theres a query here, namely this one, which has an empty answer, and yet there are points to the left of this xcoordinate. o thats bad because its very hard to get within a constant factor of zero. o pretty much immediately youve got to draw a line here and say OK, 0 is all points, 1 is these points, 2 is going to be these points. n general, theres suffixes of the points, and so the total space will be quadratic. o the first two properties will be correct because youre just looking in 0, or 1, or whatever. Everything looks fine, but your right does not have linear size. o no good. First try, failed. econd times the charm. You need a little more sophistication in how we do this partitioning, how we build our array, and well get it. didnt read this before. This one line that says maximize common suffix. have no idea what this means, but maybe it will mean something by the end. Lets see. OK, this is the part read. o xi is going to be so we had a yi Thats going to be the same as before. This is why did the first attempt. This definition remains the same. o largest y where we have some sparse query in i minus 1. want to look at what that xcoordinate is. ts just that here it says theres some x. What is that x? Lets just look at the maximum possible x that it could be. This will turn out to be really useful. The maximum xcoordinate where less than or equal to xi, comma less than or equal to yi is sparse and i minus 1. OK, we know theres something we can put in here that makes yi sparse. o look at the largest possible such x. o that means any query so we have this new point. ts not an actual point in our problem, but its a query, xi, yi. And its dense, oh sorry, its sparse. ts bad. We know that any query up here is dense. That was the definition of yi. And now we also know that any query over here, guess, thats saying a lot. But these queries are also dense. Because again, if youre far enough to the right, thats going to be basically everything. o lets get rid of that as well. And this is a problem, queries over here are also potentially a problem. We dont know. t doesnt seem like much, but it will be enough. Were going to redefine i as well. o heres the fun part. f we have some i minus 1, were going to define a new thing, which is Pi minus 1, which is this. This is a funny thing, but it is this part of the point set. This is Pi minus 1. o the points we care about are kind of here, but lets just take everything to the left of this xcoordinate. Why not? ts a thing. That is Pi minus 1. o i minus 1 is everything in this picture. First, lets restrict to x, then the next step is were going to restrict to y. But its in a funny way. This is the i, the next s set. Take the previous set and we intersect it with a funny thing. ts harder to write algebraically than it is to draw the picture. o its intersected with a union, which is basically dare draw it on the same picture? Wheres my red? ts going to be less than or equal to y. This thing is going to be i. Well see why, eventually, this works. still dont know what maximize common suffix means, but well get there. o were looking at the points below the line. Thats what we did before. We used to say i is just the intersection with less than or equal to yi. But things are just a little bit messier because of this restriction. Do really not have a P here? OK, heres the difference. The reason we have to go through this business. The array that were going to store is not the is. is are still too big, potentially. What were going to store are the Pis. Pi minus 1. And then in the end, were in a store i. i, again, has constant size. The final i has constants size. probably should have used a different letter, k or whatever. We keep doing this until we get down to something constant sized, then we store it. Thats the easy case. ntil then, we just store the Pis, because really, we know that all the queries up here and over here are OK. Theyre nice and dense. We sort of only care about the points to the left of the line. OK, but essentially, the i has to pick up the slack and we have to include these points in the next i. Whereas, before, we did not. Before we just took things below the line. Now we have to take things that are below the line or to the right of the vertical line. This is essentially necessary for correctness. o we kind of win some, we lose some. But it turns out all is well. o know this is weird, but lets jump to the analysis. These claims, in particular, that the array has linear size. Lets think about that and it will become clear why the heck weve made these choices. nless you have a question first. ADENE: s there any relationship between the i here and the i on the first try? ERK DEANE: No, this definition of i is no longer in effect. 0 is correct, and all the is are still sorted by x. Were no longer doing this. nstead of this rule, were doing this rule. This part is the same, but we have this extra union, which contradicts the previous rule. o the yi definition is the same. orry, its a little weird. xi is new, Pi is new, and i is new. At this point, its this algebraic weird thing. Heres the cool thing. For the space bound, the claim is Pi minus 1 intersect i is less than or equal to 1 over alpha times Pi minus 1. This is hard to even interpret what it means, but its good news. o remember, alpha is a number bigger than 1. ts what we use in the definition of density, and you could set this parameter to whatever you want, say 2. o then were going to get that this thing, whatever it is, is at most half the size of the previous one. claim this is good news. claim it means that these Pis essentially are geometrically decreasing in size, which is how we get thats not quite right, but this will give us a charging scheme. which will prove that the whole thing has linear size. First, why is this true? t could really only be true for sparsity from the alpha. Right, so we said oh, density is good. f we have dense, theres nothing to do. Just put the points in x order, were done. parse is bad. But actually, sparse tells us something. t tells us there are a lot of points that are not in the answer. o were looking at this query, xi yi. And wed like to just say oh, start at negative infinity, and just take all the points up to here. f were dense, that is within a constant factor of the number of points that are actually in the answer, which is down here. f were sparse, that means there are a lot of points up here. ost of the points have to be up here in order to be sparse. And thats actually what this is saying if you expand the definitions. o Pi minus 1, that was all the stuff to the left. o thats this thing. This is what we would get if we just did a linear scan from left to right. Versus were considering the points in Pi minus 1, which just restricts to x, and then were looking at i. i does this business. But if we restrict to the i points that are to the left of the line so were looking at, basically, this left portion, which was this white rectangle, intersected with this funny red rectangle, which was kind of awkward the intersection is just this. Thats the answer for this query, xi yi. OK, so this is the size of the answer for xi yi. And this was the number of points in less than or equal to xi star. We wanted to just do a linear scan like this. But this is the correct answer and because we know that this point is sparse that was the definition of xi and yi, it was the maximum sparse point. o its a sparse point, therefore we know that this does not hold. o the number of points less than or equal to x comma star is greater than alpha times the number of points in the correct range. And if got it right, that should be this. You could put alpha over here without the one over and guess this is strictly greater. No big deal. o thats the definition of sparsity. o this is the cool thing we know. Now, were going to use this is now a numbered less than 1. Question? ADENE: o for Pi minus 1, we add them as the number of points less than xi star. But for example ERK DEANE: Yes, thats the definition here. ADENE: like Pi, you dont have that block in the top left corner, right? ERK DEANE: Right. After we restrict to i, yeah, weve thrown away all of these points. ADENE: Right. o if you take the next Pi, its not necessarily going to be the points less than or equal to xi ERK DEANE: ts true. When say points, dont mean all points. mean points in i minus 1. m dropping that because it gets awkward to keep talking about. o thats a correctness issue, essentially. You have to argue that we can throw away these points and its safe. Once we do, then you could just ignore their existence. You can ignore their existence because you already solved all the dense queries, which are over here, or over here, which involve those points. And so we now know that were only going to be doing queries from here down. Otherwise, you look at P0. o forget about those. Forget about those points. Now youre going to be searching in one of these structures. o you can forget about all the points over here. o thats that argument. Once youve restricted to i minus 1 and you dont have to look at any other points, among those points, this is going to be all the points less than or equal to xi. But thats how we were defining bar sparse. We said sparse in i minus 1. o its among those points we have sparsity. o this is the definition of what we have. OK, the claim is its a good thing. Heres the charging scheme. o this is by sparsity. o m going to charge storing Pi minus 1 to Pi minus 1 minus i. This algebra, have to interpret every single time, but thats fine. Lets look at the picture. OK, Pi minus 1 remember, was this white rectangle over here. Everything to the left of the line. We have to store Pi. We want that the sum of the sizes of the Pis is good. And so heres my charging scheme. We have to store Pi minus 1. m going to charge it to these points. What are those points? Those are the points that are inside the white rectangle, but outside the red Lshape. o thats these points. This is Pi minus 1 minus i. Those are the points that m throwing away. Thats good. o if charge them now, will never charge them in the future because just threw them away. They are not in the next i. Each point overall in the point set only gets charged once. OK, how much does it get charged? How do these things relate to each other in size? Thats where we use this thing. t gets confusing to think about intersection versus difference, but the point is if we look at the Pi minus ones that are in i, thats a small fraction. Think of alpha as 100. o then the Pi minus 1 so this part down here thats in i, this is only 1/100 of the whole white rectangle. o that means this part is 99/100 of the Pi. o if we charged the storing of the entire rectangle to these guys, were only losing a very small factor like 100/99 or something. t isnt actually exactly 100/99, believe. worked it out and the factor of charging, assuming did it correctly, is 1 over 1 minus 1 over alpha, which works out to alpha over alpha minus 1. t doesnt really matter, but the point is its constant. think thats easy to believe. aybe its actually easiest to think about when alpha is 2. At most, half the points are here. At least, half the points are here. And so were charging storing the entire point set to these points, which will never get charged again. o were only charging with a factor of two. Thats all we need, a constant factor. OK, therefore, this thing has linear size. Thats the cool thing. We get more though. We also get the query bound we want. Lets think about the query bound. This is fun. Think about where the query is. t used to be over here. We do a search in 0, or we do a search in 1, or we do a search in 2. Wed never look at multiple is because thered be no point. Either 0 was dense, and were fine, just do it. Or you have to jump to 1, skip some guys up top, do the search in there. Fine. We no longer have that luxury over here because were using Pis instead of is. o it actually may be the search starts in P1, but then has to go through P2, and has to go through P3. But its OK because the farther we go right, we have this sparsity condition that tells us basically the points were looking at are the number of points were looking at are getting smaller and smaller. o ll wave my hands a little bit here, but the claim is its a geometric series. This needs a formal proof, but we wont go through it here. Decreasing so this is the query bound. The number of scanned points is order output size. o you have to check that no matter where you start in Pi thats the little bit tricky part. Were not looking at all of Pi. Were looking at some of Pi and then were going to the right from there. Actually, is that true? aybe we always look at all of Pi. Let me think about this. think we do, actually. orry. Thats what we did before. We basically figure out where we are in ycoordinate. That was the overall structure. We had a Van Emde Boas search tree on y. o all we know at this point is the ycoordinate of our search. And so we use that to determine which of the Pis we go to, based on where the yi becomes no longer dense. And then were going to have to search through that entire Pi and potentially more of them because this is no longer an i. ts just doing the things to the left. And so if were lucky, the Pi were looking at, or the query were doing, is not to the right of this point. OK, maybe its right here. That would be great. Then all our answers are done. f our query is here, that would have been dense, so we would have done it at an earlier stage. Our query might be down here though. When the querys down here, we need to report on these points. Then were going to have to do more and thats going to be Pi plus 1. o well do more and more Pis until we get to our actual query here. But in any case, the claim is that this is geometrically decreasing by the same charging scheme. OK, thats two out of the three claims. Theres one more, which is closely related. ts still about the query problem. What we havent shown is that we actually find all the points. This is what you might call correctness. To prove this, what we need to say what we claim is that after you do the P1s and now you do the P2s. Well, ll tell you. The claim is that you visited some xcoordinates here. The Pis were all the things up to some xcoordinate. laim that the very next point in here, in P2, has a smaller xcoordinate than what you just did. think that should be clear because presumably there are some points in here, and so the very next Pi, its restricted within this red thing, but its going to be up to some xcoordinate. o youre basically starting over. Every time you go to the Pis, youre starting over in x. Go back to minus infinity in x. o the idea is the picture will look something like this. You start at minus infinity, you read some points. At some point, you run out of the Pis. Then you start over again, you read some smaller set of the points. aybe you get a little farther. You start over again, read a little farther. At some point, youre going to reach your threshold x. Thats when you stop. o thats correctness. feel like need another sentence there. Once your Pi encompasses your x range, thats going to have your answer. Then youre done. o thats this moment. And so the only worry is that an early Pi, basically, or maybe the next Pi does this, and then we do this or something like this. That never happens basically because youre always resetting x range. And so your x will always start over to something less than what you had. And so the termination condition, which probably didnt write down here, but which is stop when your xcoordinate is bigger than what you want. Never terminates early. Therefore we get all the points we care about. OK, a little bit handwavy, but that is why this structure works. ts a very weird set up, but linear sized, and you just jump into the right point in the array, start reading, throw away the points that arent in your range because they just happen to be there. Those would be these points up here. Throw away duplicates. Just output the points in your range and it gives you, magically, all the points in here by a linear scan. still find this so weird, but its true. Truth is stranger than fiction, guess. Theyre fun facts. You can actually compute this thing in the sorting bound. o preprocessing is just sort. wont prove that here. o this was two sided. Let me briefly tell you how to solve three sided and four sided. We basically already did this one, which was ll remind you what it looks like. o you have a binary tree, and in each node you store two augmented structures. One which can do ranged queries like this, and one which can do inverted range queries like this. This should look familiar. And so you do a search on lets say we want to do this thing. o we have x1, x2, y2. You search for x1, you search for x2. You find the LA and then in this subtree, you do a search. n this subtree, you already know that youre less than x2, and so you do the x1, y2 search in this node. And then in the right subtree, you do the x2, y2 search. You take the union of those two results and that is this query. Thats how we did it before. No difficulty here. And the point is, you can build this, put it in a Van Emde Boas layout. You do this search, you do this search, you find the LA in log base B of N to check that everything works, cache obviously. Then these structures are just structures which we already built, and so yes, we lose a lag factor because every point appears in log data structures, but thats it. Everything else works the same. o we get N log N space log base B of N plus output over B query. Because now we just have to do two queries instead of one. We dont theres a log factor. Thats the trick we did before OK, that was easy. One more. o that was three sided. Next is four sided. Four sided, of course, we could do exactly the same thing. Lose another log factor in space. aintain log base B of N plus output over B query time. But want to do slightly better and this is a trick we could have done in internal memory as well. But have two minutes to show it to you. o heres a bonus. Didnt have to do this in external memory context, but we can. Four sided. o were going to do the same thing, but not on a binary tree. Take this binary tree, this is sorted by x, suppose. This is key on x. nstead of making it binary, make it root log o imagine taking the binary tree, taking little chunks, which have size square root log N. ts capital N. And imagine contracting those chunks into single nodes. o we have a single note which has square root log N. hildren has square root log N children. This is all static. And so on. Otherwise, the same. The augmentation is going to be a little bit different. f we look at a node, were going to store the same things we had before, which was this kind of query, and this kind of query. Were going to store a little bit more. Namely, for any interval of children, like here you have some start child and some end child. want to store for all the points that are down there. For this thing, store a regular binary search tree on y for those points. Why? Because if we do a search OK, same deal we find the LA of x1, x1? dont know. Lets say its on x. Well have to do it again on y whatever. o heres the LA. Lets say theres a lot of children. OK, maybe here is x1 and here is x2. o in this subtree, we do this sorry, we do this range query because we want to go from x1 to infinity. Over in this subtree, we want to do this range query because we want to go from negative infinity to x2. But then theres all this stuff in the middle. dont want to have to do a query for every single tree. nstead, have this augmentation that says for this interval, here are all the points sorted by xcoordinate. guess were doing it this way. Fine, so then it is a range query. want to know what are all the points. Whoa, this is confusing. feel like ve missed something here. No, this on y. orry. These points ve got sorted by y. o should draw it the other way. These points we already know are inbetween x1 and x2 in x. Weve already solved the x problem here. o now just need to restrict to the y range from y1 to y2. n these trees, these already match in x. just need to make sure they match in y. o do a regular 1D range tree. search for y1, search for y2, take all the points in between. This is cheap if just have a regular old binary search tree. Now, this thing has linear size. This thing has sorry, think actually need should have a three sided range query. Thanks. These should be three sided because here know that ve got the right side covered already in this tree, ve got the left side covered already in this tree, but still need the remaining three sides. n here, only need these two sides because ve already got x1 and x2 covered. OK, so this is cheap. only need a linear space data structure. This thing is not so cheap. m using the previous data structure. This thing, which has N log N size, these are three sided range queries. orry for drawing it wrong. o need two three sided structures. Then need actually a whole bunch of these structures because this was for every interval. But conveniently, theyre only log N intervals because theres root log N children. o root log N squared is log N. o theres root N, but then we need log N of them. And so thats why these things balance out. ee? o normally, this would be N log squared N because every point would appear in log N trees. But now the height of my tree is merely log N over log log N with a factor 2 out here because have a square root here. OK, so the tree has height log N over log log N. o each point only appears in log N over log log N structures. Each of them needs a structure size N log N. o we end up with N log squared N over log log N space. Kind of crazy, but this is how you get that last little bit of log log N space improvement by contracting nodes, doing a simpler data structure for these middle children, and just focusing on The left child and the right child you have to do one three sided call, but then the middle is a very simple two sided call. ts just a 1D structure and so its really cheap. Thats it.","o remember, alpha is a number bigger than 1. ts what we use in the definition of density, and you could set this parameter to whatever you want, say 2. o then were going to get that this thing, whatever it is, is at most half the size of the previous one. Just start at the beginning, scan, and as long as this alpha is some constant its going to be a constant bigger than 1, then the number of points in the answer is proportional sorry, the number of points we had to scan through is proportional to the number of points in the answer, and so were done. And then were going to have to search through that entire Pi and potentially more of them because this is no longer an i. ts just doing the things to the left. This is Pi minus 1. o the points we care about are kind of here, but lets just take everything to the left of this xcoordinate. ake this, the amount of stuff that you have to traverse here, be linear in out in the number of points that are actually in this range. Were going to use some of those techniques that we built there, and in particular, you may recall there was this idea that if we have a bunch of points, regular 2D range searching is give you a rectangle, give me all the points in the rectangle. The batched version is give you a whole bunch of queries want to simultaneously and were going to achieve the sorting bound N/B log base /B of N/B plus the size of the output over B. And this is generally the optimal bound you could hope for. But if we restrict to the i points that are to the left of the line so were looking at, basically, this left portion, which was this white rectangle, intersected with this funny red rectangle, which was kind of awkward the intersection is just this. And so if were lucky, the Pi were looking at, or the query were doing, is not to the right of this point. o this just stores all the points sorted by y. o if you want to do this query, use search for that value of y, then each of these positions in between two keys in here has a pointer to an array. But this is the correct answer and because we know that this point is sparse that was the definition of xi and yi, it was the maximum sparse point. To prove this, what we need to say what we claim is that after you do the P1s and now you do the P2s. And the claim is each buffer, we set to a size of K to the 3/2 because the number of buffers is about square root of K because theres one per leaf of this funnel. OK, the question is what do you put in this triangle to do the merge? f were dense, that is within a constant factor of the number of points that are actually in the answer, which is down here. The array that were going to store is not the is. p here, every possible query along this line is going to be dense because everything to the left of the xcoordinate will be in the output. Once youve restricted to i minus 1 and you dont have to look at any other points, among those points, this is going to be all the points less than or equal to xi. o this is the definition of what we have. Yeah, we have to do the points in order by y. ADENE: o do we want to just sort by y, and then . This would be good for us because it says ultimately what were trying to do here is do a scan in x. ts the right thing to do here. The second thing is that the number of scanned points, the length of that step here, is order the size of the output. o first you sort by x. Things are nicely ordered by x. o we get these nice horizontal slabs in the decomposition, but now when we merge Now were going to sort by y. o were going to reorder the points and thats what lets us do the sweep. And just to give you an idea of where were going, the array were imagining here is first we write down all the points, then well write down some subset of the points, 1, then some subset of that subset, and so on until we get down to a constant size structure. o the idea is that if you look at a particular query, it will be dense in one of these is. And so you do a search on lets say we want to do this thing. m not going to go through the recurrence, but if you add up the total size of this thing, it is linear size in the output, K cubed. And to do that, we need to basically count how many answers there are, and this is what well do. Kind of crazy, but this is how you get that last little bit of log log N space improvement by contracting nodes, doing a simpler data structure for these middle children, and just focusing on The left child and the right child you have to do one three sided call, but then the middle is a very simple two sided call. o that each of them has been sorted by y. Now were doing a merge, so were considering all the corners, and all the points, and increasing the ycoordinate as we do that binary merge. And then the only thing we need to do is whenever we encounter a point as opposed to a corner, because were storing them all together, we add got this right R to its counter. think that should be clear because presumably there are some points in here, and so the very next Pi, its restricted within this red thing, but its going to be up to some xcoordinate. But some queries are going to be sparse, and for that were going to use 1, 2, and so on. And as we do it, then we get the information we want about rectangles and points. And so the only worry is that an early Pi, basically, or maybe the next Pi does this, and then we do this or something like this. ntil then, we just store the Pis, because really, we know that all the queries up here and over here are OK. This is a funny thing, but it is this part of the point set. By the time we get to here, this one goes all the way cross R, and so we increment L. And when we get to the upper left corner, we decrement L. ay oh, that rectangles over. ll double check this, but in order for this sweep to work so its like you first sort by x. You We are in some sense doing divide and conquer by x because we did this sort by x. But the merge short is on y. t makes more sense. Then for this particular ycoordinate, we could just basically start at the beginning of the array, start scanning, and just report all the points that are actually in our range. Then were going to have to do more and thats going to be Pi plus 1. o well do more and more Pis until we get to our actual query here. We sort of only care about the points to the left of the line. But its OK because the farther we go right, we have this sparsity condition that tells us basically the points were looking at are the number of points were looking at are getting smaller and smaller. f we look at a node, were going to store the same things we had before, which was this kind of query, and this kind of query. o in this subtree, we do this sorry, we do this range query because we want to go from x1 to infinity. And then symmetrically, we do R. ts the number of active rectangles with the right corners in R that span L. o thats this guy, R, guess, this guy is L. n general, there might be a lot of them, so you count them. When were sorting this kind of thing, we get log base /B, but when youre searching, the best you can do is log base B. We actually proved a lower bound about this in the first memory hierarchy lecture. At some point you decide theres a query here, namely this one, which has an empty answer, and yet there are points to the left of this xcoordinate. And so for example, when we see this point, and R is currently one, then we know that this point appeared in some rectangle that spanned L. o we increment this points counter. OK, but essentially, the i has to pick up the slack and we have to include these points in the next i. Whereas, before, we did not. o the first step of this algorithm is to first figure out how big those buffers have to be so that we dont have to allocate them too large. You did one search at the beginning, which costs you log base B of N, then you do this linear scan, and you get the right answer, magically. This is going to be nice because its like two scans, until one of these guys empties, and then you pause this merge, and then say OK, m going to fill this entire buffer, which will recursively do stuff until its completely full or run out of input elements, whichever comes first, and then resume this merge. should say to throw away duplicates, you have to throw away points that are not in the range lesser or equal to x, comma lesser or equal to y. till, we claim the number of scan points is proportional to the output size. At that point, were done, and then we can afford to look through all the points. We have whatever we want in the points and corners in this slab. And if recall, in 3D we ended up doing this thing in linear space with this fancy first you do a search on the left coordinate and then you just walk. OK, so this is the size of the answer for xi yi. The idea with distribution sweep is that not only can we use this cool funnel sort algorithm to sort, but we can think of it as doing a divide and conquer on the key value. This is where were going to put the results and this will have size K cubed. ts a very weird set up, but linear sized, and you just jump into the right point in the array, start reading, throw away the points that arent in your range because they just happen to be there. f theres this many points, well output them all in that much over B. This is what we call a regular range search, but m going to distinguish it and call it a four sided range search because a rectangle has four sides. OK, and the big question is how do you set the buffer size? Precondition, if youre going to do a fill, right now the buffer is empty, and then at the end of the fill youd like this to be completely full. Just output the points in your range and it gives you, magically, all the points in here by a linear scan. Now we have to take things that are below the line or to the right of the vertical line. But you could think of the other versions and we actually did this when we were doing the 3D problem. o xi is going to be so we had a yi Thats going to be the same as before. You do this search, you do this search, you find the LA in log base B of N to check that everything works, cache obviously. o pretty much immediately youve got to draw a line here and say OK, 0 is all points, 1 is these points, 2 is going to be these points. And the batch version is where youre given a whole bunch of rectangles, and a whole bunch of points up front, and you want to find all the points that live in all the rectangles. Because the whole point is were trying to merge by y. OK, we also have some rectangles, and their corners are what we have represented. Once we have Kfunnels, funnel sort is just going to be N to the 1/3 way merge sort with an N to the 1/3 funnel as the merger. t will tell you how many and this is actually necessary as a first step because one of the hard parts in solving these kinds of problems or reporting problems, is that the output could be big. Think of alpha as 100. o then the Pi minus 1 so this part down here thats in i, this is only 1/100 of the whole white rectangle. n this subtree, you already know that youre less than x2, and so you do the x1, y2 search in this node. We want that the sum of the sizes of the Pis is good. Over in this subtree, we want to do this range query because we want to go from negative infinity to x2. o thats the algorithm and as said, m not going to analyze it, but its the same kind of analysis. OK, this is why wanted this to be both x and y. But really, the divide and conquer is happening on x, but we are doing a merge sort on y. Finally clear. Were looking at some of Pi and then were going to the right from there. want to know what are all the points. And so we now know that were only going to be doing queries from here down. Number of memory transfers to merge them is K cubed over B times log base /B of K cubed over B. Theres a plus K term and when you plug this into an actual sorting algorithm, you need to think about that, but thats not a big deal. We have to store Pi minus 1. m going to charge it to these points. Every time you go to the Pis, youre starting over in x. Go back to minus infinity in x. o the idea is the picture will look something like this. The number of points in lesser or equal to x star is at most, alpha times the number of points in the answer. o were given, essentially we have whatever we want on the points and corners in here. f you want to do this merge, really what youd like to do is fill this output buffer. And so when you have a super linear buffer or a bunch of very large buffers that sum to linear size, you essentially need to carve that tree, which you do by recursive carving of the tree. ost of the points have to be up here in order to be sparse. o were going to have the inputs down at the bottom of this funnel. Fine, so replace or say augment the binary merge, which is, in the end, the only part of the algorithm other than the recursion. want to store for all the points that are down there. You take the union of those two results and that is this query. o this is the cool thing we know. ts going to be less than or equal to y. This thing is going to be i. Well see why, eventually, this works. All right, so we want to do m going to assume that they are this kind of quarter plain query. And the other thing is that the array has size order N. Thats the biggest surprise to me. And so we use that to determine which of the Pis we go to, based on where the yi becomes no longer dense. Heres the L points and the R points. ts hard to beat the sorting bound, and then once you pay the sorting bound, this is the optimal linear time to just write down the output. o if you take the next Pi, its not necessarily going to be the points less than or equal to xi ERK DEANE: ts true. imilarly, there are these rectangles that completely span L, and so therefore none of the corners are inside L. But we need to know that these points are in there. o the picture is going to be something like this. And so the total size of all the buffers is K squared, which is not too big. And this was the number of points in less than or equal to xi star. o largest y where we have some sparse query in i minus 1. want to look at what that xcoordinate is. We can only merge if we want the sorting bound N/B log base /B of N/B we can only afford K being up to n to the 1/3. ADENE: Because in the notes, it said x and then y. ERK DEANE: Yeah, know in the notes it says y. t used to say x. believe, were dividing and conquering on x, but were sorting by y, and thats the confusion. And now we want to merge these two things and merging here is all about counting how many rectangles contain each point. And particularly, this will tell you for each point, does it appear in any of the rectangles in the set? o those are the three things we need to prove about the algorithm, which will now tell you. And so the best you can hope to achieve is log base B of N for search in one dimension. really wanted to introduce this lecture because the next thing were going to cover, which is a way to do orthogonal 2D range search and cache obviously, is super cool. o were going to do the same thing, but not on a binary tree. o the actual batched orthogonal range searching is your given N points, and N rectangles, and you want to know which points are in which rectangles. You look at all the points that have xcoordinate less than or equal to x2. We can only up to n the 1/3 because of this cubic thing. f we have some i minus 1, were going to define a new thing, which is Pi minus 1, which is this. And in general, if y is near the top, like its at the top most point, or maybe the next of top most point, or maybe a little bit farther down, it depends on the point set, then a lot of queries are going to be dense. o you can count the number of outputs per merge and so then theres a natural strategy, which is you build a new funnel structure where these buffers have the right size. f these buffers have to get much bigger in order to store those answers, then life is kind of tough because then this data structure gets too big, and then my analysis goes out the window because things that used to fit in cache, no longer fit in cache. You find the LA and then in this subtree, you do a search. And where the analysis breaks, essentially, is if you have a giant buffer because one of the outputs potentially, the output size here is quadratic. And then in the right subtree, you do the x2, y2 search. What were missing are things like this rectangle, where none of the corners are inside R. o R knew nothing about this rectangle, and yet it has points that are contained in it. o if these are two rays and an edge, this you might call a three sided rectangle, and you can go all the way down to two sides. At this point, if you believe in funnel sort, you should believe that range counting is easy to do, and ve just hand waved the range reporting part. t gets confusing to think about intersection versus difference, but the point is if we look at the Pi minus ones that are in i, thats a small fraction. This is the i, the next s set. aybe ll write down the things we need to prove as well before we get there, so you can think about them as were writing down the algorithm.",0.1396479463537301
57,57,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. ERK DEANE: All right. Time for some more geometry, and, in particular, some more fractional cascading, which is a cool topic we saw last lecture. And then were going to do a different kind of data structure called kinetic data structures, where you have moving data. And that will actually be most of the lecture. But last time, we saw this nice general black box transformation, fractional cascading, and we didnt really see a lot of applications of it. We saw a simple example in orthogonal range searching. But want to show you today a much cooler one in the original fractional cascading papers. o remember, in general, what it lets you do is search for a common element x, find its predecessor and successor in k sorted lists, in order log n plus k time, instead of the obvious k times log n, where n is the length of each list. And the general version that we talked about is, if youre navigating a graph, and at each node of the graph, you have one of these lists, then you can instantly know where x fits in that list in constant time per thing, as long as you spend log n time to get started, provided your graph has bounded degree. That was necessary to do all the fractional cascading stuff of copying half into the next level up. o were going to use this result to get a lag factor improvement in our old friend orthogonal range searching. o lets do orthogonal range searching. o last time, we saw how to do two dimensional orthogonal range searching in log n per query. That was using fractional cascading, or, really, half of fractional cascading, which was just a crosslinking idea. This time were going to do 3D orthogonal range searching in log n time. Our space is going to go up by a couple log factors. But this is pretty cool. n general, for d dimensions, this gives us log to the d minus two, whereas last class, we saw how to do log the d minus 1. Now, this is static only. You could probably do polylog update. But lets not worry about updates. Were just trying to do static. o 3D orthogonal range search using fractional cascading this is kind of a tour de force. Theres a really cool result. ts in the ""Applications of Fractional ascading"" paper by hazelle and Guibas. And it proceeds in four easy steps. Each of the steps are easy, but its kind of amazing where we get to. Were going to start out with a two dimensional restricted orthogonal range query. Well see why at the very end. But this is actually from another paper of hazelle in the same year. OK, in general, remember, in 3D, were trying to do querying with a rectangle a1, b1, a2, b2, a3, b3. We want to know all the points in that rectangle. o m going to start out with a very restricted form, which is only two dimensional, for whatever reason. And well see why later. ycoordinate and zcoordinate, skipping x and the left end point doesnt exist. o you go all the way up to b2, and you go all the way up to b3. This is a quarter plane in two dimensions. want to know all the points in there. ts the same as saying all the points that are dominated by this point. Both y and zcoordinates are dominated by that yz coordinate. This is b2, b3. o we can solve this in log n time plus k. But want to be a little bit more precise about what that log in is, and say that this costs whatever it costs to search for b3, the zcoordinate, in a z list in a list of zcoordinates of points plus order k. write it this way because if we have many searches among lists, then we can speed things up. o dont want to just think of it as log n. want to think of it as one of these fractional cascading operations. o this is the time bound want to get for finding the k points in this search range. Now, heres the fun part. Were going to transform this into a kind of stabbing ray problem, like we saw last class, and in the retroactive stuff, and so on. o lets suppose have some points. m just going to draw an arbitrary arrangement. Hopefully, its reasonably interesting. Those are the points want to be able to query. And if m given, say, a query like this one, want to know all the points in here. o what m going to do is draw a leftward horizontal ray from the query point. And m going to draw maybe use a color upward vertical rays from each of the points. OK. And where theres crossings, those correspond to points that are in the query quarter plane. OK, so same problem but now, thinking about rays. o heres a cool thing you can do with this approach. o we want to preprocess these vertical rays so that, then, we can stab with a horizontal ray. And its actually easier to think of it as coming from the left, because thats kind of a consistent xcoordinate, and walking to the right. d like to find this intersection, then find this one, then find this one. Eventually, get to the desired ycoordinate. This is the y direction, and this is z direction. Then stop. o if could get started over here in log n time, and then do a walk in constant time per intersection, d be golden. That is possible. And the way that hazelle did this m going to erase the query ray is to decompose the plane in a pretty simple way. Were going to draw a horizontal segment from each point. Were going to extend it to the right until it hits something, extend it to the left. n this case, it goes off to infinity. Here, extend this guy. extend this guy. extend this guy. extend this guy. And extend this one out here. m going to add one more point over here a little more exciting. o this stops there. This goes to there. ADENE: One more to the right. ERK DEANE: One more up here thanks. OK. o this is kind of decomposition into slabs or bricks or something. t looks good. And so the idea is, over here, theres, at most, n different rays that make it all the way to the left. Do a search. Thats your z search. o thats this search for b3 in the z list. o remember, our goal is to get to here. o we search for that zcoordinate over here. We say, OK, it falls here. Then enter this face. d like to then navigate to this face, say, OK, thats where am now. By crossing this edge, know that this point is actually in my answer. And then cross this edge, so know that this point is in my answer. Then cross this ray, so know that this point is in my answer. Then say, OK, reached my desired ycoordinate. top. o if can do each of these traversals in constant time, d be all set. o do one search at the beginning, then constant time per query. We know how to solve this problem. We can do it with range trees and log n time preparation. But this is a particular way to solve it that will work with fractional cascading when we do many of them. OK. ool. an do this in constant time? aybe not, because its this sort of scenario. f draw a whole bunch of points like this, theyll each have a segment. And in general, this face will have large degree. And so need to sort of find my zcoordinate again, somewhere in here. t turns out, with, essentially, fractional cascading again, you can avoid that. f you have many segments here, just extend half of them. o maybe ll extend this one over and this one over. t looks like fractional cascading. m taking half of the elements here, inserting them into the previous list, which is the left side of the face. f this now has too many, well, half of them get promoted. But it decreases exponentially. And so the total amount of extra edges m adding here is only linear. o linear space m not going to prove this formally here, but its the same idea as fractional cascading. We just need it as a tool to get to 3D. You can extend these things, and then every face will have bounded degree. And so you can just look at every single rightward edge in constant time, and figure out which one has your zcoordinate. Follow that edge. And so every time youre crossing a ray and getting an output, you can pay only constant time to get it. Pretty cool. Thats the first step. Question. ADENE: o when you say that a particular face has too many crossings, what do you define too many? ERK DEANE: ore than a constant. n general, you just look at the right side of a face, and just take half of those things and propagate it to the left. Just do that right to left in one pass. think you might also need to do it left to right, if you want to have bounded leftward degree. But m not sure that really matters. think you just do one pass right to left, and half the guys keep getting promoted. And its the same thing, where youre promoting from the L primes, not from the L. o its everybody who came from the right plus whatever you originally had. Half of them get promoted to the left, but because its geometrically decreasing, all is OK. This is earlier than fractional cascading, but would guess its what motivated them to then do general fractional cascading. Other questions? know this is a little bit vague. But the more exciting stuff, to me, is the next three steps. o lets move on to those. . o this is a tool for doing two dimensional quarter plane searching. The next thing were going to do is make it three dimensional. This is actually something we already know how to do. And were going to do it in exactly the same way we knew how to before. uppose you have a three dimensional query, and two of the intervals start at minus infinity, but the new xcoordinate is a regular interval. You can specify both endpoints. want to do this in log n searches plus k. k is the size of the output. How do do this using one? Two words range tree. Yep. Easy. Just do 1D range tree on x. And then each node stores that data structure one on points in the subtree. And so just like before, you get log n subtrees that represent your x interval. You look at the root of each one, and it stores a one data structure. You query each of them for b2 and b3 for that interval among the y and zcoordinates. And each of them costs a search plus order k. The ks sum up to order k. o we end up doing log n searches of that type plus k time. And we can now solve this kind of 3D query. This is really easy. This is what we did last class. The cool thing, of course, is that, by doing log n searches, its always searching for the same thing b3 in various c lists. We know by fractional cascading this is actually log n time. Were doing log n searches in k lists and slightly different k here, sorry. ts actually log n lists. But we know from this bound, were going to get order log n plus k. But dont want to do fractional cascading yet, because were not done. This is a sort of three dimensional orthogonal range query. But want to put a2 here and a3 here. Were going to do that step by step. First step is a2. Were going to do it in exactly the same way, twice. Again, want to do it in log n searches plus k. ts the same time bound want to put into a2. The cost will be a log n factor in space. And its a cool transformation. ts a general transformation. Whenever you have a data structure has a minus infinity, you can turn it into a lower bound, magically almost the same way as we did here, except were not going to lose a log factor in time only in space. o m going to say its kind of like a range tree on the ycoordinate. ycoordinate is the one that we want to extend. And you may remember there was this brief question last time. n a 1D range tree, what key does a node store? t just has to store something thats in between whats in the left subtree and whats in the right subtree. o proposed you could store max of left subtree. And thats enough to do a search. Then you know whether you should go in the left subtree or the right subtree. Just compare with that key. o same as before were going to store that key. Except now, m making it explicit, because we really need to know what that key is. A node v will also store two of these. o its going to store the two data structure on the points in the right subtree not the entire subtree, just the right subtree. And its going to store a yinverted two structure on points in the left subtree. o this is the new part. And what does yinverted mean? t means that you can search for boxes like this. o regularly, a two structure is sort of leftcentered, and the left end point is undefined, and it goes up to b3. Now, want something thats rightcentered. t goes up to infinity on the right side. But you can start at an arbitrary a2. How do make such a data structure do exactly the same thing, but with this inverted, which means, do exactly this, but with this inverted? Easy to do just twice as many data structures. o you can think of that as one prime and two prime. ll call this two prime, explicitly. OK, now, the big question is, why is this enough? o lets do that. How do do a query in data structure three? The basic idea is simple. Were going to walk down the tree. Now, before we walk down the tree, and the interval is represented by log n different subtrees we cant afford that anymore. We can really only afford a constant number of calls to this data structure, if were not going to get an extra log blowup. o dont want to walk and then fork and then visit all these subtrees. could do that. But it turns out, with this structure, can be a little bit more efficient. Because, essentially, want to do an interval like this. The queries m allowed is, can do a query thats infinite to the left, and can do a query thats infinite to the right. o the intersection of those would be what want. cant really compute intersection. o it take too much time to list those. But if, somehow, could get this left endpoint in another way, using the tree, then can do a leftward infinity query. o thats what were going to do. And once say walk down the tree, its pretty clear what you have to do. When you visit a node, if the key of the node is less than what are you searching for a2, b2 so a2 is less than b2. And if the key of the node is to the left, that means that the stuff were interested to is to the right. o walk right. f the key of the node is greater than the interval, then walk left. Thats sort of the easy case. And then the interesting case is this fork. Before, we had to do a lot of work at the fork. Now, claim we only need to do constant work at the fork. o if the key falls between a2 and b2, so our interval is stabbed by the key of the node, then want to query the two data structure two is the right number yeah and want to query the two prime data structure. o m going to do two calls to data structure two. And so only get a constant factor blowup. And what could possibly search for? Well, two m able to do what is it a1, b1, minus infinity b2, and minus infinity b3. Were not fixing the zcoordinate yet. And with two prime, can do the left endpoint bounded. Once ve set things up, this is, like, the only thing you could possibly do. But claim its actually the right answer. OK, what does this mean? Two prime is were doing rightward infinity searches in the left subtree. o heres v. Here is the left subtree of v, and the right subtree of v. o this is a bunch of points down here, a bunch of points down here. We know that the interval looks something like this. t straddles this node. Thats what this means. And so what wed really like is this stuff plus this stuff. That looks good. Because this goes to infinity on the right. Thats the two prime search. And this goes to infinity on the left. Thats the two search. As long as you restrict to this subtree, which is what weve always been doing, and, in particular, here, too, its only on the points that are in the right subtree. Two prime is only on the points in the left subtree. And so thats it. Were golden. Actually, we only need to do a rightward infinity search, and a leftward infinity search. o that is data structure three. And its really easy. Once you have the ability to do a 3D search even if this one was minus infinity b1 if we just could do would you call that octants octant search in 3D, then we could just sit there and apply this transformation, and turn each one of them into doublesided, with a log n penalty each time in space, but no extra query time. We do a single log n traversal of this tree. o this part costs log n. And then we reduce to the previous problem its constant number of calls. o could have written minus infinity here. But pay an extra log factor in space. o this one we get for free, because were using range trees. We do one transformation. We get this one. And now were basically done, because we just do this again on the zcoordinate, and we get a1, b1, a2, b2, a3, b3, as desired. For our queries, just ditto on z, and using three in place of two. o we want to add in this a3. We build a three data structure. We build three prime data structures in exactly the same way. Three primes are on the left. Threes are on the right. Every node in this range tree on z. And we do a log n search at the beginning. Then we do a constant of calls to the data structure three. Data structure three does a log n search at the beginning constant number of calls to data structure two. Data structure two does our usual range tree thing identifies log n different calls to data structure one. Data structure one is a search and a list, plus every time walk to the right and spend constant time, that is an element of my output. How much time has this taken total? Normally, theres log n of these, plus order of k. Normally, that would be log squared n plus k. But with fractional cascading we have to check this is valid within fractional cascading. Weve got a graph of data structures here, but each node has only constant degree. You can come from your parent. You can go to your left child. You can go to your right child. And you can go to previous dimension data structure, or the inverted version of it. o the degree five if you count in and out degree. And so fractional cascading applies. Were always searching for the same thing not quite, actually a little bit of a cheat. Because of this inversion thing, well sometimes be searching for b3 in the z list, but well also sometimes be searching for a3 in the z list. But hey just a factor of two. o we have two fractional cascading data structures one for searching for b3, one for searching for a3. ts the inverted versions of one, and the uninverted versions of one zinverted. But thats fine. And in the end, we get log n to do the first search, and then plus the number of searches. Number of searches is log n plus k, where k is the size of the output. o total time is log n plus k. This is pretty amazing. We can do 3D orthogonal range queries, still in log n. And if you go to higher dimensions, the best we know, basically, is to use range trees. And so you get, in general, log to the d minus two n plus k. o last class, we could do one. Now, we improved it by one more. Questions? ADENE: Yeah, have a quick question. question. o we do three, and then we do four. And its not hard to imagine that same , when you might try to do the same argument for step three, step four, step five. ERK DEANE: Why cant we keep doing this for all dimensions? o what three is, and also four, is using the same transformation, which is, if have a minus infinity something interval, can transform it into an a2, b2 interval. o can make a onesided interval into twosided. The trouble is actually getting the onesided interval. o we started, in one and two, just getting up to three dimensions. And thats where things are hard. o fine, in two dimensions, we can do all sorts of fancy tricks. We saw one way to do it last time. This is a particularly cute way to do it that lets you fractionally cascade. We could add a dimension. To add a dimension, we just use range trees. This is kind of pathetic. Every time we do this, were going to pay a log n factor. o we could afford to do it once and get to 3D. We paid a log n factor here, but we were lucky fractional cascading will remove one log factor, but only one. f we had to go to four dimensions, wed have to use another level of range trees. And then wed get a log squared searches. And then we have to pay unit cost for every search. o well get log squared for four dimensions. o its just getting up to the right number of dimensions thats hard. What youre seeing here is that onesided intervals are just the same as twosided intervals, if you dont mind extra space. The space here is, think, log cubed n log cubed n. Data structure one is linear space, but every other level, we lost a log factor. o one of those was to get up a dimension by range trees. The other two were to convert the onesided intervals into twosided intervals. And that generalizes. You could do that as many times as you want. The hard part is just getting the right number of intervals in the first place. And thats where we pay log per dimension. o kind of annoying you cant do log n for any dimension, but pretty sure thats impossible. There are models under which its impossible, but were not going to get into that. Yeah? ADENE: o when our query is log n plus k, k is actually the number of points that are coming back, because ERK DEANE: Yeah. Good question. Here, k has to be this is for what we call range reporting, where you really want to list everybody in there. And if we wanted to do range counting queries, which just give me the number of elements that match, dont think this will work. n particular, our seed data structure up here had to pay for everything. t doesnt know how many times its going to have to walk to the right. ts got to actually do it. o range counting not so much. Our previous data structures could do range counting, without the plus k, just paying log to the d minus one. But this is just for range reporting. Good question. dont think anyone knows how to do range counting faster. ADENE: And the reason were only hitting points we know about is because the one data structure is on the bottom, so we never actually ERK DEANE: Right. o you need to check, why is it only order k, where k is the actual output size? Because by the time we get down to the one data structural level, were guaranteed that x already matches. ts already in our interval a1, b1 by the range tree. And were guaranteed that these two open intervals the minus infinity actually is a2 here, and it actually is a3 here, or, actually, something bigger than it. o were guaranteed whatever this thing outputs is a result. And were never doing overlapping intervals. o we never double charge. Yeah. You do need to check that. Theres a lot of pointers to follow here, but it works. All right. You look convinced. Lets move on to kinetic data structures. The idea with kinetic data structures is, you have moving data. Deal with it. o normally, were thinking of data that at best, its dynamic data, meaning we can delete something and then reinsert it. But what if everything is constantly changing? o normally, OK, ve got some points in my data structure. But now, what if they also have velocities? And maybe some guys just sitting there stationary, but some of them are moving relative to it. And my operations this is kind of like time travel, but now were going to time travel into the future, which we do all the time. But wed like to do it really quickly, and say, OK, advanced time by five units. And now, in that frame, do an orthogonal range query or something. Do some kind of query. Were always going to be doing queries in the present. o this is not fancy time travel. This is regular, forward time travel. We just want to quickly say, jump forward 10 time units, do some queries, jump forward some other time units. Theres actually another operation, which is, ah, this point is no longer moving in that direction. Now, its moving in this direction. o the operations are, advance to time t so this is like setting now equal to t and change a point x to have some new f of t trajectory. ADENE: thought it was supposed to be arbitrary. ERK DEANE: Well, arbitrary trajectory is not not quite. m going to restrict what those trajectories are. But that is the remaining question. What kind of f of ts do we allow? m drawing the picture in d dimensions, lets say. And most of the work in kinetic structures in 2D a little bit in 3D but m going to focus today mostly on 1D. Because its easy to analyze, clean. Theres a lot of open questions in 2D. o we can also think of these theres points on a line. They have velocities, accelerations, who knows what. That would be OK, lets say models of trajectories. The simplest one that would be affine f of t equals a plus bt. a and b here would be points. a and d dimensions are just values in one dimension. o the motivation is, maybe, you have cell phones or cars or something. You have some current estimates on which way theyre going and at what speed. Then this would be the simple model. And if either those two things change, you have to do a change operation. o you pay for that every time they change their trajectory. But otherwise, its going to be super efficient, because advance is going to be super fast. Thats the plan. But maybe its not just position and speed. aybe also have acceleration and stuff. And then the extension of that would be bounded degree algebraic, which is, you have some polynomial of bounded degree sorry, that should be c but bounded. And the reason we care about bounded is really the following. Theres an even more general model which we call pseudo algebraic. o we would like to bound the cost of this advance operation. What wed like is that when we advance time a large amount, stuff doesnt change crazy number of times. And pseudo algebraic says that if you look at anything you care about we call this a certificate of interest. Well be talking a lot about certificates today. ertificate is something like, is this point left of that point. ts a Boolean question about the moving points. ts either true or false. And what d like is that have some point. t has some crazy trajectory. have another point. t has some crazy trajectory. dont want, is this point left of this point, to change an unbounded number of times. d like it to be constant, as long as no change operations are called. o for a single trajectory, d like these to flip a constant number of times. Now, if youre algebraic with bounded degree, then that will be the case. But more generally, as long as you sort of switch between left and right bounded number of times, then that particular certificate will only change unbounded number of times. But in general, anything where all the certificates care about change a constant number of times is just as good as algebraic. This is really why we like this. think ve talked enough about trajectory models. o its not totally generic. But it covers a lot of things you might care about. ADENE: certificate of interest? ERK DEANE: ertificate of interest is just a Boolean function on a constant number of data points over time a constant number of trajectories, should say. ll probably actually define certificate right now. ADENE: Good. ERK DEANE: ertificates m going to define them more as a tool that we use to build data structures, but theyre really the same thing that mean here. o pretty much all kinetic data structures follow this unified approach. Kinetic data structures are actually a pretty new thing, introduced in 1999. o as data structures go, thats new. And over the last however many years that is 13 theres been a bunch of kinetic data structures. o what were going to do is store the data structure that is accurate now. o this will make queries about the present really easy. t just is . You look at the data structure, you do a query. And so the hard part becomes, how do do advance? How do advance time and make the new data structure, which is correct, about the new now? The way we do that is with certificates. o what m going to do is additionally store, basically, a proof that this data structure is valid. We can say conditions which are currently true and as long as they remain true, the data structure remains valid. o these conditions are true now, and sorry under which data structure is accurate or correct. And those conditions are true now. OK, so for example well, well get to examples in a moment. Well just keep abstract for a little bit longer. Each of these certificates, you can figure out a failure time. o you have some certificate, like, this point is to the left of that point. And you just look ahead in time. f you have some constant defined trajectories, you just see, when is the time when they will be aligned vertically? After that time, theyre going to switch who is left of whom. o just compute that failure time. m going to assume thats another assumption about the trajectory model that this takes constant time for trajectory. do that for every certificate, and then put those failure times into a priority queue. Do you want to ask your question? ADENE: o is it required that a certificate needs to be like, that weve checked that a certificate is true. t just needs to be bounded by order one, or ERK DEANE: Yeah. o m assuming checking a certificate takes constant time, whether it currently holds, and computing the failure time takes constant time. Yeah. mean, you could assume it, or maybe you pay a little more. mean, this is how were going to build the data structure. o whatever it costs, it will cost. But think everything well talk about and pretty much every certificate out there is sort of a constant size thing. ome of them are bigger. But well get to the costs involved. aybe we wont worry about time too much. o priority queue is going to take log n time, where n is the number of certificates in the data structure, to find, when is the next failure? o if want to do an advance, basically, m going to do discrete event simulation. find the next event when something changes, i.e. a certificate fails. m going to fix that event fix that certificate make the data structure correct again. Then advance to the next failure. Repeat. As long as there arent too many certificates to mess me up, this advance will be fast. o yeah. o heres how were going to implement advance. Basically in all kinetic data structures, we just say, while t is greater than or equal to the next failure of the priority queue, we advance to that moment in time, when something interesting happens. We do an event. ll write it this way. And then we set now to t. And this event thing has to somehow fix the data structure and fix the certificates. o that is the challenge, is, how do you deal with an event when one of your certificates breaks? o if you have a data structure, and you want to make it, kinetic you just first write down some certificates under which its guaranteed to be valid, and then see how to fix them as things happen. Then theres the analysis issue, which we will get to. Lets start with an example before we get to analyzing, so this becomes a little more concrete. ADENE: have another question. ERK DEANE: Yeah. ADENE: o are the change operations sort of online, or do you have the sequence of changes that youre going to ERK DEANE: All of these operations are online. o at any moment, someone says advance, someone says change, or someone says query. And query is respect to now. o we have no idea which of these are coming, in what order, whatever. dont think anyones studied the case where you know up front all the things that are going to happen. Though there are, presumably, applications for that. mean, time can just be a euphemism for a dimension, or whatever. But a lot of the kinetic people really want to be tracking points and maintaining whats happening. o lets do a 1D problem a simple one sort of the most basic problem, which is a predecessor problem. nsert, delete. We wont worry too much about insert and delete here. ts hard enough, because the points are moving around and predecessor and successor. o want to know, on the line, have some points. Theyre moving. Now, query is, at the current time, whos to the left and whos to the right of this query? OK. How do we maintain this? This is a problem well be studying a lot in this class. But the basic structure for solving predecessor, insert, delete, predecessor, is ADENE: Binary search tree. ERK DEANE: Binary search tree balanced binary search tree. OK, so lets use a log n high AVL trees whatever. o what do we need for certificates? was going to use this as an example, but its actually a little tricky to think about what the certificates are. Because the binary search tree property is x plus or equal to x greater or equal to x. Thats kind of a lot of certificates. f want to compare x to every single guy in here and compare x to every single guy in here, that would be a, think, quadratic number of certificates, in general. Almost everyone has a relation. d prefer to get away with fewer certificates. Because then, less certificates will fail. o cute idea really only need to compare x with this one and this one. the max in the subtree and the min in the subtree. n general, if look at the data in sorted order, it has to stay sorted order, where its not going to be sorted. But this is an inorder traversal. norder traversal is something we can understand without knowing what the data is. Because remember, data is constantly changing. We cant really use the keys here. But we can use the abstract shape of the tree and do an inorder traversal, and say, look, as long as x i is less than or equal to x i plus one in the inorder traversal for all i, then this is a valid binary search tree. f an inorder traversal stays sorted, were golden. o those are my certificates. Theres only n of them. o thats nice. And we need to check that we can compute a failure time. This is usually really easy. But well go through the exercise of writing it down. o want to know, among all times greater than or equal to now, when will x i am doing strict here? This should probably be greater than. Yeah, so thats why have an infimum. OK, take the earliest moment when x i of t is greater than x i of plus one, which is the opposite of what want, and take the infimum of those times. And so that will be the moment of transition when theyre equal. And then boom its going to jump over. m assuming these things are continuous. ADENE: Thats why you take the infimum? ERK DEANE: Yeah. o it would be an infimum, because these guys are going to cross. mean, dont care about this kind of happening. But if its going to actually go across, then therell be the moment of transition where theyre equal. And thats going to be this infimum. OK. How do you compute that? Well, it depends what these trajectory functions are like. f its algebraic, then this is just a polynomial thing. You can do it in bounded degree. You can do it in constant time. Thats our model. OK, so you put them into a priority queue. Do this advance. And now, the question is, how do you process an event? When one of these things happens, youre about to transition to x i being bigger than x i plus one. What do you do? o thats the real heart of the data structure. Although, really, the heart of a kinetic data structure is the choice of certificates. f you choose certificates well, then youre going to be efficient. We havent defined efficient yet. We will. Otherwise, youre not going to be so fast. o its all about using certificates right. The rest is kind of straightforward. o lets suppose that this certificate is about to fail. And were guaranteed by this algorithm that it fails now. We have advanced to the time when it is about to fail. We process that event. o now is the time when these two things are equal. Right after now, we will get to greater than. o heres what do. wap them in the binary search tree. o right now, maybe just in general, its going to look something like this. We have x i, x i plus one. Or it could be the reverse scenario, where x i plus one is a leaf, and x i is the predecessor. Right now, theyre equal in value. o m just going to interchange them, move x i up here. o its a little confusing. But this is x i plus one, and this is x i. Replace those. These are really pointers to the trajectories, however theyre described. nterchange them. ts still valid as a binary search tree. Actually, this binary search tree never becomes invalid, because at this moment, theyre equal. And after now, things will continue to be OK, because this guy will be bigger than this one. Thats the assumption. We just need to fix the certificates. Fixing the data structure was pretty trivial constant time. o what certificates do we need to do? We need to add this new certificate. m going to call it x i prime is less than or equal to x i plus one prime. o this is actually x i plus one, formerly known as x i plus one, formerly known as x i. But itd be really confusing. o m going to use the primes to be the new data structure. Because this is always the kind of certificate we want. We also need to update certificates. o there used to be an x i minus one less than or equal to x i. We want to turn that into x i minus one less than or equal to x i prime. And we used to have an x i less than or equal to x i plus one. We want to turn that into x i prime less than or equal oh, that one, we already did. orry. o want x i plus one to x i plus two. And want to do, now, x i plus one prime to x i plus two. OK, basically, wherever the primes happen, which is x i and x i plus one whatever certificates theyre involved in, you have to update them meaning, rip out the old one, put in the new one. And the main issue here is that you have to maintain the priority queue. o youve got to take them out of the priority queue, recompute their failure times, put them back in the priority queue with that failure time. But the point is, this is a constant number of updates. n general, pretty much, as long as the things youre messing with the items youre changing are involved in a small number of certificates each, then this will be cheap. m writing this explicitly. But in general, its just, update whatever certificates those points were involved in. The issue is how many things you change, and how many certificates each of those things is in. n this case, changing a constant number of things each of them is in a constant number of certificates. o its not constant time, but a constant number of calls to priority queue updates so log n time. ADENE: Question. ERK DEANE: Yeah, question. ADENE: o but how do you find the things that you just changed in the priority queue of the failure times. ERK DEANE: How do you find them in the priority queue? How do you find these x is and x i plus ones? Every point knows what certificates its in, meaning it has a point or two a list of certificates its in. Theres only two of them. And each certificate has a pointer into its existence in the priority queue. o then you know where it is in the priority queue and you can rip it out. Yeah. Lots of pointers and crosslinking. Yeah? ADENE: o in something thats pseudo algebraic, any certificate of interest changes o of one times. But here, theres o of n squared even though were only taking n certificates at a time, theres o of n squared possible certificates in each of them. ERK DEANE: Youre asking about efficiency, which well go to now. This looks disconcerting, because even though pseudo algebraic gives us that each certificate wont change too many times, there are quadratic number of potential certificates. They could all become relevant. And indeed, that is the case, if you have these points, spaced out a lot, and then you have these points, spaced out a little four and four. And these guys are all moving constant velocity this way. And the white guys are stationary. Then what happens? Well, theres event, event, event, event, event, event, event, event, event, event, event, event, event, event, event, event, event, event, event, event, event. Youre going to get n squared events. You get, like, n over two each time you cross a white point. call this OK. Why? Because of efficiency. The claim is, this is sort of the best you could hope to do. o in that sense, its as good as you can hope to do. f you want to maintain predecessors lets put it this way. f we need to know ""know"" is a sort of vague thing the sorted order of the points, then you need an event if were going to keep a data structure that is always accurate now so this is sort of an assumption then you need an event every time you have an order change. Thats sort of a tautology. But its a perspective. And whats happening here is that we have an event every time theres an order change. o sometimes, yeah, its going to be bad. Worst case here is quadratic. You can actually prove for pseudo algebraic, it is order n squared events. o this was really a lower bound. But it is also order n squared all the time, because, if you look at any pair of guys, theyre only going to change a constant number of times whos above whom, for pseudo algebraic, if there are no change events. o for efficiency, because its really hard to analyze if points are changing their trajectories all the time, assume theres no changes. We basically just advance to infinity. How much could that possibly cost? Thats how kinetic people like to analyze things. And for this problem, for maintaining sorted order at all times, the worst case answer is theta n squared. This data structure achieves theta n squared, so we consider it worst case optimal, in this weird sense. OK, this is if you really want to maintain the sorted order. Now, we didnt say we wanted to maintain the sorted order. We said we want to maintain a predecessor data structure. But it feels kind of like those are the same thing, maybe. dont know if theres a formal sense in which this is the case. And efficiency, in general, is the vaguest part of kinetic. And for each problem, you have to think hard to understand, what does efficient mean for this problem? But maybe dont even worry about efficiency what does a lower bound mean but bottom line is worst case, n squared events, if there are no change operations. o you can think of the running time, if you want, of jumping to infinity as order n squared in the worst case. Thats how we analyze these things. There are other things wed like to analyze. Efficiency is one of them. We have three others, which sort of hinted at. Theres responsiveness, which is time spent to do an event. o when a certificate is invalidated, how much time does it take to do that? Theres locality, which is closely related. This is the number of oh, see number of certificates per data object. said over here, this is good, because each item x i is only involved in a constant number of certificates. o we say the locality is constant constant number of certificates per object. sually, locality implies responsive. As long as you can update the data structure, you can also update the certificates in whatever you pay a log factor, because youre updating a priority queue. But you can update all those events, or redo all the certificates in an event, provided you are local. o locally usually implies responsive, more or less. And then whats the other one compact. This is about space. And we just like the number of certificates to be small. o in this case, number of certificates is linear. Thats that up here. o we consider the structure to be optimally compact. You need at least n. o in our case, were getting order n here, order one here, order log n here. And efficiency guess youd call constant, kind of. Efficiency is the ratio of how many events you do divided by how many events you need to do. And here, it is optimal, in a certain vague sense. orry, thats a little unsatisfying. But its unsatisfying. Thats the literature for that problem. Were going to do another problem, which is more satisfying, would say. And it kind of shows you why kinetic problems are pretty interesting. Data structures are often pretty darn simple, and the fun part is the analysis. Or, where it can get intricate is the analysis. o lets solve kinetic heap. Our goal is to be able to do find min. And yeah, theres also insertions and deletions. But m not going to think about them. Because theyre not really that different. Theyre kind of like a change. Lets not worry about it. The goal is to maintain the minimum. Now, if you look at this point set and their velocities, how many times does the minimum change? Once. nitially, the min is this. Eventually, the min will be this one change to the minimum. o this data structure is a bad way to maintain the minimum. Yes, it does maintain the minimum. But its going to spend a quadratic number of events in this situation, where the min only changes once. o can we get a ideally, you have one certificate, which is like, this point is the minimum. But thats not such a good certificate, because then, every point is involved in it. And how do you know when its going to be violated? guess you could compute it in, like, linear time or something. ts not a good plan. We need to break down the certificates that only involve a constant number of things and maintain the minimum. And somehow, want to get below quadratic number of events, in the worst case. And you can. And you do it with a heap. Remember heaps? tore min heap. o min heap has this property. You have x, y, and z. Then x is less than or equal to y, and x is less than or equal to z. This is nice, because local property. These are my certificates. For every node x, have to have this property. o have order n certificates. Thats good. had order n certificates before. But claim these, magically, are easier to maintain than those, even though they look almost the same kind of crazy. dont know if really need to write it, but how do we do an event? f x and y are about to invert in order so currently, x lets me go to y. n a moment, x will be bigger than y. swap x and y, update all the certificates constant for each. Theres two items m moving. Each one is involved in a constant of certificates three now, instead of two but constant number of certificate changes and stuff ADENE: Question. ERK DEANE: log n time. update the priority queue. Question? ADENE: How is it three? ERK DEANE: Why is it three? x is involved in a certificate with its two children, and with its parent. ts parent has a relation, as well. Yeah. o you have to be careful in counting locality. How many certificates in each object in? OK so were responsive log n time, local, constant, certificates per object, compact, linear number certificates all these are the same. Big issue is about efficiency. How many events is it, in the worst case? How many events do need? OK, here, for kinetic heap, where want to maintain the minimum at all times, theres actually a very clear lower bound. The number of events or lets say, the number of changes to the min think this is what some people call external events. Like, you cant control when the min changes. The user controls that by how they set up the trajectories. hanges to the min is at least n in the worst case. ts pretty easy to set this up. You have a point, another point moving at constant velocity to the left, it will overtake. You have another point farther away, with a bigger velocity, to the left. t will overtake after this one overtakes and, you know, something like this. Youve got to make sure they dont all cross at the same moment. But this one will go first, and then this guy will cross over. And eventually o its easy to set up. The min can change a linear number of times. claim in this data structure, well, of course, the min will change, at most, a linear number of times. Well, its not clear its at most. But thats true. What we care about though, is, were storing way more certificates than the min. We have a linear number of certificates. claim the total number of events in this data structure, if there are no change operations, if we just advance to infinity number of events is order n log n. And so we call the efficiency log n, because youre log in factor away from this lower bound. This is the interesting part. And this requires a proof. o why is the number of events order n log n? Were going to prove this by amortization, with a somewhat tedious potential function. But its sort of the obvious one. At a time t, wanted to find my potential at time t to be, how many events will happen in the future? This is the thing want to bound. o want to prove this is order n log n at time zero. But lets think about how it changes over time. o m going to rewrite this as follows. m going to um over all items x. And were thinking about the potential at time t. o look x is in some node. And which node its in changes over time. But at time t, its in some node. look at all the descendants of that node. look at those items. And see which of those items will overtake x in a future time. Those are the ones that care about. When you overtake an ancestor, thats when an event happens. claim these two things are equal. probably dont even need to prove that. o this is in parentheses. Dont worry about this being equal. Think of this as the potential function. Now, want to look at an event and see how this changes. need a little bit more notation. m going to call this thing phi of t comma x. o phi of t is the sum over all x of this thing, which is phi of t comma x number of descendants of x at this time that will, in the future, overtake x, meaning their key will get larger than x. OK. can also expand phi of t, x in a simple way. ts kind of trivial. But if look at each child of x theres two of them call that y and then measure how many descendants of y at time t will overtake x? o its almost the same words, but changed one of the xs to a y but only one of them. orry ""future"" write greater than ""t."" This is a different quantity slightly different which m going to call phi of t comma x comma y. just need to be able to talk about this. o this is mostly to introduce notation. OK, and descendants of y this is including y itself. o theres y and all of its children descendants whatever which are those that will overtake x. f add that up for both children, that is the total number of descendants of x that will overtake x. o this equality is trivial. ainly, wanted to introduce that. OK. o what now? Lets look at an event, and see what changes. o try a little bigger x, y, z. Lets say we need to change it to y, x, z. Because y is about to overtake x. How does the potential change? OK, well, potential is the sum of all these phi t, xs. o which of these phi t, xs change? claim that phi of m going to call it tplus, which is the moment right after now infintesimally larger claim it does not change for most vertices. t only changes for x and y. f x and y are the guys that are switching order, this will not change for any others. Why is that? Because were looking at number of descendants to some vertex that will overtake that vertex. o do the descendants change? No. Descendants of x and y change, but no one else, their descendants change. This is just a swap of two adjacent elements. Will overtaking change? No. mean, overtaking doesnt change. f youre going to be overtaken by x before, you still be overtaken by x. And its still a descendant unless it was y. y is the only one for which it changed. OK, so we only have to think about x and y and how their potentials are changing. OK. o lets look at x. x is pretty simple. x went down. o we care about now the descendants of x. Those are what used to be the descendants of y. o what are the descendants of y that will overtake x? That is phi of t, x, y. Thats the definition here. The descendants of y that will overtake x we had that written down. o that is its new potential, except as a minus 1, because y used to be a descendant that would overtake x. But it just overtook x not going to happen again. Not gonna happen again. m assuming here affine motion, suppose. t could really happen a constant number of times. But lets keep it simple. OK. That was easy. Next ones a little harder. The other one is y. What is the new potential of y? y has new descendants now. t used to just have this many descendants. t still has those descendants. t now has x, which doesnt matter, if we assume it will never be overtaken again. And now, it has this whole subtree of z. o we have whatever it used to have, which is phi of t, y. Thats all its old descendants. And now we have this new thing phi of t, y, z. f you plug that into here, that is the number of descendants of z that will overtake y. Thats the new thing that we add on. o this is an increase. This was a decrease and this is an increase. We hope that theyre about the same. n fact, they will be basically equal. o first claim is this is, at most, this other thing phi of t, y phi of t, x, z. o m just replacing this y with an x. Why is this true? Because this is overtaking y. f youre going to overtake y at this time, you would overtake x at this time. Because x and y are equal right now. m pretty sure this is actually equal. dont know why wrote my notes ""less than or equal to."" Less than or equal to is all need. But think, at this moment in time, when theyre actually equal to each other, it doesnt matter whether you have x and y here. You will overtake one if and only if you overtake the other at this moment. ADENE: ERK DEANE: No, see. Right. Actually, this is about future time. n future time, we know that y is always going to be less than x, because its moving up. n future time, if you overtake y, youll certainly overtake x, because no, the other way around. f you overtake x, meaning you go below x, one of these ways overtaking is actually going up in the tree, but its actually getting smaller in value. f you get smaller than y, you rise above y. That means you will have already risen above x. You already be a value smaller than x. o hopefully, that is the way that its less than or equal to. Because thats what we need. OK, good. Now, understand. ADENE: o at this point, were assuming affine? ERK DEANE: Were assuming affine. Yeah. orry. Forgot to mention that. think this works for pseudo algebraic, but this proof does not, assuming that x and y only switch places once. OK, then we have this simple equation, which is phi of t, x equals phi of t, x, y plus phi of t, x, z, where you just sum over the two children. o this is phi of t, x, z. o need more space, guess. o phi of t, x, z, that is phi of t, x minus phi of t, x, y. o we have, lets say, on the lefthand side, is less than or equal to phi of t, y plus this is what we had before t of x, z. But now, that is the same as phi of t, x minus the other child. orry y. OK, that was just that equation. Now, do things simplify? Hope so. Phi of t, y these two things OK. Not so pretty. What we care about is the sum of these things. We care about sum the overall potential. o we want phi of tplus is phi of t plus something. How it changes is, well, how does phi of t prime x change? Well, it went down a lot. t went all the way down to phi of t, x, y. Well, actually, we just add these together. Add them up. This is messy. orry. Lets look at this sum phi of t plus x and y. o we get this thing phi of t, x, y minus 1. And then we get this thing so phi of t, y, plus phi of t, x, minus phi of t, x, y. ts an elaborate way of saying this cancels with this. o we are left with the old value phi of t, x, plus phi of t, y, minus 1. o that means phi sorry, this is less than or equal to. This means phi of tplus is less than or equal phi of t minus 1. n other words, every time an event happens, the potential goes down by at least one. This is basically confirming this intuition, but roughly. But our definition was this the number of descendants of x that overtake x in the future. How big could this thing be? t is, at most, the sum over all nodes of the number of descendants of that node. o it is, by definition, at most, n log n. f we look at phi of 0, its order n log n. And what were doing is using a balanced heap here. And so for order n log n, initially, in every event that happens, we go down by one. And were never negative. That thing is always nonnegative. Then the number of events is, at most, order n log n. ool? OK, only have one more page of notes to cover in zero minutes about 15 seconds. Let me quickly tell you about whats on this page, and you can read it at home. ts a little survey of lots of different kinetic data structures, in particular, for more than one dimension. o theres a lot of work on 2D convex hull. You have moving points in two dimensions. You want to maintain the convex hull. The number of events the best we know how to achieve is order n to the two plus epsilon a little bit bigger than n squared. And theres a lower bound that it can change n squared times. o its almost optimal unknown how to do that in 3D. A problem that we solved in the open problems in this class two years ago is smallest enclosing disk. o you have points moving in the plane, and you want to maintain, what is the smallest enclosing disk of those points? Number of events we got was n to the 3 plus epsilon, which is a little bit painful. Best lower round is n squared so still a gap there. Those closely related to another open problem, which is, you want to maintain something called a Delaunay triangulation. f you know what that is, great. f not, its a big open problem how many times can the Delaunay triangulation change if you have just points moving along straight lines? Best upper bound is n cubed. Best lower bound is n squared so similar linear gap. f you just want to maintain some triangulation on your points, you can do better. Best upper bound is n to the 2.33333. Best lower bound is n squared. o a smaller gap just cube root of n gap. ollision detection is probably the most obvious application here. You have a video game. Things are moving with known velocities. You update those velocities like, you have bullets, and people running around, and whatever you want to know when they bounce into each other walls, which are stationary. When does that happen? Theres pretty good algorithms for kinetic collision detection. Although, its very unclear what efficient means. Because you want to optimize for the case when not many collisions happen. And these algorithms do in a certain sense, theyre optimal. But its much harder to state what that sense is. inimum spanning tree is another one have. This is tough. Easy thing to do is, you do kinetic predecessor on the entire sorted order of the edge lengths. And then you just run a thing that processes the edges in order. o that gets a quadratic number of events. ts unknown whether you can do any better for minimum spanning tree. o that was your quick survey of kinetic. And that ends geometry. The end.","o we can solve this in log n time plus k. But want to be a little bit more precise about what that log in is, and say that this costs whatever it costs to search for b3, the zcoordinate, in a z list in a list of zcoordinates of points plus order k. write it this way because if we have many searches among lists, then we can speed things up. And if the key of the node is to the left, that means that the stuff were interested to is to the right. And now we have this new thing phi of t, y, z. f you plug that into here, that is the number of descendants of z that will overtake y. Thats the new thing that we add on. And in the end, we get log n to do the first search, and then plus the number of searches. f we need to know ""know"" is a sort of vague thing the sorted order of the points, then you need an event if were going to keep a data structure that is always accurate now so this is sort of an assumption then you need an event every time you have an order change. o priority queue is going to take log n time, where n is the number of certificates in the data structure, to find, when is the next failure? o phi of t, x, z, that is phi of t, x minus phi of t, x, y. o we have, lets say, on the lefthand side, is less than or equal to phi of t, y plus this is what we had before t of x, z. But now, that is the same as phi of t, x minus the other child. claim the total number of events in this data structure, if there are no change operations, if we just advance to infinity number of events is order n log n. And so we call the efficiency log n, because youre log in factor away from this lower bound. m going to call this thing phi of t comma x. o phi of t is the sum over all x of this thing, which is phi of t comma x number of descendants of x at this time that will, in the future, overtake x, meaning their key will get larger than x. OK. o if the key falls between a2 and b2, so our interval is stabbed by the key of the node, then want to query the two data structure two is the right number yeah and want to query the two prime data structure. Once you have the ability to do a 3D search even if this one was minus infinity b1 if we just could do would you call that octants octant search in 3D, then we could just sit there and apply this transformation, and turn each one of them into doublesided, with a log n penalty each time in space, but no extra query time. o the operations are, advance to time t so this is like setting now equal to t and change a point x to have some new f of t trajectory. The claim is, this is sort of the best you could hope to do. o this is the time bound want to get for finding the k points in this search range. want to do this in log n searches plus k. k is the size of the output. But we can use the abstract shape of the tree and do an inorder traversal, and say, look, as long as x i is less than or equal to x i plus one in the inorder traversal for all i, then this is a valid binary search tree. o you have some certificate, like, this point is to the left of that point. And my operations this is kind of like time travel, but now were going to time travel into the future, which we do all the time. o what were going to do is store the data structure that is accurate now. ADENE: And the reason were only hitting points we know about is because the one data structure is on the bottom, so we never actually ERK DEANE: Right. And now, it has this whole subtree of z. o we have whatever it used to have, which is phi of t, y. Thats all its old descendants. And the general version that we talked about is, if youre navigating a graph, and at each node of the graph, you have one of these lists, then you can instantly know where x fits in that list in constant time per thing, as long as you spend log n time to get started, provided your graph has bounded degree. Basically in all kinetic data structures, we just say, while t is greater than or equal to the next failure of the priority queue, we advance to that moment in time, when something interesting happens. ADENE: o but how do you find the things that you just changed in the priority queue of the failure times. o remember, in general, what it lets you do is search for a common element x, find its predecessor and successor in k sorted lists, in order log n plus k time, instead of the obvious k times log n, where n is the length of each list. o if you have a data structure, and you want to make it, kinetic you just first write down some certificates under which its guaranteed to be valid, and then see how to fix them as things happen. Data structure one is a search and a list, plus every time walk to the right and spend constant time, that is an element of my output. And so you get, in general, log to the d minus two n plus k. o last class, we could do one. The way we do that is with certificates. o we care about now the descendants of x. Those are what used to be the descendants of y. o what are the descendants of y that will overtake x? But this is a particular way to solve it that will work with fractional cascading when we do many of them. o can we get a ideally, you have one certificate, which is like, this point is the minimum. And we just like the number of certificates to be small. OK, take the earliest moment when x i of t is greater than x i of plus one, which is the opposite of what want, and take the infimum of those times. But the point is, this is a constant number of updates. As long as you restrict to this subtree, which is what weve always been doing, and, in particular, here, too, its only on the points that are in the right subtree. And so the idea is, over here, theres, at most, n different rays that make it all the way to the left. OK, this is if you really want to maintain the sorted order. And were going to do it in exactly the same way we knew how to before. Then we do a constant of calls to the data structure three. n general, you just look at the right side of a face, and just take half of those things and propagate it to the left. And then we set now to t. And this event thing has to somehow fix the data structure and fix the certificates. Here, k has to be this is for what we call range reporting, where you really want to list everybody in there. The number of events the best we know how to achieve is order n to the two plus epsilon a little bit bigger than n squared. o you have points moving in the plane, and you want to maintain, what is the smallest enclosing disk of those points? o you can think of the running time, if you want, of jumping to infinity as order n squared in the worst case. This is the thing want to bound. Each one is involved in a constant of certificates three now, instead of two but constant number of certificate changes and stuff ADENE: Question. o theres y and all of its children descendants whatever which are those that will overtake x. f add that up for both children, that is the total number of descendants of x that will overtake x. o this equality is trivial. Now, query is, at the current time, whos to the left and whos to the right of this query? Because this is always the kind of certificate we want. ycoordinate is the one that we want to extend. We can really only afford a constant number of calls to this data structure, if were not going to get an extra log blowup. But our definition was this the number of descendants of x that overtake x in the future. And if m given, say, a query like this one, want to know all the points in here. The other one is y. What is the new potential of y? But it is also order n squared all the time, because, if you look at any pair of guys, theyre only going to change a constant number of times whos above whom, for pseudo algebraic, if there are no change events. And its actually easier to think of it as coming from the left, because thats kind of a consistent xcoordinate, and walking to the right. o first claim is this is, at most, this other thing phi of t, y phi of t, x, z. o m just replacing this y with an x. Why is this true? ADENE: o when our query is log n plus k, k is actually the number of points that are coming back, because ERK DEANE: Yeah. But we know from this bound, were going to get order log n plus k. But dont want to do fractional cascading yet, because were not done. We have advanced to the time when it is about to fail. OK, basically, wherever the primes happen, which is x i and x i plus one whatever certificates theyre involved in, you have to update them meaning, rip out the old one, put in the new one. The number of events or lets say, the number of changes to the min think this is what some people call external events. Every node in this range tree on z. And we do a log n search at the beginning. But this is x i plus one, and this is x i. Replace those. mean, this is how were going to build the data structure. And now, the question is, how do you process an event? was going to use this as an example, but its actually a little tricky to think about what the certificates are. We can do 3D orthogonal range queries, still in log n. And if you go to higher dimensions, the best we know, basically, is to use range trees. Whenever you have a data structure has a minus infinity, you can turn it into a lower bound, magically almost the same way as we did here, except were not going to lose a log factor in time only in space. And the main issue here is that you have to maintain the priority queue. OK, then we have this simple equation, which is phi of t, x equals phi of t, x, y plus phi of t, x, z, where you just sum over the two children. You can do it in constant time. o that is the challenge, is, how do you deal with an event when one of your certificates breaks? We need to break down the certificates that only involve a constant number of things and maintain the minimum. But if look at each child of x theres two of them call that y and then measure how many descendants of y at time t will overtake x? And the way that hazelle did this m going to erase the query ray is to decompose the plane in a pretty simple way. o try a little bigger x, y, z. Lets say we need to change it to y, x, z. Because y is about to overtake x. How does the potential change? You have x, y, and z. Then x is less than or equal to y, and x is less than or equal to z. This is nice, because local property. But think, at this moment in time, when theyre actually equal to each other, it doesnt matter whether you have x and y here. dont want, is this point left of this point, to change an unbounded number of times. This is actually something we already know how to do. t is, at most, the sum over all nodes of the number of descendants of that node. The issue is how many things you change, and how many certificates each of those things is in. o then you know where it is in the priority queue and you can rip it out. ADENE: o are the change operations sort of online, or do you have the sequence of changes that youre going to ERK DEANE: All of these operations are online. o its going to store the two data structure on the points in the right subtree not the entire subtree, just the right subtree. o dont want to just think of it as log n. want to think of it as one of these fractional cascading operations. o it is, by definition, at most, n log n. f we look at phi of 0, its order n log n. And what were doing is using a balanced heap here. Because the binary search tree property is x plus or equal to x greater or equal to x. Thats kind of a lot of certificates. And then we get this thing so phi of t, y, plus phi of t, x, minus phi of t, x, y. ts an elaborate way of saying this cancels with this. t doesnt know how many times its going to have to walk to the right. f x and y are about to invert in order so currently, x lets me go to y. n a moment, x will be bigger than y. swap x and y, update all the certificates constant for each. Every time we do this, were going to pay a log n factor. This is really why we like this. Because this is overtaking y. f youre going to overtake y at this time, you would overtake x at this time. o what certificates do we need to do? Number of searches is log n plus k, where k is the size of the output. n this case, changing a constant number of things each of them is in a constant number of certificates. n future time, we know that y is always going to be less than x, because its moving up. We want to know all the points in that rectangle. Lets look at this sum phi of t plus x and y. o we get this thing phi of t, x, y minus 1. Each of the steps are easy, but its kind of amazing where we get to. o we are left with the old value phi of t, x, plus phi of t, y, minus 1. o that means phi sorry, this is less than or equal to. The queries m allowed is, can do a query thats infinite to the left, and can do a query thats infinite to the right. o thats what were going to do. And want to do, now, x i plus one prime to x i plus two. ERK DEANE: ertificates m going to define them more as a tool that we use to build data structures, but theyre really the same thing that mean here. That is phi of t, x, y. Thats the definition here. an do this in constant time? And then were going to do a different kind of data structure called kinetic data structures, where you have moving data. o heres v. Here is the left subtree of v, and the right subtree of v. o this is a bunch of points down here, a bunch of points down here. dont know if really need to write it, but how do we do an event? But in general, anything where all the certificates care about change a constant number of times is just as good as algebraic. think you might also need to do it left to right, if you want to have bounded leftward degree. This time were going to do 3D orthogonal range searching in log n time. Again, want to do it in log n searches plus k. ts the same time bound want to put into a2. o this part costs log n. And then we reduce to the previous problem its constant number of calls. How do advance time and make the new data structure, which is correct, about the new now? Easy thing to do is, you do kinetic predecessor on the entire sorted order of the edge lengths. And we used to have an x i less than or equal to x i plus one. o its almost the same words, but changed one of the xs to a y but only one of them. And thats going to be this infimum. And once say walk down the tree, its pretty clear what you have to do. o in this case, number of certificates is linear. And how do you know when its going to be violated?",0.1146507383441181
58,58,"LNAN HEN: Hi. Welcome back to recitation. n the lecture, youve learned eigenvalues and eigenvectors of a matrix. One of the many important applications of them is solving a higherorder linear differential equation with constant coefficients. A typical example is like what ve written on the board here. y is a function of t, and y and its derivatives satisfy this equation. As you can see, it involves y, y prime, and all the way to its third derivative. o our first goal is to solve this differential equation for its general solution using the method of matrix. o the very first thing that we should do is to find out which matrix we should be working with. o after that, we also want to say something about the explanation of this matrix A*t. We want to find out the first column of this matrix exponential. Why dont you hit the pause now, and try to write down this matrix A by yourself. But before you continue, make sure you come back to this video and check with me youve got the correct A. ll see you in a while. OK, lets work together to transform this problem into linear algebra. The idea is to put y double prime, y prime, and y together as a vector. And let me call this vector u. o of course, vector u is also a function in t. o this is vector u. f this is u, whats going to be u prime? OK, u prime is going to be so take derivative of every coordinate here thats going to be y triple prime, y double prime, and y prime. o this is our u prime t. And my goal is to write u prime as a matrix, call it A, times vector u itself. o want to put a matrix here. And want to create this matrix by incorporating this differential equation. f you move everything except y triple prime to the righthand side of the equation, you can read y triple prime is equal to negative 2 times y double prime so y triple prime is negative 2 times y double prime plus y prime thats plus 1 times y prime plus 2y. Thats 2y, right? That gives you the first row. Then look at the second coordinate, this y double prime. y double prime is simply itself. o you read y double prime is equal to 1 y double prime, then 0, 0. Thats the second row. Well, same thing happens to the last row. y prime is again itself. o thats 0, 1, and 0. That is our matrix A. Did you get the right answer? o we have transformed this equation, this thirdorder ordinary differential equation of y into a firstorder differential equation of u(t). Although u(t) is a vector, but if we can solve this equation for u, we have all the information we need for y. o lets plan on solving this equation. n order to solve this equation, we will need the eigenvalues and eigenvectors of this matrix A. Again, this is a good practice for you. Why dont you pause the video again, and try to complete this problem on your own. When youre ready, m going to come back and show you how did it. Lets finish up everything together. o as we said, we need the eigenvalues and eigenvectors of matrix A, and that involves computing the determinant of the following matrix. o want to compute the determinant of A minus lambda times the identity matrix . Lets write it out. Thats the determinant of 2 minus lambda, 1 2; 1, negative lambda, 0; and 0, 1, negative lambda. o we need the determinant of this three by three matrix. Do it in your favorite way. You can either use the big summation formula, or you can do by cofactor along any row or any column. The correct answer should be this is equal to 1 minus lambda times 1 plus lambda times 2 plus lambda. And this polynomial has three roots: 1, 1, and 2. These are the eigenvalues were looking for. o let me write it here. Lambda_1 is equal to 1. Lambda_2 is equal to 1. And lambda_3 is equal to 2. o now what we need is the eigenvector corresponding to each eigenvalue. Lets take lambda_1 for example. The eigenvector of A corresponding to lambda_1 is in the null space of the matrix A minus lambda_1*, so in this case its A minus . o its in the null space of this matrix. n other words, we are looking for a vector, lets call it , a column vector a, b, and c, such that this matrix multiplying gives me 0. o if you write it out, thats going to be A minus is is equal to 0. OK. ould we choose constants a, b, c such that this is always true? Well if you read the last row, so the last dot product, it says that b has to be equal to c. And if you read the second row it says that a has to be equal to b. Which means a is equal to b is equal to c. And if this relation is true, the first product is always going to be 0. o that simply means we can choose the first eigenvector, the eigenvector corresponding to lambda_1, to be x_1 is equal to transpose. o we choose the first eigenvector to be the column vector with all the coordinates being 1. And you can do the same thing to lambda_2 and lambda_3. But please allow me to skip the computation here. m going to write out the answer for you. o x_2 the eigenvector corresponding to the second eigenvalue, is going to be equal to 1, 1, and 1. And x_3 is going to be 4, 2, and 1. Now weve got everything we need in order to create the general solutions for u(t) o we have eigenvalues, we have the corresponding eigenvectors. What should be u(t)? The general solution for u(t) is equal to some constant _1 times e to the power lambda_1*t so in this case, e to the power t. Then times the first eigenvector, x_1. Plus some other constant _2 times e to the power lambda_2*t so e to the power negative t times x_2. Thats the second eigenvector. Then plus some other constant, _3 times e to the power lambda_3t so negative 2t times x_3. That gives you the general solution for u. As we just said, if you know what u is, you have all the information you need for y. Just in case youre curious about what y is, you can just read the last coordinate of x_1, x_2, and x_3. And you can see that all of them are 1. o y(t) is simply equal to _1 e to the power lambda t plus _2 e to the power negative t plus _3 e to the power negative 2t. And the choice of _1, _2, and _3 is completely arbitrary. o that completes the first part of this question. n the second part, we want to say something about the exponential of A*t. o let me first give you the recipe to cook up the exponential of A*t. The exponential of A*t is equal to the product of three matrices. o you usually we denote them by times e to the power capital lambda t times inverse. And you may ask what is, and what this matrix is. o is the matrix that has x_1, x_2, and x_3 being its column vectors. o is x_1, x_2, and x_3. Let me copy it down here. o thats 1, 1, 1; 1 1, 1; 4, 2, 1. And the matrix in the middle, e to the power lambda*t is a diagonal matrix. o e to the power lambda*t, its a diagonal matrix, and its diagonal entries are given by e to the power lambda_1*t so thats e to the power t then e to the power lambda_2*t negative t and e to the power lambda_3*t negative 2t. 0 everywhere else. o thats e to the power lambda*t. Then the exponential of this At is given by the product of these three matrices. t looks a bit complicated because it involves the inverse of . But luckily, we only want the first column of the result. o if we consider this product, we can see: the product of the first two matrices is relatively easy, because this is a diagonal matrix, and we know that is given by these columns. o the result of the product of these two is simply multiplying the columns of by these coefficients respectively. o you expect to get e to the power lambda_t x_1 times e to the power sorry. The second column should be e to the power negative t, x_2. The third column should be e to the power negative 2t, x_3. And here, what we should put is inverse. But we dont need everything from inverse, because as we just said, we only need the first column of this result. And the first column of this product is going to be given by linear combinations of these columns, and the coefficients are going to be given by the first column inverse. o our goal should be just to get the first column of inverse. Then what is the first column of inverse? Well, the formula for inverse is inverse is going to be the reciprocal of the determinant of , so 1 over determinant of , times the transpose of a matrix . This matrix , the entries of this matrix are given by cofactors of matrix . And then you take transpose, you divide everything by the determinant of . The result will be inverse. And we only need the first column of this matrix. Lets try to write the first column out. Well again, do it in your favorite way to compute the determinant of . The result should be 1 over 6. o the determinant of is 6. Then what is the first column of transpose? Well we can read it from here. This spot, the (1, 1) spot, should be the cofactor of this spot here. That negative 1 minus negative 2, which is 1, so we put 1 here. Now this spot will be the cofactor of this entry here. so thats 1 minus negative 2, thats 3. But this is (1, 2) entry, so you should put a negative sign in the front. Then the last spot should be the cofactor of this entry here, which is 1 minus negative 1, thats 2. omething else here. Two warnings. First, dont forget this transpose sign. econd, dont forget this negative sign. Weve got the first column of inverse, and thats all we need. o we put it here. Thats 1 over 6, 1/2, and 1/3. Thats good enough for me. Now can read out the first column of exponential of A*t. o the first column of the exponential of A*t, m going to write it here. Thats going to be equal to the linear combination of these columns. o thats 1/6 of the first column, thats e to the power t over 6 times x_1. Plus this times this, so thats going to be minus 1/2 of e to the power negative t times x_2. Then plus 1/3 of e to the power negative 2t times x_3. Thats the first column of the exponential A*t. And then with the other two columns. Thats the answer. f you want more practice, you can certainly complete this inverse, and then you can also complete the exponential of A*t. But will leave the rest to you. OK, hope this example shows you that linear algebra can be a powerful tool in solving higherorder ordinary differential equations with constant coefficients. And we have demonstrated the standard procedure to do it, and we also practiced how to calculate the exponential of a matrix. Thanks for watching, and see you next time.","Well if you read the last row, so the last dot product, it says that b has to be equal to c. And if you read the second row it says that a has to be equal to b. Which means a is equal to b is equal to c. And if this relation is true, the first product is always going to be 0. o that simply means we can choose the first eigenvector, the eigenvector corresponding to lambda_1, to be x_1 is equal to transpose. Plus this times this, so thats going to be minus 1/2 of e to the power negative t times x_2. And the first column of this product is going to be given by linear combinations of these columns, and the coefficients are going to be given by the first column inverse. o thats 1/6 of the first column, thats e to the power t over 6 times x_1. Well, the formula for inverse is inverse is going to be the reciprocal of the determinant of , so 1 over determinant of , times the transpose of a matrix . The general solution for u(t) is equal to some constant _1 times e to the power lambda_1*t so in this case, e to the power t. Then times the first eigenvector, x_1. n other words, we are looking for a vector, lets call it , a column vector a, b, and c, such that this matrix multiplying gives me 0. o if you write it out, thats going to be A minus is is equal to 0. Now can read out the first column of exponential of A*t. o the first column of the exponential of A*t, m going to write it here. o thats e to the power lambda*t. Then the exponential of this At is given by the product of these three matrices. And you can see that all of them are 1. o y(t) is simply equal to _1 e to the power lambda t plus _2 e to the power negative t plus _3 e to the power negative 2t. o e to the power lambda*t, its a diagonal matrix, and its diagonal entries are given by e to the power lambda_1*t so thats e to the power t then e to the power lambda_2*t negative t and e to the power lambda_3*t negative 2t. And we only need the first column of this matrix. n the second part, we want to say something about the exponential of A*t. o let me first give you the recipe to cook up the exponential of A*t. The exponential of A*t is equal to the product of three matrices. o x_2 the eigenvector corresponding to the second eigenvalue, is going to be equal to 1, 1, and 1. Thats the first column of the exponential A*t. And then with the other two columns. And the matrix in the middle, e to the power lambda*t is a diagonal matrix. The second column should be e to the power negative t, x_2. n order to solve this equation, we will need the eigenvalues and eigenvectors of this matrix A. Again, this is a good practice for you. Then the last spot should be the cofactor of this entry here, which is 1 minus negative 1, thats 2. omething else here. o after that, we also want to say something about the explanation of this matrix A*t. We want to find out the first column of this matrix exponential. Then what is the first column of inverse? o if we consider this product, we can see: the product of the first two matrices is relatively easy, because this is a diagonal matrix, and we know that is given by these columns. o as we said, we need the eigenvalues and eigenvectors of matrix A, and that involves computing the determinant of the following matrix. And x_3 is going to be 4, 2, and 1. The eigenvector of A corresponding to lambda_1 is in the null space of the matrix A minus lambda_1*, so in this case its A minus . Weve got the first column of inverse, and thats all we need. The result should be 1 over 6. o the determinant of is 6. Then what is the first column of transpose?",0.2361702127659574
59,600,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: o by now you have seen pretty much every possible trick there is in basic probability theory, about how to calculate distributions, and so on. You have the basic tools to do pretty much anything. o whats coming after this? Well, probability is useful for developing the science of inference, and this is a subject to which were going to come back at the end of the semester. Another chapter, which is what we will be doing over the next few weeks, is to deal with phenomena that evolve in time. o socalled random processes or stochastic processes. o what is this about? o in the real world, you dont just throw two random variables and go home. Rather the world goes on. o you generate the random variable, then you get more random variables, and things evolve in time. And random processes are supposed to be models that capture the evolution of random phenomena over time. o thats what we will be doing. Now when we have evolution in time, mathematically speaking, you can use discrete time or continuous time. Of course, discrete time is easier. And thats where were going to start from. And were going to start from the easiest, simplest random process, which is the socalled Bernoulli process, which is nothing but just a sequence of coin flips. You keep flipping a coin and keep going forever. Thats what the Bernoulli process is. o in some sense its something that you have already seen. But were going to introduce a few additional ideas here that will be useful and relevant as we go along and we move on to continuous time processes. o were going to define the Bernoulli process, talk about some basic properties that the process has, and derive a few formulas, and exploit the special structure that it has to do a few quite interesting things. By the way, where does the word Bernoulli come from? Well the Bernoullis were a family of mathematicians, wiss mathematicians and scientists around the 1700s. There were so many of them that actually and some of them had the same first name historians even have difficulty of figuring out who exactly did what. But in any case, you can imagine that at the dinner table they were probably flipping coins and doing Bernoulli trials. o maybe that was their passtime. OK. o what is the Bernoulli process? The Bernoulli process is nothing but a sequence of independent Bernoulli trials that you can think of as coin flips. o you can think the result of each trial being heads or tails. ts a little more convenient maybe to talk about successes and failures instead of heads or tails. Or if you wish numerical values, to use a 1 for a success and 0 for a failure. o the model is that each one of these trials has the same probability of success, p. And the other assumption is that these trials are statistically independent of each other. o what could be some examples of Bernoulli trials? You buy a lottery ticket every week and you win or lose. Presumably, these are independent of each other. And if its the same kind of lottery, the probability of winning should be the same during every week. aybe you want to model the financial markets. And a crude model could be that on any given day the Dow Jones is going to go up or down with a certain probability. Well that probability must be somewhere around 0.5, or so. This is a crude model of financial markets. You say, probably there is more into them. Life is not that simple. But actually its a pretty reasonable model. t takes quite a bit of work to come up with more sophisticated models that can do better predictions than just pure heads and tails. Now more interesting, perhaps to the examples we will be dealing with in this class a Bernoulli process is a good model for streams of arrivals of any kind to a facility. o it could be a bank, and you are sitting at the door of the bank. And at every second, you check whether a customer came in during that second or not. Or you can think about arrivals of jobs to a server. Or any other kind of requests to a service system. o requests, or jobs, arrive at random times. You split the time into time slots. And during each time slot something comes or something does not come. And for many applications, its a reasonable assumption to make that arrivals on any given slot are independent of arrivals in any other time slot. o each time slot can be viewed as a trial, where either something comes or doesnt come. And different trials are independent of each other. Now theres two assumptions that were making here. One is the independence assumption. The other is that this number, p, probability of success, is constant. Now if you think about the bank example, if you stand outside the bank at 9:30 in the morning, youll see arrivals happening at a certain rate. f you stand outside the bank at 12:00 noon, probably arrivals are more frequent. Which means that the given time slot has a higher probability of seeing an arrival around noon time. This means that the assumption of a constant p is probably not correct in that setting, if youre talking about the whole day. o the probability of successes or arrivals in the morning is going to be smaller than what it would be at noon. But if youre talking about a time period, lets say 10:00 to 10:15, probably all slots have the same probability of seeing an arrival and its a good approximation. o were going to stick with the assumption that p is constant, doesnt change with time. Now that we have our model what do we do with it? Well, we start talking about the statistical properties that it has. And here theres two slightly different perspectives of thinking about what a random process is. The simplest version is to think about the random process as being just a sequence of random variables. We know what random variables are. We know what multiple random variables are. o its just an experiment that has associated with it a bunch of random variables. o once you have random variables, what do you do instinctively? You talk about the distribution of these random variables. We already specified for the Bernoulli process that each Xi is a Bernoulli random variable, with probability of success equal to p. That specifies the distribution of the random variable X, or Xt, for general time t. Then you can calculate expected values and variances, and so on. o the expected value is, with probability p, you get a 1. And with probability 1 p, you get a 0. o the expected value is equal to p. And then we have seen before a formula for the variance of the Bernoulli random variable, which is p times 1p. o this way we basically now have all the statistical properties of the random variable Xt, and we have those properties for every t. s this enough of a probabilistic description of a random process? Well, no. You need to know how the different random variables relate to each other. f youre talking about a general random process, you would like to know things. For example, the joint distribution of X2, with X5, and X7. For example, that might be something that youre interested in. And the way you specify it is by giving the joint PF of these random variables. And you have to do that for every collection, or any subset, of the random variables you are interested in. o to have a complete description of a random processes, you need to specify for me all the possible joint distributions. And once you have all the possible joint distributions, then you can answer, in principle, any questions you might be interested in. How did we get around this issue for the Bernoulli process? didnt give you the joint distributions explicitly. But gave them to you implicitly. And this is because told you that the different random variables are independent of each other. o at least for the Bernoulli process, where we make the independence assumption, we know that this is going to be the product of the PFs. And since have told you what the individual PFs are, this means that you automatically know all the joint PFs. And we can go to business based on that. All right. o this is one view of what a random process is, just a collection of random variables. Theres another view thats a little more abstract, which is the following. The entire process is to be thought of as one long experiment. o we go back to the chapter one view of probabilistic models. o there must be a sample space involved. What is the sample space? f do my infinite, long experiment of flipping an infinite number of coins, a typical outcome of the experiment would be a sequence of 0s and 1s. o this could be one possible outcome of the experiment, just an infinite sequence of 0s and 1s. y sample space is the set of all possible outcomes of this kind. Heres another possible outcome, and so on. And essentially were dealing with a sample space, which is the space of all sequences of 0s and 1s. And were making some sort of probabilistic assumption about what may happen in that experiment. o one particular sequence that we may be interested in is the sequence of obtaining all 1s. o this is the sequence that gives you 1s forever. Once you take the point of view that this is our sample space its the space of all infinite sequences you can start asking questions that have to do with infinite sequences. uch as the question, whats the probability of obtaining the infinite sequence that consists of all 1s? o what is this probability? Lets see how we could calculate it. o the probability of obtaining all 1s is certainly less than or equal to the probability of obtaining 1s, just in the first 10 tosses. OK. This is asking for more things to happen than this. f this event is true, then this is also true. Therefore the probability of this is smaller than the probability of that. This event is contained in that event. This implies this. o we have this inequality. Now whats the probability of obtaining 1s in 10 trials? This is just p to the 10th because the trials are independent. Now of course theres no reason why chose 10 here. The same argument goes through if use an arbitrary number, k. And this has to be true for all k. o this probability is less than p to the k, no matter what k choose. Therefore, this must be less than or equal to the limit of this, as k goes to infinity. This is smaller than that for all ks. Let k go to infinity, take k arbitrarily large, this number is going to become arbitrarily small. t goes to 0. And that proves that the probability of an infinite sequence of 1s is equal to 0. o take limits of both sides. ts going to be less than or equal to the limit shouldnt take a limit here. The probability is less than or equal to the limit of p to the k, as k goes to infinity, which is 0. o this proves in a formal way that the sequence of all 1s has 0 probability. f you have an infinite number of coin flips, whats the probability that all of the coin flips result in heads? The probability of this happening is equal to zero. o this particular sequence has 0 probability. Of course, m assuming here that p is less than 1, strictly less than 1. Now the interesting thing is that if you look at any other infinite sequence, and you try to calculate the probability of that infinite sequence, you would get a product of (1p) times 1, 1p times 1, 1p, times p times p, times 1p and so on. You keep multiplying numbers that are less than 1. Again, m making the assumption that p is between 0 and 1. o 1p is less than 1, p is less than 1. You keep multiplying numbers less than 1. f you multiply infinitely many such numbers, the infinite product becomes 0. o any individual sequence in this sample space actually has 0 probability. And that is a little bit counterintuitive perhaps. But the situation is more like the situation where we deal with continuous random variables. o if you could draw a continuous random variable, every possible outcome has 0 probability. And thats fine. But all of the outcomes collectively still have positive probability. o the situation here is very much similar. o the space of infinite sequences of 0s and 1s, that sample space is very much like a continuous space. f you want to push that analogy further, you could think of this as the expansion of a real number. Or the representation of a real number in binary. Take a real number, write it down in binary, you are going to get an infinite sequence of 0s and 1s. o you can think of each possible outcome here essentially as a real number. o the experiment of doing an infinite number of coin flips is sort of similar to the experiment of picking a real number at random. When you pick real numbers at random, any particular real number has 0 probability. o similarly here, any particular infinite sequence has 0 probability. o if we were to push that analogy further, there would be a few interesting things we could do. But we will not push it further. This is just to give you an indication that things can get pretty subtle and interesting once you start talking about random processes that involve forever, over the infinite time horizon. o things get interesting even in this context of the simple Bernoulli process. Just to give you a preview of whats coming further, today were going to talk just about the Bernoulli process. And you should make sure before the next lecture guess between the exam and the next lecture to understand everything we do today. Because next time were going to do everything once more, but in continuous time. And in continuous time, things become more subtle and a little more difficult. But we are going to build on what we understand for the discrete time case. Now both the Bernoulli process and its continuous time analog has a property that we call memorylessness, whatever happened in the past does not affect the future. Later on in this class were going to talk about more general random processes, socalled arkov chains, in which there are certain dependences across time. That is, what has happened in the past will have some bearing on what may happen in the future. o its like having coin flips where the outcome of the next coin flip has some dependence on the previous coin flip. And that gives us a richer class of models. And once we get there, essentially we will have covered all possible models. o for random processes that are practically useful and which you can manipulate, arkov chains are a pretty general class of models. And almost any real world phenomenon that evolves in time can be approximately modeled using arkov chains. o even though this is a first class in probability, we will get pretty far in that direction. All right. o now lets start doing a few calculations and answer some questions about the Bernoulli process. o again, the best way to think in terms of models that correspond to the Bernoulli process is in terms of arrivals of jobs to a facility. And theres two types of questions that you can ask. n a given amount of time, how many jobs arrived? Or conversely, for a given number of jobs, how much time did it take for them to arrive? o were going to deal with these two questions, starting with the first. For a given amount of time that is, for a given number of time periods how many arrivals have we had? How many of those Xis happen to be 1s? We fix the number of time slots lets say n time slots and you measure the number of successes. Well this is a very familiar random variable. The number of successes in n independent coin flips or in n independent trials is a binomial random variable. o we know its distribution is given by the binomial PF, and its just this, for k going from 0 up to n. And we know everything by now about this random variable. We know its expected value is n times p. And we know the variance, which is n times p, times 1p. o theres nothing new here. Thats the easy part. o now lets look at the opposite kind of question. nstead of fixing the time and asking how many arrivals, now let us fix the number of arrivals and ask how much time did it take. And lets start with the time until the first arrival. o the process starts. We got our slots. And we see, perhaps, a sequence of 0s and then at some point we get a 1. The number of trials it took until we get a 1, were going to call it T1. And its the time of the first arrival. OK. What is the probability distribution of T1? What kind of random variable is it? Weve gone through this before. The event that the first arrival happens at time little t is the event that the first t1 trials were failures, and the trial number t happens to be a success. o for the first success to happen at time slot number 5, it means that the first 4 slots had failures and the 5th slot had a success. o the probability of this happening is the probability of having failures in the first t 1 trials, and having a success at trial number 1. And this is the formula for t equal 1,2, and so on. o we know what this distribution is. ts the socalled geometric distribution. Let me jump this through this for a minute. n the past, we did calculate the expected value of the geometric distribution, and its 1/p. Which means that if p is small, you expect to take a long time until the first success. And then theres a formula also for the variance of T1, which we never formally derived in class, but it was in your textbook and it just happens to be this. All right. o nothing new until this point. Now, lets talk about this property, the memorylessness property. We kind of touched on this property when we discussed when we did the derivation in class of the expected value of T1. Now what is the memoryless property? ts essentially a consequence of independence. f tell you the results of my coin flips up to a certain time, this, because of independence, doesnt give you any information about the coin flips after that time. o knowing that we had lots of 0s here does not change what believe about the future coin flips, because the future coin flips are going to be just independent coin flips with a given probability, p, for obtaining tails. o this is a statement that made about a specific time. That is, you do coin flips until 12 oclock. And then at 12 oclock, you start watching. No matter what happens before 12 oclock, after 12:00, what youre going to see is just a sequence of independent Bernoulli trials with the same probability, p. Whatever happened in the past is irrelevant. Now instead of talking about the fixed time at which you start watching, lets think about a situation where your sister sits in the next room, flips the coins until she observes the first success, and then calls you inside. And you start watching after this time. What are youre going to see? Well, youre going to see a coin flip with probability p of success. Youre going to see another trial that has probability p as a success, and these are all independent of each other. o what youre going to see starting at that time is going to be just a sequence of independent Bernoulli trials, as if the process was starting at this time. How long it took for the first success to occur doesnt have any bearing on what is going to happen afterwards. What happens afterwards is still a sequence of independent coin flips. And this story is actually even more general. o your sister watches the coin flips and at some point tells you, oh, something really interesting is happening here. got this string of a hundred 1s in a row. ome and watch. Now when you go in there and you start watching, do you expect to see something unusual? There were unusual things that happened before you were called in. Does this means that youre going to see unusual things afterwards? No. Afterwards, what youre going to see is, again, just a sequence of independent coin flips. The fact that some strange things happened before doesnt have any bearing as to what is going to happen in the future. o if the roulettes in the casino are properly made, the fact that there were 3 reds in a row doesnt affect the odds of whether in the next roll its going to be a red or a black. o whatever happens in the past no matter how unusual it is at the time when youre called in, whats going to happen in the future is going to be just independent Bernoulli trials, with the same probability, p. The only case where this story changes is if your sister has a little bit of foresight. o your sister can look ahead into the future and knows that the next 10 coin flips will be heads, and calls you before those 10 flips will happen. f she calls you in, then what are you going to see? Youre not going to see independent Bernoulli trials, since she has psychic powers and she knows that the next ones would be 1s. he called you in and you will see a sequence of 1s. o its no more independent Bernoulli trials. o whats the subtle difference here? The future is independent from the past, provided that the time that you are called and asked to start watching is determined by someone who doesnt have any foresight, who cannot see the future. f you are called in, just on the basis of what has happened so far, then you dont have any information about the future. And one special case is the picture here. You have your coin flips. Once you see a one that happens, once you see a success, you are called in. You are called in on the basis of what happened in the past, but without any foresight. OK. And this subtle distinction is whats going to make our next example interesting and subtle. o heres the question. You buy a lottery ticket every day, so we have a Bernoulli process thats running in time. And youre interested in the length of the first string of losing days. What does that mean? o suppose that a typical sequence of events could be this one. o what are we discussing here? Were looking at the first string of losing days, where losing days means 0s. o the string of losing days is this string here. Lets call the length of that string, L. Were interested in the random variable, which is the length of this interval. What kind of random variable is it? OK. Heres one possible way you might think about the problem. OK. tarting from this time, and looking until this time here, what are we looking at? Were looking at the time, starting from here, until the first success. o the past doesnt matter. tarting from here we have coin flips until the first success. The time until the first success in a Bernoulli process we just discussed that its a geometric random variable. o your first conjecture would be that this random variable here, which is 1 longer than the one we are interested in, that perhaps is a geometric random variable. And if this were so, then you could say that the random variable, L, is a geometric, minus 1. an that be the correct answer? A geometric random variable, what values does it take? t takes values 1, 2, 3, and so on. 1 minus a geometric would take values from 0, 1, 2, and so on. an the random variable L be 0? No. The random variable L is the length of a string of losing days. o the shortest that L could be, would be just 1. f you get just one losing day and then you start winning, L would be equal to 1. o L cannot be 0 by definition, which means that L + 1 cannot be 1, by definition. But if L +1 were geometric, it could be equal to 1. Therefore this random variable, L + 1, is not a geometric. OK. Why is it not geometric? started watching at this time. From this time until the first success, that should be a geometric random variable. Wheres the catch? f m asked to start watching at this time, its because my sister knows that the next one was a failure. This is the time where the string of failures starts. n order to know that they should start watching here, its the same as if m told that the next one is a failure. o to be asked to start watching at this time requires that someone looked in the future. And in that case, its no longer true that these will be independent Bernoulli trials. n fact, theyre not. f you start watching here, youre certain that the next one is a failure. The next one is not an independent Bernoulli trial. Thats why the argument that would claim that this L + 1 is geometric would be incorrect. o if this is not the correct answer, which is the correct answer? The correct answer goes as follows. Your sister is watching. Your sister sees the first failure, and then tells you, OK, the failures or losing days have started. ome in and watch. o you start to watching at this time. And you start watching until the first success comes. This will be a geometric random variable. o from here to here, this will be geometric. o things happen. You are asked to start watching. After you start watching, the future is just a sequence of independent Bernoulli trials. And the time until the first failure occurs, this is going to be a geometric random variable with parameter p. And then you notice that the interval of interest is exactly the same as the length of this interval. This starts one time step later, and ends one time step later. o conclusion is that L is actually geometric, with parameter p. OK, it looks like m missing one slide. an cheat a little from here? OK. o now that we dealt with the time until the first arrival, we can start talking about the time until the second arrival, and so on. How do we define these? After the first arrival happens, were going to have a sequence of time slots with no arrivals, and then the next arrival is going to happen. o we call this time that elapses or number of time slots after the first arrival until the next one we call it T2. This is the second interarrival time, that is, time between arrivals. Once this arrival has happened, then we wait and see how many more it takes until the third arrival. And we call this time here, T3. Were interested in the time of the kth arrival, which is going to be just the sum of the first k interarrival times. o for example, lets say Y3 is the time that the third arrival comes. Y3 is just the sum of T1, plus T2, plus T3. o were interested in this random variable, Y3, and its the sum of interarrival times. To understand what kind of random variable it is, guess we should understand what kind of random variables these are going to be. o what kind of random variable is T2? Your sister is doing her coin flips until a success is observed for the first time. Based on that information about what has happened so far, you are called into the room. And you start watching until a success is observed again. o after you start watching, what you have is just a sequence of independent Bernoulli trials. o each one of these has probability p of being a success. The time its going to take until the first success, this number, T2, is going to be again just another geometric random variable. ts as if the process just started. After you are called into the room, you have no foresight, you dont have any information about the future, other than the fact that these are going to be independent Bernoulli trials. o T2 itself is going to be geometric with the same parameter p. And then you can continue the arguments and argue that T3 is also geometric with the same parameter p. Furthermore, whatever happened, how long it took until you were called in, it doesnt change the statistics about whats going to happen in the future. o whatever happens in the future is independent from the past. o T1, T2, and T3 are independent random variables. o conclusion is that the time until the third arrival is the sum of 3 independent geometric random variables, with the same parameter. And this is true more generally. The time until the kth arrival is going to be the sum of k independent random variables. o in general, Yk is going to be T1 plus Tk, where the Tis are geometric, with the same parameter p, and independent. o now whats more natural than trying to find the distribution of the random variable Yk? How can we find it? o fixed k for you. Lets say k is 100. m interested in how long it takes until 100 customers arrive. How can we find the distribution of Yk? Well one way of doing it is to use this lovely convolution formula. Take a geometric, convolve it with another geometric, you get something. Take that something that you got, convolve it with a geometric once more, do this 99 times, and this gives you the distribution of Yk. o thats definitely doable, and its extremely tedious. Lets try to find the distribution of Yk using a shortcut. o the probability that Yk is equal to t. o were trying to find the PF of Yk. k has been fixed for us. And we want to calculate this probability for the various values of t, because this is going to give us the PF of Yk. OK. What is this event? What does it take for the kth arrival to be at time t? For that to happen, we need two things. n the first t 1 slots, how many arrivals should we have gotten? k 1. And then in the last slot, we get one more arrival, and thats the kth one. o this is the probability that we have k 1 arrivals in the time interval from 1 up to t. And then, an arrival at time t. Thats the only way that it can happen, that the kth arrival happens at time t. We need to have an arrival at time t. And before that time, we need to have exactly k 1 arrivals. Now this is an event that refers t1. n the previous time slots we had exactly k 1 arrivals. And then at the last time slot we get one more arrival. Now the interesting thing is that this event here has to do with what happened from time 1 up to time t 1. This event has to do with what happened at time t. Different time slots are independent of each other. o this event and that event are independent. o this means that we can multiply their probabilities. o take the probability of this. What is that? Well probability of having a certain number of arrivals in a certain number of time slots, these are just the binomial probabilities. o this is, out of t 1 slots, to get exactly k 1 arrivals, p to the k1, (1p) to the t1 (k1), this gives us tk. And then we multiply with this probability, the probability of an arrival, at time t is equal to p. And so this is the formula for the PF of the number of the time it takes until the kth arrival happens. Does it agree with the formula in your handout? Or its not there? ts not there. OK. Yeah. OK. o thats the formula and it is true for what values of t? . t takes at least k time slots in order to get k arrivals, so this formula should be true for k larger than or equal to t. For t larger than or equal to k. All right. o this gives us the PF of the random variable Yk. Of course, we may also be interested in the mean and variance of Yk. But this is a lot easier. ince Yk is the sum of independent random variables, the expected value of Yk is going to be just k times the expected value of your typical t. o the expected value of Yk is going to be just k times 1/p, which is the mean of the geometric. And similarly for the variance, its going to be k times the variance of a geometric. o we have everything there is to know about the distribution of how long it takes until the first arrival comes. OK. Finally, lets do a few more things about the Bernoulli process. ts interesting to talk about several processes at the time. o in the situation here of splitting a Bernoulli process is where you have arrivals that come to a server. And thats a picture of which slots get arrivals. But actually maybe you have two servers. And whenever an arrival comes to the system, you flip a coin and with some probability, q, you send it to one server. And with probability 1q, you send it to another server. o there is a single arrival stream, but two possible servers. And whenever theres an arrival, you either send it here or you send it there. And each time you decide where you send it by flipping an independent coin that has its own bias q. The coin flips that decide where do you send it are assumed to be independent from the arrival process itself. o theres two coin flips that are happening. At each time slot, theres a coin flip that decides whether you have an arrival in this process here, and that coin flip is with parameter p. And if you have something that arrives, you flip another coin with probabilities q, and 1q, that decides whether you send it up there or you send it down there. o what kind of arrival process does this server see? At any given time slot, theres probability p that theres an arrival here. And theres a further probability q that this arrival gets sent up there. o the probability that this server sees an arrival at any given time is p times q. o this process here is going to be a Bernoulli process, but with a different parameter, p times q. And this one down here, with the same argument, is going to be Bernoulli with parameter p times (1q). o by taking a Bernoulli stream of arrivals and splitting it into two, you get two separate Bernoulli processes. This is going to be a Bernoulli process, thats going to be a Bernoulli process. Well actually, m running a little too fast. What does it take to verify that its a Bernoulli process? At each time slot, its a 0 or 1. And its going to be a 1, youre going to see an arrival with probability p times q. What else do we need to verify, to be able to tell to say that its a Bernoulli process? We need to make sure that whatever happens in this process, in different time slots, are statistically independent from each other. s that property true? For example, what happens in this time slot whether you got an arrival or not, is it independent from what happened at that time slot? The answer is yes for the following reason. What happens in this time slot has to do with the coin flip associated with the original process at this time, and the coin flip that decides where to send things. What happens at that time slot has to do with the coin flip here, and the additional coin flip that decides where to send it if something came. Now all these coin flips are independent of each other. The coin flips that determine whether we have an arrival here is independent from the coin flips that determined whether we had an arrival there. And you can generalize this argument and conclude that, indeed, every time slot here is independent from any other time slot. And this does make it a Bernoulli process. And the reason is that, in the original process, every time slot is independent from every other time slot. And the additional assumption that the coin flips that were using to decide where to send things, these are also independent of each other. o were using here the basic property that functions of independent things remain independent. Theres a converse picture of this. nstead of taking one stream and splitting it into two streams, you can do the opposite. You could start from two streams of arrivals. Lets say you have arrivals of men and you have arrivals of women, but you dont care about gender. And the only thing you record is whether, in a given time slot, you had an arrival or not. Notice that here we may have an arrival of a man and the arrival of a woman. We just record it with a 1, by saying there was an arrival. o in the merged process, were not keeping track of how many arrivals we had total. We just record whether there was an arrival or not an arrival. o an arrival gets recorded here if, and only if, one or both of these streams had an arrival. o that we call a merging of two Bernoull of two processes, of two arrival processes. o lets make the assumption that this arrival process is independent from that arrival process. o what happens at the typical slot here? m going to see an arrival, unless none of these had an arrival. o the probability of an arrival in a typical time slot is going to be 1 minus the probability of no arrival. And the event of no arrival corresponds to the first process having no arrival, and the second process having no arrival. o theres no arrival in the merged process if, and only if, theres no arrival in the first process and no arrival in the second process. Were assuming that the two processes are independent and thats why we can multiply probabilities here. And then you can take this formula and it simplifies to p + q, minus p times q. o each time slot of the merged process has a certain probability of seeing an arrival. s the merged process a Bernoulli process? Yes, it is after you verify the additional property that different slots are independent of each other. Why are they independent? What happens in this slot has to do with that slot, and that slot down here. These two slots so what happens here, has to do with what happens here and there. What happens in this slot has to do with whatever happened here and there. Now, whatever happens here and there is independent from whatever happens here and there. Therefore, what happens here is independent from what happens there. o the independence property is preserved. The different slots of this merged process are independent of each other. o the merged process is itself a Bernoulli process. o please digest these two pictures of merging and splitting, because were going to revisit them in continuous time where things are little subtler than that. OK. Good luck on the exam and see you in a week.","And then we multiply with this probability, the probability of an arrival, at time t is equal to p. And so this is the formula for the PF of the number of the time it takes until the kth arrival happens. o this is the probability that we have k 1 arrivals in the time interval from 1 up to t. And then, an arrival at time t. Thats the only way that it can happen, that the kth arrival happens at time t. We need to have an arrival at time t. And before that time, we need to have exactly k 1 arrivals. o whatever happens in the past no matter how unusual it is at the time when youre called in, whats going to happen in the future is going to be just independent Bernoulli trials, with the same probability, p. The only case where this story changes is if your sister has a little bit of foresight. And the time until the first failure occurs, this is going to be a geometric random variable with parameter p. And then you notice that the interval of interest is exactly the same as the length of this interval. We already specified for the Bernoulli process that each Xi is a Bernoulli random variable, with probability of success equal to p. That specifies the distribution of the random variable X, or Xt, for general time t. Then you can calculate expected values and variances, and so on. The time until the kth arrival is going to be the sum of k independent random variables. After the first arrival happens, were going to have a sequence of time slots with no arrivals, and then the next arrival is going to happen. And with probability 1 p, you get a 0. o the expected value is equal to p. And then we have seen before a formula for the variance of the Bernoulli random variable, which is p times 1p. The probability is less than or equal to the limit of p to the k, as k goes to infinity, which is 0. o this proves in a formal way that the sequence of all 1s has 0 probability. Were interested in the time of the kth arrival, which is going to be just the sum of the first k interarrival times. o the probability of an arrival in a typical time slot is going to be 1 minus the probability of no arrival. o at least for the Bernoulli process, where we make the independence assumption, we know that this is going to be the product of the PFs. o the probability that this server sees an arrival at any given time is p times q. o this process here is going to be a Bernoulli process, but with a different parameter, p times q. And this one down here, with the same argument, is going to be Bernoulli with parameter p times (1q). And its the time of the first arrival. The event that the first arrival happens at time little t is the event that the first t1 trials were failures, and the trial number t happens to be a success. o what youre going to see starting at that time is going to be just a sequence of independent Bernoulli trials, as if the process was starting at this time. o conclusion is that the time until the third arrival is the sum of 3 independent geometric random variables, with the same parameter. What happens in this time slot has to do with the coin flip associated with the original process at this time, and the coin flip that decides where to send things. The time its going to take until the first success, this number, T2, is going to be again just another geometric random variable. o the model is that each one of these trials has the same probability of success, p. And the other assumption is that these trials are statistically independent of each other. For example, what happens in this time slot whether you got an arrival or not, is it independent from what happened at that time slot? o T2 itself is going to be geometric with the same parameter p. And then you can continue the arguments and argue that T3 is also geometric with the same parameter p. Furthermore, whatever happened, how long it took until you were called in, it doesnt change the statistics about whats going to happen in the future. o now that we dealt with the time until the first arrival, we can start talking about the time until the second arrival, and so on. And if this were so, then you could say that the random variable, L, is a geometric, minus 1. an that be the correct answer? The time until the first success in a Bernoulli process we just discussed that its a geometric random variable. And then you can take this formula and it simplifies to p + q, minus p times q. o each time slot of the merged process has a certain probability of seeing an arrival. And this is because told you that the different random variables are independent of each other. Now the interesting thing is that this event here has to do with what happened from time 1 up to time t 1. And you have to do that for every collection, or any subset, of the random variables you are interested in. The Bernoulli process is nothing but a sequence of independent Bernoulli trials that you can think of as coin flips. And we want to calculate this probability for the various values of t, because this is going to give us the PF of Yk. And were going to start from the easiest, simplest random process, which is the socalled Bernoulli process, which is nothing but just a sequence of coin flips. After you are called into the room, you have no foresight, you dont have any information about the future, other than the fact that these are going to be independent Bernoulli trials. At each time slot, theres a coin flip that decides whether you have an arrival in this process here, and that coin flip is with parameter p. And if you have something that arrives, you flip another coin with probabilities q, and 1q, that decides whether you send it up there or you send it down there. o your first conjecture would be that this random variable here, which is 1 longer than the one we are interested in, that perhaps is a geometric random variable. ince Yk is the sum of independent random variables, the expected value of Yk is going to be just k times the expected value of your typical t. o the expected value of Yk is going to be just k times 1/p, which is the mean of the geometric. Therefore the probability of this is smaller than the probability of that. The other is that this number, p, probability of success, is constant. o the probability of successes or arrivals in the morning is going to be smaller than what it would be at noon. o what is the Bernoulli process? This event has to do with what happened at time t. Different time slots are independent of each other. o in the situation here of splitting a Bernoulli process is where you have arrivals that come to a server. And its going to be a 1, youre going to see an arrival with probability p times q. What else do we need to verify, to be able to tell to say that its a Bernoulli process? The same argument goes through if use an arbitrary number, k. And this has to be true for all k. o this probability is less than p to the k, no matter what k choose. And each time you decide where you send it by flipping an independent coin that has its own bias q. The coin flips that decide where do you send it are assumed to be independent from the arrival process itself. o the probability of this happening is the probability of having failures in the first t 1 trials, and having a success at trial number 1. o after you start watching, what you have is just a sequence of independent Bernoulli trials. What happens at that time slot has to do with the coin flip here, and the additional coin flip that decides where to send it if something came. o we have everything there is to know about the distribution of how long it takes until the first arrival comes. And the additional assumption that the coin flips that were using to decide where to send things, these are also independent of each other.",0.202581664910432
60,601,"Lets keep building our table of Laplace transforms. And now well do a fairly hairy problem, so m going to have to focus so that dont make a careless mistake. But lets say we want to take the Laplace transform and this is a useful one. Actually, all of them weve done so far are useful. ll tell you when we start doing notsouseful ones. Lets say we want to take the Laplace transform of the sine of some constant times t. Well, our definition of the Laplace transform, that says that its the improper integral. And remember, the Laplace transform is just a definition. ts just a tool that has turned out to be extremely useful. And well do more on that intuition later on. But anyway, its the integral from 0 to infinity of e to the minus st, times whatever were taking the Laplace transform of times sine of at, dt. And now, we have to go back and find our integration by parts neuron. And mine always disappears, so we have to reprove integration by parts. dont recommend you do this all the time. f you have to do this on an exam, you might want to memorize it before the exam. But always remember, integration by parts is just the product rule in reverse. o ll just do that in this corner. o the product rule tells us if we have two functions, u times v. And if were take the derivative of u times v. Lets say that theyre functions of t. These are both functions of t. could have written u of x times v of x. Then that equals the derivative of the first times the second function, plus the first function times the derivative of the second. Now, if were to integrate both sides, get uv this should be review is equal to the integral of u prime v, with respect to dt but m just doing a little bit of shorthand now plus the integral of uv prime. m just trying to help myself remember this thing. And lets take this and subtract it from both sides. o we have this integral of u prime v is going to be equal to this, uv minus the integral of uv prime. And, of course, this is a function of t. Theres a dt here and all of that. But just have to do this in the corner of my page a lot, because always forget this, and with the primes and the integrals and all that, always forget it. One way, if you did want to memorize it, you said, OK, the integration by parts says if take the integral of the derivative of one thing and then just a regular function of another, it equals the two functions times each other, minus the integral of the reverse. Right? Here, when you take the subtraction, youre taking the one that had a derivative, now it doesnt. And the one that didnt have a derivative, now it does. But anyway, lets apply that to our problem at hand, to this one. Well, we could go either way about it. Lets make u prime is equal to well do our definition u prime is equal to e to the minus st, in which case you would be the antiderivative of that, which is equal to minus 1 over s e to the minus st, right? And actually, this is going to be an integration by parts twice problem, so m just actually going to define the Laplace transform as y. Thatll come in useful later on. And think actually did a very similar example to this when we did integration by parts. But anyway, back to the integration by parts. o thats u. And let me do v in a different color. o when v if this is u prime, right? This is u prime, then this is v. o v is equal to sine of at. And then what is v prime? Well, thats just a cosine of at, right? The chain rule. And now, were ready to do our integration. o the Laplace transform, and ll just say thats y, y is equal to y is what were trying to solve for, the Laplace transform of sine of at that is equal to u prime v. defined u prime in v, right? Thats equal to that. The integral of u prime times v. That equals uv. o thats minus 1 over s e to the minus st, times v, sine of at, minus the integral. And when you do the integration by parts, this could be an indefinite integral, an improper integral, a definite integral, whatever. But the boundary stays. And we can still say, from 0 to infinity of uv prime. o u is minus 1 over s e to the minus st, times v prime, times a cosine of at fair enough dt. Well, now we have another hairy integral we need to solve. o this might involve another integration by parts, and it does. Lets see if we can simplify it at Lets take the constants out first. Let me just rewrite this. o we get y is equal to minus e to the minus st over s, sine of at. o you have a minus minus plus a over s a divided by s, and then these two negative signs cancel out times the integral from 0 to infinity, e to the minus st, cosine of at, dt. Lets do another integration by parts. And ll do this in a purple color, just so you know this is our second integration by parts. Over here. Lets define once again, u prime is equal to e the minus st. o this is u prime. Then u is equal to minus 1 over s e to the minus st. Well make v equal to cosine of at. The hardest part about this is not making careless mistakes. And then v prime just want it to be on the same row is equal to minus a sine of at, right? The chain rule, derivative of cosine is minus sine. o lets substitute that back in, and we get this is going to get hairy; actually, it already is hairy y is equal to minus e to the minus st over s, sine of at, plus a over s, times OK. ntegration by parts. uv. o thats minus 1 over s e to the minus st, times v, times cosine at, minus the integral from 0 to infinity. This problem is making me hungry. ts taking so much glucose from my bloodstream. m focusing so much not to make careless mistakes. Anyway, integral from 0 to infinity. And now, we have uv prime, so u is minus 1 over s e to the minus st. Thats u. And then v prime times minus a. o lets make that minus cancel out with this one. o that becomes a plus. a sine of at, dt. m starting to see the light at the end of the tunnel. o then, lets simplify this thing. And, of course, were going to have to evaluate this whole thing, right? Actually, were going to have to evaluate everything. Lets just focus on the indefinite integral for now. Were going to have to take this whole thing and evaluate lets just say that y is the antiderivative and then evaluate it from infinity to 0. From 0 to infinity. o y is equal to minus e to the minus st over s, sine of at. Now lets distribute this. inus a over s squared, e to the minus st, cosine of at. Right? OK, now want to make sure dont make a careless mistake. OK. Now, lets multiply this times this and take all of the constants out. o we have an a and an s. a over s. Theres a minus sign. We have a plus a to the s. o well have a minus a squared over s squared, times the integral from 0 well, said m just worrying about the indefinite integral right now, and well evaluate the boundaries later. e to the minus st, sine of at, dt. Now, this is the part, and weve done this before, its a little bit of a trick with integration by parts. But this expression, notice, is the same thing as our original y. Right? This is our original y. And were assuming were doing the indefinite integral, and well evaluate the boundaries later. Although we could have kept the boundaries the whole time, but it would have made it even hairier. o we can rewrite this integral as y. That was our definition. And actually, just realized m running out of time, so ll continue this hairy problem in the next video. ee you soon.","Lets make u prime is equal to well do our definition u prime is equal to e to the minus st, in which case you would be the antiderivative of that, which is equal to minus 1 over s e to the minus st, right? o we have this integral of u prime v is going to be equal to this, uv minus the integral of uv prime. o y is equal to minus e to the minus st over s, sine of at. And now, we have uv prime, so u is minus 1 over s e to the minus st. o lets substitute that back in, and we get this is going to get hairy; actually, it already is hairy y is equal to minus e to the minus st over s, sine of at, plus a over s, times OK. o we get y is equal to minus e to the minus st over s, sine of at. o the Laplace transform, and ll just say thats y, y is equal to y is what were trying to solve for, the Laplace transform of sine of at that is equal to u prime v. defined u prime in v, right? And then v prime just want it to be on the same row is equal to minus a sine of at, right? But anyway, its the integral from 0 to infinity of e to the minus st, times whatever were taking the Laplace transform of times sine of at, dt. o thats minus 1 over s e to the minus st, times v, sine of at, minus the integral. o thats minus 1 over s e to the minus st, times v, times cosine at, minus the integral from 0 to infinity. Then u is equal to minus 1 over s e to the minus st. o you have a minus minus plus a over s a divided by s, and then these two negative signs cancel out times the integral from 0 to infinity, e to the minus st, cosine of at, dt. This is u prime, then this is v. o v is equal to sine of at. Were going to have to take this whole thing and evaluate lets just say that y is the antiderivative and then evaluate it from infinity to 0. One way, if you did want to memorize it, you said, OK, the integration by parts says if take the integral of the derivative of one thing and then just a regular function of another, it equals the two functions times each other, minus the integral of the reverse. Now, if were to integrate both sides, get uv this should be review is equal to the integral of u prime v, with respect to dt but m just doing a little bit of shorthand now plus the integral of uv prime. e to the minus st, sine of at, dt. We have a plus a to the s. o well have a minus a squared over s squared, times the integral from 0 well, said m just worrying about the indefinite integral right now, and well evaluate the boundaries later. Lets say we want to take the Laplace transform of the sine of some constant times t. Well, our definition of the Laplace transform, that says that its the improper integral. o u is minus 1 over s e to the minus st, times v prime, times a cosine of at fair enough dt. But lets say we want to take the Laplace transform and this is a useful one.",0.2791327913279133
61,602,"Hello everyone. o far in the series on data structures, we have talked about some of the linear data structures like array, linked list, stack and queue. n all these structure, data is arranged in a linear or sequential manner, so we can call them linear data structures and weve also talked about tree which is a nonlinear data structure. Tree is a hierarchical structure. Now as we understand data structures are ways to store and organize data, and for different kinds of data we use different kinds of data structures. n this lesson, were going to introduce you to another non linear data structure that has got its application in a wide number of scenarios in computer science. t is used to model and represent a variety of systems and this data structure is graph. When we study data structures, we often first study them as mathematical or logical models. Here also, we will first study graph as a mathematical or logical model and we will go into implementation details later. Okay so lets get started. A graph just like a tree is a collection of objects or entities that we call nodes or vertices, connected to each other through a set of edges. But in a tree connections are bound to be in a certain way. n a tree that our rules dictating the connection among the nodes. n a tree with N Nodes, we must have exactly N 1 edges. One edge for each parent child relationship. As we know an edge in a tree is for a parent child relationship and all nodes in a tree except the root node would have apparent would have exactly 1 parent and thats why if they are N nodes, it must be exactly N 1 edges. n a tree, all nodes must be reachable from the root and there must be exactly one possible path from root to a node. Now in a graph there are no rules dictating the connection among the nodes. A graph contains a set of nodes and a set of edges and edges can be connecting nodes in any possible way. Tree is only a special kind of graph. Now graph as a concept has been studied extensively in mathematics. f you have taken a course on discrete mathematics then you must be knowing about graphs already. n computer science, we basically study and implement the same concept of graph from mathematics. The study of graph is often referred to as graph theory. n pure mathematical terms we can define graph something like this. A graph G is in order pair of a set V of vertices and a set E of edges. Now m using some mathematical jargon here. An ordered pair is just a pair of mathematical objects in which the order of objects in the pair matters. This is how we write and represent an ordered pair, objects separated by comma put within parenthesis. Now because the order here matters. We can say that V is the first object in the pair and E is the second object. An ordered pair A, B is not equal to B, A unless A and B are equal. n our definition of graph here, first object in the pair must always be a set of vertices and the second object must be a set of edges thats why we are calling the pair an ordered pair. We also have concept of unordered pair. An unordered pair is simply a set of two elements. Order is not important here. We write an unordered pair using curly brackets or braces. Because the order is not important here, unordered pair A, B is equal to B, A. t doesnt matter which object is first and which object is second. Okay coming back, so a graph of is an ordered pair of a set of vertices and a set of edges and G = (V,E) is a formal mathematical notation that we use to define a graph. Now have a graph drawn here in the right. This graph is 8 vertices and 10 edges. What want to do is want give some names to these vertices because each node in a graph must have some identification. t can be a name or it can be an index. m naming these vertices as V1, V2 V3, V4, V5 and so on, and this naming is not indicative of any order. There is no 1st, 2nd and 3rd Node here. could give any name to any node. o my set of vertices here is this. We have 8 elements in the set V1, V2, V3, V4, V5, V6, V7 and V8. o this is my set of vertices for this graph. Now whats my set of edges. To answer this we first need to know how to represent an edge. An edge is uniquely identified by its 2 endpoints, so we can just write the names of the two endpoint of an edge as a pair and it can be a representation for the edge. But edges can be of two types. We can have a directed edge in which connection is oneway or we can have an undirected edge in which connection is two way. n this example graph that m showing here, edges are undirected but if you remember the tree that had shown earlier then we had directed edges in that tree. With this directed edge that m showing you here, we are saying that there is link or path from vertex to V but we cannot assume a path from V to . This connection is one way. For a directed edge, one of the endpoints would be the origin and other end point would be the destination and we draw the edge with an arrow head pointing towards the destination. For our edge here, origin is and destination is V. A directed edge can be to represented as an ordered pair, first element in the pair can be the origin and second element can be the destination. o with this directed edge represented as ordered pair (,V), we have a path from to V. f we want a path from V to , when need to draw another directed edge here with V as origin and as destination and this edge can be the represented as ordered pair (V,), the upper one here is (,V) and the below one is (V,) and they are not same. Now if the edge is undirected, the connection is 2 way and undirected edge can be to represented as an unordered pair here because the edge is bi directional origin and destination are not fixed. We only need to know what two end points have been connected by the edge. o now that we know how to present edges, we can write the set of edges for this example graph here. We have an undirected edge between V1 and V2 then we have 1 between V1 and V3 and then be have V1 V4. This is really simple and just go ahead and write all of them. o this is my set of edges. Typically in a graph, all edges would either be directed or undirected. ts possible for a graph to have both directed and undirected edges but we are not going to study such graphs, we are only going to study graphs in which all edges would either be directed or undirected. A graph with all directed edges is called a directed graph or digraph and a graph with all undirected ages is called an undirected graph. There is no special name for an undirected graph. sually, if the graph directed, we explicitly say that its directed graph or digraph. o these are two types of graphs. Directed graph or digraph in which edges are unidirectional or ordered pairs and undirected graph in which edges are bidirectional or unordered pairs. Now many realworld systems and problems can be modeled using a graph. Graphs can be used to represent any collection of objects having some kind of pairwise relationship. Lets have a look at some of the interesting examples. A social network like Facebook can be represented as an undirected graph. A user would be a node in the graph and if 2 user are friends, there would be an edge connecting them. A real social network would have millions and billions of nodes. can show only few in my diagram here because m short of space. Now social network is an undirected graphs because friendship is a mutual relationship. f m your friend, you are my friend too. o connections have to be 2 way. Now once a system is modeled as a graph a lot of problems can easily be solved by applying standard algorithms in graph theory. Like here in this social network, lets say we want to do something like suggest friends to a user. Lets say we want to suggest some connections to Rama. One possible approach to do so can be suggesting friends of friends who are not connected already. Rama has 3 friends, Ella, Bob and Katie and friends of 3 that are not connected to Rama already can be suggested. There is no friend of Ella which is not connected to Rama already. Bob however, has 3 friends Tom, am, and Lee that are not friends with Rama so they can be suggested and katie has two friends Lee and wati that are not connected to Rama.We have counted Lee already, so in all we can suggest these for users to Rama. Now even though we described this problem in context of a social network. This is a standard graph problem. The problem here in pure graph terms is finding all nodes having lenght of shortest path from a given node equal to 2. tandard algorithms can be applied to solve this problem. Well talk about concepts like path in a graph in some time. For now just know that the problem that we just described in context of a social network is a standard graph problem. Okay so a social network like Facebook is an undirected graph Now lets have a look at another example. nterlinked web pages on the internet or the World Wide Web can be represented as a directed graph. A web page that would have a unique address or RL would be a node in the graph and we can have a directed edge if a page contains link to another page. Now once again, there are billions of pages on the web but can show only few here. The edges in this graph are directed because that relationship is not mutual this time. f page A has a link to page B then its not necessary that page B will also have a link to page A. Lets say one of the pages on mycodeschool.com has a tutorial on graph and on this page have put a link to Wikipedia article on graph. Lets assume that in this example graph that am showing you here. Page P is my mycodeschool tutorial on graph with this address or RL mycodeschool.com/videos/graph and lets say, page Q is the Wikipedia article on graph with this RL Wikipedia/org/wiki/graph. Now on my page that is page P, have put a link to the Wikipedia page on graph. f you are on page P, you can click on this link and go to page Q but Wikipedia has not reciprocated to my favor by putting a link back to my page. o if you are on page Q you cannot click on the link can come to page P. onnection here is one way and thats why we have drawn a directed egde here. Okay now once again if we are able to present web as a directed graph, we can apply standard graph theory algorithms to solve problems and to perform tasks. One of the tasks that search engines like Google perform very regularly is web crawling. earch engines use a program called web crawler that systematically browsers the World Wide Web to collect and store data about web pages. earch engines can then use this data to provide quick and accurate results against search queries. Now even though in this context, we are using a nice and heavy term like web crawling. Web crawling is basically graph traversal or in simpler words, act of visiting all nodes in a graph and no prizes for guessing that there are standard algorithms for graph traversal. We will be studying graph traversal algorithms in a later lessons. Okay now the next thing that want to talk about is concept of a weighted graph. ometimes in a graph, all connections cannot be treated as equal. ome connections can be preferable to others like for example we can represent intercity road network that is the network of highways and free ways between cities as an undirected graph. m assuming that all highways would be bidirectional. ntracity road network that is road network within a city would definitely have oneway roads and so ntracity network must be represented as a directed graph but intercity road network in my opinion can be represented as an undirected graph. Now clearly we cannot treat all connections as equal here. Roads would be of different lengths and to perform a lot of tasks to solve a lot of problems, we need to take length of roads into account. n such cases, we associate some weight or cost with every edge. We label the edges with their weights. n this case weight can be lenght of the roads, so what to do here is ll just label this edges with some values for the lenghts. Lets say these values are in kilometers and now edges in this graph are weighted and this graph can be called weighted graph. Lets say in this graph, we want to pick the best route from city A to city D. Have a look at these four possible routes, am showing them in different colors. Now if would treat all edges as equal then would say that the green route through B and and a red route through E and F are equally good. Both these paths have to three edges and this yellow route through E is the best because we have only two edges in this path. But with different weights assigned to the connections, need to add up weights of edges in a path to calculate total cost. When m taking weight into account shortest route is through B and . onnections have different weights and this is really important here in this graph. Actually, we can look at all the graphs as weighted graphs An unweighted graph can basically be seen as a weighted graph in which weight of all the edges is same and typically we assume to weight as one. Okay so we have represented intercity cities road network as weighted undirected graph. ocial network was an unweighted undirected graph and World Wide Web was an unweighted directed graph and this one is a weighted undirected graph. Now this was intercity road network. think intracity road network that is road network within a city can be modeled as a weighted directed graph because in a city that would be some one ways. ntersections in interest citys road network would be Nodes and road segments would be our edges, and by the way we can also draw an undirected graph as directed. ts just that for each undirected edge we will have 2 directed edges. We may not be able to redraw a directed graph has undirected but we can always redraw an undirected graph as directed. Okay ll stop here now. This much is good for an introductory lesson. n next lesson, we will talk about some more properties of graph. This is it for this lesson. Thanks for watching !","o with this directed edge represented as ordered pair (,V), we have a path from to V. f we want a path from V to , when need to draw another directed edge here with V as origin and as destination and this edge can be the represented as ordered pair (V,), the upper one here is (,V) and the below one is (V,) and they are not same. o now that we know how to present edges, we can write the set of edges for this example graph here. Now if the edge is undirected, the connection is 2 way and undirected edge can be to represented as an unordered pair here because the edge is bi directional origin and destination are not fixed. Okay coming back, so a graph of is an ordered pair of a set of vertices and a set of edges and G = (V,E) is a formal mathematical notation that we use to define a graph. A graph G is in order pair of a set V of vertices and a set E of edges. Lets say these values are in kilometers and now edges in this graph are weighted and this graph can be called weighted graph. For our edge here, origin is and destination is V. A directed edge can be to represented as an ordered pair, first element in the pair can be the origin and second element can be the destination. o this is my set of vertices for this graph. Actually, we can look at all the graphs as weighted graphs An unweighted graph can basically be seen as a weighted graph in which weight of all the edges is same and typically we assume to weight as one. ts possible for a graph to have both directed and undirected edges but we are not going to study such graphs, we are only going to study graphs in which all edges would either be directed or undirected. n our definition of graph here, first object in the pair must always be a set of vertices and the second object must be a set of edges thats why we are calling the pair an ordered pair. The edges in this graph are directed because that relationship is not mutual this time. ome connections can be preferable to others like for example we can represent intercity road network that is the network of highways and free ways between cities as an undirected graph. We can have a directed edge in which connection is oneway or we can have an undirected edge in which connection is two way. An edge is uniquely identified by its 2 endpoints, so we can just write the names of the two endpoint of an edge as a pair and it can be a representation for the edge. o this is my set of edges. ntersections in interest citys road network would be Nodes and road segments would be our edges, and by the way we can also draw an undirected graph as directed. A graph contains a set of nodes and a set of edges and edges can be connecting nodes in any possible way. As we know an edge in a tree is for a parent child relationship and all nodes in a tree except the root node would have apparent would have exactly 1 parent and thats why if they are N nodes, it must be exactly N 1 edges. A web page that would have a unique address or RL would be a node in the graph and we can have a directed edge if a page contains link to another page. n this case weight can be lenght of the roads, so what to do here is ll just label this edges with some values for the lenghts. A graph just like a tree is a collection of objects or entities that we call nodes or vertices, connected to each other through a set of edges. o my set of vertices here is this. We can say that V is the first object in the pair and E is the second object. Both these paths have to three edges and this yellow route through E is the best because we have only two edges in this path.",0.1783216783216783
62,603,"ay have some matrix a lets say a is n by n, so it looks something like this. Youve seen this before, a 1 1, a 1 2, all the way to a 1 n. When you go down the rows you get a 2 1, that goes all the way to a 2 n. And lets say that theres some row here, lets say row i, it looks like a i 1, all the way to a i n. And then you have some other row here, a j, its a j 1 all the way to a j n. And then you keep going all the way down to a n 1, a n 2, all the way to a n n. This is just an n by n matrix, and you can see that took a little trouble to write out my row a, my ith row here and my jth row here. And just to kind of keep things a little simple, let me just define just for notational purposes, you can view these as row vectors if you like, but havent formally defined row vectors so wont necessarily go there. But lets just define the term r i, well call that row i, to be equal to a i 1, a i 2, all the way to a i n. You can write it as a vector if you like, like a row vector. We havent really defined operations on row vectors that well yet, but think you get the idea. We can then replace this guy with r 1, this guy with r 2, all the way down. Let me do that, and ll do that in the next couple of videos because itll simplify things, and think make things a little bit easier to understand. o can rewrite this matrix, this n by n matrix a, can rewrite it as just r i. Actually, this just looks like a vector, its just a row vector. Let me write it as a vector like that. And m being a little bit handwavy here because all of our vectors have been defined as column vectors, but think you get the idea. o lets call that r 1, and then we have r 2 is the next row, all the way down. You keep going down, you get to r i thats this row right there r i. You keep going down, you get r j, and then you keep going down until you get to the nth row. And each of these guys are going to have n terms because you have n columns. o thats another way of writing this same n by n matrix. Now what m going to do here is, m going to create a new matrix lets call that swapping the swap matrix of i and j. o m going to swap i and j, those two rows. o whats the matrix going to look like? Everything else is going to be equal. You have row 1 assuming that 1 wasnt one of the i or js, it could have been. Row 2, all the way down to now instead of a row i there you have a row j there, and you go down and instead of a row j you have a row i there. And you go down and then you get r n. o what did we do? We just swapped these two guys. Thats what the swap matrix is. Now think it was in the last video or a couple of videos ago, we learned that if you just swap two rows of any n by n matrix, the determinant of the resulting matrix will be the negative of the original determinant. o we get the determinant of s, the swap of the ith and the j rows is going to be equal to the minus of the determinant of a. Now, let me ask you an interesting question. What happens if those two rows were actually the same? What if r i was equal to r j? f we go back to all of these guys, if that row is equal to this row? That means that this guy is equal to that guy, that the second column the second column for that row all the way to the nth guy is equal to the nth guy. Thats what mean when say what happens if those two rows are equal to each other. Well, if those two rows are equal to each other, than this matrix is no different than this matrix here, even though we swapped them. f you swap two identical things, youre just going to be left with the same thing again. o if let me write this down if row i is equal to row j, then this guy, then s, the swapped matrix, is equal to a. Theyll be identical. Youre swapping two rows that are the same thing. o that implies a determinant of the swapped matrix is equal to the determinant of a. But we just said, if the swap matrix, when you swap two rows, it equals a negative of the determinant of a. o this tells us it also has to equal the negative of the determinant of a. o what does that tell us? That tells us if a has two rows that are equal to each other, if we swap them, we should get the negative of the determinant, but if two rows are equal were going to get the same matrix again. o if a has two rows that are equal so if row i is equal to row j then the determinant of a has to be equal to the negative of the determinant of a. We know that because the determinant of a, or a is the same thing as the swapped version of a, and the swapped version of a has to have the negative determinant of a. o these two things have to be equal. Now what number is equal to a negative version of itself? f just told you x is equal to negative x, what number does x have to be equal to? Theres only one value that it could possibly be equal to. x would have to be equal to 0. o the takeaway here is, lets say if you have duplicate rows you can extend this if you have three or four rows that are the same leads you to the fact that the determinant of your matrix is 0. And that really shouldnt be a surprise. Because if you have duplicate rows, remember what we learned a long time ago. We learned that a matrix is an invertible if and only if the reduced row echelon form is the identity matrix. We learned that. But if you have two duplicate rows lets say these two guys are equal to each other you could perform a row operation where you replace this guy with this guy minus that guy, and youll just get a row of 0s. And if you get a row of 0s, youre never going to be able get the identity matrix. o we know that duplicate rows could never get reduced row echelon form to be the identity. Or, duplicate rows are not invertible. And we also learned that something is not invertible if and only if its determinant is equal to 0. o we now got to the same result two different ways. One, we just used some of what we learned. When you swap rows, it should become the negative, but if you swap the same row, you shouldnt change the matrix. o the determinant of the matrix has to be the same as itself. o if you have duplicate rows, the determinant is 0. Which isnt something that we had to use using this little swapping technique, we could have gone back to our requirements for invertability think was five or six videos ago. But just wanted to point that out. f you see duplicate rows. and actually if you see duplicate columns ll leave that for you to think about if you see duplicate rows or duplicate columns, or even if you just see that some rows are linear combinations of other rows and m not showing that to you right here then you know that your determinant is going to be equal to 0.","o if a has two rows that are equal so if row i is equal to row j then the determinant of a has to be equal to the negative of the determinant of a. We know that because the determinant of a, or a is the same thing as the swapped version of a, and the swapped version of a has to have the negative determinant of a. o these two things have to be equal. x would have to be equal to 0. o the takeaway here is, lets say if you have duplicate rows you can extend this if you have three or four rows that are the same leads you to the fact that the determinant of your matrix is 0. That tells us if a has two rows that are equal to each other, if we swap them, we should get the negative of the determinant, but if two rows are equal were going to get the same matrix again. o that implies a determinant of the swapped matrix is equal to the determinant of a. But we just said, if the swap matrix, when you swap two rows, it equals a negative of the determinant of a. o this tells us it also has to equal the negative of the determinant of a. o what does that tell us? Youve seen this before, a 1 1, a 1 2, all the way to a 1 n. When you go down the rows you get a 2 1, that goes all the way to a 2 n. And lets say that theres some row here, lets say row i, it looks like a i 1, all the way to a i n. And then you have some other row here, a j, its a j 1 all the way to a j n. And then you keep going all the way down to a n 1, a n 2, all the way to a n n. This is just an n by n matrix, and you can see that took a little trouble to write out my row a, my ith row here and my jth row here. o we get the determinant of s, the swap of the ith and the j rows is going to be equal to the minus of the determinant of a. Now, let me ask you an interesting question. and actually if you see duplicate columns ll leave that for you to think about if you see duplicate rows or duplicate columns, or even if you just see that some rows are linear combinations of other rows and m not showing that to you right here then you know that your determinant is going to be equal to 0. Now think it was in the last video or a couple of videos ago, we learned that if you just swap two rows of any n by n matrix, the determinant of the resulting matrix will be the negative of the original determinant. But lets just define the term r i, well call that row i, to be equal to a i 1, a i 2, all the way to a i n. You can write it as a vector if you like, like a row vector. o if you have duplicate rows, the determinant is 0. But if you have two duplicate rows lets say these two guys are equal to each other you could perform a row operation where you replace this guy with this guy minus that guy, and youll just get a row of 0s. f we go back to all of these guys, if that row is equal to this row? o if let me write this down if row i is equal to row j, then this guy, then s, the swapped matrix, is equal to a. Theyll be identical. Now what m going to do here is, m going to create a new matrix lets call that swapping the swap matrix of i and j. o m going to swap i and j, those two rows.",0.2023026315789473
63,604,"n this lesson, were going to write code to delete a node from binary search tree. n most data structures deletion is tricky. n case of binary search trees too, its not so straightforward. o lets first see what all complications we may have while trying to delete a node from binary search tree. have drawn a binary search tree of integers here. As we know in a binary search tree for each node value of all nodes in its left subtree is lesser and value of all nodes right subtree is greater. For example, in this tree if ll pick this node with value 5 then we have 3 and 1 in its left subtree which are lesser and we have 7 and 9 in its right subtree which are greater, and you can pick any other node in the tree and this property will be true else the tree is not a BT. Now when we need to delete a node, this property must be conserved. Lets try to delete some nodes from this example tree and see if we can rearrange these things and conserve the property of binary search tree or not. What if want to delete this node the value 19. To delete a node from tree we need to do two things, we need to remove to reference of the node from its parent so the node is detached from the tree. Here we will cut this link we will set right child of this node with value 17 as null and the second thing that we need to do is reclaim the memory allocated to the node being deleted. that is wipe off the node object from memory. This particular node with value 19 that youre trying to delete here is leaf node. t has no children and even if we take this guy out by simply cutting this link that is removing its reference from its parent and then wiping it off from memory there is no problem, property of binary search tree that for each node value of nodes in left should be lesser and value of nodes in right should be greater is conserved. o deleting a leaf node, a node with no children is really easy. n this tree, these four nodes with values 1, 9, 13 and 19 are leaf nodes. To delete any of these, we just need to cut the link and wipe off the node that is clear it from memory. But what if we want to delete a non leaf node, what if in this example we want to delete this node with value 15, cant just cut this link because if ll cut this link, we will detach not just the node with value 15 but this complete subtree. We have two more nodes in the subtree we could have had a lot more. We need to make sure that all other nodes except the node with value 15 thats been deleted remain in the tree. o what do we do now. This particular node that were trying to delete here has two children on two subtrees. ll come back to case of node with two children later because this is not so easy to crack. What we want to discuss first is the case when node being deleted would have only one child. f the not being deleted would have only one child like in this example this node with value 7 this guy has only one child. This guy has a right child but does not have left child. For such a node what we can do is we can link its parent to this only child, so the child and everything below the child we could have some more nodes below 9 as well will remain attached to the tree and only the node being deleted will be detached. Now were not losing any other node than the node with value 7. This is my tree after the deletion. s this still a binary search tree. Yes, it is. Only the right subtree of Node with value five has changed. Earlier we had 7 and 9 in right subtree of 5 and now we have 9 which is fine. What if we were having some more nodes below 9. Here in this tree, i can have a node in left of 9 and the value in this node has to be lesser so than 12, greater than 5, greater than 7 and lesser than 9. We are left with only 1 choice. We can only have 8 here, in right we can have something lesser than 12 and greater than 5, 7 and 9 all in our between 9 and 12. Okay so if the original tree was this much after deletion this is how my tree will look like. Okay so are we good now. s the tree in right a BT. Well yes, it is. When we are setting this node with value 9 as right child of the node with value 5. We are basically setting this particular subtree as right subtree of the node with value 5. Now this subtree is already in right of 5 so value of all nodes in this subtree is already greater than 5 and the subtree itself of course is a binary search tree. Any subtree in a binary search tree will also be a binary search tree, so even after deletion, even after the rearrangement property of the tree that for each node, nodes in left should be lesser and nodes in right should be greater in value is conserved. o this is what we need to do to delete a node with just one child or a node with just one subtree connect its parent to its only child and then wipe it off from memory. There are only two nodes in this tree that have only one child, lets try to delete this other one with value 3. All we need to do here is set 1 as left child of 5. Once again if there were some more nodes below 1, then also there was no issue. Okay so now were good for two cases, we good for leaf nodes and we are good with nodes with just one child and now we should think about the third case, what if a node has two children, what should be do in this case. Lets come back to this node value 15 that we were trying to delete earlier.With two children we cant do something like connect parent to one of two children. While trying to delete 15 if we will connect 12 to 13. f we will make 13 to right child of 12 then we will include 13 and anything below 13 that is we will include the left subtree of 15 but we will lose the right subtree a 15 that is 17 and anything below 17. imilarly, if we will make 17 the right child then we will lose to left subtree of 15 that is 13 and anything below 13. Actually this case is tricky and before talk about a possible solution, want to insert some more nodes here. want to have some more node in subtrees of 13 and 17 the reason m inserting some more nodes here is because want to discuss a generic case and thats why want these two subtrees to have more than one node. Okay coming back when m trying to delete this node my intent basically is to remove this value 15 from the tree. y delete function will have signature something like this. t will take pointer reference to the root node and value to be deleted as argument. o here, am deleting this particular node because want to remove 15 from the tree. what m going to do now is something with which can reduce case 3 to either case 1 or case 2. ll wipe off 15 from this node and ll fill in some other value in this node. Ofcourse cant fill in any random value. What ll do is, ll look for the minimum in right subtree of this node and ll fill in that value here. inimum in right subtree of this node is 17. o have filled 17 here. We now have two nodes with value 17 but notice that this node has only one child. We can delete this node because we know how to delete a node with only one child and once this node is deleted my tree will be good. The final arrangement will be a valid arrangement for my BT, but why minimum in right subtree, why not value in any other leaf node or any other node with one child. Well we also need to conserve this property that for each node, nodes left should have lesser value nodes in right should have greater value. For this node if m bringing in the minimum from its right subtree then because m bringing in something from its right subtree it will be greater and the previous value 17 is greater than 15. o all the elements in left ofcourse will be lesser and because its the minimum in right subtree, all the elements in right of this guy would either be greater or equal.We will have a duplicate that will be equal. Once the duplicate is removed everything else will be fine. n a tree or subtree if a node has minimum value it wont have a left child because if theyre is a left child there is something lesser and this is another property that were exploiting. Give this some thought. n a tree or subtree node with minimum value will not happen left child, there may or may not be a right child. f we would have a right child, like here we have a right child. o here we are reducing case 3 to case 2. f there was no child we would have reduced case 3 to case 1. Okay so lets get rid of the duplicate. ll build a link like this and after deletion this is what my tree will look like. o this is what we need to do in case 3, we need to find a minimum in right subtree of targeted node then copy or fill in this value and finally we need to delete the duplicate or the node with minimum value from right subtree. There was another possible approach here and must talk about it. nstead of going for minimum in right, we could also go far maximum any left subtree. aximum left subtree would ofcourse be greater than our equal to all the values in left, maximum left subtree of node with value 15 is 14. m copying 14 here. Now all the nodes in left a lesser than equal to 14 and because were picking something from left subtree it will still be lesser than the value being deleted. 14 is less than 15. o all the nodes in this right subtree will still be greater and if were picking maximum in a tree or subtree then that node will not have right child because if we have something in right we have something greater, so the value cant be maximum. The node may have left child. n this case a node with value 14 doesnt have a left child so we are basically reducing case 3 to case 1. ll simply get rid of this node. o we are looking good even after deletion. n case 3, we can apply any of these methods and this is all in logic part. Lets now write called for this logic. ll write c++ and we will use recursion if youre not very comfortable applying recursion on trees then make sure you watch earlier lesson in this series. You can find link to them and description of this video. n my ode here i have defined node as a structure with three fields. We have 1 field to store data and we have 2 fields that are pointers to node to store addresses of left and right children and want to write a function named delete that should take pointer to root node and the data to be deleted as argument, and this function should return pointer to root node because that root may change after deletion. What were passing to delete function is only local copy of roots address. f the address is changing we need to return it back. To delete a given value or data to the first need to find it in the tree and once we find the node containing that data, we can try to delete it. Remember only identity of tree that we pass to functions is address of the root node and to perform any action on the tree we need to start at root. o lets first search for the node with this data. First ll cover a corner case. f the root is null that is if the tree is empty, we can simply return. can say return root or return null here, they will mean the same because root is null, else if the data that we are looking for is less than the data in root then its in the left subtree. The problem can be reduced to deleting the data are from left subtree. We need to go and find the data in left subtree, so they can make a recursive call to delete function passing address of left child and the data to be deleted. Now the root of the left subtree that is the left child of this current node may change after deletion but the good thing is, the delete function will return address of the modified root of the left subtree. o we can set the return as left child of current node. Now if data that were trying to delete is greater than the data in root, we need to go and delete the data from right subtree, and if the data is neither greater nor lesser that is if its equal then we can try deleting the node containing that data. Now lets handle the 3 cases 1 by 1. f there is no child we can simply delete that node what ll do here is that first wipe off to node from memory and this is how ll do it. What we have in root right now is address of to node to be deleted. m using to delete operator here thats used to deallocate memory of an object in heap. n you would use free function. Now root is a dangling pointer because object in heap is deleted but root still has its address. o we can set root as null, and now we can return root. Reference of this node in its parent will not be fixed here. Once this recursive call finishes then somewhere in these 2 statement in any of these two statements in any of these two else ifs the link will be corrected. hope this is making sense. Okay now lets handle other cases. f only the left child is null then what want to do is first want to store the address of current node that m trying to delete in a temporary pointer to node and now want to move to root, this pointer named root to the right child, so the right child becomes the root of this subtree and now we can delete the node that has been pointed to by temp. We will use to delete operator. n we would be using free function. and now they can return root.imilarly if the right child is null, ll first store address of current root in a temporary pointer to node then ll make the left child new root of the subtree so we will move to the left child and then ll delete the previous root whose address have in temp and finally ll return root. Actually we need to return root in all cases, so ill remove this return statement from our all these if and else if and write one return root after everything. Lets talk about the 3rd now. n case of two children what we need to do is, we need to search for minimum element in right subtree of the node that were trying to delete. Lets say this function findmin will you give me address of the node with minimum value in tree or subtree, so im calling this function findmin and m collecting the return in a pointer to node named temp. Now should set the data in current node that i am trying to delete as this minimum value and now the problem is getting reduced to deleting this minimum value from the right subtree of current node. With this much code think m done with delete function. This looks good to me. Lets quickly run this code on an example tree and see if this works or not. have drawn a binary search tree here. Lets say these values outside these nodes are addresses of the nodes. Now want to delete number 15 from this tree, so ll make a call to delete function passing address of the root which is 200 and 15 the value to be deleted. n delete function for this particular call control will come to this line, a recursive call will be made. Execution of this call delete 200 , 15 will pause and it will resume only after this function below delete 350 , 15 returns. Now for this call below well go inside the third else in case 3, here we will find with minimum value in right which is 17 which is 400, the value is 17 address is 400. First we will set the data in node 350 as 17 and we are making a recursive call to delete 17 from right subtree of 350. We have only one node in right subtree of 350. Here we have case 1. n this call will simply delete the node at 400 and return null. Remember root will be returned in all calls in the end. Now delete 350 , 15 will resume and in this resumed call, we will set address of right child of node at 350 as null. As you can see the link in parent is being corrected, when the recursion is unfolding and the function call corresponding to the parent is resuming and now this guy can return, and now in this call, we will resume at this line. o right child of node at 200 will set as 350. ts already 350 but it will be written again and now this call can also finish. o hope you got some sense of how this recursion is working. You can find link to all the source code and code to test the delete function in description of this video. This is it for this lesson. Thanks for watching.","f only the left child is null then what want to do is first want to store the address of current node that m trying to delete in a temporary pointer to node and now want to move to root, this pointer named root to the right child, so the right child becomes the root of this subtree and now we can delete the node that has been pointed to by temp. o this is what we need to do in case 3, we need to find a minimum in right subtree of targeted node then copy or fill in this value and finally we need to delete the duplicate or the node with minimum value from right subtree. For example, in this tree if ll pick this node with value 5 then we have 3 and 1 in its left subtree which are lesser and we have 7 and 9 in its right subtree which are greater, and you can pick any other node in the tree and this property will be true else the tree is not a BT. Here we will cut this link we will set right child of this node with value 17 as null and the second thing that we need to do is reclaim the memory allocated to the node being deleted. For such a node what we can do is we can link its parent to this only child, so the child and everything below the child we could have some more nodes below 9 as well will remain attached to the tree and only the node being deleted will be detached. What we have in root right now is address of to node to be deleted. n case of two children what we need to do is, we need to search for minimum element in right subtree of the node that were trying to delete. We can delete this node because we know how to delete a node with only one child and once this node is deleted my tree will be good. 14 is less than 15. o all the nodes in this right subtree will still be greater and if were picking maximum in a tree or subtree then that node will not have right child because if we have something in right we have something greater, so the value cant be maximum. Now the root of the left subtree that is the left child of this current node may change after deletion but the good thing is, the delete function will return address of the modified root of the left subtree. For this node if m bringing in the minimum from its right subtree then because m bringing in something from its right subtree it will be greater and the previous value 17 is greater than 15. o all the elements in left ofcourse will be lesser and because its the minimum in right subtree, all the elements in right of this guy would either be greater or equal.We will have a duplicate that will be equal. We have 1 field to store data and we have 2 fields that are pointers to node to store addresses of left and right children and want to write a function named delete that should take pointer to root node and the data to be deleted as argument, and this function should return pointer to root node because that root may change after deletion. Now should set the data in current node that i am trying to delete as this minimum value and now the problem is getting reduced to deleting this minimum value from the right subtree of current node. As we know in a binary search tree for each node value of all nodes in its left subtree is lesser and value of all nodes right subtree is greater. To delete a node from tree we need to do two things, we need to remove to reference of the node from its parent so the node is detached from the tree. Now want to delete number 15 from this tree, so ll make a call to delete function passing address of the root which is 200 and 15 the value to be deleted. and now they can return root.imilarly if the right child is null, ll first store address of current root in a temporary pointer to node then ll make the left child new root of the subtree so we will move to the left child and then ll delete the previous root whose address have in temp and finally ll return root. t has no children and even if we take this guy out by simply cutting this link that is removing its reference from its parent and then wiping it off from memory there is no problem, property of binary search tree that for each node value of nodes in left should be lesser and value of nodes in right should be greater is conserved. We need to go and find the data in left subtree, so they can make a recursive call to delete function passing address of left child and the data to be deleted. Now lets handle the 3 cases 1 by 1. f there is no child we can simply delete that node what ll do here is that first wipe off to node from memory and this is how ll do it. To delete a given value or data to the first need to find it in the tree and once we find the node containing that data, we can try to delete it. Now if data that were trying to delete is greater than the data in root, we need to go and delete the data from right subtree, and if the data is neither greater nor lesser that is if its equal then we can try deleting the node containing that data. But what if we want to delete a non leaf node, what if in this example we want to delete this node with value 15, cant just cut this link because if ll cut this link, we will detach not just the node with value 15 but this complete subtree. Any subtree in a binary search tree will also be a binary search tree, so even after deletion, even after the rearrangement property of the tree that for each node, nodes in left should be lesser and nodes in right should be greater in value is conserved. Now this subtree is already in right of 5 so value of all nodes in this subtree is already greater than 5 and the subtree itself of course is a binary search tree. o this is what we need to do to delete a node with just one child or a node with just one subtree connect its parent to its only child and then wipe it off from memory. As you can see the link in parent is being corrected, when the recursion is unfolding and the function call corresponding to the parent is resuming and now this guy can return, and now in this call, we will resume at this line. Okay so now were good for two cases, we good for leaf nodes and we are good with nodes with just one child and now we should think about the third case, what if a node has two children, what should be do in this case. what m going to do now is something with which can reduce case 3 to either case 1 or case 2. ll wipe off 15 from this node and ll fill in some other value in this node. There are only two nodes in this tree that have only one child, lets try to delete this other one with value 3. When we are setting this node with value 9 as right child of the node with value 5.",0.2248618784530386
64,605,"All right. o lets get started. o were still talking about conditional expectation, right? And so today well finish conditional expectation as a topic in its own right, which of course, doesnt mean you can then forget it because everything in this course is about thinking conditionally. But as its own topic, well finish that today. o wanted to start with just a couple quick examples of conditional expectation where youre conditioning on a random variable. Last time we were talking about conditioning on an event versus conditioning on a random variable. o well do a couple quick examples, then derive some properties, and then do some more difficult examples. But just to start with a couple easy examples just to help get the notation and concepts in mind. o, here is a simple example. Lets just start with a normal, x is standard normal. And lets let y = x squared, okay? And then suppose we want E(Y given X). This is just a practice, you know what does the concept mean? E(Y given X) is E(X squared given X). This notation means we get to treat x as known and then we try to give our best prediction for x squared. Best is in the sense of minimizing mean squared error. But in a certain sense, its the best prediction. f we know X, we know X squared, so obviously our best prediction would be X squared, which equals Y. Okay, so now thats a very easy calculation, but if we didnt get X squared here, then theres something very suspicious about this, right? We get to know X and somehow predicting something else doesnt make sense. o this should be very very clear. Now lets see what would happen if we went the other way around. ame example, but now lets do E(X given Y) instead of E( Y given X). o thats E (X given X squared). o we get to observe X squared. Now we treat X squared as known, but we dont know X. Okay what do you think this is? Zero, why? Negative, yeah. This is just 0 and you can do some big calculation, but you shouldnt have to cuz you just think about whats the conditional distribution. ince if we know, if we get to observe that in fact X squared = a, so were treating it as knowns so m just going to call it a, that we get to know a, then we know that x is plus or minus the square root of a, but by symmetry those are equally likely. All right, this is only giving us information about the magnitude, its not giving us any information about the sign. ince the normal is symmetric, then its equally likely. o this is equally likely to be square root of a or minus square root of a. Those are equally likely. f you average square root of a and minus square root of a youll get zero. o this doesnt say that X and X squared are independent, right? We saw before that theyre uncorrelated. But theyre definitely not independent. But this just says that X squared doesnt help very much with predicting X. Just as a number in this sense, right. We know the magnitude but we dont know anything about the sign so we just have to guess one number and we may as well say zero. All right, lets do another example, just another quick example. Okay, so suppose we have a stick and we break off, we have these stick breaking type problems. We have a stick, lets say it has length 1 and break off a random piece, and by random here mean uniform. o we break off a piece, throw out the other piece. o now we only have one random piece then break that piece again, okay? o break off another piece. And suppose we want the expected value, or the conditional expectation, for the length of the second piece. o in terms of the picture, what were doing is were first picking x. have to put it somewhere for the sake of the picture. Thats x, but lets assume that x is uniform between 0 and 1. m just translating what just said into probability notation. o thats the first break point. o we break the stick here, and we keep this part, throw out the other piece. And then, now we just have this piece from 0 to x, then pick a random break point in this piece. Lets say there, thats y. Okay, and the question is then, whats the length of this piece, right, or the distribution, or the conditional expectation, that kind of thing. o to write that out conditionally, we would just write y given x is uniform (0, x). o this notation would not make any sense if didnt write given x here, right, because just wanna specify a distribution. But what this notation means, its just shorthand for saying that if we know that big X equals little x, then its going to be uniform between 0 and little x. And this is just short hand for that. But you can always think of it back in terms of conditioning on big X equals little x. o thats just short hand saying if we get to treat x as known, then were picking a random point from here to here. All right, so thats the setup. Okay, now lets compute E(Y given X=x). o thats going to be a function of little x. This just says that we know the first break point is here, call this point little x. This one is anywhere from zero to little x uniformly, so on average it would be little x over 2. And so E(Y given capital X)= capital X /2 because we just changed lowercase x to big X. And as said, you can just think of this as short hand for this. ts easier to write this and to work with it once you understand what it means. But its not essentially a different concept. Okay so thats a random variable, right? E(Y given X) is a random variable and its a function of capital X. And lets just quickly see what happens if we then take, so this is a random variable, we can then ask, whats its expectation? o if we now take E(E(Y given X)), That makes perfect sense to do that, right? Because thats a random variable, can take its expectation. Expected value of x is onehalf, cuz thats uniform zeroone. o onehalf of onehalf is onefourth. And also said at the very end last time, we didnt prove it yet, but well prove it today that just in general not just for this problem. E(E (Y given X)) is just E(Y). o that would be a quick way to get the expected value of that second piece after we break twice. And onefourth seems pretty intuitive, right? Because on average, youre taking half the stick and then half the stick again. o that seems reasonable, but as weve seen many times our intuitions could be wrong, but in this case its pretty intuitive. And that actually proves that thats true, at least once we know that this equals this, which stated but we havent proven yet, okay? o those are just a couple of quick examples. o now lets talk about kind of the general. Properties of conditional expectation. Theres like three or four main properties and once you understand those few properties then you can derive all kinds of stuff about conditional expectation. o these are very, very useful properties. ll even write useful properties. Although we wouldnt do them if they were useless. Okay. Property one, similar to what we were just doing over there, but just want to kind of write that as a general statement. f we have E of, lets say, each of X times Y, given X. Now we know that if we have a constant in front, we can take it out. Right. Well in this case were treating X as known, so from our point this is capital X. o this h of X is random variable. h is just any function. ould be X cubed, E to the X, whatever. ts a function of X, were treating X as known, so we know h of X so we can take it out, because were treating it as a constant. o that just becomes h of X, E of Y, given X. o thats really what we implicitly were doing up here, just took out the X squared, and were left with a 1 inside, the expected value of 1 given anything is 1, cuz its always 1, okay? o thats called taking out whats known. Okay, so we use that a lot to simplify when we see a function of X there and were conditioning on X. We can take it out. Thats good. Okay, and secondly, E of Y given X equals E of Y, if X, and Y are independent. This is not if and only if. We just saw an example over there, where E of X given X squared is zero, we now that theyre not independent, but if they are independent then we can just get rid of the condition. Thats just clear from the definition, right, because the definition of this says that we work with the conditional distribution given X. The conditional distribution of Y given X is no different from the unconditional one, because theyre independent. o being given X doesnt help at all for predicting Y. o then, thats just true. Okay. Third one is the one we we just stated, E of E of Y given X equal E of Y. o we have the conditional expectation. Take its expectation and just get the unconditional expectation. This one needs some proof. This one goes by different names, depending on where you look. But ll call it either iterated expectation or in this department, we like to call it Adams Law for Reasons that we might get to later. Anyway whatever you call it its an extremely useful fact. ts main use is not to say that this equals this, maybe should have written it the other way around, this equals this. ts more useful the other way around just like in law of total, and why do we care about conditional probability? Well, one reason is just that we gather evidence and we condition on the evidence, right? But the other reason is even we want unconditional probability, then we keep using the law of total probability, and reduce it down to conditional, right? This is an analogous to that, this is actually a generalization of the law of total probability. o its says we want the expected value of Y, but we dont know how to do it, we can try to cleverly choose X to make the problem simpler, where E of Y given X is simpler to work with, and then take the expected value of that. Thats basically what we did over here, E of Y given X, could immediately just write down thats X over 2, right, cuz we know that conditional distribution. ts harder to just say right away, whats the unconditional distribution of Y, right, cuz it has the conditional structure built in. o thats an extremely useful property. Well prove this in a few minutes. Just want to state one more property, think. And that one is that if we take Y minus E of Y given X, thats a natural thing to look at because were thinking of this as the prediction, that is were using X to predict Y. E of Y given X is our prediction. o, Y minus that is just how far off is the prediction, right? Thats the actual value of Y, minus the predicted value of Y, okay? And then the statement is that if we multiply this by any function of X, youll always get 0, h is any function of x, h of x is any function of x. n words, this says that, This thing, Y minus E of Y, given X, which in statistics is called a residual. ts just whats left over after you try to predict Y and then the difference is uncorrelated with any function of X. Because if we computed the covariance of these two things, ll just write out the covariance, Y minus E of Y, given X, just the definition of covariance. The covariance would be exactly this thing that we wrote here, E of, m just writing the same thing again. E of Y minus E of Y, given X, h of x, and then minus, definition of covariance. Expected value of this times this, minus E of this, E of this, right? o its minus E of Y minus E of Y, given X. Looks complicated, but it will simplify, E of h of X. Just writing down what the covariance is. But this thing is 0. We know thats 0, right, just by iterated expectation and linearity. Thats E of Y minus E of Y, so this part is just 0. o we only have this term. o in other words what that shows is that this is actually the covariance of this and this and it says its 0. We havent shown that yet. o let me draw a picture to show geometrically what this says, and then think well prove this property assuming the third one, then well prove the third one, okay? All right, so first, heres a picture, and whether this picture makes sense or not kind of depends on how much linear algebra youve had. o, if you havent had much then you can ignore the picture. But if you have, then this picture will help with your intuition for some of these properties, okay? o the picture is like this. We start with Y, and were representing it as a point. o the whole, not the whole idea, but a big part of the idea of linear algebra is you start treating vectors in an abstract way, right? Where a vector could be a function, t could be a cow. t could be anything. Well, it doesnt matter what it is. All that matters is the axioms that you have certain operations, right? o if you have an operation on cows that satisfies the axioms of a vector space, then you treat them as vectors and you need to just work with them, right? And so its an axiomatic thing. And a big strength of that approach is that we all have at least some intuition for what goes on in r2 and r3, right, in Euclidean space, in the plane, and things like that. ts harder when you have infinite dimensional spaces, or even four or five dimensional spaces, its harder to figure out whats going on, like. But a lot of the geometric intuition still applies. Okay, so were thinking of this as a random variable, which, remember, formally speaking, a random variable is a function, but we are treating that function as if its just a point or a vector, okay. And then in our picture, m gonna draw a plane, and its not literally a plane, but were just visualizing it as a plane. This plane consists of all possible functions of x. o its a collection of random variables. ts a plane through the origin. ts not really a plane, but it goes through the origin because one function of X is just zero. o every constant is contained in here. And X is in here somewhere, and X squared is in here, and E to the X is in here. Any function of X, thats this plane. Okay, now what were doing geometrically when we do conditional expectation is a projection. o were taking Y and were going, project it down into the plane, E (Y|X). E (Y|X) is a function of X, right. keep emphasizing that. o E (Y|X) is in this plane. E(Y|X) is defined to be the point in this plane thats closest to Y, okay? o that tells us why is it true, if Y is already function of X, then E(Y|X) = Y. Because that says if its already in the plane, you dont need to project it anywhere. But if Y is not already a function of X, then youre projecting it down to whatever function of X is closest in a certain sense. The inner product of X,Y is E(X,Y). Thats just for those of you whove seen inner products before, thats what it is. The inner product is just a fancy word for a dot product, right? Youve all seen dot products in L and for r2 and r3 and thats just the generalization of that concept. You can check that this has the properties of an inner product. The only assumption here is that were working with functions of X. All our random variables we want to assume have finite variance. o implicitly assuming finite variance in this picture. Okay, so anyway, we project Y to E(Y) given X, and then just thinking geometrically if we want this residual vector, YE(Y|X) thats just gonna be a vector like this, right, the vector from here to here. Okay, and that, just from projection, you know how if have a point above this table, and wanna project it down, m gonna go perpendicularly down til hit the table, right? o thats perpendicular. o all this is saying, all number (4) says in this picture is that this vector from here to here is perpendicular to the plane, right? o take any function of X and this residual is gonna be perpendicular. o thats what it says geometrically. And lets see, what does this statement say? E(E(Y|X)). o its just, This is a function of X and were taking its average and we say we got E(Y). ll talk more about the intuition for this one later when we also get to the version of this for variance. But first, lets prove number (4), assuming number (3). Then well prove number (3). o thats just a picture to keep in mind, okay? Thats not a proof, though, so we still need to prove these things. o lets just calculate that. That proof of (4), just wanna calculate it and see if we get 0. Hopefully, we will. Okay, proof of (4). o lets just take this thing and use linearity. o m not gonna rewrite that whole expression, but m just gonna use linearity. m just gonna look at what m gonna distribute, this time this minus this time this. o its E(Yh(x)) E(Y|x)h(X)). m just rewriting the same thing except splitting it into two terms using linearity. Okay, now lets just try to see what could we do with this to try to simplify it. This looks as simple as we can get it. o just leave that alone. Lets try to simplify this part. E of something. Well, kind of like really wanna apply Adams law, cuz have this E(E), but then theres like this h(X) here, okay? o cant directly apply it. o what do you think we should do with h(X)? Here we know X, so lets actually put it back. o thats called putting back whats known, but it follows from taking out whats known that you can put back whats known, Right, were treating that as known so can write it here, write it there, its fine okay? o now so we have E(E(h(x)Y|X)), right? Now its exactly, well, should have put it back over here, right, move it over there. That should be Y times h(x). o thats of the form where we can apply Adams law now, right? o thats E(Yh(x)) E(Yh(x)) = 0. Okay, so its really just linearity, taking out whats known or putting back whats known, and iterated expectation. That proves property (4), assuming iterated expectation. Okay, so now we really need to check that this iterated expectation formula is correct. Okay, so lets do that. Okay, so just to simplify notation, lets do it in the discrete case. ontinuous case is analogous. Proof of (3) discrete case just to simplify our notation And lets let, o were trying to find E of E of Y given X, so lets give it a name, g of X, okay? o were gonna let E of Y given X. Remember, its a function of X as keep saying, so we may as well give it a name, g of X. o really all were trying to do here, thats just E of g of X, thats all were trying to do, okay. o we need to find the E of g of X, and show that that just reduces to E of Y, right. Okay, so lets just do that, E of g of X, well, weve dealt with things that looks like E of g of X many times before already, just LOT, right. o lets just write down discrete LOT, in the continuous case, we could write down continuous LOT. o by LOT, thats just the sum over X of, g of X times the probability that X = little x. Now, lets write down whats g of little x. g of little x is E of Y given, this is how we define conditional expectation, s that, We started by conditioning on big X = little x, call that g of X. Then we changed little x to big X to get g of capital X. o thats just what g of little x is. Okay, so just used the definition. o far all ve done is used LOT, used the definition. Now, again, lets just use the definition of this, okay? uz dont like memorizing proofs or anything, and dont remember how to do this. o all m gonna do is just plug into the definition and hope it works, okay? o, lets just see, whats the definition of this thing, E of Y given X = x? Well, again, dont like memorizing definitions any more than like memorizing proofs, but know the definition of expectation, and then conditional just means make it conditional. o m just gonna write down that the definition, just y. Right, if were unconditional, we would just do P of Y = y here, right? That would just be E of Y, but its conditional, so we just put given X = x. All right, and then we have this probability X = x here, Which is outside of that sum. But actually, if we want, we can bring it inside of the sum, okay? o, because this depends on X and were summing over Y. o somehow, we have to reduce this down to, o if we want, we can bring this in here because this is a function of X and were summing over Y. o somehow were hoping that this will reduce down to just the expected value of Y. o somehow we have to get rid of all of these xs here somehow have to go away. o a very, very common trick when were dealing with a double sum or a double integral is to swap the order of summation or swap the order of integration, okay? Especially in the discrete case that as long as the sum converge absolutely, its a completely valid thing to do. You just rearranging the whole a + b is b + x. o m gonna add up the same thing in a different order. o m just gonna say sum over y first and then sum over x. Thats the same thing as summing over x and then summing over y. Were just rearranging terms. Were just adding in a different order. Okay, and then, thats y p times y = y. Let me write it this way, X = x. can write it this way because remember thats the joint PF. But remember the joint PF is the conditional PF times the marginal PF, thats the marginal, right? Thats the marginal PF of X, thats the condition PF of Y given X. o we multiply this thing times this thing, thats just the joint PF, so we may as well write it that way. Okay, so, Now, notice since swapped the order of summation, something good happens, which is that this y doesnt depend on x, so we can pull this y out. o that y goes right there, okay? o thats the sum over y of y. o just imagine pulling this y out and lets just stare at this sum here. We have the joint PF and were summing up over all x. Well, thats exactly how we got the marginal, right? To get a marginal from a joint, we just took the joint PF and we sum up over X, well get the marginal of Y, if we sum over Y, well get the marginal of X. n this case, were summing up over X. Just like remember those 2x2 tables we were drawing? Just add up a row or add up a column we get to get the marginal? o were summing up over x, that gives us the marginal distribution of Y. o therefore, by definition thats just E of Y. o really the only trick here was to write this as a double sum and then swap the order of summation which is often a useful trick and proving things. Other than that, would just plugged into LOT, plugged into the definition. And just used whats a conditional distribution and marginal and joint distribution which we talked about before, okay? o that is the proof of this property, and want to do some more examples. First, one more, One more definition of a conditional thing. o definition of conditional variance, cuz we have conditional expectation, and think this would be a good time to get to conditional variance. ts defined analogously. o the variance of Y given X. Lets just try to think intuitively. Either we could write it as E of Y squared. sually the way we do variances, E of Y squared the square of E of Y. o lets just write down the same thing, except make everything given x, right. Because this says, we get to treat X as known, given that information, whats the variance of Y, okay? o a natural thing to do would be E of Y squared given x E of Y given X squared, Which is correct. But remember we also define variance a different way, that was the expected square difference from the mean. o lets try to also write that down that definition. f we define it the other way, we did like E of Y E of Y. f it was just unconditional, we would do Y its mean and square that thing. However, were trying to make it conditional, so were gonna put Y , we get to treat X as known, so instead of E of Y, were using E of Y given X to make it conditional, and were squaring this thing. Now if just close the parentheses here, that would be wrong. And you could immediately see that would be wrong just by thinking about what kind of object this is. f just put closed parentheses here, then thats just gonna be a number cuz this is random variable. Taking its expected out, well get a number. But actually this equation or just this expression makes it clear that variance in y given x should be a function of x. As were treating x as known, okay? And then whats the variance of y as a function of x. That means we need another given Xhere. o all this is saying is like throughout this problem, everything is given x. o we cant forget one of the given xs, everything is based on the assumption that we know x. Okay, so just wrote that these two things are equal, we didnt prove that these are equal. We kind of hoped that there will be something kind of strange if they were not equal. uz intuitively, were just doing variance except everything is given x. o it should work out, but it should still be checked. ts good practice, so ll probably put this on the next strategic practice cuz it is good practice to check that this equals this. Just practice with, because at this point we reduced it to conditional expectation, so you an just use the properties of conditional expectation, all right? o thats variance and then, okay, now we have one more property, property 5, ll write it up there, easier to see. These are four properties of conditional expectation, and it would be sad not to get at least one property of conditional variance. o the property is that the variance of y equals the expected value of the conditional variance, plus the variance of the conditional expectation. o its a pretty coollooking formula, right? This is the unconditional variance of Y. And somehow we want to like, so imagine were trying, we have this quantity y and we want its variance and we dont know how to get it. o we kinda want to do some condition on something to make the problem easier. o the condition on x, but then should we do the variance of y given x first? Or should we take the conditional expectation and then take the variance of that? Not so obvious, right? This says two ways you could do this, add them together. This property is called Eves Law because its EVVE. That actually should be EVVE but we abbreviate to Eve, Eves law, okay? o, that explains some of the etymology here for Adams law. Especially when you see the proof of Eves law, which is also very very good practice to prove this. o, m going to put this on the next strategic practice, too. You should try it yourself first, then you can study the proof that will put in the strategic practice. Let me explain the intuition of this a little bit and then, well do an example of how to use Adams law and Eves law together to get the mean and variance. o heres kind of an intuitive picture. magine we have different groups of people, okay? Just to have a simple picture in mind, lets say we have three groups, okay? And then theres lots of people inside each group. And then, just have a concrete example in mind, maybe think of y as height, so you have some population of people which consists of three subpopulations. You wanna know the mean and the variance for the heights of people in this population, or make up your own example. o these are the three subpopulations and each subpopulation may have its own mean and variance, right? But you want the overall, okay? o its kinda hard to think about this entire population all at once. ts much easier to think about each subpopulation, all right? o notice that there are two types of variability going on. One is that different subpopulations may have differences in height, right? o we have differences between populations, then you have variability within each population, right? o within each population, unless everyone in that subpopulation is the same height, you have variability within each of these and you have variability between them, okay? o if you want, m thinking of x in this case, itll be like, this wold be x = 1, x = 2, x = 3. o x takes three values, x just says which, if you take a random person from this population, which subpopulation are they in, okay? o that would be the x. o if we do e of y given x equals 1, that was just be the mean for this population, right? o really, what this says is, this term here is saying look at the average within each population. And then take the average, take the variance of those numbers, right? o thats really looking between populations. And this is saying look within each population, this ones within, this ones between. This says look within each population, take its variance and then average those numbers. This one says look, Replace each population by just its average height and take the variance of those, okay? o it is pretty intuitive that there are those two types of variabilities, but whats kind of cool is that this just says you can just add them, right? ntuitively theres two types of variability, but its pretty nice that to get the overall variance, you just add those two things. t sounds too good to be true, but thats how it works. Okay, so let me do an example. All right, so imagine that were studying prevalence of a certain disease and the fraction of people, so you have some country, or lets say some state that consists of different cities, and different cities have different prevalences of the disease, okay? o, this is just an example. o, suppose that you pick a sample of basically, here is how you do your sampling. ometimes this is done in practice cuz like, guess if youve been studying the entire state ideally ,maybe you would get a simple random sample, of people from the state and you want to see how many of them have the disease. Or maybe rather than using simple random sample you stratify in certain ways and so on. They can get into it in a sampling class which is not our topic here. But sometimes for practical or other reasons the way that these kind of things work is, you pick a random city, right? And then go into the city, collect the sample from the city, its easier, right? And then we wanna make some conclusions. All right, so just to try to formalize that, what m saying is we pick a random city in some state. And then pick random sample of people in that city. Of people in that city, And then, you test each of those people for the disease that youre studying. Lets say this is a random sample consisting of n people. o n is our sample size which we treat as fixed. Okay, so pick a random city, then go to the city, get n people, test them all for the disease and let X equal number of people with the disease in the sample. And lets let Q = dp, true proportion of people infected in the random city. Proportion of people in the random city. o that is once weve selected the random city, then Q is just literally how many people in that city have the disease divided by the number of people in that city. But m using a capital letter cuz initially its a random variable, because different cities have different prevalences of the disease, and were picking a random city, so thats a random variable. o you can think of this as a random probability, right? uz this is gonna be a number between 0 and 1, which is the proportion of people who have the disease, but the city is random. Okay, so who have the disease. By city with the disease, mean people with the disease, not the cities with the disease. o there are a lot of questions we could, this is a pretty general. o you can see how this kind of setup has lots of applications in epidemiology. But it doesnt have to be disease. t could be political opinions or whatever you want. ts just its just that we have. That is its similar to what was just saying here. n that were assuming that we have variability between different cities have different political opinions or different disease characteristics. And within each city theres also variation, right? o we have those two types of variation, how do we deal with that? Okay, so there are lots of things we could ask about this setup. But for right now, lets just find the mean and the variance of X. To do that, though we need some assumption about the distribution of Q, okay. o the most commonly used choice in practice would be to pick a beta distribution. o were gonna assume that Q is Beta a, b where a and b are known. Because a Beta as we were just saying when we were doing the Beta its a very flexible family. t takes continuous values between 0 and 1. We know Q has to be between 0 and 1. And we know the Beta is a conjugate prior for the binomial, so it has a lot of nice properties. o its mathematically convenient but its also a pretty flexible family to work with. By playing around with a and b, you could get a variety of distributions that hopefully would accurately reflect what you wanna do, as far as what the distribution of Q is like. o well assume a beta. You can assume something else if you want and then do a similar calculation but the Beta would be the most popular choice here, and also happens to be convenient, okay. o now, so thats Q. Were also implicitly, man its basically set in words here, but were implicitly assuming x given Q is binomial n, Q. That is once we know that the true value of what proportion of people in that city have the disease, then were doing binomial. Hypergeometric might be a little bit better. But were either assumed sampling with replacement or that the n is small enough compared to the population size that it is essentially binomial, okay. o now were all set to find what we want. E of x, so its very, very natural here to use conditional expectation, right? uz the whole problem was set up in a way where conditional on which city youre in. Then we have a good sense of whats going on unconditionally then you have to kinda combine all these different cities thats harder to think about. ts easier if you kind of zoom in on one city first. o that suggests, okay, just condition on Q. o this is gonna be E of E of X given Q. E of X given Q, well given Q, we just have a binomial n, Q, expected value of binomial of n, Q will be n, Q. o thats just E(nQ), n is just a constant. o then its just na/(a+b) because a Beta ab has expected value a/(a+b). o its a quick calculation at that point once you condition. All right, so now lets do the variants. o again, were gonna do this by thinking conditionally. We have a between cities and a within city term. Eaves Law says this is the expected value of the variants of X given Q. Plus the variance of the expected value of X given Q. Then we just have to work out what are these two things Okay, so for the first term, the variance of X given Q, of given Q is just a binomial, and we know that this, if we treat Q as a constant, this has variance, n, Q, 1Q, right? And the n, so lets just write that down. o its just the expected value of the variance of X given Q is just nQ 1 Q. And then, For the other term, the variance of X given Q, just get that from the binomial, all right. For the other term, if the variance of E of X given Q, well, we just said that E of X given Q, is n,Q. o just want the variance of n,Q. N comes out as a squared, so just give me n squared times the variance of Q. Now we just have to compute those two things. And for the Beta distribution, those things are actually both pretty easy because when we see this Q(1Q), that kinda reminds you of what the Beta looks like, right? o to compute those two things, Lets just do those on the side, we just need to compute those two quantities and then were done. o we need to know the expected value, the end comes out here, so all we need is E of Q(1Q). Okay, so lets just do that, just for a quick practice with a Beta and with LOT. Well, one thing we could do is just say this is E of Q E of Q squared. And we already know E of Q and we could get E of Q squared. That parts really fine. But lets just do it directly using LOT. o m just gonna write down, LOT, right? o this is just Q. m going to change capital Q to lower case q. Q(1Q), and then we integrate the beta density. And lets just simplify the beta density has Q to the a1, but we are multiplying by q. o now its q to the a. And then has a 1Q to the b1, but we also have this 1Q, so that becomes to the b. Dq times the normalizing constant of the beta, which is gamma a plus b over gamma of a, gamma of b. Well, it looks like this complicated thing until you realize that is just another beta integral, right? o, this is just gamma of a+b over gamma a gamma b. And then we just have to multiply and divide by whatever we need in order to make this exactly the integral of a beta PDF. o m just imagining putting in the normalizing constant of the beta, which in this case is a beta of a+1, b+1, okay. o this is gonna be gamma of a+1, gamma of b+1, divided by gamma of a+b+2. Times one because m multiplying and dividing by this thing so that this is exactly integral of the beta density. And then, right? Okay, so now lets just simplify this thing that looks ugly with the gammas. But hopefully we can simplify it. f we remember the fact that gamma of x+1=x gamma of x, just use that fact a bunch of times. o gamma of a+1 is a gamma of a. o this gamma of a is going to cancel. Thats b gamma of b. o theres going to be a b there and then, in the denominator, gamma of a+b. Gamma of a+b+2 is a+b+1 times gamma of a+b+1. But gamma of a+b+1 is a plus b gamma of a+b. o that cancels. o we just get this expression in terms of a and b. And similarly, you can get the variance of q. Kind of the nice way to write the variance of a beta is mu, 1mu over a+b+1 where mu is the mean of the beta, so mu = a over a+b. You check this in exactly the same way, okay? o then were done with, mean, you can do some algebra to simplify, if just plug those things in, and then thats the answer. Okay, so thats all for today.","Eaves Law says this is the expected value of the variants of X given Q. Plus the variance of the expected value of X given Q. Then we just have to work out what are these two things Okay, so for the first term, the variance of X given Q, of given Q is just a binomial, and we know that this, if we treat Q as a constant, this has variance, n, Q, 1Q, right? That would just be E of Y, but its conditional, so we just put given X = x. All right, and then we have this probability X = x here, Which is outside of that sum. o its just the expected value of the variance of X given Q is just nQ 1 Q. And then, For the other term, the variance of X given Q, just get that from the binomial, all right. And lets just simplify the beta density has Q to the a1, but we are multiplying by q. o now its q to the a. And then has a 1Q to the b1, but we also have this 1Q, so that becomes to the b. Dq times the normalizing constant of the beta, which is gamma a plus b over gamma of a, gamma of b. Well, it looks like this complicated thing until you realize that is just another beta integral, right? But for right now, lets just find the mean and the variance of X. To do that, though we need some assumption about the distribution of Q, okay. Lets say there, thats y. Okay, and the question is then, whats the length of this piece, right, or the distribution, or the conditional expectation, that kind of thing. And that one is that if we take Y minus E of Y given X, thats a natural thing to look at because were thinking of this as the prediction, that is were using X to predict Y. E of Y given X is our prediction. This is the unconditional variance of Y. And somehow we want to like, so imagine were trying, we have this quantity y and we want its variance and we dont know how to get it. For the other term, if the variance of E of X given Q, well, we just said that E of X given Q, is n,Q. o that suggests, okay, just condition on Q. o this is gonna be E of E of X given Q. E of X given Q, well given Q, we just have a binomial n, Q, expected value of binomial of n, Q will be n, Q. o thats just E(nQ), n is just a constant. o that would be the x. o if we do e of y given x equals 1, that was just be the mean for this population, right? o its says we want the expected value of Y, but we dont know how to do it, we can try to cleverly choose X to make the problem simpler, where E of Y given X is simpler to work with, and then take the expected value of that. o m just gonna write down that the definition, just y. Right, if were unconditional, we would just do P of Y = y here, right? Well, one thing we could do is just say this is E of Q E of Q squared. o that just becomes h of X, E of Y, given X. o thats really what we implicitly were doing up here, just took out the X squared, and were left with a 1 inside, the expected value of 1 given anything is 1, cuz its always 1, okay? ince if we know, if we get to observe that in fact X squared = a, so were treating it as knowns so m just going to call it a, that we get to know a, then we know that x is plus or minus the square root of a, but by symmetry those are equally likely. o by LOT, thats just the sum over X of, g of X times the probability that X = little x. Now, lets write down whats g of little x. g of little x is E of Y given, this is how we define conditional expectation, s that, We started by conditioning on big X = little x, call that g of X. Then we changed little x to big X to get g of capital X. o thats just what g of little x is. o were gonna let E of Y given X. Remember, its a function of X as keep saying, so we may as well give it a name, g of X. o really all were trying to do here, thats just E of g of X, thats all were trying to do, okay. o in other words what that shows is that this is actually the covariance of this and this and it says its 0. o, because this depends on X and were summing over Y. o somehow, we have to reduce this down to, o if we want, we can bring this in here because this is a function of X and were summing over Y. o somehow were hoping that this will reduce down to just the expected value of Y. o somehow we have to get rid of all of these xs here somehow have to go away. o we need to find the E of g of X, and show that that just reduces to E of Y, right. Third one is the one we we just stated, E of E of Y given X equal E of Y. o we have the conditional expectation. E(Y given X) is a random variable and its a function of capital X. And lets just quickly see what happens if we then take, so this is a random variable, we can then ask, whats its expectation? And then the statement is that if we multiply this by any function of X, youll always get 0, h is any function of x, h of x is any function of x. n words, this says that, This thing, Y minus E of Y, given X, which in statistics is called a residual. o we just get this expression in terms of a and b. And similarly, you can get the variance of q. Kind of the nice way to write the variance of a beta is mu, 1mu over a+b+1 where mu is the mean of the beta, so mu = a over a+b. f we have E of, lets say, each of X times Y, given X. Now we know that if we have a constant in front, we can take it out. o now, so thats Q. Were also implicitly, man its basically set in words here, but were implicitly assuming x given Q is binomial n, Q. That is once we know that the true value of what proportion of people in that city have the disease, then were doing binomial. Thats just clear from the definition, right, because the definition of this says that we work with the conditional distribution given X. The conditional distribution of Y given X is no different from the unconditional one, because theyre independent. o all this is saying, all number (4) says in this picture is that this vector from here to here is perpendicular to the plane, right? Just practice with, because at this point we reduced it to conditional expectation, so you an just use the properties of conditional expectation, all right? ts just whats left over after you try to predict Y and then the difference is uncorrelated with any function of X. Because if we computed the covariance of these two things, ll just write out the covariance, Y minus E of Y, given X, just the definition of covariance. uz this is gonna be a number between 0 and 1, which is the proportion of people who have the disease, but the city is random. Thats E of Y minus E of Y, so this part is just 0. o we only have this term. We just saw an example over there, where E of X given X squared is zero, we now that theyre not independent, but if they are independent then we can just get rid of the condition. Okay, so we use that a lot to simplify when we see a function of X there and were conditioning on X. We can take it out. Thats the marginal PF of X, thats the condition PF of Y given X. o we multiply this thing times this thing, thats just the joint PF, so we may as well write it that way. o, lets just see, whats the definition of this thing, E of Y given X = x? f we define it the other way, we did like E of Y E of Y. f it was just unconditional, we would do Y its mean and square that thing. o the property is that the variance of y equals the expected value of the conditional variance, plus the variance of the conditional expectation. o, this is just gamma of a+b over gamma a gamma b. And then we just have to multiply and divide by whatever we need in order to make this exactly the integral of a beta PDF. Because this says, we get to treat X as known, given that information, whats the variance of Y, okay? But what this notation means, its just shorthand for saying that if we know that big X equals little x, then its going to be uniform between 0 and little x. And this is just short hand for that. o its just, This is a function of X and were taking its average and we say we got E(Y). o all this is saying is like throughout this problem, everything is given x. o we cant forget one of the given xs, everything is based on the assumption that we know x. Okay, so just wrote that these two things are equal, we didnt prove that these are equal. o it is pretty intuitive that there are those two types of variabilities, but whats kind of cool is that this just says you can just add them, right? Okay, so lets just do that, E of g of X, well, weve dealt with things that looks like E of g of X many times before already, just LOT, right. Or should we take the conditional expectation and then take the variance of that? sually the way we do variances, E of Y squared the square of E of Y. o lets just write down the same thing, except make everything given x, right. And the n, so lets just write that down. o that is the proof of this property, and want to do some more examples. But you can always think of it back in terms of conditioning on big X equals little x. o thats just short hand saying if we get to treat x as known, then were picking a random point from here to here. However, were trying to make it conditional, so were gonna put Y , we get to treat X as known, so instead of E of Y, were using E of Y given X to make it conditional, and were squaring this thing. Okay, so anyway, we project Y to E(Y) given X, and then just thinking geometrically if we want this residual vector, YE(Y|X) thats just gonna be a vector like this, right, the vector from here to here. Proof of (3) discrete case just to simplify our notation And lets let, o were trying to find E of E of Y given X, so lets give it a name, g of X, okay? o the condition on x, but then should we do the variance of y given x first? This is just 0 and you can do some big calculation, but you shouldnt have to cuz you just think about whats the conditional distribution.",0.1236979166666666
65,606,"n the last video we defined a transformation that rotated any vector in R2 and just gave us another rotated version of that vector in R2. n this video, m essentially going to extend this, so m going to do it in R3. o m going to define a rotation transformation. ll still call it theta. Theres going to be a mapping this time from R3 to R3. As you can imagine, the idea of a rotation in an angle becomes a little bit more complicated when were dealing in three dimensions. o in this case were going to rotate around the xaxis, let me call it so this is going to rotate around the xaxis. And what we do in this video, you can then just generalize that to other axes. And if you want to rotate around the xaxis, and then the yaxis, and then the zaxis by different angles, you can just apply the transformations one after another. And were going to cover that in a lot more detail in a future video. But this should kind of give you the tools to show you that this idea that we learned in the previous video is actually generalizeable to multiple dimensions, and especially three dimensions. o let me just be clear, what were going to be doing here. Let me draw some axes. Thats my xaxis. That is my yaxis. And this is my zaxis. Of course, this is R3. But any vector here in R3 will be rotating it counterclockwise around the xaxis. Well be rotating like that. o if had a vector m just drawing it in the zy plane because its a little bit easier to visualize but if have a vector sitting here in the zy plane, it will still stay in the zy plane. But itll be rotated counterclockwise by an angle of theta, just like that. Now, a little harder to visualize is a vector that doesnt just sit in the zy plane. f we have some vector that has some xcomponent that comes out like that, then some ycomponent and some zcomponent, it looks like that. Then when you rotate it, its z and its ycomponents will change, but its xcomponent will stay the same. o then it might look something like this. Let me see if can give it justice. o then the vector when rotate it around might look something like that. Anyway, dont know if m giving it proper justice but this was rotated around the xaxis. think you understand what that means. But just based on the last video, we want to build a transformation. Let me call this rotation 3 theta. Or let me call it 3 rotation theta now that were dealing in R3. And what we want to do is we want to find some matrix, so can write my 3 rotation sub theta transformation of x as being some matrix A times the vector x. ince this is a transformation from R3 to R3 this is of course going to be a 3 by 3 matrix. Now in the last video we learned that to figure this out, you just have to apply the transformation essentially to the identity matrix. o what we do is we start off with the identity matrix in R3, which is just going to be a 3 by 3. ts going to have 1, 1, 1, 0, 0, 0, 0, 0, 0. Each of these columns are the basis vectors for R3. Thats e1, e2, e3 m writing it probably too small for you to see but each of these are the basis vectors for R3. And what we need to do is just apply the transformation to each of these basis vectors in R3. o our matrix A will look like this. Our matrix A is going to be a 3 by 3 matrix. Where the first column is going to be our transformation, 3 rotation sub theta, applied to that column vector right there, 1, 0, 0. And then m going to apply it to this middle column vector right here. You get the idea, dont want to write that whole thing again. m going to apply 3 rotation sub theta to 0, 1, 0. And then m going to apply it ll do it here 3 rotation sub theta. m going to apply it to this last column vector, so 0, 0, 1. Weve seen this multiple times. o lets apply it. Lets rotate each of these basis vectors for R3. Lets rotate them around the xaxis. o the first guy, if were to draw an R3, what would he look like? He only has directionality in the x direction right? f we call this the xdimension, if the first entry corresponds to our xdimension, the second entry corresponds to our ydimension. And the third entry corresponds to our zdimension. This vector would just be a unit vector that just comes out like that, right? o if m going to rotate this vector around the xaxis, whats going to happen to it? Well, nothing. t is the xaxis. o when you rotate it, its not changing its direction or its magnitude or anything. o this vector right here is just going to be the vector 1, 0, 0. Nothing happens when you rotate it. Now these are a little bit more interesting. To do these, let me just draw my zyaxis. Let me just draw my Z. o thats my zaxis and this is my yaxis right here. Now this basis vector just goes in the y direction by 1. o this basis vector just looks like that. And it has a length of 1. And then when you rotate it around the xaxis, when draw it like this, you could imagine the xaxis is just popping out of your computer screens. o could draw it like this is like the tip of the arrow just popping out. nstead of drawing it at an angle like this, m drawing it straight out of the computer screen. o if you were to rotate this vector right here, this blue vector right here, by an angle of theta, itll look like this. And weve done this in the previous video. What are its new coordinates? First of all, will its xcoordinate have changed it all? ts xcoordinate was 0 before, because it doesnt break out into the xdimension. t just stays along the zy plane. t was 0 before. When you rotate it, its still on a zy plane. o its x direction, or its xcomponent, wont change at all. o the x direction is still going to be 0. And then whats its new y direction? Well, here we do exactly what we did in the last video. We figure out this is going to be its new guess dont want to draw a vector there necessarily but this length right here is going to be its new ycomponent. And this length right here is going to be its new zcomponent. o whats its new ycomponent? We did this in the last video so wont go into as much detail, but what is cosine of theta? The length of this vector is 1, right? These are the standard basis vectors. And one of the things that makes them a nice standard basis vector is that their lengths are 1. o we know that the cosine of this angle is equal to the adjacent side over the hypotenuse. The adjacent side is this right here. And what is the hypotenuse? ts equal to 1. o this adjacent side, which we said is going to be our new second component, our second entry, is going to be equal to cosine of theta, right? Thats A. You can just ignore the 1s. This going to be equal to cosine of theta. And whats going to be its new zcomponent? Well, sine of theta is equal to the opposite side, this side over 1. o it just equals its opposite side. And the length of that opposite side is this vectors, once its rotated, is its new zcomponent. o youve got a sine theta right there. Now we just have to do everything in the z direction. o this z basis vector right there, what does it look like on this graph? Let me just actually redraw it just to make things a little bit cleaner. o thats my zaxis and this is my yaxis. And my zbasis vector e3, it starts off looking something like that. t just goes only in the z direction. o first of all, lets just rotate it by an angle of theta. o m going to rotate it like that. Thats an angle of theta. ts former x entry was 0. t did not break out in the x direction at all. And of course were still just in the zy plane so it wont be moving out in the x direction. o its still going to be a 0 up here. Now what about its new ycomponent? ts new ycoordinate, guess we can call it, is going to be this length, or its going to be this coordinate right here. And how can we figure that out? Well, that length is the same thing as that length. And if we call this the opposite side of the angle, we know that the sine of theta is equal to this opposite side over the length of this vector, which is just 1. o its just equal to the opposite side. o the opposite side is equal to sine of theta. But our new coordinate is to the left of the zaxis, so this is going to be a negative version. We did this in the last video. o its just going to be a negative sine of theta. This point right here, that coordinate. o its minus sine of theta. And then finally, whats its new zcoordinate going to be? Thats going to be this length right here. And we know that this length, if we call that adjacent, we know that the cosine of our theta is equal to this divided by 1. o its equal to that adjacent side, so just put a cosine of theta right there. And we get our transformation matrix. Were done. Our transformation matrix A is this. o we can now say our new transformation that this video is about. call it a 3 because its a rotation in R3. aybe should call it 3 sub X because its a rotation around the xaxis, but think you get the idea. t is equal to this matrix right up here maybe could rewrite it. Let me do it this way. Let me delete all of this so dont have to rewrite. o my transformation that this videos is about, 3 rotation theta of x, that transformation is equal to this matrix times whatever vector x have in R3. And you might say, hey, al, that looks exactly like what you did in the second. f you remember the last video when we defined our rotation in R2, we had a transformation matrix that looked very similar to this. And that makes sense because were essentially just rotating things counterclockwise in the zy plane. Now you might say, hey, al, why is this even useful? You extended it to three dimensions or to R3, saw what you did in R2. Why is this useful? ts kind of a limited case where youre just rotating around the xaxis. And did it for two reasons. One to show you that you can generalize to R3. But the other thing is, if you think about it, a lot of the rotations that you might want to do in R3 can be described by a rotation around the xaxis first which we did in this video then by rotation around the yaxis and then maybe some rotation around the zaxis. This is just a special case where were dealing with rotation around the xaxis. But you could do the exact same process to define transformation matrices for rotations around the yaxis or the zaxis, and then you can apply them one after another. And well talk a lot about that in the future when we start applying one transformation after the other. But anyway, hopefully you found this slightly useful. ts a slight extension of what we did in R2.","o this vector right here is just going to be the vector 1, 0, 0. And if we call this the opposite side of the angle, we know that the sine of theta is equal to this opposite side over the length of this vector, which is just 1. o its just equal to the opposite side. o in this case were going to rotate around the xaxis, let me call it so this is going to rotate around the xaxis. But the other thing is, if you think about it, a lot of the rotations that you might want to do in R3 can be described by a rotation around the xaxis first which we did in this video then by rotation around the yaxis and then maybe some rotation around the zaxis. We figure out this is going to be its new guess dont want to draw a vector there necessarily but this length right here is going to be its new ycomponent. And what we want to do is we want to find some matrix, so can write my 3 rotation sub theta transformation of x as being some matrix A times the vector x. ince this is a transformation from R3 to R3 this is of course going to be a 3 by 3 matrix. ts new ycoordinate, guess we can call it, is going to be this length, or its going to be this coordinate right here. And this length right here is going to be its new zcomponent. Now in the last video we learned that to figure this out, you just have to apply the transformation essentially to the identity matrix. And we know that this length, if we call that adjacent, we know that the cosine of our theta is equal to this divided by 1. o its equal to that adjacent side, so just put a cosine of theta right there. And what we need to do is just apply the transformation to each of these basis vectors in R3. o if m going to rotate this vector around the xaxis, whats going to happen to it? o my transformation that this videos is about, 3 rotation theta of x, that transformation is equal to this matrix times whatever vector x have in R3. o what we do is we start off with the identity matrix in R3, which is just going to be a 3 by 3. ts going to have 1, 1, 1, 0, 0, 0, 0, 0, 0. But our new coordinate is to the left of the zaxis, so this is going to be a negative version. The length of this vector is 1, right? And one of the things that makes them a nice standard basis vector is that their lengths are 1. o we know that the cosine of this angle is equal to the adjacent side over the hypotenuse. o if you were to rotate this vector right here, this blue vector right here, by an angle of theta, itll look like this. ts equal to 1. o this adjacent side, which we said is going to be our new second component, our second entry, is going to be equal to cosine of theta, right? This going to be equal to cosine of theta. o m going to rotate it like that. And then m going to apply it to this middle column vector right here. And then when you rotate it around the xaxis, when draw it like this, you could imagine the xaxis is just popping out of your computer screens. n this video, m essentially going to extend this, so m going to do it in R3. o the x direction is still going to be 0. And the length of that opposite side is this vectors, once its rotated, is its new zcomponent. And what we do in this video, you can then just generalize that to other axes. Thats going to be this length right here. And if you want to rotate around the xaxis, and then the yaxis, and then the zaxis by different angles, you can just apply the transformations one after another. And of course were still just in the zy plane so it wont be moving out in the x direction. Where the first column is going to be our transformation, 3 rotation sub theta, applied to that column vector right there, 1, 0, 0. And then m going to apply it ll do it here 3 rotation sub theta. o its just going to be a negative sine of theta.",0.314453125
66,607,"n our previous lesson, we talked about level order traversal of binary tree which is basically breadthfirst traversal. Now in this lesson we are going to discuss these three depthfirst algorithms preorder, inorder and postorder. have drawn a binary tree here datatype filled in the nodes is character now as we had discussed in earlier lessons, in depthfirst traversal of binary tree if we go in one direction then we visit all the nodes in that direction or in other words we visit the complete subtree in that direction and then only we go in other direction. n this example tree that i have drawn here if m at root and m going left then ll visit all the nodes in this left subtree and then only can go right and once again when ll go right ll visit all the nodes in this right subtree if you can see in this approach we are reducing the problem in a selfsimilar or recursive mannner, we can say that in total visiting all the nodes in the tree is visiting the root node visiting the left subtree and visiting the right subtree remember by visiting a node we mean reading or processing the data in that node and by visiting a subtree we mean visiting all the nodes in the subtree indepth first strategy relative order of visiting the left subtree, right subtree and the root can be different, for example we can first visit the right subtree then the root and then the left subtree or we can first visit the root and then left subtree and then the right subtree conventionally left subtree is always visited before right subtree we this constraint we will have three permutations, we can first visit the root and then the left subtree and then the right subtree and such a traversal will be called preorder traversal or we can first visit the left subtree then the root and then the right subtree and such a traversal will be called inorder traversal and we can also go leftright and then root and such a traversal will be called post order traversal, left and right subtree will be visited recursively in same manner as the original tree, so in preorder once again for the subtrees we will go root, left and then right in inorder will keep going left, root and then right. The actual implementation of these algorithms really easy and intuitive lets first see code for preorder traversal. first written the algorithm in words here, in preorder traversal we first need to visit the root and then left subtree and then the right subtree now want to write a function that should take pointer or reference to root node as argument and print data in all the nodes in preorder lets say visiting a node for us is printing the data in that node in or ++ my method signature will look something like this: this function will take address of the root node as argument, argument type is pointer to Node. ll define node as a structure with three fields like this, data type in this definition is character and there are two fields to store the addresses of left and right children 55 00:03:15,349 00:03:20,609 now in preorder function ll first visit or print the data in root node and now ill make a recursive call to visit the left subtree have made a recursive call here and to this call m passing address of the left child of my current root because left child will be the root of left subtree and ll have another call like this to visit the right subtree there is one more thing that we need to add in this function and we will be done we cannot go into recursion infinitely, we need to have a base condition where we should exit if a tree or a subtree is empty or in other words for any call if root is null we can return or exit. Now with this much of code m done with my preorder function. this will work fine in or ++ actually in make sure you right struct space node instead of righting just node, rest of the things are fine it will be good to visualize this recursion so lets now quickly see how this preorder function will work if this example tree that m showing in right here is pass to it ll redraw this tree and show it like this hear m depicting node as a structure with three fields lets say the leftmost cell here is to store the address of left child the cell in middle is to store the data and the right most cell is to store the address of right child Now lets assume some addresses for these nodes, lets say the root node is at address 200 and ll assume some random addresses for other notes as well and now can fill in left and right fields for each node and as we know the identity of tree that we always keep with us is reference or address of the root node this is what we pass to all the functions, in our implementation we often use a variable of type pointer to node named root to store the address of root node, we can name this variable anything, we can name this variable root or we can name this variable rootPtr but this is just a pointer this particular block that m showing here is for pointer to node and all these rectangles with three cells are nodes, this is how things are organized in memory now for this tree, let say we are making a call to this preorder function ll make a call to preorder passing it address 200 for this call root is not null so we will not return at first line in this function we will go ahead and print the data in this node at address 200. ll write output for all print statements here and now this function will make a recursive call execution of this particular function call will pause, it will resume only after this recursive call preorder(150) finishes this second call is to visit this left subtree this call preorder(150) is to visit this left subtree, address of the left child of node at 200 is 150 once again for this call root is not null, so we will go ahead and print the data, data in node at 150 is D and now once again there will be a recursive call with this call preorder 400. We are saying that we are going to visit this subtree once again we will print the data and make another recursive call now we have made a call to visit this particular subtree with just one node for this call we will print the data and now for node at 250 address of left child is zero or null we will make a call preorder(0) but for this call we will simply return because the address in this variable root will be null. We have hit the base condition for our recursion call to preorder(0) will finish and preorder(250) will resume. Now in this particular function call will make another call for right subtree for node at 250, even the right child is null we will have another recursive call passing address 0 but this once again simply will return and now call to preorder(250) will finish and call to preorder(400) will resume. Now in call to preorder(400) we will make another recursive call to preorder(180) with this call preorder(180), we are visiting this particular subtree with just one node for this call first we will print the data and then we will make a recursive call to preorder(0) now preorder(0) will simply return and then we will have another call to preorder(0) for right child of 180, the recursion will go on like this theres one thing that want to talk about you thats happening in this whole process even though we are not using any extra memory explicitly in our function because of the recursion we are growing the function call stack we have discussed memory management a number of times in our earlier lessons you can check description of this video for link to one of those lessons. As we know for each function call we allocate some amount of memory in what we call stack section of applications memory and this allocated memory is reclaimed when the function call finishes at this stage of execution of my recursion for this example my call stack will look something like this: m writing P as shortcut for preorder because m short of space here lets say we made a call to preorder passing it address 200 from main function, main function will be at bottom of stack at any time only the call at top of stack will be executing and all other calls will be paused, call stack keeps growing and shrinking during execution of a program because memory is allocated for a new function call and its reclaimed when a function call finishes so even though we are not using any extra memory explicitly here we are using memory implicitly in the call stack so space complexity which is measure of rate of growth of extra memory used with input will depend upon the maximum amount of extra memory used in the call stack. ll talk about space complexity once more later for now lets come back to this recursion that was executing called to this preorder(0) will finish and preorder(180) will resume memory allocated for execution of preorder(0) will be reclaimed now for preorder(180) both recursive calls have finished so this guy will also finish even for preorder(400) both calls have finished so preorder(150) will resume. Now this guy will make a recursive call to preorder function passing it address 450 address of its right child, memory in the stack will be allocated for execution of preorder(450). Now in this call we will first print the data and then we will make two recursive calls to preorder passing address zero each time because for this node at 450 both children are null, both calls will simply return and then preorder(450) will finish and now preorder(150) will also be done if you can see the call stack will grow on till we reach a leaf node, a node with no children and then it will start shrinking again maximum growth of call stack to do this recursion will depend upon maximum depth or height of the tree. We can say that extra space used will be proportional to height of the tree or in other words space complexity of this algorithm is O(h), where h is height of the tree. Okay coming back to the recursion we are done with preorder(150) so preorder(200) will resume and now we will make a call to visit this particular subtree, in this call we will print J and then we will make a call passing address 60 so now we are visiting this particular subtree, here we will first print ""G"" and then this guy will make a call to preorder(0) which will simply return and then there will be another call to preorder(500) here we will print """" 187 00:11:19,160 00:11:22,610 and then we will two recursive calls passing address zero every time because node at 500 is a leaf node with no children after this guy finishes preorder(60) will resume, now this guy will also finish and preorder(350) will resume and now we will have a call to preorder(700) which once again is a leaf node, so ""K"" which is data in this node will be printed and then we will two calls passing address zero which will simply return. Now at this stage all these calls can finish, we are done visiting all the nodes finally we will return back to the caller of preorder(200) which probably would be the main function so this is preorder traversal for you hope you got how this recursion works, code for inorder and postorder will be very similar in inorder traversal my base case will be the same so ll say if root is null then return or exit if root is not null first need to visit the left subtree m visiting the left subtree with this recursive call then need to visit the root so now m writing this printf statement to print the data and now can visit the right subtree so this second recursive call, and this is my inorder function, inorder traversal of this example tree that have drawn here will be this. This particular binary trees actually also a binary search tree and inorder traversal of a binary search tree would give us elements in the tree in sorted order. Okay lets now write code for postorder. For this function once again the base case will be the same so ll say if root is null, return or exit if root is not null. first need to visit the left subtree so have made this recursive call, then the right subtree so ll have this another recursive call and now can visit the root node postorder traversal for this example tree will be this. o this is preorder, inorder and postorder for you you can check the description of this video for link to all the source code. Lets now quickly talk about time and space complexity of these algorithms. Time complexity of all these three algorithms is O(n), if you could see then there was one function call corresponding to each node where we were actually visiting that node, where we were actually printing the data in that node so running time should actually be proportional to number of nodes theres a better formal and mathematical way of proving that time complexity of these algorithms is O(n), you can check the description of this video for link to that space complexity as we had discussed earlier will be O(h), where h is height of the tree, height of a tree in worstcase will be (n1) so in worstcase space complexity of these algorithms can be O(n) and in best or average case height of the tree will be O(log n) so we can say that in best or average case space complexity will be O(log n). ll stop here now. n coming lessons we will solve some problems on binary tree. Thanks for Watching.","this will work fine in or ++ actually in make sure you right struct space node instead of righting just node, rest of the things are fine it will be good to visualize this recursion so lets now quickly see how this preorder function will work if this example tree that m showing in right here is pass to it ll redraw this tree and show it like this hear m depicting node as a structure with three fields lets say the leftmost cell here is to store the address of left child the cell in middle is to store the data and the right most cell is to store the address of right child Now lets assume some addresses for these nodes, lets say the root node is at address 200 and ll assume some random addresses for other notes as well and now can fill in left and right fields for each node and as we know the identity of tree that we always keep with us is reference or address of the root node this is what we pass to all the functions, in our implementation we often use a variable of type pointer to node named root to store the address of root node, we can name this variable anything, we can name this variable root or we can name this variable rootPtr but this is just a pointer this particular block that m showing here is for pointer to node and all these rectangles with three cells are nodes, this is how things are organized in memory now for this tree, let say we are making a call to this preorder function ll make a call to preorder passing it address 200 for this call root is not null so we will not return at first line in this function we will go ahead and print the data in this node at address 200. ll write output for all print statements here and now this function will make a recursive call execution of this particular function call will pause, it will resume only after this recursive call preorder(150) finishes this second call is to visit this left subtree this call preorder(150) is to visit this left subtree, address of the left child of node at 200 is 150 once again for this call root is not null, so we will go ahead and print the data, data in node at 150 is D and now once again there will be a recursive call with this call preorder 400. Now at this stage all these calls can finish, we are done visiting all the nodes finally we will return back to the caller of preorder(200) which probably would be the main function so this is preorder traversal for you hope you got how this recursion works, code for inorder and postorder will be very similar in inorder traversal my base case will be the same so ll say if root is null then return or exit if root is not null first need to visit the left subtree m visiting the left subtree with this recursive call then need to visit the root so now m writing this printf statement to print the data and now can visit the right subtree so this second recursive call, and this is my inorder function, inorder traversal of this example tree that have drawn here will be this. We are saying that we are going to visit this subtree once again we will print the data and make another recursive call now we have made a call to visit this particular subtree with just one node for this call we will print the data and now for node at 250 address of left child is zero or null we will make a call preorder(0) but for this call we will simply return because the address in this variable root will be null. Now in call to preorder(400) we will make another recursive call to preorder(180) with this call preorder(180), we are visiting this particular subtree with just one node for this call first we will print the data and then we will make a recursive call to preorder(0) now preorder(0) will simply return and then we will have another call to preorder(0) for right child of 180, the recursion will go on like this theres one thing that want to talk about you thats happening in this whole process even though we are not using any extra memory explicitly in our function because of the recursion we are growing the function call stack we have discussed memory management a number of times in our earlier lessons you can check description of this video for link to one of those lessons. ll define node as a structure with three fields like this, data type in this definition is character and there are two fields to store the addresses of left and right children 55 00:03:15,349 00:03:20,609 now in preorder function ll first visit or print the data in root node and now ill make a recursive call to visit the left subtree have made a recursive call here and to this call m passing address of the left child of my current root because left child will be the root of left subtree and ll have another call like this to visit the right subtree there is one more thing that we need to add in this function and we will be done we cannot go into recursion infinitely, we need to have a base condition where we should exit if a tree or a subtree is empty or in other words for any call if root is null we can return or exit. Okay coming back to the recursion we are done with preorder(150) so preorder(200) will resume and now we will make a call to visit this particular subtree, in this call we will print J and then we will make a call passing address 60 so now we are visiting this particular subtree, here we will first print ""G"" and then this guy will make a call to preorder(0) which will simply return and then there will be another call to preorder(500) here we will print """" 187 00:11:19,160 00:11:22,610 and then we will two recursive calls passing address zero every time because node at 500 is a leaf node with no children after this guy finishes preorder(60) will resume, now this guy will also finish and preorder(350) will resume and now we will have a call to preorder(700) which once again is a leaf node, so ""K"" which is data in this node will be printed and then we will two calls passing address zero which will simply return. Now in this particular function call will make another call for right subtree for node at 250, even the right child is null we will have another recursive call passing address 0 but this once again simply will return and now call to preorder(250) will finish and call to preorder(400) will resume. Now in this call we will first print the data and then we will make two recursive calls to preorder passing address zero each time because for this node at 450 both children are null, both calls will simply return and then preorder(450) will finish and now preorder(150) will also be done if you can see the call stack will grow on till we reach a leaf node, a node with no children and then it will start shrinking again maximum growth of call stack to do this recursion will depend upon maximum depth or height of the tree. first need to visit the left subtree so have made this recursive call, then the right subtree so ll have this another recursive call and now can visit the root node postorder traversal for this example tree will be this.",0.1162790697674418
67,608,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer highquality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: OK, guess we might as well start a minute early since those of you who are here are here. Were coming to the end of course. Were deep in chapter 7 now talking about random walks and detection theory. Well get into martingales sometime next week. There are four more lectures after this one. The schedule was passed out at the beginning of the term. dont know how did it, but somehow left off the last Wednesday of class. The final is going to be on Wednesday morning at the ice rink. dont know what the ice rink is like. t doesnt sound like an ideal place to take a final, but assume they must have desks there and all that stuff. We will send out a notice about that. This is the last homework set that you will have to turn in. We will probably have another set of practice problems and problems on but not things you should turn in. We will try to get solutions out on them fairly quickly, also. o you can do them, but also look at the answers right after you do them. OK, so lets get back to random walks. And remember what we were doing last time. A random walk, by definition, you have a sequence of D random variables. You have partial sums of those random variables. sub n is a sum of the first n of those D random variables. And the sequence of partial sums 1, 2, 3, and so forth, that sequence is called a random walk. And if you graph the random walk, its something which wanders up and down usually. And sometimes, if the mean of X is positive, it wanders off to infinity. f the mean of X is negative, it wanders off to minus infinity. f the mean of X is 0, it simply diffuses somewhat as time goes on. And what were trying to find that is exactly how do these things work. o our focus here is going to be on thresholdcrossing problems. Namely, whats the probability that this random walk is going to cross some threshold by or at some particular value of n? f you have two thresholds, one above and one below, whats the probability its going to cross the one above? Whats the probability its going to cross the one below? And if it crosses one of these, when does it cross it? f it crosses it, how much of an overshoot is there? All of those problems just come in naturally by looking at a sum of D random variables. But here were going to be trying to study them in some consistent manner looking at the thresholds particularly. Weve talked a little bit about two particularly important applications. One is . And even far more important than that is this question of detection, or making decisions, or hypothesis testing, all of which are the same thing. You remember we did show that there was at least one thresholdcrossing problem that was very, very easy. ts the threshold problem where the underlying random variable is binary. You either go up by 1 or you go down by 1 on each step. And the question is, whats the probability that you will cross some threshold at some k greater than 0? And it turns out that since you can only go up 1 each time, the probability of getting up to some point k is the probability you ever got up to 1. Given that you got up to 1, its the probability that you ever got up to 2. Given you got up to 2, its the probability you ever got up to 3. That doesnt mean that you go directly from 2 to 3. After you go to 2, you wander all around, and eventually you make it up to 3. f you do, then the question is, do you ever get from 3 to 4, and so forth. And we found that the solution to that problem was p over 1 minus p to the kth power of p is less than or equal to 1/2. And we solved that problem, if you remember, back when we were talking about stop when youre ahead if youre playing coin tossing with somebody. And so lets go further and look particularly at this problem of detection, and decisions, and hypothesis testing, which is really not a particularly hard problem. But its made particularly hard by statisticians who have so many special rules, peculiar cases, and almost mythology about making decisions. And you can imagine why because as long as you talk about probability, everybody knows youre talking about an abstraction. As soon as you start talking about making a decision, it suddenly becomes real. mean, you look at a bunch of data and you have to do something. You look at a bunch of candidates for a job, you have to choose one. Thats always very difficult because you might not choose the right one. You might choose a very poor one. But you have to do your best. f youre investing in stocks, you look at all the statistics of everything. And finally you say, thats where m going to put my money. Or if youre looking for a job you say, thats where m going to work, and you hope that thats going to work out well. There are all these situations where you can evaluate probabilities until youre sick in the head. They dont mean anything. ts only when you make a decision and actually do something with it that it really means something. o it becomes important at this point. The model we use for this, since were studying probability theory well, actually, were studying random processes. But were really studying probability theory. You probably noticed that by now. ince were studying probability, we study all these problems in terms of a probabilistic model. And in the probabilistic model, theres a discrete and, in most cases, binary random variable, H, which is called the hypothesis random variable. The sample values of H, you might as well call them 0 and 1. Thats the easiest things to call binary things. Theyre called the alternative hypotheses. They have marginal probabilities because its a probability model. You have a random variable. t can only take on the value 0 and 1, so it has to have probabilities of being 0 and 1. Along with that, there are all sorts of other random variables. The situation might be as complicated as you want. But since were making decisions, were making decisions on the basis of some set of alternatives. And here, since were trying to talk about random walks, and martingales, and things like that, also we restrict our attention to particular kinds of observations. And the particular kind of observation that we restrict attention to here is a sequence of random variables, which we call the observation. You observe Y1. You observe Y2. You observe Y3, and so forth. n other words, you observe a sample value of each of those random variables. There are a whole sequence of them. And we assume, to make life simple for ourselves, that each of these are independent, conditional on the hypothesis. And theyre identically distributed conditional on the hypothesis. Thats what this says right here. This makes one more assumption that assumes that these observations are continuous random variables. That doesnt make much difference, there are just a few peculiarities that come in if these are discrete random variables. There also a few peculiarities that come in when theyre continuous. And there are a lot of peculiarities that come in when theyre absolutely arbitrary. But for the time being, just imagine each of these are continuous random variables. o for each value of n, we look at n observations. We can calculate the probability density that those observations would occur conditional on hypothesis 0. We can find the conditional probability they could occur conditional on hypothesis 1. And since theyre D, thats equal to this product here. Excuse me, they are not D, they are conditionally D. onditional on the hypothesis. Namely, the idea is the world is one way or the world is another way. f the world is this way, then all of these hypotheses are D. Youre doing the same experiment again and again and again, but its based on the same underlying hypothesis. Or, the underlying hypothesis is this over here. You make the number of observations all based on this same hypothesis, and you make as many of these D observations conditional on that observation as you choose. And when youre all done, what do you do? You have to make your decision. OK, so this is a very simpleminded model of this very complicated and very important problem. But its close enough to the truth that we can get a lot of observations from it. Now, spent a lot last time talking about this. pend a lot of time this time talking about it because when we use a probability model for this, when we say that were studying probability theory. And therefore, were going to use probability, we have suddenly allied ourselves completely with people called Bayesian statisticians or Bayesian probabilists. And we have gone against, turned our back on people called NonBayesians, or sometimes classical. hate using the word ""classical"" because like the word ""classics."" like the classics for such an unusual point of view. And the unusual point of view is that we refuse to take a probability model. We accept the fact that on all the observations, all the observations are probabilistic. We assume we have a nice model for them, which makes sense. We can do whatever we want with that model. We can change the model. We can do whatever we want with a model. But if you once assume that these two hypotheses that youre trying to choose between, that they have a priori probabilities, then people get very upset about it because they say, well, if what the a priori probabilities are, why do you have to do a hypothesis test? You already understand everything there is to know about the problem. And they feel this is very strange. ts not strange because you use probability models. You use models to try to understand certain things about reality. And you assume as many things as you want to assume about it. And when you get all done, you either use all the assumptions or you dont use them. What were going to find today is that when you use this assumption of a probability model, you can answer the questions that these classical statisticians go to great pains to answer. And you can ask them very, very simply. o that after we assume the a priori probabilities, we can calculate certain things which dont depend on those a priori probabilities. And therefore, we know two things. One, we know that if we did know the a priori probabilities, it wouldnt make any difference. And two, we know that if we can estimate the a priori probabilities, it makes a great deal of difference. And three and this is the most important point you make 100 observations of something. omebody else says, dont believe you, and comes in and makes another 100 observations. omebody else makes another 100 observations. Now, even if the second person doesnt believe what the first person has done, it doesnt make sense as a scientist to completely eliminate all of that from consideration. Namely, what you would like to do is say well, since this person has found such and such, the a priori probabilities have changed. And then can go on and make my 100 observations. can either make a hypothesis test based on my 100 observations or can make a hypothesis test assuming that the other person did their work well. can make it based on all of these observations. f you try to do that those two things in a classical formulation, you run into a lot of trouble. f you try to do them in this probabilistic formulation, its all perfectly straightforward. Because you can either start out with a model in which youre taking 200 observations or you can start out with a model in which you take 100 observations. And then suddenly, the world changes. This hypothesis takes on, perhaps a different value. You take another hundred observations. o you do whatever you want to within a probabilistic formulation. But the other thing is, all of you that patiently have lived with this idea of studying probabilistic models all term long. You might as well keep on living with it. The fact that were now interested in making decisions should not make you think that everything youve learned up until this point is baloney. And to move from here to a classical statistical formulation of the world would really be saying, dont believe in probability theory. ts that bad. o here we go. m sorry, we did that. We were there. Assume that on the basis of observing a sample value of this sequence of observations, we have to make a decision about H. We have to choose H equals 0 or H equals 1. We have to detect whether or not H is 1. When you do this detection, you would think in the real world that youve detected something. f youve made a decision about something, that youve tested a hypothesis and you found that which is correct. Not at all. When you make decisions, you can make errors. And the question of what kinds of errors youre making is a major part of trying to make decisions. mean, those people who make decisions and then cant believe that they might have made the wrong decision are the worst kind of fools. And you see them in politics. You see them in business. You see them in academia. You see them all over the place. When you make a decision and youve made a mistake, you get some more evidence. You see that its a mistake and you change. The whole 19th century was taken up with mean, the scientific community was driven by physicists in those days. And the idea of Newtons laws was the most sacred thing they had. Everybody believed in Newtonian mechanics in those days. When quantum mechanics came along, this wasnt just a minor perturbation in physics. This was a most crucial thing. This said, everything weve known goes out the window. We cant rely on anything anymore. But the physicists said, OK, guess we made a mistake. Well make new observations. We have new observations that can be made. We now see that Newtonian mechanics works over a certain range of things. t doesnt work in another ranges of things. And they go on and find new things. Thats the same thing we do here. We take these models. We evaluate our error probabilities. And evaluating them, we then say, well, weve got to go on and take some more measurements. Or we say were going to live with it. But we face the fact that there are errors involved. And in doing that, you have to take a probabilistic model. f you dont take a probabilistic model, its very hard for you to talk honestly about what error probabilities are. o both ways well, m preaching and m sorry. But ve lived for a long time with many statisticians, many of whom get into my own field and who cause a great deal of trouble. o the only thing can do it urge you all to be cautious about this. And to think the matter through on your own. m not telling you to take my point of view on it. m telling you, dont take other peoples point of view without thinking it through. The probability experiment here really mean, every probability model we view in terms of the real world, as you have this set of probabilities, a set of possible events. You do the experiment. Theres one sample point that comes out. And after the one sample point comes out, then you know what the result of the experiment is. Here, the experiment consists both of what you normally view as the experiment. Namely, taking the observations. And it also involves a choice of hypotheses. Namely, theres not a correct hypothesis to start with. The experiment involves God throws his dice. Einstein didnt believe that God threw dice, but do. And after throwing the dice, one or the other of these hypotheses turns out to be true. All of these observations point to that or they point to the other and you make a decision. OK, so the experiment consists both on choosing the hypothesis and on taking a whole sequence of observations. Now, the other thing to not forget in this because you really have to get this model in your mind or youre going to get very confused with all the things we do. The experiment consists on a whole sequence of observations, but only one choice of hypothesis. Namely, you do the experiment. Theres a hypothesis that occurs, and theres a whole sequence of observations which are all D conditional on that particular hypothesis. o thats the model were going to be using. And now life is quite simple once weve explained the model. We can talk about the probability that H is equal to either 0 or 1, conditional on the sample point weve observed. ts equal to the a priori probability of that hypothesis times the density of the observation conditional on the hypothesis divided by just a normalization factor. Namely, the overall probability of that observation period, which is the sum of probability that 0 is a correct hypothesis times this plus probability that 1 is a correct hypothesis times the density given 1. This denominator here is a pain in the neck, as you can see. But you can avoid ever dealing with a denominator if you take this for H equals 0, divide by this for H equals 1, and then you have this term divided by this term all divided by this term for l equals 1 divided by the same thing. o the ratio, the probability that H equals 0 given y over the probability that H is 1 equals y is just this ratio here. Now, whats the probability of error if we make a decision at this point? f ve got in this particular sequence Y, this quantity here is, in fact, the probability that hypothesis 0 is correct in the model that we have chosen. o this is the probability that H is equal to 0 given Y. f we select 1 under these conditions, if we select hypothesis 1, if we make a decision and say, m going to guess that 1 is the right decision. That means that this is the probability youve made a mistake. Because this is the probability that H is actually 0 rather than 1. This quantity here is the probability that youve made a mistake given that 1 is the correct hypothesis. o here we are sitting here with these probabilities of error. We dont have to do any calculations for them. Well, you might have to do a great deal of calculation to calculate this and to calculate this. But otherwise, the whole thing is just sitting there for you. o what do you do if you want to minimize the probability of error? This was the probability that youre going to make an error if you choose 1. This is the probability of error if you choose 0. We want to minimize the probability of error and we see the observation Y, we want to pick the one of these which is largest. And thats all there is to it. This is the decision rule that minimizes the probability of an error. ts based on knowing what P0 and P1 is. But otherwise, probability that H equals l is the correct hypothesis given the observation is probability that H equals L given Y. We maximize the a posteriori probability of choosing correctly by choosing the maximum over l of probability that H equals l given Y. This choosing directly, maximizing the a posteriori probability is called the AP rule, aximum A posteriori Probability. You can only solve the AP problem if you assume that you know P0 P1. We do know P0 and P1 if weve selected a probability model. o when we select this probability model, weve already assumed what these a priori probabilities are, so we now make our observation. And after making our observation, we make a decision. And at that point, we have an a posteriori probability that each of the hypotheses is correct. Anybody has any issues with this? mean, it looks painfully simple when you look at this way. And if it doesnt look painfully simple, please ask now or forever hold your peace as they say. Yeah? ADENE: o can you explain how you get the equation? an you explain how you get the equation on the first line? PROFEOR: On the first line right up here? Yes, use Bayes law. ADENE: o what is that? o thats P of A given B is equal to P of B given A? PROFEOR: Yes. ADENE: dont quite see how to P of A given B is equal to P of B given A times P of A divided by P of A and B. f you take this over there then its am stating Bayes law in a funny way? ADENE: o the thing on the bottom is P of B? OK. PROFEOR: What? ADENE: OK, get it. PROFEOR: mean, might not be explained it well. ADENE: . PROFEOR: Except if you start out with P of A given B is equal to P of B given A times P of B divided by P of A. This quantity here is P of Y. o we have probability that H equals l times probability of Y given l divided by the probability of l to start with. OK, so you maximize the a posteriori probability by choosing the maximum of these. ts called the AP rule. And it doesnt require you to calculate this quantity, which is sometimes a mess. All it requires you to do is to compare these two quantities, which means you have to compare these two quantities. ADENE: ts 10 oclock. PROFEOR: Well, excuse me. Yes. Yes, know. These things become clearer if you state them in terms of what you call the likelihood ratio. Likelihood ratio only works when you have two hypotheses. When you have two hypotheses, you call the ratio of one of them to the other one the likelihood ratio. Why do put 0 up here and 1 down here? Absolutely no reason at all, its just convention. And unfortunately, its a convention that not everybody follows. o some people have one convention and some people have another convention. f you want to use the other convention, just imagine switching 1 and 1 in your mind. Theyre both just binary numbers. Then, when you want to look at this AP rule, the AP rule is choosing the larger of these two things, which we had back here. Thats choosing whether this is larger than this, or vice versa, which is choosing whether this ratio here is greater than the ratio of P1 to P0. o thats the same, thats the same thing. o the AP rule is to calculate the likelihood ratio for this given observation y. And if this is greater than P1 over P0, you select H equals 0. f its less than or equal to P1 over P0, you select H1. Why do put the strict equality here and the strict inequality here? Again, no reason whatsoever. When you have equality, it doesnt make any difference which you choose. o you could flip a coin. ts a little easier if you just say, were going to do this under this condition. o we state condition this way. We calculate the likelihood ratio. We compare it with a threshold. The threshold here is P1 over P0. And then we select something. Why did put a little hat over this? ADENE: Estimation. PROFEOR: What? ADENE: Because its an estimation. PROFEOR: What? ADENE: ts an estimation? PROFEOR: Well, its not really an estimation. ts a detection. mean, estimation you usually view as being analog. Detection you usually view as being digital. And thanks for bringing that up because its an important point. But in this model, H is either 0 or 1 in the result of this experiment. We dont know which it is. This is what weve chosen. o H hat is 0 does not mean that H itself is 0. o this is our choice. t might be wrong or it might be right. any decision rules, including the most common and the most sensible, are rules that compare lambda of y to a fixed threshold, say, eta, is P1 over P0, which is independent of y, which is just a fixed threshold. The decision rules then vary only in the way that you choose the threshold. Now, what happens as soon as call this eta instead of P1 over P0? y test becomes independent of these a priori probabilities that statisticians have thought about for so long. Namely, after a couple of lines of fiddling around with these things, suddenly all of that has disappeared. We have a threshold test. The threshold test says, take this ratio everybody agrees that theres such a ratio that exists and compare it with something. And if its bigger than that something, you choose 0. f its less than that thing, you choose 1. And thats the end of it. OK, so we have two questions. One, do we always want to use a threshold test or are there cases where we should use things other than a threshold test? And the second question is, if were going to use a threshold test, where should we set the threshold? mean, theres nothing that says that you really want to minimize the probability of error. mean, suppose your test is to see whether mean, something in the news today. mean, youd like to take an experiment to see whether your nuclear plant is going to explode or not. o you come up with one decision, its not going to explode. Or another decision, you decide it will explode. Presumably on the basis of that decision, you do all sorts of things. Do you really want to make it a maximum a posteriori probability decision? No. You recognize that if its going to explode, and you choose that its not going to explode and you dont do anything, there is a humongous cost associated with that. f you decide the other way, theres a pretty large cost associated with that also. But theres not really much comparison between the two. But anyway, you want to do something which takes those costs into account. One of the problems in the homework does that. ts really almost trivial to readjust this problem, so that you set the threshold to involve the costs also. o if you have arbitrary costs in making errors, then you change the threshold a little bit. But you still use a threshold test. Theres something called maximum likelihood that people like for making decisions. And maximum likelihood says you calculate the likelihood ratio. And if the likelihood ratio is bigger than 1, you go this way. f its less than 1, you go this way. ts the AP test if the two a priori probabilities are equal. But in many cases, you want to use it whether or not the a priori probabilities are equal. ts a standard test, and there are many reasons for using it. Aside from the fact that the a priori probabilities might be chosen that way. o anyway, thats one other choice. When we go a little further day, well talk about a NeymanPearson test. The NeymanPearson test says, for some reason or other, want to make sure that the probability that my nuclear plant doesnt blow up is less than, say, 10 to the minus fifth. Why 10 to the minus fifth? Pull it out of the air. aybe 10 to the minus sixth, that point our probabilities dont make much sense anymore. But however we choose it, we choose our test to say, we cant make the probability of error under one hypothesis bigger than some certain amount alpha than what test will minimize the probability of error under the other hypothesis. Namely, if have to get one thing right, or have to get it right almost all the time, whats the best can do on the other alternative? And thats the NeymanPearson test. That is a favorite test among the nonBayesians because it doesnt involve the a priori probabilities anymore. o its a nice one in that way. But well see, we get it anyway using a probability model. OK, lets go back to random walks just a little bit to see why were doing what were doing. The logarithm of the threshold ratio is logarithm of this lambda of y. m taking m observations. m putting that in explicitly, is the sum from N equals 1 to m of the log of the individual ratio. n other words, when you under hypothesis 0, if calculate the probability of vector y given H equals 0, m finding the probability of n things which are D. o what m going to find this probability density is taking the product of the probabilities of each of the observations. ost of you know now that any time you look at a probability, which is a product of observations, what youd really like to do is to take the logarithm of it. o youre talking about a sum of things rather than a product of things because we all know how to add independent random variables. o the log of this likelihood ratio, which is called the log likelihood ratio as you might guess, is just a sum of these likelihood ratios. f we look at this for each m greater than or equal to 1, then given H equals 0, its a random walk. And given H equals 1, its another random walk. ts the same sequence of sample values in both cases. Namely, as an experimentalist, were taking these observations. We dont know whether H equals 0 or H equals 1 is what the result of the experiment is going to be. But what we do know is we know what those values are. We can calculate this sum. And now, if we condition this on H equals 0, then this quantity, which is fixed, has a particular probability of occurring. o this is a random variable then under the hypothesis H equals 0. ts a random variable under the hypothesis H equals 1. And this sum of random variables behaves in a very different way under these two hypotheses. Whats going to happen is that under one hypothesis, the expected value of this log likelihood ratio is going to linearly increase with n. f we look at it under the other hypothesis, its going to linearly decrease as we increase n. And a nifty test at that point is to say, as soon as it crosses a threshold up here or a threshold down here, were going to make a decision. And thats called a sequential test in that case because you havent specified ahead of time, m going to take 100 tests and then make up my mind. Youve specified that m going to take as many tests as need to be relatively sure that m getting the right decision, which is what you do in real life. mean, theres nothing fancy about doing sequential tests. Those are the obvious things to do, except theyre a little more tricky to talk about using probability theory. But anyway, thats where were headed. Thats why were talking about hypothesis testing. Because when you look at it in this formulation, we get a random walk. And it gives us a nice example of when you want to use random walks crossing a threshold as a way of making decisions. OK, so thats why were doing what were doing. Now, lets go back and look at threshold tests again, and try to see how were going to make threshold tests, what the error probabilities will be, and try to analyze them a little more than just saying, well, a AP test does this. Because as soon as you see that a AP test does this, you say, well, suppose use some other test. And what am going to suffer from that? What am going to gain by it? o its worthwhile to, instead of looking at even just threshold tests, to say, well, lets look at any old test at all. Now, any test means the following. have this probability model. ve already bludgeoned you into accepting the fact thats the probability model were going to be looking at. And we have this well, we have the likelihood ratio, but we dont care about that for the moment. But we make this observation. We got to make a decision. And our decision is going to be either 1 or 0. How do we characterize that mathematically? Or how do we calculate it if we want a computer to make that decision for us? The systematic way to do it is for every possible sequence of y to say ahead of time to give a formula, which sequences get mapped into 1 and which sequences get mapped into 0. o were going to call a set A the set of sample sequences that get mapped into hypothesis 1. Thats the most general binary hypothesis test you can do. That includes all possible ways of choosing either 1 or 0. Youre forced to hire somebody or not hire somebody. You cant get them to work for you for two weeks, and then make a decision at that point. Well, sometimes you can in this world. But if its somebody you really want and other people want them, too, then youve got to decide, m going to go with this person or m not going to go with them. o under all observations that youve made, you need some way to decide which ones make you go to decision 1. Which ones make you go to decision 0. o we will just say arbitrarily, theres a set A of sample sequences that map into hypothesis 1. And the error probability for each hypothesis using test A is given by and well just call Q sub 0 of A this is our name for the error probability. Have twisted this up? No. Q sub 0 of A is the probability that actually choose its the probability that choose A given that the hypothesis is 0. Q sub 1 of A is the probability that choose 1. Blah, let me start that over again. Q0 of A is the probability that m going to choose hypothesis 1 given that hypothesis 0 was the correct hypothesis. ts the probability that Y is in A. That means that H hat is equal to 1 given that H is actually 0. o thats the probability we make an error given the hypothesis, the correct hypothesis is 0. Q1 of A is the probability of making an error given that the correct hypothesis is 1. f have a priori probabilities, m going back to assuming a priori probabilities again. The probability of error is? ts P0 times the probability make an error given that H equals zero. P1 a priori probability of 1 given that make an error given 1. add these two up. can write it this way. Dont ask for the time being. ll just take the P0 out, so its Q0 of A plus P1 over P0 Q1 of A. o thats what ve called eta times Q1 of A. For the threshold test based on eta, the probability of error is the same thing. But that A there is an eta. hope you can imagine that quantity there is an eta. This is an eta. o its P0 times Q0 of eta plus eta times Q1 of eta. o the eta probability, under this crazy test that youve designed, is P0 times this quantity. nder the AP test, probability of error is this quantity here. What do we know about the AP test? t minimizes the error probability under those a priori probabilities. o what we know about it is that this quantity is less than or equal to this quantity. Take out the P0s and it says that this quantity is less than or equal to this quantity. Pretty simple. Lets draw a picture that shows what that means. Heres a result that we have. We know because of maximum a posteriori probability for the threshold test that this is less than or equal to this. This is the minimum error probability. This is the error probability you get with whatever test you like. o lets draw a picture on a graph where the probability of error given H equals 1 is on the horizontal axis. The probability of error conditional on H equals 0 is on this axis. o can list the probability of error for the threshold test, which sits here. can list the probability of error for this arbitrary test, which sits here. And know that this quantity is greater than or equal to this quantity. o the only thing have to do now is to sort out using plain geometry, why these numbers are what they are. This number here is Q0 of eta plus eta times Q1 of eta. Heres Q1 of eta. This distance here is Q1 of eta. We have a line of slope minus eta there that weve drawn. o this point here is, in fact, Q0 of eta plus eta times Q1 of eta . Thats just plain geometry. This point is Q0 of A plus eta times Q1 of A. Another line of slope minus et. What weve shown is that this is less than or equal to this. Thats because of the AP rule. This has to be less than or equal to that. o what have we shown here? Weve shown that for every test A you can imagine, when you draw that test on this twodimensional plot of error probability given H equals 1 versus error probability given H equals 0. Every test in the world lies Northeast of this line here. Yeah? ADENE: an you say again exactly what axis represents what? PROFEOR: This axis here represents the error probability given that H equals 1 is the correct hypothesis. This axis is the error probability given that 0 is the correct hypothesis. o weve defined Q1 of eta and Q0 of eta as those two error probabilities. sing the threshold test, or using the AP test where eta is equal to P0 over P1. And this point here is whatever it happens to be for any test that you happen to like. You might have a supervisor who wants to hire somebody and you view that person is a threat to yourself, so youve taken all your observations and you then make a decision. f the person is any good, you say, dont hire him. f the person is good you say, hire them. o just the opposite of what you should do. But whatever you do, this says this is less than or equal to this because of the AP rule. And therefore, this point lies up in that direction of this line here. You can do this for any eta that you want to do it for. o for every eta that we want to use, we get some value of Q0 of eta and Q1 of eta. These go along here in some way. You can do the same argument again. For every threshold test, every point lies Northeast of the line of slope minus eta through that threshold test. We get a whole family of curves when eta is very big, the curve of slope minus eta goes like this. When eta is very small, it goes like this. We just think of ourselves plotting all these curves, taking the upper envelope of them because every test has to lie Northeast of every one of those lines. o we take the upper envelope of all of these lines, and we get something that looks like this. We call this the error curve. And this is the upper envelope of the straight lines of slope minus eta that go through the threshold tests at eta. You get something else from that, too. This curve is convex. Why is the curve convex? Well, you might like to take the second derivative of it, but thats a pain in the neck. But the fundamental definition of convexity is that a onedimensional curve is convex if all of its tangents lie underneath the curve. Thats the way weve constructed this. ts the upper envelope of a bunch of straight lines. Yes? ADENE: an you please explain, what is u of alpha? PROFEOR: of alpha is just what ve called this upper envelope. This upper envelope is now a function. ADENE: Whats the definition? PROFEOR: What? ADENE: What is the definition? PROFEOR: The definition is the upper envelope of all these straight lines. ADENE: For changing eta? PROFEOR: What? ADENE: For changing eta? PROFEOR: Yes. As eta changes, get a whole bunch of these points. got a whole bunch of these points. take the upper envelope of all of these straight lines. mean, yes, youd rather see an equation. But if you see an equation its terribly ugly. mean, you can program a computer to do this. as easily as you can program it to follow a bunch of equations. But anyway, m not interested in actually solving for this curve in particular. am particularly interested in the fact that this upper envelope is, in fact, a convex curve and that the threshold tests lie on the curve. The other tests lie Northeast of the curve. And thats the reason you want to use threshold tests. And it has nothing to do with a priori probabilities at all. o you see, the thing weve done is to start out assuming a priori probabilities. Weve derived this neat result using a priori probabilities. But now we have this error curve. Well, to give you a better definition of what u of alpha is, u of alpha is the error probability under hypothesis 1 if the error probability under hypothesis 0 was alpha. You pick an error probability here. You go up to that point here. Theres a threshold test there. You read over there. And at that point, you find the probability of error given H equals 1. ADENE: How do you know that the threshold tests lie on the curve? PROFEOR: Well, this threshold test here is outhwest of all tests. And therefore, it cant lie above this upper envelope. Now, ve cheated you in one small way. f you have a discrete test, what youre going to wind up with is just a finite set of these possible points here. o youre going to wind up with the upper envelope of a finite set of straight lines. o the straight line is actually going to be its still convex, but its piecewise linear. And its piecewise linear, and the threshold tests are at the points of that curve. And in between those points, you dont quite know what to do. o since you dont quite know what to do in between those points, as far as the maximum a posteriori probability test goes, you can reach any one of those points, sometimes using one test on one corner of guess its easier if draw it. And didnt want to get into this particularly because its a little messier. o you could have this kind of curve. And the notes talk about this in detail. o the threshold test correspond to this point. This point says always decide one. Dont pay any attention to the tests at all, just say think one is the right hypothesis. mean, this is the testing philosophy of people who dont believe in experimentalism. Theyve already made up their mind. They look at the results. They say, thats very interesting. And then they say, m going to choose this. These other points are our particular threshold tests. f you want to get error probabilities in the middle here, what do you do? You use a randomized test. ometimes you use this. ometimes you use this. You flip a coin and choose whichever one of these you want to choose. o what this says is the NeymanPearson test, which is the test that says pick some alpha, which is the error probability under hypothesis 1 that youre willing to tolerate. o you pick alpha. And then it says, minimize the error probability of the other kind, so you read over there. And the NeymanPearson test, what it does is it minimizes the error probability under the other hypothesis. Now, when this curve is piecewise linear, the NeymanPearson test is not a threshold test, but its a randomized threshold test. ometimes when youre at a point like this, you have to use this test and this test sometimes. For most of the tests that you deal with, NeymanPearson test is just the threshold test thats at that particular point. Any questions about that? This is probably one of these things you have to think about a little bit. Yes? ADENE: When you say you have to use this test or this test, are you talking about threshold or are you talking about because this is always its either H equals 0 or H equal 1, right? What do you mean when you say you have to randomize between the two tests? mean threshold tests if have a finite set of alternatives, and m doing a threshold test on that finite set of alternatives, only have a finite number of things can do. As increase the threshold, suddenly get to the point where this ratio of likelihoods includes one more point. And then it gets to the point where it includes one other point and so forth. o that what happens is that this upper envelope is just the upper envelope of a finite number of points. And this upper envelope of a finite number of points, the threshold tests are just the corners there. o sometimes have to randomize between them. f you dont like that, ignore it. Because for most tests you deal with, almost all books on statistics that ve ever seen, it just says the NeymanPearson test looks at the threshold curve, at this error curve. And it chooses accordingly. Yes? ADENE: You can put the previous slide back? You told us that because of maximum a posteriori probability, if eta is equal to P0 divided by P1, then the probability of error is minimized. And so the errors of the test A are . But if we start changing eta from 0 to infinity, it doesnt have to be anymore. , which means the error is not necessarily minimized. o the argument doesnt hold anymore. PROFEOR: As change eta, m changing P1 and P0 also. n other words, now what m doing is m saying, lets look at this threshold test, and lets visualize what happens as change the a priori probabilities. o m suddenly becoming a classical statistician instead of a Bayesian one. But know what the answers are from looking at the Bayesian case. OK, so lets move on. mean, we now sort of see that these tests well, one thing weve seen is when you have to make a decision under this kind of probabilistic model weve been talking about namely, two hypotheses, D random variable is conditional on each hypothesis. Those hypothesis testing problems turn into random walk problems. We also saw that the when started looking at when the system becomes empty, and how long it takes to start to fill up again, that problem is a random walk problem. o now want to start to ask the question, whats the probability that a random walk will cross a threshold? m going to apply the hernoff bound to it. You remember the hernoff bound? We talked about it a little bit back on the second week of the term. We were talking about the arkov inequality and the hebyshev inequality. And we said that the hernoff inequality was the same sort of thing, except it was based on e to the rZ rather than x or x squared. And we talked a little bit about its properties. The major thing one uses the hernoff bound for is to get good estimates very, very far away from the mean. n other words, good estimates of probabilities that are very, very small. ve grown up using these all my life because ve been concerned with error probabilities in communication systems. You typically want error probabilities that run between 10 to the minus fifth and 10 to the minus eighth. o you want to look at points which are quite far away. mean, you take a large number of you take a sum of a large number of variables, which correspond to a code. And you look at error probabilities for this rather complicated thing. But youre looking very, very far away from the mean, and youre looking at very large numbers of observations. o instead of the kinds of things where we deal with things like the central limit theorem where youre trying to figure out what goes on close to the mean, here youre trying to figure out what goes on very far from the mean. OK, so what the hernoff bound says is that the probability that a random variable Z is greater than or equal to some constant b. We dont even need sums of random variables here, its just a hernoff bound is a bound on the tail of a distribution. s less than or equal to the moment generating function of that random variable. g sub Z of r is the expected value of e to the rZ. These generating functions, you can calculate them if you want to. Times e to the minus rb. This is the arkov inequality for the random variable e to the rZ. And go back and review chapter 1. think its section 1.43 or something. ts the section that deals with the arkov inequality, the hebyshev inequality, and the hernoff bound. And as told you once when we talked about these things, hernoff is still alive and well. Hes a statistician at Harvard. He was somewhat embarrassed by this inequality becoming so famous because he did it as sort of a throwoff thing in a paper where he was trying to do something which was much more mathematically sophisticated. And now the poor guy is only known for this thing that he views as being trivial. But what the bound says is the probability of Z is greater than or equal to b is this inequality. trangely enough, the probability that Z is less than or equal to b is bounded by the same inequality. But one of them, r is bigger than 0. And the other one, r is less than 0. And you have to go back and read that section to understand why. Now, this is most useful when its applied to a sum of random variables. dont know of any applications for it otherwise. o if the momentgenerating function oh, incidentally, also. When most people talk about momentgenerating functions, and certainly when people talked about momentgenerating functions before the 1950s or so, what they were always interested in is the fact that if you take derivatives of the momentgenerating functions, you generate the moments of the random variable. f you take the derivative of this with respect to r, evaluate it at r equals 0, you get the expected value of Z. f you take the second derivative evaluated at r equals 0, you get the expected value of Z squared, and so forth. You can see that by just taking the derivative of that. Here, were looking at something else. Were not looking at what goes on around r equals 0. Were trying to figure out what goes on way on the far tails of these distributions. o if gX of r is e to the rX, then e to the e to the r n n is the sum of these random variables is the expected value of the product of e to the rXi. Namely, its e to the r. ome of Xi. o that turns into a product. The expected value of a product of a finite number of terms is the product of the expected value. o its gX or r to the nth power. o if want to write this, now m applying the hernoff bound to the random variable sub n. Whats the probability that sub n is greater than or equal to n times a? ts gX to the n of r times e to the minus rna. Thats what the hernoff bound says. This is the hernoff bound over on the other side of the distribution. This only makes sense and has interesting values when a is bigger than the mean or when a is less than the mean. And when r is greater than 0 for this one and less than 0 for this one. Now, this is easier to interpret and its easier to work with. f you take that product of terms g to the r to the nth power and you visualize the logarithm of g to the X. Visualize the logarithm of g to the X, then you get this quantity up here. You get the probability that n is greater than or equal to na is this e to the n times gamma x of r minus ra. Gamma is the logarithm of the momentgenerating function. The logarithm of the momentgenerating function is always called the semiinvariant momentgenerating function. The name is, again, because people were originally interested in the momentgenerating properties of these random variables. f you sit down and take the derivatives, can probably do it here. ts simple enough that wont get confused. The derivative with respect to r of the logarithm of g of r is first derivative of r divided by g of r. And the second derivative is then the natural log of g of r. Taking the derivative of that is equal to g double prime of r over g of r squared. Tell me if m making a mistake here because usually do when do this. inus g of r and g prime of r. Probably divided by this squared. Lets see. s this right? Who can take derivatives here? ADENE: First term doesnt have a square in it. PROFEOR: What? ADENE: First term doesnt have a square in the denominator. PROFEOR: First term? Yeah. Oh, the first thing doesnt have a square. No, youre right. ADENE: econd one doesnt have PROFEOR: And the second one, lets see. We have we just have g prime of r squared divided by g of r squared. And we evaluate this at r equals 0. This term becomes 1. This term becomes 1. This term becomes the second moment x squared bar. And this term becomes x bar squared. And this whole thing becomes the variance of the moment of the random variable rather than the second moment. All of these terms might be wrong, but this term is right. And m sure all of you can rewrite that and evaluate it at r equals 0. o thats why its called the semiinvariant momentgenerating function. t doesnt make any difference for what were interested in. The thing that were interested in is that this exponent here as you visualize doing this experiment and taking additional observations, what happens is the probability that you exceed na that the nth sum exceeds n times some fixed quantity a is going down exponentially with Now, is this bound any good? Well, if you optimize it over r, ts essentially exponentially tight. o, in fact, it is good. What does it mean to be exponentially tight? Thats what dont want to define carefully. Theres a theorem in the notes that says what exponentially tight means. And it takes you half an hour to read it because its being stated very carefully. What it says essentially is that if take this quantity here and subtract add an epsilon to it. Namely, e to the n times this quantity minus epsilon. o have an e to the minus n epsilon, see it sitting in there? When take this exponent and reduce it just a little bit, get a bound that isnt true. This is greater than or equal to the quantity with an epsilon. n other words, you cant make an exponent thats any smaller than this. You can take coefficients and play with them, but you cant make the exponent any smaller. OK, all of these things you can do them by pictures. know many of you dont like doing things by pictures. keep doing them by pictures because keep trying to convince you that pictures are more rigorous than equations are. At least, many times. f you want to show that something is convex, you try to show that the second derivative is positive. That works sometimes and it doesnt work sometimes. mean, it works as a function is continuous and has a continuous first derivative. t doesnt work. otherwise. When you start taking tangents of the curve, and you say the upper envelope of the tangents to the curve all lie below the function, then it works perfectly. Thats what a convex function is by definition. How do we derive all this stuff? What were trying to do is to find mean, this inequality here is true for all r, for all r greater than 0 so long as a is greater than the mean of X. ts true for all r for which this momentgenerating function exists. omentgenerating functions can sometimes blow up, so they dont exist everywhere. o its true wherever the momentgenerating function exists. o we like to find the r for which this bound is tightest. o what m going to do is draw a picture and show you where its tightest in terms of the picture. What ve drawn here is the semiinvariant momentgenerating function. Why didnt put that down? This is gamma of r. Gamma of r at 0, its the log of the momentgenerating function at 0, which is 0. ts convex. You take its second derivative. ts second derivative at r equals 0 is pretty easy. ts second derivative of other values or r you have to struggle with it. But when you struggle a little bit, it is convex. f youve got a curve that goes down like this, then it goes back up again. ometimes goes off towards infinity. ight do whatever it wants to do. ometimes at a certain value of r, it stops existing. uppose take the simplest random variable you know about. You only know two simple random variables. One of them is a binary random variable. The other ones an exponential random variable. uppose take the exponential random variable with density alpha times e to the minus alpha X. Where does this momentgenerating function exist? You take alpha and multiply it by e to the rX when integrate it. Where does this exist? mean, dont bother to integrate it. f r is bigger than alpha, this exponent is bigger than this exponent. And this thing takes off towards infinity. f r is less than a, the whole thing goes to 0. gX of r exists for r less than alpha in this case. And in general, if you look at a momentgenerating function, if the tail of that distribution function is going to 0 exponentially, you find the rate at which its going to 0 exponentially. And thats where the momentgenerating function cuts off. t has to cut off. You cant show a result like this, which says something is going to 0, faster than it could possibly be going to 0. o we have to have that kind of result. But anyway, we draw this curve. This is mu sub X of r. And then we say, how do we graphically minimize gamma of r minus r times a? Well, what do because ve done this before and know how to do it mean, its not the kind of thing where if you sat down you would immediately settle on this. look at some particular value of r. f take a line of slope gamma prime of r, thats a tangent to this curve because this curve is convex. o if take a line through here of this slope and look at where this line hits here, where does it hit? t hits at gamma sub X of r, this point here, minus gamma X of r oh. Well, what ve done is ve already optimized the problem. m trying to find the probability that n is greater than or equal to na. m trying to minimize this exponent here, gamma X of r minus ra. nfortunately, really start out by taking the derivative of that and setting it equal to 0, which is what you would all do, too. When set the derivative of this equal to 0, get gamma prime of r minus a is equal to 0, which is what this says. o then we take a line of slope gamma x of r equals 0. ts tangent at this point here. You look at this point over here and you get the minimum value of the gamma X of r minus r0 a. o what this says is when you vary a, you can go through this maximization tilting this curve around. mean, a determines the slope of this line here. f use a smaller value of a, the slope is smaller. t hits in here. f take a larger value of a, it comes in further down and the exponent gets bigger. Thats not surprising. want to find out the probability that sub n is greater than or equal to a. As increase a, expect this exponent to keep going down as make a bigger and bigger because its harder and harder for it to be greater than or equal to a. o anyway, when you optimize this, you get something exponentially tight. And this is what its equal to. And would recommend that you go back and read the section of chapter 1, which goes through all of this in a little more detail. Let me go passed that. Dont want to talk about that. Well, when do this optimization, if what m looking at is the probability that sub n is greater than or equal to some alpha rather than n times a when m do this optimization and m looking at what happens at different values of n, it turns out that when n is very big, you get something which is tangent there. As n gets smaller, you get these tangents that come down that comes in to there, and then it starts going back out again. This e to the r star is the tightest the bound ever gets. Thats the n at which errors in the hypothesis testing usually occur. ts the point at which its the n for which n greater than or equal to alpha is most likely to occur. And if you evaluate that for our friendly binary case again, X equals 1 or X equals minus 1, what you find when you evaluate that point alpha r star is that r star is equal to log 1 minus P over P. And our bound of probability union of n is greater than or equal to alpha is approximately e to the minus alpha r star is 1 minus P over P to the minus alpha. mean, why do torture you with this? Because we solved this problem at the beginning of the lecture, remember? The probability that the sum sub n for this binary experiment is greater than or equal to k is equal to 1 minus P over P to the minus k. Thats what its equal to exactly. When go through all of this hernoff bound stuff, get the same answer. Now, this is a much harder way to do it, but this is a general way of doing it. And thats a very specialized way of doing it. o well talk more about this next time.","ts the probability that Y is in A. That means that H hat is equal to 1 given that H is actually 0. o thats the probability we make an error given the hypothesis, the correct hypothesis is 0. o if gX of r is e to the rX, then e to the e to the r n n is the sum of these random variables is the expected value of the product of e to the rXi. n other words, when you under hypothesis 0, if calculate the probability of vector y given H equals 0, m finding the probability of n things which are D. o what m going to find this probability density is taking the product of the probabilities of each of the observations. ost of you know now that any time you look at a probability, which is a product of observations, what youd really like to do is to take the logarithm of it. f you take that product of terms g to the r to the nth power and you visualize the logarithm of g to the X. Visualize the logarithm of g to the X, then you get this quantity up here. We know because of maximum a posteriori probability for the threshold test that this is less than or equal to this. But what the bound says is the probability of Z is greater than or equal to b is this inequality. This is the probability of error if you choose 0. You get the probability that n is greater than or equal to na is this e to the n times gamma x of r minus ra. The thing that were interested in is that this exponent here as you visualize doing this experiment and taking additional observations, what happens is the probability that you exceed na that the nth sum exceeds n times some fixed quantity a is going down exponentially with Now, is this bound any good? We want to minimize the probability of error and we see the observation Y, we want to pick the one of these which is largest. Whats going to happen is that under one hypothesis, the expected value of this log likelihood ratio is going to linearly increase with n. f we look at it under the other hypothesis, its going to linearly decrease as we increase n. And a nifty test at that point is to say, as soon as it crosses a threshold up here or a threshold down here, were going to make a decision. You look at this point over here and you get the minimum value of the gamma X of r minus r0 a. o what this says is when you vary a, you can go through this maximization tilting this curve around. PROFEOR: Except if you start out with P of A given B is equal to P of B given A times P of B divided by P of A. This quantity here is P of Y. o we have probability that H equals l times probability of Y given l divided by the probability of l to start with. Now, the other thing to not forget in this because you really have to get this model in your mind or youre going to get very confused with all the things we do. The probability that the sum sub n for this binary experiment is greater than or equal to k is equal to 1 minus P over P to the minus k. Thats what its equal to exactly. Well, when do this optimization, if what m looking at is the probability that sub n is greater than or equal to some alpha rather than n times a when m do this optimization and m looking at what happens at different values of n, it turns out that when n is very big, you get something which is tangent there. o this is the probability that H is equal to 0 given Y. f we select 1 under these conditions, if we select hypothesis 1, if we make a decision and say, m going to guess that 1 is the right decision. All of these observations point to that or they point to the other and you make a decision. o what do you do if you want to minimize the probability of error? But whatever you do, this says this is less than or equal to this because of the AP rule. The probability of error is? But if you once assume that these two hypotheses that youre trying to choose between, that they have a priori probabilities, then people get very upset about it because they say, well, if what the a priori probabilities are, why do you have to do a hypothesis test? And this is what its equal to. f ve got in this particular sequence Y, this quantity here is, in fact, the probability that hypothesis 0 is correct in the model that we have chosen. Q sub 0 of A is the probability that actually choose its the probability that choose A given that the hypothesis is 0. After you go to 2, you wander all around, and eventually you make it up to 3. f you do, then the question is, do you ever get from 3 to 4, and so forth. When set the derivative of this equal to 0, get gamma prime of r minus a is equal to 0, which is what this says. ADENE: When you say you have to use this test or this test, are you talking about threshold or are you talking about because this is always its either H equals 0 or H equal 1, right? And at that point, you find the probability of error given H equals 1. Assume that on the basis of observing a sample value of this sequence of observations, we have to make a decision about H. We have to choose H equals 0 or H equals 1. want to find out the probability that sub n is greater than or equal to a. As increase a, expect this exponent to keep going down as make a bigger and bigger because its harder and harder for it to be greater than or equal to a. o anyway, when you optimize this, you get something exponentially tight. o what we know about it is that this quantity is less than or equal to this quantity. ll just take the P0 out, so its Q0 of A plus P1 over P0 Q1 of A. o thats what ve called eta times Q1 of A. For the threshold test based on eta, the probability of error is the same thing. For most of the tests that you deal with, NeymanPearson test is just the threshold test thats at that particular point. The derivative with respect to r of the logarithm of g of r is first derivative of r divided by g of r. And the second derivative is then the natural log of g of r. Taking the derivative of that is equal to g double prime of r over g of r squared. And at that point, we have an a posteriori probability that each of the hypotheses is correct. You can do this for any eta that you want to do it for. f you take the derivative of this with respect to r, evaluate it at r equals 0, you get the expected value of Z. f you take the second derivative evaluated at r equals 0, you get the expected value of Z squared, and so forth. And the error probability for each hypothesis using test A is given by and well just call Q sub 0 of A this is our name for the error probability. OK, so what the hernoff bound says is that the probability that a random variable Z is greater than or equal to some constant b. We dont even need sums of random variables here, its just a hernoff bound is a bound on the tail of a distribution. This was the probability that youre going to make an error if you choose 1. And this is the upper envelope of the straight lines of slope minus eta that go through the threshold tests at eta. This is the error probability you get with whatever test you like. mean, we now sort of see that these tests well, one thing weve seen is when you have to make a decision under this kind of probabilistic model weve been talking about namely, two hypotheses, D random variable is conditional on each hypothesis. You make the number of observations all based on this same hypothesis, and you make as many of these D observations conditional on that observation as you choose. But however we choose it, we choose our test to say, we cant make the probability of error under one hypothesis bigger than some certain amount alpha than what test will minimize the probability of error under the other hypothesis. o the ratio, the probability that H equals 0 given y over the probability that H is 1 equals y is just this ratio here. o what this says is the NeymanPearson test, which is the test that says pick some alpha, which is the error probability under hypothesis 1 that youre willing to tolerate. When you have two hypotheses, you call the ratio of one of them to the other one the likelihood ratio. And it turns out that since you can only go up 1 each time, the probability of getting up to some point k is the probability you ever got up to 1. This axis is the error probability given that 0 is the correct hypothesis. This is the decision rule that minimizes the probability of an error. And the NeymanPearson test, what it does is it minimizes the error probability under the other hypothesis. And this point here is whatever it happens to be for any test that you happen to like. And we found that the solution to that problem was p over 1 minus p to the kth power of p is less than or equal to 1/2. We dont know whether H equals 0 or H equals 1 is what the result of the experiment is going to be. Q1 of A is the probability of making an error given that the correct hypothesis is 1. f have a priori probabilities, m going back to assuming a priori probabilities again. o we take the upper envelope of all of these lines, and we get something that looks like this. And after the one sample point comes out, then you know what the result of the experiment is. Well, what do because ve done this before and know how to do it mean, its not the kind of thing where if you sat down you would immediately settle on this. And we have this well, we have the likelihood ratio, but we dont care about that for the moment. And if you evaluate that for our friendly binary case again, X equals 1 or X equals minus 1, what you find when you evaluate that point alpha r star is that r star is equal to log 1 minus P over P. And our bound of probability union of n is greater than or equal to alpha is approximately e to the minus alpha r star is 1 minus P over P to the minus alpha. This quantity here is the probability that youve made a mistake given that 1 is the correct hypothesis. Namely, the overall probability of that observation period, which is the sum of probability that 0 is a correct hypothesis times this plus probability that 1 is a correct hypothesis times the density given 1. f you want to get error probabilities in the middle here, what do you do? Now, whats the probability of error if we make a decision at this point? o if want to write this, now m applying the hernoff bound to the random variable sub n. Whats the probability that sub n is greater than or equal to n times a? ADENE: dont quite see how to P of A given B is equal to P of B given A times P of A divided by P of A and B. f you take this over there then its am stating Bayes law in a funny way? Now, lets go back and look at threshold tests again, and try to see how were going to make threshold tests, what the error probabilities will be, and try to analyze them a little more than just saying, well, a AP test does this. What were trying to do is to find mean, this inequality here is true for all r, for all r greater than 0 so long as a is greater than the mean of X. ts true for all r for which this momentgenerating function exists. What were going to find today is that when you use this assumption of a probability model, you can answer the questions that these classical statisticians go to great pains to answer. ometimes when youre at a point like this, you have to use this test and this test sometimes. When you start taking tangents of the curve, and you say the upper envelope of the tangents to the curve all lie below the function, then it works perfectly. What weve shown is that this is less than or equal to this. o the AP rule is to calculate the likelihood ratio for this given observation y. And if this is greater than P1 over P0, you select H equals 0. f its less than or equal to P1 over P0, you select H1. You told us that because of maximum a posteriori probability, if eta is equal to P0 divided by P1, then the probability of error is minimized. This is gamma of r. Gamma of r at 0, its the log of the momentgenerating function at 0, which is 0. ts convex. t can only take on the value 0 and 1, so it has to have probabilities of being 0 and 1. ts equal to the a priori probability of that hypothesis times the density of the observation conditional on the hypothesis divided by just a normalization factor. The probability experiment here really mean, every probability model we view in terms of the real world, as you have this set of probabilities, a set of possible events. And the unusual point of view is that we refuse to take a probability model. And thats all there is to it. And in general, if you look at a momentgenerating function, if the tail of that distribution function is going to 0 exponentially, you find the rate at which its going to 0 exponentially. Take out the P0s and it says that this quantity is less than or equal to this quantity. But otherwise, probability that H equals l is the correct hypothesis given the observation is probability that H equals L given Y. We maximize the a posteriori probability of choosing correctly by choosing the maximum over l of probability that H equals l given Y. This choosing directly, maximizing the a posteriori probability is called the AP rule, aximum A posteriori Probability. We can talk about the probability that H is equal to either 0 or 1, conditional on the sample point weve observed. Q0 of A is the probability that m going to choose hypothesis 1 given that hypothesis 0 was the correct hypothesis. PROFEOR: This axis here represents the error probability given that H equals 1 is the correct hypothesis. And the particular kind of observation that we restrict attention to here is a sequence of random variables, which we call the observation. What do you mean when you say you have to randomize between the two tests? nfortunately, really start out by taking the derivative of that and setting it equal to 0, which is what you would all do, too. The NeymanPearson test says, for some reason or other, want to make sure that the probability that my nuclear plant doesnt blow up is less than, say, 10 to the minus fifth. Q sub 1 of A is the probability that choose 1. You recognize that if its going to explode, and you choose that its not going to explode and you dont do anything, there is a humongous cost associated with that. Namely, if have to get one thing right, or have to get it right almost all the time, whats the best can do on the other alternative? And then it says, minimize the error probability of the other kind, so you read over there. o since you dont quite know what to do in between those points, as far as the maximum a posteriori probability test goes, you can reach any one of those points, sometimes using one test on one corner of guess its easier if draw it. mean, theres nothing that says that you really want to minimize the probability of error. But in this model, H is either 0 or 1 in the result of this experiment. Well, you might like to take the second derivative of it, but thats a pain in the neck. Well, to give you a better definition of what u of alpha is, u of alpha is the error probability under hypothesis 1 if the error probability under hypothesis 0 was alpha. Namely, whats the probability that this random walk is going to cross some threshold by or at some particular value of n? And the second question is, if were going to use a threshold test, where should we set the threshold? Then, when you want to look at this AP rule, the AP rule is choosing the larger of these two things, which we had back here. Because this is the probability that H is actually 0 rather than 1. ADENE: How do you know that the threshold tests lie on the curve? And its piecewise linear, and the threshold tests are at the points of that curve. You cant show a result like this, which says something is going to 0, faster than it could possibly be going to 0. o we have to have that kind of result. You can see that by just taking the derivative of that. And three and this is the most important point you make 100 observations of something. And m sure all of you can rewrite that and evaluate it at r equals 0. o thats why its called the semiinvariant momentgenerating function. mean, you look at a bunch of data and you have to do something. And if the likelihood ratio is bigger than 1, you go this way. o for every eta that we want to use, we get some value of Q0 of eta and Q1 of eta. And so the errors of the test A are . ts the point at which its the n for which n greater than or equal to alpha is most likely to occur. This is the arkov inequality for the random variable e to the rZ. This is probably one of these things you have to think about a little bit. And know that this quantity is greater than or equal to this quantity. f the world is this way, then all of these hypotheses are D. Youre doing the same experiment again and again and again, but its based on the same underlying hypothesis. And in doing that, you have to take a probabilistic model. m putting that in explicitly, is the sum from N equals 1 to m of the log of the individual ratio. nder the AP test, probability of error is this quantity here. And even far more important than that is this question of detection, or making decisions, or hypothesis testing, all of which are the same thing. When most people talk about momentgenerating functions, and certainly when people talked about momentgenerating functions before the 1950s or so, what they were always interested in is the fact that if you take derivatives of the momentgenerating functions, you generate the moments of the random variable. am particularly interested in the fact that this upper envelope is, in fact, a convex curve and that the threshold tests lie on the curve. Given that you got up to 1, its the probability that you ever got up to 2. And the question is, whats the probability that you will cross some threshold at some k greater than 0?",0.1025072488487122
68,609,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer highquality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. ANA BELL: o were given this definition for car. We saw that on the previous slide. want to add a method thats going to change the color of the car, and these are my four choices. And it looks like you guys are getting it right, which is awesome. o to find a method to change the color of the car, which does this. o we know that self has to be the first parameter, so we can automatically eliminate A and and its between B and D. And remember said you have to be conscious about whose data attribute youre accessing, and in this case, we want to change the color of a particular instance of the car, right? o we have to say self.color instead of just color. f we just said color, then color would refer to just a variable inside the class, not a data attribute of a particular object.","o to find a method to change the color of the car, which does this. o we know that self has to be the first parameter, so we can automatically eliminate A and and its between B and D. And remember said you have to be conscious about whose data attribute youre accessing, and in this case, we want to change the color of a particular instance of the car, right? want to add a method thats going to change the color of the car, and these are my four choices.",0.3738317757009345
69,610,"n this problem, well be looking at an ambulance that is traveling back and forth in interval of size l. ay from 0 to l. At some point in time, theres an accident occurring, lets say at location x. And well assume the accident occurs in a random location so that x is uniformly distributed between 0 and l. Now, at this point in time, lets say the ambulance turns out to be at location y. Again, well assume that y is a uniform random variable between 0 and l, and also that x and y are independently distributed. The question were interested in answering is how long it would take an ambulance to respond to travel from point y to point x. Lets call this time T. And in particular, we want to know everything about distribution of T. For example, what is the DF of T given by the probability of big T, less than or equal to little t, or the PDF, which is done by differentiating the DF once we have it. Now, to start, well express T you as a function of X and Y. ince we know that the ambulance travels at a speed V V meters or V units of distance per second then we can write that big T is simply equal to Y minus X, absolute value the distance between X and Y, divided by the speed at which the ambulance is traveling at, V. o now if we look at the probability of T less than or equal to little t, this is then equal to the probability that Y minus X divided by V less than or equal to little t. We now take off the absolute value by writing the expression as negative vt less equal to Y minus X less equal to positive vt. Here we multiply v on the other side of t, and then took out the absolute value sign. As a final step, well also move X to the other side of inequalities by writing this as X minus vt less equal to y less equal to x plus vt. To compute this quantity, well define a set A as a set of all points that satisfies this condition right here. n particular, its a pair of all X and Y such that X minus vt less equal to little y less equal to X plus vt, and also that X is within 0 and l, and so is Y. o the set A will be the set of values well be integrating over. Now that we have A, we can express the above probability as the integral of all X and Y, this pair within the set A, integrating the PDF of f of X, Y, little x, little y. Lets now evaluate this expression right here in a graphical way. On the right, were plotting out what we just illustrated here, where the shaded region is precisely the set A. As we can see, this is a set of values of X and Y where Y is sandwiched between two lines, the upper one being X plus vt right here, and the lower line being X minus vt, right here. o these are the values that correspond to the set A. Now that we have A, lets look f of x, y. We know that both x and y are uniform random variables between 0 and l, and therefore, since theyre independent, the probability density of x and y being at any point between 0 and l is precisely 1 over l squared, where l squared is the size of this square box right here. o given this picture, all we need to do is to multiply by 1 over l squared the area of the region A. And depending on the value of T, well get different answers as right here. f T is less than 0, obviously, the area of A diminishes to nothing, so we get 0. f T is greater than l over V, the area of A fills up the entire square, and we get 1. Now, if T is somewhere in between 0 and l over v, we will have 1 over l squared, multiply by the area looking like something like that right here the shaded region. Now, if you wonder how we arrive at exactly this expression right here, here is a simple way to calculate it. What we want is 1 over l squared times the area A. Now, area A can be viewed as the entire square, l squared, minus whatevers not in area A, which is these two triangles right here. Now, each triangle has area 1/2, l minus vt squared. This multiply 2, and this, after some algebra, will give the answer right here. At this point, we have obtained the probability of big T less equal to little t. Namely, we have gotten the DF for T. And as a final step, we can also compute the probability density function for T. Well call it little f of t. And we do so by simply differentiating the DF in different regions of T. To begin, well look at t between 0 and l over v right here at differentiating the expression right here with respect to t. And doing so will give us 2v over l minus 2v squared t over L squared. And this applies to t greater or equal to 0, less than l/v. Now, any other region, either t less than 0 or t greater than l/v, we have a constant for the DF, and hence its derivative will be 0. o this is for any other t. We call it otherwise. Now, this completely characterized the PDF of big T, and hence, weve also finished a problem.","Now, to start, well express T you as a function of X and Y. ince we know that the ambulance travels at a speed V V meters or V units of distance per second then we can write that big T is simply equal to Y minus X, absolute value the distance between X and Y, divided by the speed at which the ambulance is traveling at, V. o now if we look at the probability of T less than or equal to little t, this is then equal to the probability that Y minus X divided by V less than or equal to little t. We now take off the absolute value by writing the expression as negative vt less equal to Y minus X less equal to positive vt. At this point, we have obtained the probability of big T less equal to little t. Namely, we have gotten the DF for T. And as a final step, we can also compute the probability density function for T. Well call it little f of t. And we do so by simply differentiating the DF in different regions of T. To begin, well look at t between 0 and l over v right here at differentiating the expression right here with respect to t. And doing so will give us 2v over l minus 2v squared t over L squared. n particular, its a pair of all X and Y such that X minus vt less equal to little y less equal to X plus vt, and also that X is within 0 and l, and so is Y. o the set A will be the set of values well be integrating over. o given this picture, all we need to do is to multiply by 1 over l squared the area of the region A. And depending on the value of T, well get different answers as right here. o these are the values that correspond to the set A. Now that we have A, lets look f of x, y. We know that both x and y are uniform random variables between 0 and l, and therefore, since theyre independent, the probability density of x and y being at any point between 0 and l is precisely 1 over l squared, where l squared is the size of this square box right here. The question were interested in answering is how long it would take an ambulance to respond to travel from point y to point x. Lets call this time T. And in particular, we want to know everything about distribution of T. For example, what is the DF of T given by the probability of big T, less than or equal to little t, or the PDF, which is done by differentiating the DF once we have it. Now that we have A, we can express the above probability as the integral of all X and Y, this pair within the set A, integrating the PDF of f of X, Y, little x, little y. Lets now evaluate this expression right here in a graphical way.",0.346793349168646
70,611,"The following content is provided by T OpenourseWare under a reative ommons license. Additional information about our license and OpenourseWare in general is available at ocw.mit.edu. PROFEOR: OK. o ll start a little formally. That this is 18.086 in pring semester of 2006. And were lucky to have it videotaped for OpenourseWare. o, let me begin by remembering the main topics from what you might call the first half, 18.085, the Fall course. And then, most important to us, so it gets a star, are the main topics for this course. o 18.085 began with applied linear algebra. The discrete equations of mechanics, and physics and engineering. And the type of matrices that it involved, so we learned what positive definite matrices are. Then the center of the course was differential equations. Ordinary differential equations, so that was 1D, partial differential equations like Laplace, that was 2D, with boundary values. o that led to systems of equations. And then the third big topic was Fourier methods. All of Fourier ideas: series, integrals discrete transform. o thats like the first half of the course. Well, thats 18.085. OK. And here we are. o this course has, also, three major topics. The first, the one we start on today, is differential equations that start from initial values. o m thinking of the wave equation, where were given the displacement of velocity. The heat equation. Blackcholes equation, which comes from finance, is a version of the heat equation. Ordinary differential equations, thats going to be today. And nonlinear equations, Naviertokes ultimately. But not heavily Naviertokes. o this is where we begin. And then, a second big topic is how to solve. Because this course is really applied math and scientific computing. The second big topic is, how do you solve a large linear system A*x equal b? Do you do it by direct methods, elimination? With renumbering of nodes, and there are lots of tricks that makes elimination very successful. Or do you do it by iterative methods? ultigrid is one. o this is really modern scientific computing were talking about. Were really at the point where people are computing. And then the third topic, which is a little different, is optimization, minimization. Linear programming would be one example, but among many, many. o thats a major area in applied math. OK. o this is our topic, differential equations. And was asked before the tape started about my own background. And my thesis was in this topic. Because a key question will be stability. s the difference method stable? Youll see that thats a requirement to be any good. You really have to sort out what and that usually puts a limitation on the time step. o were going to have a time step. And it may be limited by the requirement of stability. And the fact that stability and convergence success of the method are so intimately linked. There was a key paper by Lax and Richtmyer that well look at. ts known now as the Lax Equivalence Theorem, stability and convergence. Well see it in detail. Actually when was a grad student, by good chance, there was a seminar, and was asked to report on some paper and it happened to be that one. And just, looking back, think, gosh, was lucky. Because that paper, the LaxRichtmyer paper with the Lax Equivalence Theorem, set the question of stability. And then years of effort went into finding tests for stability how do you decide, is the method stable? o well see some of that. OK. ince then ve worked on other things, well, wavelets would be one that relates to Fourier. And other topics too. And lots of linear algebra. But its a pleasure to look back to the beginning of . OK. o thats the OpenourseWare site which does have the course outline, and other information about the course. ome of which have mentioned informally before the class. This is our central website, with no decimal in 18086, which will have notes coming up, ATLAB things, problems. Thats our content. t will be the 086 page. OK. o thats the course outline in three lines. And then the web page and a handout will give you half a dozen subheadings under those. OK. o thats the course. And actually its the center of scientific computing. There would be other courses at T covering material like that, because its just so basic you cant go without it. All right, so now m ready to start on the first lecture, which will be ordinary differential equations. And wont always have things so clearly and beautifully written on the board, but today m better organized. o heres our problem. Ordinary differential equations. o were given initial values u at t equals 0. OK. o theres always an initial value here. The question is, whats the equation that makes it evolve? OK. o, often well think in terms of one equation. The unknown will be u, u of t. Or, in reality there are typically N equations. N might be 6 or 12 or something in a small problem. But also, N could be very large. Well see how easily that happens. OK. And often, to discuss stability or understand a method and try it, the linear case is the natural one. o ll use a*u with a small a to make us think that weve got a scalar, or a capital A to indicate were talking about a matrix. o, the correspondence between these and the more general so these are linear. These are likely to be not linear. But theres a natural match between the number a there corresponds to the partial of f with respect to u. And of course we see it. f that was f, then its derivative is indeed a. And in this matrix case, there would be a Jacobian matrix. f we have N righthand sides, because have N us, and the matrix that comes in is the derivative of righthand side in equation i with respect to unknown j. o its a N by N matrix, and of course, this is the case where its a constant. Yeah, should have said, these are not only linear, but constant coefficients. o we know of course the solution, its an exponential, e to the a*t times u_0, the initial bunch. o we know everything about it. Nevertheless, its the best test case for difference methods. And let me just comment that we do have to pay attention to the possibility that a could be complex. ostly ll think of a as a real, and often negative. Often the equation will be stable. A negative a means that e to the a*t decreases. A negative definite matrix, negative eigenvalues, mean that the matrix exponential decays. And if that matrix is symmetric, those eigenvalues are all real, thats a key case. ymmetric matrices have real eigenvalues, and theyre the most important, most attractive class. And a negative definite matrix might come from a diffusion term, like second derivative, d second u, dx squared. But a convection term, du/dx, from a first derivative, will not be symmetric. n fact, maybe itll be antisymmetric. t will push the eigenvalues into the complex plane. And therefore, we have to pay attention to the possibility of complex a. As you know, the whole point of eigenvalues is that we can understand matrices to a very large extent by diagonalizing, by reducing them to N scalar equations with the eigenvalues as the little as in those scalar equations. o anyway, thats the set up. Now want to say something about methods. want to introduce some of the names. What am thinking here? This topic, todays topic of ordinary differential equations, is not going to last long in 18.086. might still be discussing it tomorrow. Friday, mean. But not much after that. Well soon be on to partial differential equations. o want to sort of tell you about ordinary differential equations, well, in kind of an organized way, so that you know the competing methods. And also so that you know this key distinction between nonstiff is a typical equation, u prime equal minus 4u would be a nonstiff, completely normal equation. And for that, those are the relatively easy ones to solve. We can use explicit differences. And ll say what that word explicit means. What want to do is just, like, help you organize in your mind. o nonstiff are the average, everyday differential equations. For those we can use explicit methods that are fast. And we can make them accurate. And explicit means so this is the first meaning that compute the new and ll use capital at time step n plus 1. o thats capital at the new time. Natural to call that the new time. an be computed directly from at the old time, maybe at the time before that, maybe at the times before that. n EE terms its causal. The new comes explicitly by some formula that well construct from the previous computations. ts fast. Of course, not only use _n but used f of _n. o should say from _n itself and from f of _n at time t_n, and similarly for earlier times. You see how fast that is? Because the only new calculation of f is this one. The previous step produced _n, and at the new step, this step, we plug it in to find out what the slope is. And that may be the expensive calculation, because f could involve all sorts of functions, all sorts of physical constants. t can be complicated or not. but if it is, we only have to do it once here. Thats explicit. Now, whats the opposite? mplicit. o the point about implicit methods is the difference equation, the formula, involves not just _(n+1), the new value, but also the slope at that new value, at that new unknown value. o it means that an implicit equation could be nonlinear. Because if f is not linear, then this term could involve cosines, or exponentials or whatever, of the unknown _(n+1). o that, you cant do it as fast. o this is definitely slower per step. But, the advantage, and the reason it gets used for stiff problems, is that on stiff problems it will allow a much larger step. o its this constant tradeoff of speed versus stability. These are less stable, as well see, and well get to see what stability means. These are slower but more stable, implicit methods. OK. didnt yet say what stiff is. ll say that in a moment. OK. o explicit versus implicit. _(n+1) immediately or _(n+1) only by maybe using Newtons method to solve an equation, because _(n+1) appears in a complicated formula. And we have an equation to solve. OK. o now, this part of the board begins to identify different methods. And ll follow the same twocolumn system that nonstiff versus stiff. o this will be explicit and those will be implicit. OK. o the one method that occurs to everybody right away is Eulers method. And that will be the first method well construct, of course. o thats the first idea anybody would have. t has the minimum accuracy. ts first order. The order of accuracy will be an issue for all methods. And Euler has p equal 1, order p equal 1, first order. o its too crude. mean, if you wanted to tracking a space station with Eulers method, you would lose it real fast, or you would have to take delta t so small that it would be impossible. o Eulers method is the first one you think of, but not the one you finish with. And similarly on the implicit side, backward Euler, well see, is again the first implicit method you think of. But in the end, you probably want to do that. OK. o those are two specific methods that are easy. The first ones well write down. Now then comes families of methods, especially these Adams families. o AdamsBashforth is explicit, Adamsoulton are implicit, and the coefficients are in books on numerical analysis and will be on the web. And ll say more about them of course. And see the first ones. o what AdamsBashforth, how does does it differ from Adamsoulton? OK. o those are multistep methods. Number two was multistep methods. To get more accurate we use more old values. o Adamsoulton and AdamsBashforth, or especially AdamsBashforth, would use several old values to get the order of accuracy up high. OK. Now this third category. Of course, actually Euler would be the first of the AdamsBashforth, and backward Euler might be early in Adamsoulton. Or maybe backward Euler is early in backward differences. OK. o what want to say is, RungeKutta is like a different approach to constructing methods. You might know what some of these already. And its a onestep method. Thats a method that just produces, by sort of halfsteps, you could say, it gets to _(n+1). But its explicit. And the code in ATLAB, the code ODE fortyfive, or maybe should say ODE four five, is like the workhorse of ODEs. guess m hoping youll try that. Youll find ODE45. And ll try to get it onto the web, but you could just easily discover the syntax to call it, and apply it to an equation. Yeah ll get examples on the web, and please do the examples. o this is the 4, 5 typically means that it use a fourthorder RungeKutta, so thats pretty accurate. To get up to a fourth order means the error up at time t equal 1, say, is delta t to the fourth. o by cutting delta t in half, you divide the error by 16. And then it also uses a fifthorder one. And maybe can point to these words. What makes ODE45 four five good, successful. mean, how does it work? t slows down or speeds up. t speeds up when it can, it slows down when it has to. And speed up means a bigger delta t, slow down means a smaller delta t, for the sake of stability or accuracy. Yeah. o if the equation is suddenly doing something, if the solution is suddenly doing something, you know, important and quick, then probably at that period delta t will get reduced automatically by ODE45. tll cut it in half, cut in half again, half again. Every good code is constantly estimating, using internal checks to estimate what error its making. About the error. Just a quick word about the error. o ODE45, it will, unless you tell it otherwise, the default accuracy that it will adjust for is relative. o ODE45 will have a relative accuracy so can just squeeze in a few words of 10 to the minus 3. tll shoot for that. And an absolute accuracy of 10 to the minus 6. o it will try to keep the error it will plan to keep the error, and itll tell you if it has a problem, below 10 to the minus 6. And you could set that differently. OK. hope you experiment a little with ODE45. And you can either plot the solutions, some exponential. mean, push it. Like, let delta t be pretty big, and see where it breaks down. Well, guess ODE45 is engineered not to break down. f you try a delta t itll decide on what delta t it can do. o well also code, just quickly, some methods by ourselves, and see for a large delta t, what problems could happen. OK. And then backwards. This says backward differences. Thats the category of implicit methods for stiff equation, and backward Euler would be the very first. And ODE15 is the code that, is the most used code, perhaps, for stiff equations. And it will do the same thing, it will vary delta t. t was will vary the order. o, if it has to slow down, it may slow. And if things are happening too quickly, it may change to a loworder, safe, secure method. When things are, you know, when the satellite is buzzing out in space, and it can take giant steps or it can have highorder accuracy, say in astronomy, of course, they would go up to eighth order and higher, tracking, estimating where stars would go or planets, these will do that automatically, up to a certain point. And we could write codes, and of course people have, for AdamsBashforth and Adamsoulton, varying delta t, varying p. guess heres a comment. f had given this lecture yesterday, would have emphasized number two, AdamsBashforth and Adamsoulton, because books do it and basically learned this subject from the major textbooks on ODEs. But had a conversation with one of the computational scientists in the math department, and that changed the lecture. Because learned that although books tend to discuss those, the Adams methods, in practice these methods, the methods three RungeKutta and backward differences are the most used. And thats reflected in the fact that these two major codes that are available in ATLAB are in the number three category. OK. o now, thats a picture with name but not details, right? And havent even said what stiff means. o somewhere ve written something about stiff. Yeah. OK. o stiff means so, if had only e to the minus t in the solution, then the solution would decay at that rate, e to the minus t. That time step would adjust to keep it accurate. And if have only e to the minus 99t, then again that of course is a faster decay, much faster decay so the time step would adjust to be much smaller, probably about 1/99 or something. Anyway, much smaller, to capture within each step the correct behavior of e to the minus 99t, and no problem. o in other words, its not the minus 99 that makes it stiff. f it was only the minus 99 in other words, if had a equal minus 99 up there, wouldnt call it a stiff equation. What makes it stiff is the combination here. You see, whats going to happen as time gets anywhere, this is going to control u of t. The decay of u of t is going to have that time constant 1. But this is going to control so ll say but this would control delta t for explicit methods. And just picked minus 99, but it could have been minus 999 or far worse. o it could be very stiff. This word stiff and identifying this class of problems, which now is familiar to everybody who has to compute, didnt come, you know, with Gauss and so on. t came much more recently. But its become very important now, to distinguish stiff from nonstiff. o where does stiff problems arise? Well you can see how they arise in chemical processes with very different decay rates, biological processes, control theory, all sorts of cases where theres a big dynamic range of rates. And they also arise in systems, because the eigenvalues could be minus 1 and minus 99. can easily create a matrix that has those two eigenvalues. o it would be a system let me create such a matrix. think probably, if put minus 50s on the diagonal, that gives trace sum down the diagonal is minus 100. And that should equal the sum of the eigenvalues. o, so far so good. And now, if want these two particular numbers, could put 49 here. o that matrix has those two eigenvalues. mean, this is the kind of matrix, well you might say ill conditioned would be a word that you hear for a matrix like this. The condition number is the ratio so the condition number of this matrix would be the ratio 99 over 1. And that condition number is a number that ATLAB computes all the time. Every time you give it a matrix problem like this, it estimate, not computes. Because to compute the condition number requires solving an eigenvalue problem, and it does not want to take forever to do that exactly. But it gets an estimate. o thats only a moderate condition number of 99. Thats not a disaster, but it makes the point. OK. o thats what stiff equations are. OK. Now ve got one more board prepared. wont have this every day, but let me use it while got it. OK. ts only prepared in the sense of telling us what were going to do. o this is whats coming: a construction of methods, what stability is about, and whats convergence. o let me begin, let me get Euler constructed today. And backward Euler. o Eulers method will be the most obvious method, equals f at _n. o, in the model case, its a*_n. o thats Euler. o that tells us then that we could rewrite it _(n+1) is explicitly 1 plus a delta t _n. Right? ve moved up the delta t and moved over the _n, and its simple like that. OK. o whats the solution after n time steps? Let me stay with Euler for a moment. After n steps, every step just multiplied by this growth factor. Let me refer to that as the growth factor even if its, as hope, smaller than 1. o _n, after n steps, is this thing has multiplied n times _0. And what is the stability now? tability is suppose a is negative, typically. That means often well think of that model, a negative, so that the differential equation is completely stable. Right? e to the a*t, with a negative, is perfect. The question is, is this one perfect? o stability will be, is this number below 1? Thats the test. s that we could argue about, are we going to let it equal 1? aybe ll let it equal 1. o 1 plus a delta t smaller than 1. Thatll be the stability requirement for forward Euler. And whats the a delta t? Whats the boundary? Whats the critical value of a delta t? Remember a negative. How negative can it be, or how negative can this combination be? You see, its a combination a delta t. o let me put above here what the stability condition is. o the stability condition on Euler is going to be so do you see what it is? How negative can a delta t be? Go ahead, you dont even have to push the well, lets see. o if a delta t, this is a negative number, so this is dragging us down from 1 always. Right? a delta t is like a subtracting away from 1. o were here with the 1. And a delta t is pulling us this way. And how far, at what point are we going to meet instability? Yeah. When we hit minus 1. This is the 1 plus a delta t that graphed. o it starts at 1 with delta t equals 0. And as increase delta t, it moves this way. And eventually, it hits there. And the limit will be at a delta t equal minus 2. Because if it carries me 2 from 1, that carries me to minus 1. And if go further, m unstable. And you could easily check that Euler will blow up like crazy if you just go a little beyond that limit. Because youre taking powers of a number bigger than, or below minus 1. OK. o theres the stability limit for Euler. And thats a pretty unpleasant restriction. We will have other stability limits. Well have some stability limits over here, but were looking for numbers better than 2. This AdamsBashforth will give us numbers worse than 2. Well see those methods first thing next time. OK. Now, let me just get back OK. f drew a picture then lets see, have got another board here? Nope. f drew a picture of good Euler, so Euler doing its job, the true solution is decaying like that, and what does Euler do? Forward Euler says, it looks here, it looks at the slope, thats all it knows, it goes maybe that far. Thats a small delta t. Thats an OK method. You see, its not very accurate of course. ts lost all the curvature in the solution. o this was the true solution e to the a*t. And this is the 1 plus a delta t to the power, well, t over delta t times. Well, no, this was only one step, so that would just be 1 plus a delta t. OK. Where the bad, the unstable one, if just try to draw that, the true solution is e to the a*t, but take start with that value and the right slope. But take too big a time step and m below minus 1. o that was a case of a too large delta t. OK. Now, ve got one minute left to mention backward Euler. Well, that wont take me long. ll just change this to _(n+1). o m evaluating the slope at the new time. o this is now backward Euler, or implicit Euler. OK. o that changes this equation. Oh, lets find out the equation for implicit Euler. OK. only have one more moment, ll just pop it in this little corner. o now, heres backward Euler. All right. You can tell me what backward Euler is going to be. o _(n+1) minus _n over delta t is a*_(n+1). o let me collect the n plus 1s. o have one of them. And then minus a delta t. Right? And well, bring it over to the left side. And here just have _n. OK. o what happens at every step now? divide every step is a division by this. o could bring this into the denominator. o _n is that division times _0. And you see the stability? Whats the size of this growth factor? Remember, m thinking of a as a negative. m dealing with a stable differential equation. And so whats the main point about this growth factor, this ratio in parentheses? t is? f a is negative, what do know about that number? Think of a as minus 2, lets say. o then have 1 over 1 plus 2 delta t, because its minus a there. And its going to be smaller than 1. Right. ts going to be stable. o Euler is actually absolutely stable, backward Euler. Backward Euler has this property of Astability. There is no stability limit on negative a, no problem. Or even pure imaginary a, no problem. Always stable. o that shows how implicit methods can be greatly much more stable than explicit, just by comparing forward to backward Euler. But, just to remind you the price. The price is that this equation has to be solved. Well, it wasnt any trouble in linear case. n the linear case were just inverting a matrix for a linear system. o backward Euler for linear system, big linear system, is a matrix inversion, well a matrix linear system to solve. But for a nonlinear problem, its serious work. And yet, you have to do the work if the explicit method gives you such a small delta t that its impossible to go with. OK. Thanks for your patience on this first lecture. o, obviously m going to have one more discussion of ODEs, in which we construct these methods, see their stability and their convergence. And then we move on to the PDEs.","o the stability condition on Euler is going to be so do you see what it is? o this was the true solution e to the a*t. And this is the 1 plus a delta t to the power, well, t over delta t times. f we have N righthand sides, because have N us, and the matrix that comes in is the derivative of righthand side in equation i with respect to unknown j. o its a N by N matrix, and of course, this is the case where its a constant. Where the bad, the unstable one, if just try to draw that, the true solution is e to the a*t, but take start with that value and the right slope. Thats the category of implicit methods for stiff equation, and backward Euler would be the very first. o stiff means so, if had only e to the minus t in the solution, then the solution would decay at that rate, e to the minus t. That time step would adjust to keep it accurate. And that will be the first method well construct, of course. And what is the stability now? And an absolute accuracy of 10 to the minus 6. o it will try to keep the error it will plan to keep the error, and itll tell you if it has a problem, below 10 to the minus 6. And ll try to get it onto the web, but you could just easily discover the syntax to call it, and apply it to an equation. Of course, actually Euler would be the first of the AdamsBashforth, and backward Euler might be early in Adamsoulton. And therefore, we have to pay attention to the possibility of complex a. As you know, the whole point of eigenvalues is that we can understand matrices to a very large extent by diagonalizing, by reducing them to N scalar equations with the eigenvalues as the little as in those scalar equations. And often, to discuss stability or understand a method and try it, the linear case is the natural one. And yet, you have to do the work if the explicit method gives you such a small delta t that its impossible to go with. You see, whats going to happen as time gets anywhere, this is going to control u of t. The decay of u of t is going to have that time constant 1. And similarly on the implicit side, backward Euler, well see, is again the first implicit method you think of. But theres a natural match between the number a there corresponds to the partial of f with respect to u. And of course we see it. And explicit means so this is the first meaning that compute the new and ll use capital at time step n plus 1. o thats capital at the new time. The previous step produced _n, and at the new step, this step, we plug it in to find out what the slope is. And you see the stability? Well, no, this was only one step, so that would just be 1 plus a delta t. OK. o Eulers method is the first one you think of, but not the one you finish with. And if have only e to the minus 99t, then again that of course is a faster decay, much faster decay so the time step would adjust to be much smaller, probably about 1/99 or something. And let me just comment that we do have to pay attention to the possibility that a could be complex. That means often well think of that model, a negative, so that the differential equation is completely stable. When things are, you know, when the satellite is buzzing out in space, and it can take giant steps or it can have highorder accuracy, say in astronomy, of course, they would go up to eighth order and higher, tracking, estimating where stars would go or planets, these will do that automatically, up to a certain point. The price is that this equation has to be solved. mean, this is the kind of matrix, well you might say ill conditioned would be a word that you hear for a matrix like this. o thats like the first half of the course. o want to sort of tell you about ordinary differential equations, well, in kind of an organized way, so that you know the competing methods. You can tell me what backward Euler is going to be. but if it is, we only have to do it once here. The question is, whats the equation that makes it evolve? o AdamsBashforth is explicit, Adamsoulton are implicit, and the coefficients are in books on numerical analysis and will be on the web. And the limit will be at a delta t equal minus 2. And whats the a delta t? The condition number is the ratio so the condition number of this matrix would be the ratio 99 over 1. And the fact that stability and convergence success of the method are so intimately linked. o the point about implicit methods is the difference equation, the formula, involves not just _(n+1), the new value, but also the slope at that new value, at that new unknown value. And that should equal the sum of the eigenvalues. But, the advantage, and the reason it gets used for stiff problems, is that on stiff problems it will allow a much larger step. This is the 1 plus a delta t that graphed. But this is going to control so ll say but this would control delta t for explicit methods. And then, most important to us, so it gets a star, are the main topics for this course. Then the center of the course was differential equations. This word stiff and identifying this class of problems, which now is familiar to everybody who has to compute, didnt come, you know, with Gauss and so on. mean, if you wanted to tracking a space station with Eulers method, you would lose it real fast, or you would have to take delta t so small that it would be impossible. o we know of course the solution, its an exponential, e to the a*t times u_0, the initial bunch. All right, so now m ready to start on the first lecture, which will be ordinary differential equations. o in other words, its not the minus 99 that makes it stiff. And thats reflected in the fact that these two major codes that are available in ATLAB are in the number three category. And it may be limited by the requirement of stability. f that was f, then its derivative is indeed a. And in this matrix case, there would be a Jacobian matrix. f drew a picture of good Euler, so Euler doing its job, the true solution is decaying like that, and what does Euler do? But in the end, you probably want to do that. Because learned that although books tend to discuss those, the Adams methods, in practice these methods, the methods three RungeKutta and backward differences are the most used.",0.1243961352657004
71,612,"PATRK WNTON: You know, its unfortunate that politics has become so serious. Back when you were little it was a lot more fun. You could make fun of politicians. Heres a politician some of you may recognize. But its convenient to be able to vary what this particular politician looks like. For example, we can go from a cookie baker to radical. PATRK WNTON: We can go from superwoman to bimbo. PATRK WNTON: ocialite put socialite into this. There she is. Or we can move the slider over the other way to bag lady. Alert, asleep, sad, happy. How does that work? dont know. But bet by the end of this hour youll know how that works. And not only that, youll understand something about what it takes to recognize faces. t turns out to some theories of face recognition are based on the same principles that this program is based on. But you can kind of guess whats happening here. There are many stored images and when move those sliders its interpolating amongst them. o thats how that works. But the main subject of today is this matter of recognizing objects. Faces could be the objects, but they dont have to be. This could be an object that you might want to recognize. And want to talk to you a little bit about the history of this problem and where it stands today. ts still not solved. But its an interesting exercise to see how the attempts at solution have evolved slowly over the past 30 years. o slowly, in fact, that think if someone told me how long it would take to get to where we are 30 years ago think would have hung myself. But things do move slowly. And its important to see how slowly they move. Because they will continue to move slowly in the future. And you have to understand that thats the way things work sometimes. o to start this all off, we have to go back to the ideas of the legendary David arr, who dropped dead from leukemia in about 1980. say, the gospel according to arr, because he was such a powerful and central figure that almost anything he said was believed by a large collection of devotees. But arr articulated a set of ideas about how computer vision would work that started off by suggesting that with the input from the camera, you look for edges. And you find edge fragments. And normally they wouldnt be even as welldrawn as ve done them now. Or as badly drawn as ve done them now. But the first step, then, in visual recognition would be to form this edgebased description of whats out there in the world. And arr called that the primal sketch. And from the primal sketch, the next step was to decorate the primal sketch with some vectors, some surface normals, showing where the faces on the object were oriented. He called that the two and a half D sketch. Now why is it two and a half D? Well, its sort of 2D in the sense that its still cameracentric in its way of presenting information. But at same time, it attempts to say something about the threedimensional arrangement of the faces. o the speculation was that you couldnt get to where you wanted to go in one step. o you needed several steps to get from the image to something you could recognize. And the third step was to convert the two and a half D sketch into generalized cylinders. And the idea is this. f you have a regular cylinder, you can think of it as a circular area moving along an axis like so. o thats the description of a cylinder. A circular area moving along an axis. You can get a different kind of cylinder if you go along the same axis but you allow the size of the circle to change as you go. o for example, if you were to describe a wine bottle. t would be a function of distance along the axis that would shrink the circle appropriately to match the dimensions of a wine bottle. A fine burgundy, perceive. n any case, this one once converted into a generalized cylinder, when matched against a library of such descriptions, results in recognition. Great theory, based on the idea that you start off by looking at edges and you end up, in several steps of transformation, producing something that you could look up in a library of descriptions. Great idea. Trouble is, no one could make it work. t was too hard to do this. t was too hard to do that. And the generalized cylinders produced, if any, were too coarse. You couldnt tell the difference between a Ford and a hevrolet or between a Volkswagen and a adillac. Because they were just too coarse. o although it was a great idea based on the idea that you have to do recognition in several transformations of representational apparatus, it just didnt work. o much later, maybe 15 years later or so, we get to the next part of our story. Which is the alignment theories, most notably the one produced by himon llman, one of arrs students. o the alignment theory of recognition is based on a very strange and exotic idea. t doesnt seem strange and exotic to mechanical engineers for a while, because theyre used to mechanical drawings. But heres the strange and miraculous idea. magine this object. You take three pictures of it. You can reconstruct any view of that object. Now, have to be a little bit careful about how say that. First of all, some of the vertexes are not visible in the views that you have. o, of course, you cant do anything with those. o lets say that we have a transparent object where you can see all the vertexes. f you have three pictures of that, you can reconstruct any view of that object. Now have to be a little careful about how say that, because its not true. Whats true is, you can produce any view of that in orthographic projection. o if youre close enough to the object that you get perspective, it doesnt work. But for the most part, you can neglect perspective after you get about two and a half times as far away as the object is big. And you can presume that youve got orthographic projection. o thats a strange and exotic idea. But how can you make a recognition theory out of that? o let me show you. Well, heres one drawing of the object, need two more. Lets see. Lets have this one. And maybe one thats tilted up a little bit. ts important that these pictures not be just rotations on one axis. Because they wouldnt form what you might think of as a kind of basis set. o there are three pictures. Well call them a, b, and c. And then we want a fourth picture. Which will look like this. t doesnt have to be too precise. And well call that the unknown. And what we really want to know is if the unknown is the same object that these three pictures were made from. o let me begin with an assertion. ll need four colors of chalk to make this assertion. What want to do is want to pick a particular place on the object, like this one. And maybe the same place on this object over here. Those are corresponding places, right? o can now write an equation that the xcoordinate of that unknown object is equal to, oh, dont know, alpha x sub a plus beta x sub b plus gamma x sub c plus some constant, tau. Well, of course, thats obviously true. Because m letting you take those alpha, beta, gamma, and tau and make them anything you want. o although thats conspicuously obviously true, its not interesting. o let me take another point. And of course, can write the same equation down for this purple point. And now that m on a roll and having a great deal of fun with this, can take this point and make a blue equation. And you know m destined to do it, so ve got one more color. might as well use it. Lets just make sure get something that works here. Thats this one, thats this one. hope ve got these correspondences right. TDENT: . PATRK WNTON: Have got one off? TDENT: . PATRK WNTON: OK. o this one goes with this one, goes with this one. s that one wrong? TDENT: Yeah. PATRK WNTON: Oh, oh, oh. Of course this one, excuse me, goes down here. Right? And then this one is off as well. wouldnt get a very good recognition scheme if cant get those correspondences right. Which is one of the lessons of today. OK. Now ve got them right. And now that equation is correct. think ve got this one right already. o now can just write that down. m on a roll, m just copying this. o those are a bunch of equations. And now the astonishing part is that can choose alpha, beta, gamma, and tau to be all the same. That is, theres one set of alpha, beta, gamma, and tau that works for everything, for all four points. o you look at that puzzled. And thats OK to be puzzled. Because certainly havent proved it. m asserting it. But right away, theres something interesting about this and that is that the relationship between the points on the unknown object and the points in this stored library of images are related linearly. Thats true because its orthographic projection. Linearly related. o can generate the points in some fourth object from the points in three sample objects with linear operations. hristopher? TDENT: s that the xcoordinate of PATRK WNTON: ts the xcoordinate. hristopher asked about the xcoordinates. Each of these xcoordinates are meant to be color coded. t gets a little complicated with notation and stuff. o thats the reason m color coding the coordinates. o the orange x sub u is the xcoordinate of that particular point. TDENT: n 3D space? PATRK WNTON: No. Not in 3D space. n the image. TDENT: o its a 2D projection of it? PATRK WNTON: ts a 2D projection of it, an orthographic projection. OK? o were looking at drawings. And those coordinates over there are the twodimensional coordinates in the drawing. Just as if it were on your retina. TDENT: vertexes on the 3D projection or can curved surfaces also? PATRK WNTON: o he asked about curved surfaces. And the answer is that you have to find corresponding points on the object. o if you have a totally curved surface and you cant identify any corresponding points, you lose. But if you consider our faces, there are some obvious points, even though our face are not by any means flat like these objects. We have the tip of our nose and the center of our eyeballs and so on. o if thats true, what does that mean about recovering alpha, beta, gamma, and tau? an we find them? , what do you think? How do we go about finding them? Youre nodding your head in the right direction. TDENT: ts four equations and PATRK WNTON: plendid. ts four equations and four unknowns. Four linear equations and four unknowns. o obviously, you can solve for alpha, beta, gamma, and tau if you know that these equations are correct. o how does that help us with recognition? t helps us with recognition because we can take another point, let me say this square point here and this corresponding square point here and this corresponding square point here, and what can we do with those three points now? Weve got alpha, beta, gamma, and tau, so we can predict where its going to be in the fourth image. o we can predict that that square point is going to be right there. And if it isnt, were highly suspicious about whether this object is the kind of object we think it is. o you look at me in disbelief. Youd like me to demonstrate this, imagine. TDENT: Yeah. PATRK WNTON: OK. Let me see if can demonstrate this. o m going to do this in a slightly simplified version. m only going to allow rotation around the vertical axis. And just so you know m not cheating, theres a little slider here that rotates that third object. Lets see, why are there just two known objects and one unknown? Well thats because ve restricted the motion to rotation around the vertical axis and some translation. o now that ve spun that around a little bit, let me pick some corresponding points. Oops. Whats happened? Wow. Let me run that by again. OK. o theres one point ve selected from the model objects. The corresponding point over here on the unknown is right there. m going to be a little off. But thats OK. o let me just pick that one and then that corresponds to this one. Krishna, would you like to specify a point so people know m not cheating. Pick a point. Pick a point, Krishna. TDENT: Oh, the right? PATRK WNTON: The right? TDENT: Yeah. PATRK WNTON: This one? TDENT: Yep. PATRK WNTON: Oops. OK, lets pick it out on the model first. Now pick it over here. Boom. o all the points are where theyre supposed to be. snt that cool? Well, lets suppose that the unknown is something else. This is a carefully selected object. Because the points are all the correct positions vertically, but theyre not necessarily the correct positions in the other two dimensions. o let me pick this point, and this point, and this point, and this point. And Krishna had me pick this point. o let me pick this point. o if it thinks that the unknown is one of these obelisk objects, then we would expect to see all of the corresponding points correctly identified. But boom. All the points are off. o it seems to work in this particular example. find the alpha and beta using two images. And predict the locations of the other points. And determine whether those positions are correct. And if they are correct, then have a pretty good idea that have in fact identified the object on the right as either an obelisk or an organ, depending on which of the model choices and the unknown choices ve selected. o the only thing have left to do is to demonstrate that what said about this is true. o m going to actually demonstrate that what said about this is true using the configuration in this demonstration. Because its much too hard for me to remember matrix transformations for generalized rotation in three dimensions. o heres how its going to work. The zaxis is going up that way. Or, its going to be pointing toward you. And what m going to do is m going to rotate around this axis. And what want to do is want to find out how the xcoordinate in the image of the points move as do that rotation. o heres the xaxis. This is the coordinate that you can see. Here is the yaxis. Thats in depth, so you cant tell how far away it is. And the zaxis x, y, zaxis must be pointing out that way toward you. o now m going to consider just a single point on the object and see what happens to it. o m going to say to myself, lets put the object in some kind of standard position. dont care what it is. t can be just random, just spin it around. ome position, well call that the standard position, . And that means that the xcoordinate of the standard position is x sub s. And the ycoordinate of the standard position is y sub s. And now m going to rotate the object three times. Once to form the a picture, once to form the b picture, and once to form the c picture. And you can make those choices. Those can be anything, right? o lets say that the a picture is out here. o thats the a picture. The B picture is out here. And the unknown is up that way. And so what want to know depends on these vectors. Well call that theta sub a, and this is theta sub b. And this one is theta sub u. o would like to know how x sub a depends on x sub s and y sub s. And can never remember how to do that, because can never remember the transformation equations for rotation. o have to figure it out every time. And this is no exception. o what m going to say is that this vector that goes out to consists of two pieces. Theres the x part and the y part. And know that can rotate this vector by alpha sub a by rotating this vector and rotating that vector and adding up the results. o if rotate this vector by alpha sub a, then the contribution of that to the xcoordinate of a is going to be given by the cosine of theta sub a multiplied by x sub s. o you can just exaggerate that motion, say, well if pitch it up that way then the projection down on the xaxis is going to be this length of the vector times the cosine of the angle. Now theres also going to be a dependence on y sub s. Lets figure out what thats going to be. ve got this vector here. And m going to rotate it by theta sub a as well. f rotate that by theta sub a and see what the projection is on the xaxis, thats going to be given by the sine of the angle. But its going the wrong way, so have to subtract it off. o thats how dont have to remember what the signs are on these equations. Well, that was good. And now that m off and running can do what did before. t makes it easy to give the lecture. Because this is going to be x sub b is equal to x sub s times the cosine of theta sub b minus y sub s times the cosine of theta oh, youre letting me make mistakes. hame. can generally tell by all the troubled looks. But there should be some shouting as well. Thats the sine and thats the sine. And one more time. x sub u is equal to x sub s times the cosine of theta sub u minus y sub s times the sine of theta sub u. And forgot the b up there. o there are some equations. And we dont know what were doing. Were just going to stare at them awhile and see if they sing us a song. o lets see if they sing us a song. What about x sub a and x sub b? These are things that we see in the image. These are things that we can measure. What about all those cosines and sines of theta as and theta bs. Well, we have no idea what they are. But one thing is clear. Theyre true for all of the points on the object. Because when we rotate the object around by angle theta, were rotating all of the points through the same angle, right? o with respect to any particular view of the object here we are in the standard position. Here we are in position a. The vectors to all of the points on the object are rotated by the same angle when we go from the standard position to the a position. o that means that for all of the images in this particular rendering, with a particular rotation by theta a, theta b, and theta u, those are constants. Now remember this is for a particular theta a, a particular theta be, and a particular theta u. As long as were talking about all of the points for each of those rotations, those angles and cosines are going to be the same for all possible points on the object. OK. o now we go back to our high school algebra experts and we say, look at these first two equations, Weve got two equations and what we can now construe to be two unknowns. What are the unknowns that are left? We can measure a and b. Whatever the cosines are, theyre the same for all the pictures. o if we treat those as constants, then we can solve for x sub s and y sub s. Right? We can solve for x sub s and y sub s in terms of x sub a and x sub b and a whole bunch of constants. But, dont know, a whole bunch of constants, lets see. We can gather up all of those cosines and ratios of sines and cosines and all that stuff and put them all together. Because theyre all constants. And then we can do this. We can say x sub u is equal to well, its going to depend on x sub a and x sub b. And by the time we wash or manipulate or screw around with all those cosines, we can say that the multiplier for x sub a is some constant alpha and the multiplier for x sub b is some constant beta. o thats not a slight of hand. Thats just linear manipulation of those equations. And thats what we wanted to show, that for orthographic projection, which this is there is no perspective involved here, were just taking the projection along the xaxis we can demonstrate for this simplified situation that that equation must hold. Now want to give you a few puzzles. Because this stuff is so simple. uppose allow translation as well as rotation. Whats going to happen? TDENT: You just get the tau. Basically, you get a constant. PATRK WNTON: Yeah, you add a constant, tau. But what do we need to do in order to solve it? TDENT: ubtract them . You subtract two equations and then . PATRK WNTON: Lets see, now weve got three unknowns, right? dont know tau. dont know x sub s. And dont know y sub s. o need another equation. Where do get the other equation. TDENT: . PATRK WNTON: From another picture. Thats why up there needed four points. That covers a situation where ve got three degrees of rotation and translation. Here got by with just two pictures in this illustration. That one involved a tau translational element, so needed three pictures. And this ones got full rotation, so needed four. o great idea, works fine. The trouble is it doesnt work so fine on natural objects. t works fine on things that are manufactured because they all have identical dimensions. o if made a million of these in a factory, d have no trouble recognizing them. Because all d have to do is take three pictures, record the coordinates of some of the points, and d be done. The trouble is the natural world isnt like this. And you arent like this either. dont know, if m trying to recognize faces, its not that easy to do all this. First of all, its a little difficult to find the exact point, the exactly corresponding points. made a mistake in doing it myself. And if the computer made a mistake it would certainly make an error. Because it would be using noncorresponding points to make the prediction. o it would be way off. But this is still in the tradition of working from local features in the objects toward recognition. o having looked at that theory, we also find it a little wanting. t works great it some circumstances, doesnt seem to solve the whole recognition problem. Years pass. himon llman comes up with another theory thats not so much based on edge fragments or the location of particular features but rather on correlation. Taking a picture of, say, Krishnas face, taking a picture of the whole class, and then using that as a kind of correlation mask, running it all over the picture of the class, seeing where it maximizes out. Now thats vague. ll explain when m talking about correlation in a minute. But its basically saying, if have a picture of Krishna, where do find him? ll find him in one place. But you know what? Krishna doesnt look like anybody else. o might not find any other faces. And if my objective is to find all the faces, then maybe that idea wont work either. Or, to take another example, heres a dollar bill. We havent had raises in quite well, so this is my last one. ts got a picture of George Washington on it. And can look all over the class. And if use this is as a face detector, d be sorely disappointed. Because wouldnt find any faces. Because thank God, nobody looks exactly like George Washington. o the correlation wouldnt work very well. o that ideas a loser. But wait a minute. dont have to look for the whole face. could just look for eyes. And then could look for noses and maybe mouths. And maybe could have a library of 10 different eyes and 10 different noses and 10 different mouths. Would that idea work? Probably not so well. The trouble with that one is, d find eyeballs in every doorknob. Theres just not enough stuff there to give me a reliable correlation. o lets make this a little more concrete by drawing some pictures. Halloween is approaching. o heres a face. All right? Heres another face. o those might be faces in my prerecorded library of pumpkin faces. Now along comes this face. Whats going to happen? Well, dont know. Lets draw yet another face. dont know, that could be a pretty weird pumpkin face, suppose. But mean it to be something that doesnt look very much like a face. o if m doing a complete correlation with either of these faces in my library, neither one will match this one very well. f m looking for fine features like eyes, then ve got these eyes everywhere. o it doesnt help very much. o you can see where m going. And you can reinvent llmans great idea. What is it? We dont look for big features, like whole faces. We dont look for small features, like individual eyes. We look for intermediate features, like two eyes and a nose, or a mouth and a nose. o when we do that, then we can say, now, here are two eyes and a nose. Well, thats found in this one. And what about the combination of that nose and that mouth? Oh, thats over here. But neither of those features can be found in the fourth image. o thats the Goldilocks principle. When youre doing this sort of thing, you want things that are not too small and not too big. ve got the Rumpelstiltskin principle up there, too, by the way. Because meant to mention that arr was a genius at naming things. And even though many of his theories have faded, hes still known for these names like primal sketch and two and a half D sketch because he was such an artist at naming the concepts. He even got credit for a lot of stuff that he didnt do. Not because he was deliberately trying to get it inappropriately, but just because he was so good at naming stuff. o we had the Rumpelstiltskin principle back then. And now we have the Goldilocks principle. Not too big, not too small. But that leaves us with the final question, which is, so if what we want to do is look for intermediatesize features, how do we actually find them in a sea of faces out there? ee, might have a library, might take 10 of you and record your eyes. Take another ten, record your mouths. And they may be put together in a unique way for each of you out there. But its likely that ll fin Lanas eyes somewhere else in a crowd. And Nicolas mouth somewhere else in a crowd. o how do we in fact go about finding them? And mentioned the term correlation a couple of times now. Let me make that concrete. o lets consider a onedimensional face that looks like this. Which is a signal. And m going to consider a onedimensional image. And in that onedimensional image ve got a facsimile of the face. And the question is, what kind of algorithm could use to determine the offset in the image where the face occurs? o you can see that one possibility is you just do an integral of the signal in the face and the signal out here over the extent of the face and see how it multiplies out. Or, to make it less lawyerly and more Tish, lets say that what were going to do is were going to maximize over some parameter x the integral over x of some face, which is a function of x and the image g, which is a function of x minus that offset. o when the offset, t, is equal to this offset, then were essentially multiplying the thing by itself and integrating over the extent of the face. And that gives you a very big number if theyre lined up and a very small number if theyre not. And its even true if we add a whole lot of noise to the images. But these are images. Theyre not one dimensional. But thats OK. ts easy enough to make a modification here. Were going to maximize over translation parameters x and y. And these are no longer functions of just x, theyre also functions of y. Like so. o thats basically how it works. We wont go into details about normalization and all that sort of thing because thats the stuff of which other courses remain the custodians. o would you like to see a demonstration? OK. All right. o without realizing it, Nicola and Erica have loaned us their pictures. And they are embedded in that big field of noise. And its pretty easy to pick out Erica and Nicola, right? Because we are actually pretty good at picking faces out of these images. o lets add some noise. ts a little harder now. What m going to is m going to run this correlation program over this whole image using Nicolas face as a mask and seeing where the correlation peaks up, in spite of all the noise thats in there. Boom, there he is. dont know, maybe we can find Erica too. forgot where she was. cant find her. There she is. nfortunately the parameters arent very good here. Do you see that? Let me get another version of this. ll just do some realtime programming. ve been trying to reset the parameters so that the images in the demonstration comes out clearly up there. Lets see if this works a little better. OK, so lets add some noise. And lets find Erica. There she is. There are some other things that look a little bit like Erica. But nothing looks quite exactly like Erica. o lets try Nicolas eyes. o they stand out pretty clearly against the background. Lets see if we can find Ericas eyes. o they stand out pretty clearly against the background. Notice that it also gets Nicolas eyes. o two eyes is an intermediatesize constraint. ts loose enough that it will match more than one person. But its not so loose that its as bad as looking for one eye. ee, theyre all over the place. o two eyes and a nose, a mouth and a nose, that would be even better as an intermediate feature. But it doesnt matter what the best ones are, because you can work that out experimentally. o thats how correlation works. And its just amazing how much noise you can add and itll still pick out the right stuff. Theres Nicola. Boom. Very clear. Want to add some more noise? dont know, can see it, but thats because m a pretty good correlator, too. Boom. dont know, lets add some more noise. ts just hard to get rid of it. ts just amazing how well it picks it out. Thats good. Thats cool. Now, but the reason that this is 30 years and were still not done is there are still some questions. This is recognizing stuff straight on. How is it can recognize you in the hall from the side? Nobody knows. One possibility is that you have an ability to make those transformations. f so, then that alignment theory has a role to play. Another theory is that, well, after ve seen you once can watch you turn your head and keep recording what you look like at all possible angles. That would work. The trouble is, is there enough stuff in there? aybe. We dont know. Now what would it take to break this mechanism? Well, dont know. Lets just see if we can break the mechanism. Lets see if you can recognize some wellknown faces. Whos that? Quick. TDENT: Obama. PATRK WNTON: Oh, thats too easy. Well see if we can make some harder ones. Yeah, thats Obama. Whos this? TDENT: Bush. PATRK WNTON: Oh boy. Youre really good at this. Thats Bush. How about this guy? TDENT: Kerry. PATRK WNTON: OK. Now ve got it. ome people are starting to turn their heads. And thats not fair. PATRK WNTON: Thats not fair. Because you see whats happened is that if this kind of pumpkin in theory is correct, then when you turn the face upside down you lose the correlation of those features that have vertical components. o if you have two eyes and a nose, they wont match two eyes and a nose when theyre turned upside down. Well, lets see. Well try some more. Whos that? TDENT: Gorbachev. PATRK WNTON: Gorbachev. Who said that? Leonid, where are you? This is Gorbachev, right? You can recognize him because of the little birthmark on the top of his head. One more. Whos oh, thats easy. Who is it? Thats linton. How about this one? Do you see how insulting it is to be at T? Thats me. PATRK WNTON: And you didnt even know. Oh, god. o this might be evidence for the correlation theory. But of course, turning the face upside down would make it very difficult to do alignment, too. o it would break out alignment theory, as well. Let me get that after class, Was there a mistake, or? TDENT: No, no. was just curious stretching would break the correlation. PATRK WNTON: f what would break the structure? What? tretching? TDENT: . PATRK WNTON: Elliot asked if stretching would break the correlation. And the answer is, think, stretching in the vertical dimension is worse than stretching in the horizontal dimension. Because you get a certain amount of stretching in the horizontal dimension when you just turn your head. By the way, since our faces are basically mounted on a cylinder, this kind of transformation might actually work. Thats a sidebar to the answer to your question, Elliot. But now you say, well, OK, so this is not completely solved. You can work this out. But if you really want to work something out, let me tell you what the current questions are in computer vision. People have worked for an awful long time on this recognition stuff and, to my mind, have neglected the more serious questions. ts more serious questions are, how do you visually determine whats happening? f you could write a program that would reliably determine when these verbs are happening in your field of view, will sign your Ph.D. thesis tomorrow. There are 48 of them there. And that is todays challenge. But since were short on time, want to skip over that and perform an experiment on you. want you to tell me what m doing. TDENT: . PATRK WNTON: o the best singleword answer is? ? TDENT: Drinking. PATRK WNTON: OK, this is not a trick question. OK, the best singleword answer. hristopher, what do you think? TDENT: Toasting. PATRK WNTON: hristopher. Well, you. You. TDENT: Toasting. PATRK WNTON: What? Toasting. OK. Not a trick question. Whats happening here? Best singleword answer? TDENT: Drinking. PATRK WNTON: s drinking. Which pair look more alike? PATRK WNTON: o that cat is drinking and nobody has any trouble recognizing that. And believe its because youre telling a story. o our power of storytelling even reaches down into our visual apparatus. o the story here is that some animal has evidently had an urge to find something to drink and water is passing through that animals mouth. Thats the drinking story. o even though they look enormously different visually, the stuff at the bottom of our vision system provides enough evidence for our story apparatus so that we can give the left one and the right one different labels and recognize the cat is drinking. And thats the end of the story.","o if rotate this vector by alpha sub a, then the contribution of that to the xcoordinate of a is going to be given by the cosine of theta sub a multiplied by x sub s. o you can just exaggerate that motion, say, well if pitch it up that way then the projection down on the xaxis is going to be this length of the vector times the cosine of the angle. o you can see that one possibility is you just do an integral of the signal in the face and the signal out here over the extent of the face and see how it multiplies out. And what want to do is want to find out how the xcoordinate in the image of the points move as do that rotation. And the answer is that you have to find corresponding points on the object. But right away, theres something interesting about this and that is that the relationship between the points on the unknown object and the points in this stored library of images are related linearly. f rotate that by theta sub a and see what the projection is on the xaxis, thats going to be given by the sine of the angle. And that means that the xcoordinate of the standard position is x sub s. And the ycoordinate of the standard position is y sub s. And now m going to rotate the object three times. Because you see whats happened is that if this kind of pumpkin in theory is correct, then when you turn the face upside down you lose the correlation of those features that have vertical components. Or, to make it less lawyerly and more Tish, lets say that what were going to do is were going to maximize over some parameter x the integral over x of some face, which is a function of x and the image g, which is a function of x minus that offset. And the idea is this. And what we really want to know is if the unknown is the same object that these three pictures were made from. Because all d have to do is take three pictures, record the coordinates of some of the points, and d be done. And thats what we wanted to show, that for orthographic projection, which this is there is no perspective involved here, were just taking the projection along the xaxis we can demonstrate for this simplified situation that that equation must hold. What m going to is m going to run this correlation program over this whole image using Nicolas face as a mask and seeing where the correlation peaks up, in spite of all the noise thats in there. But that leaves us with the final question, which is, so if what we want to do is look for intermediatesize features, how do we actually find them in a sea of faces out there? This is the coordinate that you can see. We can say x sub u is equal to well, its going to depend on x sub a and x sub b. And by the time we wash or manipulate or screw around with all those cosines, we can say that the multiplier for x sub a is some constant alpha and the multiplier for x sub b is some constant beta. o if it thinks that the unknown is one of these obelisk objects, then we would expect to see all of the corresponding points correctly identified. Here we are in position a. The vectors to all of the points on the object are rotated by the same angle when we go from the standard position to the a position. Well call that theta sub a, and this is theta sub b. And this one is theta sub u. o would like to know how x sub a depends on x sub s and y sub s. And can never remember how to do that, because can never remember the transformation equations for rotation. Now remember this is for a particular theta a, a particular theta be, and a particular theta u. As long as were talking about all of the points for each of those rotations, those angles and cosines are going to be the same for all possible points on the object. First of all, some of the vertexes are not visible in the views that you have. And now the astonishing part is that can choose alpha, beta, gamma, and tau to be all the same. And if they are correct, then have a pretty good idea that have in fact identified the object on the right as either an obelisk or an organ, depending on which of the model choices and the unknown choices ve selected. What want to do is want to pick a particular place on the object, like this one. o let me just pick that one and then that corresponds to this one. And the question is, what kind of algorithm could use to determine the offset in the image where the face occurs? o the only thing have left to do is to demonstrate that what said about this is true. o we can predict that that square point is going to be right there. o now m going to consider just a single point on the object and see what happens to it. You can get a different kind of cylinder if you go along the same axis but you allow the size of the circle to change as you go. TDENT: s that the xcoordinate of PATRK WNTON: ts the xcoordinate. And what about the combination of that nose and that mouth? o even though they look enormously different visually, the stuff at the bottom of our vision system provides enough evidence for our story apparatus so that we can give the left one and the right one different labels and recognize the cat is drinking. And thats the end of the story. And you have to understand that thats the way things work sometimes. And want to talk to you a little bit about the history of this problem and where it stands today. dont know, if m trying to recognize faces, its not that easy to do all this. And if it isnt, were highly suspicious about whether this object is the kind of object we think it is. Now, but the reason that this is 30 years and were still not done is there are still some questions. o lets say that we have a transparent object where you can see all the vertexes. And then this one is off as well. How is it can recognize you in the hall from the side? o what m going to say is that this vector that goes out to consists of two pieces. TDENT: You just get the tau. f you have three pictures of that, you can reconstruct any view of that object. Do you see how insulting it is to be at T? And what m going to do is m going to rotate around this axis. And then we can do this. o although it was a great idea based on the idea that you have to do recognition in several transformations of representational apparatus, it just didnt work. And the unknown is up that way. The corresponding point over here on the unknown is right there. o when we do that, then we can say, now, here are two eyes and a nose. Great theory, based on the idea that you start off by looking at edges and you end up, in several steps of transformation, producing something that you could look up in a library of descriptions. We can measure a and b. Whatever the cosines are, theyre the same for all the pictures. But it doesnt matter what the best ones are, because you can work that out experimentally. But how can you make a recognition theory out of that? And you know m destined to do it, so ve got one more color. Theyre true for all of the points on the object. These are things that we see in the image. o lets say that the a picture is out here. And now that m on a roll and having a great deal of fun with this, can take this point and make a blue equation. o the speculation was that you couldnt get to where you wanted to go in one step. o m going to actually demonstrate that what said about this is true using the configuration in this demonstration. o thats how dont have to remember what the signs are on these equations. PATRK WNTON: The right? o the orange x sub u is the xcoordinate of that particular point. But the first step, then, in visual recognition would be to form this edgebased description of whats out there in the world. But if you really want to work something out, let me tell you what the current questions are in computer vision. But now you say, well, OK, so this is not completely solved. t helps us with recognition because we can take another point, let me say this square point here and this corresponding square point here and this corresponding square point here, and what can we do with those three points now? o with respect to any particular view of the object here we are in the standard position. This could be an object that you might want to recognize. And its just amazing how much noise you can add and itll still pick out the right stuff. Weve got alpha, beta, gamma, and tau, so we can predict where its going to be in the fourth image. But for the most part, you can neglect perspective after you get about two and a half times as far away as the object is big. And if my objective is to find all the faces, then maybe that idea wont work either.",0.1191616766467065
72,613,"What want to do in this video is make sure that were good at picking out what the normal vector to a plane is, if we are given the equation for a plane. o to understand that, lets just start off with some plane here. Lets just start off so this is a plane, m drawing part of it, obviously it keeps going in every direction. o lets say that is our plane. And lets say that this is a normal vector to the plane. o that is our normal vector to the plane. ts given by ai plus bj plus ck. o that is our normal vector to the plane. o its perpendicular. ts perpendicular to every other vector thats on the plane. And lets say we have some point on the plane. We have some point. ts the point x sub p. ll say p for plane. o its a point on the plane. Xp yp zp. f we pick the origin. o lets say that our axes are here. o let me draw our coordinate axes. o lets say our coordinate axes look like that. This is our zaxis. This is, lets say thats a yaxis. And lets say that this is our xaxis. Lets say this is our xaxis coming out like this. This is our xaxis. You can specify this is a position vector. There is a position vector. Let me draw it like this. Then it would be behind the plane, right over there. You have a position vector. That position vector would be xpi plus ypj plus zpk. t specifies this coordinate, right here, that sits on the plane. Let me just call that something. Let me call that position vector, dont know let me call that p1. o this is a point on the plane. o its p it is p1 and it is equal to this. Now, we could take another point on the plane. This is a particular point of the plane. Lets say we just say, any other point on the plane, xyz. But were saying that xyz sits on the plane. o lets say we take this point right over here, xyz. That clearly, same logic, can be specified by another position vector. We could have a position vector that looks like this. And dotted line. ts going under the plane right over here. And this position vector, dont know, let me just call it p, instead of that particular, that P1. This would just be xi plus yj plus zk. Now, the whole reason why did this set up is because, given some particular point that know is on the plane, and any other xyz that is on the plane, can find can construct a vector that is definitely on the plane. And weve done this before, when we tried to figure out what the equations of a plane are. A vector thats definitely on the plane is going to be the difference of these two vectors. And ll do that in blue. o if you take the yellow vector, minus the green vector. We take this position, youll get the vector that if you view it that way, that connects this point in that point. Although you can shift the vector. But youll get a vector that definitely lies along the plane o if you start one of these points it will definitely lie along the plane. o the vector will look like this. And it would be lying along our plane. o this vector lies along our plan. And that vector is p minus p1. This is the vector p minus p1. ts this position vector minus that position vector, gives you this one. Or another way to view it is this green position vector plus this blue vector that sits on the plane will clearly equal this yellow vector, right? Heads to tails. t clearly equals it. And the whole reason why did that is we can now take the dot product, between this blue thing and this magenta thing. And weve done it before. And they have to be equal to 0, because this lies on the plane. This is perpendicular to everything that sits on the plane and it equals 0. And so we will get the equation for the plane. But before do that, let me make sure we know what the components of this blue vector are. o p minus p1, thats the blue vector. Youre just going to subtract each of the components. o its going to be x minus xp. ts going to be x minus xpi plus y minus ypj plus z minus zpk. And we just said, this is in the plane. And this is, this right, the normal vector is normal to the plane. You take their dot product its going to be equal to zero. o n dot this vector is going to be equal to 0. But its also equal to this a times this expression. ll do it right over here. o these find some good color. o a times that, which is ax minus axp plus b times that. o that is plus by minus byp. And then let me make sure have enough colors and then its going to be plus that times that. o thats plus cz minus czp. And all of this is equal to 0. Now what m going to do is, m going to rewrite this. o we have all of these terms m looking for, right? olor. We have all of the x terms ax. Remember, this is any x thats on the plane, will satisfy this. o ax, by and cz. Let me leave that on the right hand side. o we have ax plus by plus cz is equal to and what want to do is m going to subtract each of these from both sides. Another way is, m going to move them all over. Let me do it let me not do too many things. m going to move them over to the left hand side. o m going to add positive axp to both sides. Thats equivalent of subtracting negative axp. o this is going to be positive axp. And then were going to have positive byp plus ll do that same green plus byp, and then finally plus czp. Plus czp is going to be equal to that. Now, the whole reason why did this and ve done this in previous videos, where were trying to find the formula, or trying to find the equation of a plane, is now we say, hey, if you have a normal vector, and if youre given a point on the plane where its in this case is xp yp zp we now have a very quick way of figuring out the equation. But want to go the other way. want you to be able to, if were to give you a equation for plane, where were to say, ax plus by plus cz, is equal to d. o this is the general equation for a plane. f were to give you this, want to be able to figure out the normal vector very quickly. o how could you do that? Well, this ax plus by plus cz is completely analogous to this part right up over here. Let me rewrite all this over here, so it becomes clear. This part is ax plus by plus cz is equal to all of this stuff on the left hand side. o let me copy and paste it. o just essentially flipped this expression. But now you see this, all of this, this a has to be this a. This b has to be this b. This c has to be this thing. And then the d is all of this. And this is just going to be a number. This is just going to be a number, assuming you knew what the normal vector is, what your as, bs and cs are, and you know a particular value. o this is what d is. o this is how you could get the equation for a plane. Now if were to give you equation or plane, what is the normal vector? Well, we just saw it. The normal vector, this a corresponds to that a, this b corresponds to that b, that c corresponds to that c. The normal vector to this plane we started off with, it has the component a, b, and c. o if youre given equation for plane here, the normal vector to this plane right over here, is going to be ai plus bj plus ck. o its a very easy thing to do. f were to give you the equation of a plane let me give you a particular example. f were to tell you that have some plane in three dimensions lets say its negative 3, although itll work for more dimensions. Lets say have negative 3 x plus the square root of 2 y let me put it this way minus, or lets say, plus 7 z is equal to pi. o you have this crazy mean its not crazy. ts just a plane in three dimensions. And say what is a normal vector to this plane? You literally can just pick out these coefficients, and you say, a normal vector to this plane is negative 3i plus the square root of 2 plus 2 square root of 2 j plus 7 k. And you could ignore the d part there. And the reason why you can ignore that is that will just shift the plane, but it wont fundamentally change how the plane is tilted. o a this normal vector, will also be normal if this was e, or if this was 100, it would be normal to all of those planes, because all those planes are just shifted, but they all have the same inclination. o they would all kind of point the same direction. And so the normal vectors would point in the same direction. o hopefully you found that vaguely useful. Well now build on this to find the distance between any point in three dimensions, and some plane. The shortest distance that we can get to that plane.","And lets say that this is a normal vector to the plane. And this is, this right, the normal vector is normal to the plane. The normal vector, this a corresponds to that a, this b corresponds to that b, that c corresponds to that c. The normal vector to this plane we started off with, it has the component a, b, and c. o if youre given equation for plane here, the normal vector to this plane right over here, is going to be ai plus bj plus ck. o that is our normal vector to the plane. o that is our normal vector to the plane. And say what is a normal vector to this plane? Or another way to view it is this green position vector plus this blue vector that sits on the plane will clearly equal this yellow vector, right? o this is a point on the plane. Now, the whole reason why did this set up is because, given some particular point that know is on the plane, and any other xyz that is on the plane, can find can construct a vector that is definitely on the plane. Now, the whole reason why did this and ve done this in previous videos, where were trying to find the formula, or trying to find the equation of a plane, is now we say, hey, if you have a normal vector, and if youre given a point on the plane where its in this case is xp yp zp we now have a very quick way of figuring out the equation. want you to be able to, if were to give you a equation for plane, where were to say, ax plus by plus cz, is equal to d. o this is the general equation for a plane. o its p it is p1 and it is equal to this. What want to do in this video is make sure that were good at picking out what the normal vector to a plane is, if we are given the equation for a plane. And all of this is equal to 0. o n dot this vector is going to be equal to 0. This is perpendicular to everything that sits on the plane and it equals 0. And lets say we have some point on the plane. This is a particular point of the plane. Plus czp is going to be equal to that. We take this position, youll get the vector that if you view it that way, that connects this point in that point. This is the vector p minus p1. And lets say that this is our xaxis. And they have to be equal to 0, because this lies on the plane. o we have ax plus by plus cz is equal to and what want to do is m going to subtract each of these from both sides. And we just said, this is in the plane. o lets say that is our plane. Lets say have negative 3 x plus the square root of 2 y let me put it this way minus, or lets say, plus 7 z is equal to pi.",0.2883295194508009
73,614,"NKOLA KABROV: Hi, guys. Today were going to see how one can use linear algebra to describe graphs and networks. n particular, well do the following problem. Were given this very simple graph here with five nodes and six edges. Weve already labeled them, and weve put directions on the edges. And we are asked to write down the incidence matrix A, and then to compute its kernel and the kernel of A transpose. And finally, were asked to compute the trace of A transpose A. ll give you a few moments to try the problem on your own. And then youll see my take on it. Hello again. OK, so lets first recall what an incidence matrix is. o an incidence matrix is supposed to encode how the nodes connect to the edges. n particular, it has as many rows as there are edges and as many columns as there are nodes. And were going to fill in the rows. And well fill them out as follows. o were going to use only negative 1, 1, and 0. And were going to put a negative 1 in entry i and 1 in entry j if the corresponding edge connects node i to node j. OK, let me just do it concretely. o lets look at edge number 1. o it corresponds to the first row. t connects 1 to 2. o we have a negative 1 and a 1. Then edge number 2, it connects node 2 to 3, so negative 1, 1. Edge number 3 connects node 1 to 3, so negative 1, 1. And believe you get the picture, right? o m just going to fill out the rest of the entries. All right, 4 is: negative 1, to 1. 5 is: well, 4, well, negative 1, 1 here. And 6 is: negative 1 and 1. OK. o weve constructed the matrix A. Now, well compute its null space. And were going to do it without performing any row operations whatsoever. o in order to do this, its helpful to look at the graph as an electric circuit and to assign to each of the nodes an electric potential. f we collect all the electric potentials in a vector x, then A times x is a vector with as many entries as there are edges and gives precisely the potential differences across the edges of the graph. OK, so then if A*x is to be 0, this means that across the graph, across all the edges of the graph, all potential differences are 0. Therefore, all the potentials at all the nodes need to be equal to a constant number. o therefore, we conclude that the null space of A is spanned by constant 1. OK? There are five 1s here, corresponding to the five nodes. Now what about the null space of A transpose? Adopt this analogy with electric circuits. But this time, were going to look at currents flowing across the edges of the graph. Oh, and we are going to adopt the following convention for the currents. o a current is going to be positive if it flows in the direction of the edge and negative if it flows in the opposite direction. Right. o then, what is A transpose y, where y is a vector, each of whose entries is a current on the edge? Well, the entries of A transpose y are precisely equal to the total current flowing through each of the nodes of the graph. o A transpose y being equal to 0 means that there is a balance in the circuit, that the currents that flow into each node equal the currents that flow out of it. Right. And its fairly easy to find such a configuration of currents that satisfies this balance equation. We do it by flowing around loops of the graph. o you see, this graph has three loops. The first one is this triangle up there. The second one is this square. And m just, by this curled direction, m signifying in which way m going to trace the loop. And there is third loop, is along the outer contour of the graph. But in fact, the third one can be thought of as a superposition of these two, and ll explain why in a second. o lets figure out the configuration of currents that balances these loops. o if we flow a current 1 from 1 to 2 and then flow a current of 1 along edge 2, from 2 to 3, and then we flow a current of negative 1, mind that the direction is opposite to the direction of the loop, then were going to have a balanced configuration of currents. o let me write this down. The following configuration, so 1 along edge 1, 1 along edge 2, and negative 1 along edge 3, and the rest 0, is a solution to A transpose y. Lets see what solution we get by flowing around the loop in the square. Well, we flow a current of 1 along edge 4, current of 1 along edge 5, current of 1 along edge 6, and current of negative 1 along edge 2. o lets be careful. o it was 0, then along edge 2 was negative 1. Along 3: 0; along 4: 1; along 5: 1; along 6: 1. Now we can do the same thing with the big loop and produce a vector corresponding to it. And prompt you to do it. But what youll see is that the vector that you get is precisely a sum of these two vectors. n a way, the big loop is a superposition of the small loops. OK, so we figured out what the null space of A transpose is. And now, lets concentrate our attention on finding the trace of A transpose A. Were going to do it right here. o the trace of a matrix is the sum of its diagonal entries. And weve seen this many times already, that the diagonal entries of A transpose A are precisely the magnitudes squared of the columns of A. OK? o the (1, 1) entry is the magnitude squared of the first column. The (2, 2) entry is the magnitude squared of the second column, and so on. Now what is the magnitude squared of a column of an incidence matrix? Well, each entry in a column of an incidence matrix is either 1, negative 1, or 0. o when we square these entries, we get 1s or 0s. And when we add them up, we get precisely a number which is the nontrivial entries in that column. OK? o the magnitude squared of the column is the number of nontrivial entries in it. But if we go back to the matrix A, and we count the number of nonzero entries, this is precisely the number of edges that connect with a node. OK, so the number of edges that connects with each node is called the degree of the node. n this way, trace of A transpose A will be just the sum of the degrees of the graph in the picture. o we have there are 2 edges connecting to 1, so 2, plus 3 edges connecting to 2, 3 edges connecting to 3. And weve got a 2 for the number of edges connecting to 4, and 2 for the number of edges connecting to 5. o altogether, we get 12. o you see, in this problem, we computed certain linear algebra objects without performing the usual algebraic operations, but just by looking at the graph and seeing how the linear algebra is encoded in it. hope it was most illuminating. ll see you next time.","o the magnitude squared of the column is the number of nontrivial entries in it. But if we go back to the matrix A, and we count the number of nonzero entries, this is precisely the number of edges that connect with a node. The following configuration, so 1 along edge 1, 1 along edge 2, and negative 1 along edge 3, and the rest 0, is a solution to A transpose y. Lets see what solution we get by flowing around the loop in the square. Well, the entries of A transpose y are precisely equal to the total current flowing through each of the nodes of the graph. o if we flow a current 1 from 1 to 2 and then flow a current of 1 along edge 2, from 2 to 3, and then we flow a current of negative 1, mind that the direction is opposite to the direction of the loop, then were going to have a balanced configuration of currents. OK, so the number of edges that connects with each node is called the degree of the node. And weve got a 2 for the number of edges connecting to 4, and 2 for the number of edges connecting to 5. o altogether, we get 12. o you see, in this problem, we computed certain linear algebra objects without performing the usual algebraic operations, but just by looking at the graph and seeing how the linear algebra is encoded in it. The (2, 2) entry is the magnitude squared of the second column, and so on. OK, so then if A*x is to be 0, this means that across the graph, across all the edges of the graph, all potential differences are 0. And were going to put a negative 1 in entry i and 1 in entry j if the corresponding edge connects node i to node j. OK, let me just do it concretely. And weve seen this many times already, that the diagonal entries of A transpose A are precisely the magnitudes squared of the columns of A. OK? f we collect all the electric potentials in a vector x, then A times x is a vector with as many entries as there are edges and gives precisely the potential differences across the edges of the graph. n this way, trace of A transpose A will be just the sum of the degrees of the graph in the picture. o the (1, 1) entry is the magnitude squared of the first column. But this time, were going to look at currents flowing across the edges of the graph. And we are asked to write down the incidence matrix A, and then to compute its kernel and the kernel of A transpose. o in order to do this, its helpful to look at the graph as an electric circuit and to assign to each of the nodes an electric potential. And there is third loop, is along the outer contour of the graph. o a current is going to be positive if it flows in the direction of the edge and negative if it flows in the opposite direction. OK, so we figured out what the null space of A transpose is. And now, lets concentrate our attention on finding the trace of A transpose A. Were going to do it right here. o the trace of a matrix is the sum of its diagonal entries.",0.3074324324324324
74,615,"DAVD HROKOFF: Hi everyone. m Dave. Now today, d like to tackle a problem in orthogonal subspaces. o the problem wed like to tackle: given a subspace , and suppose is spanned by two vectors, . We have a question here which is to find a basis for perp perp is another subspace which is orthogonal to . And then secondly, can every vector in R^4 be uniquely written in terms of and perp. o ll let you think about this for now, and ll come back in a minute. Hi everyone. Welcome back. OK, so why dont we tackle this problem? OK, so first off, what does it mean for a vector to be in perp? Well, if have a vector x, and perp, and x is in perp, what this means is x is going to be orthogonal to every vector in . Now specifically, is spanned by these two vectors. o its sufficient that x be perpendicular to the two basis vectors in . o specifically, can take and dot it with x, and its going to be 0. o m treating x as a column vector here. n addition, x must also be orthogonal to . o any vector x thats an perp must be orthogonal to both of these vectors. o what we can do is we can write this as a matrix equation. And we do this by combining these two vectors as rows of the matrix. o if we step back and take a look at this equation, we see that what were really asking is to find all x that are in the null space of this matrix. o how do we find x in the null space of a matrix? Well what we can do is we can row reduce this matrix and try and find a basis for the null space. o m going to just row reduce this matrix. And notice that by row reduction, we dont actually change the null space of a matrix. o if m only interested in the null space, this system is going to be equivalent to can keep the top row the same. And then just to simplify our lives, we can take the second row and subtract one copy of the first row. Now, if do that, obtain 0, 1, 1, 1. Now, to parameterize the null space, what m going to do is m going to write x out as components. o if write x with components x_1, x_2, x_3 and x_4, we see here that this matrix has a rank of 2. Now, were looking at vectors which live in R^4, so we know that the null space is going to have a dimension which is 4 minus 2. o that means there should be two vectors in the null space of this matrix. To parameterize these twodimensional vectors, what m going to do is m going to let x_4 equal some constant, and x_3 equal another constant. o specifically, m going to let x_4 equal b, and x_3 equal a. Now what we do is we take a look at these two equations, and this bottom equation will say that x_2 is equal to negative x_3 plus x_4, which is going to equal negative a x_4 plus b. And then the top equation says that x_1 is equal to negative 2*x_2 minus 2*x_3 minus 3*x_4. And if substitute in, x_2 is a plus b. x_3 is a. And x_4 is b. o when the dust settles, the as cancel and m left with minus 5b. o we can combine everything together and we end up obtaining [x_1, x_2, x_3, x_4] equals 5b, x_2 is minus a plus b, x_3 is a, and x_4 is b. And now what we can do is we can take this vector and we can decompose it into pieces which are a multiplied by a vector, and b multiplied by a vector. o youll note that this is actually a times . OK? o we have successfully achieved a parameterization of the null space of this matrix as some constant a times a vector . And now we claim that this is the entire space, perp. o perp is going to be spanned by this vector and this vector. Now notice how, if were to take either of these two vectors in and dot it with any vector in the null space, by construction, it automatically vanishes. o this concludes part one. Now for part two. an every vector v in R^4 be written uniquely in terms of and perp? The answer is yes. o how do we see this? Well, if have a vector v, what can do is can try and write it as some constant c_1 times the vector plus c_2 times the vector plus the vector c_3 . OK? o c_1 and c_2 are multiplying the vectors in , and c_3 and c_4 are multiplying the vectors in perp. o the question is, given any v, can find constants c_1, c_2, c_3, c_4, such that this equation holds? And the answer is yes. Just to see why its yes, what we can do is we can rewrite this in matrix notation, and theres kind of a handy trick. What can do is can take these columns and write them as columns of the matrix. And this whole expression is actually equivalent to this matrix multiplied by the constant, c_1, c_2, c_3, c_4. And on the righthand side, we have the vector v. Now, by construction, these vectors are linearly independent. And we know from linear algebra that if we have a matrix with linearly independent columns, the matrix is invertible. What this means is, for any v on the righthand side, we can invert this matrix and obtain unique coefficients, c_1, c_2, c_3, c_4. This then gives us a unique decomposition for v in terms of a piece which is in and a piece which is in perp. And in general this can be done for any vector space. Well d like to conclude this problem now and hope you had a good time.","Well, if have a vector x, and perp, and x is in perp, what this means is x is going to be orthogonal to every vector in . Well what we can do is we can row reduce this matrix and try and find a basis for the null space. Now, were looking at vectors which live in R^4, so we know that the null space is going to have a dimension which is 4 minus 2. o that means there should be two vectors in the null space of this matrix. o if we step back and take a look at this equation, we see that what were really asking is to find all x that are in the null space of this matrix. o perp is going to be spanned by this vector and this vector. o we can combine everything together and we end up obtaining [x_1, x_2, x_3, x_4] equals 5b, x_2 is minus a plus b, x_3 is a, and x_4 is b. And now what we can do is we can take this vector and we can decompose it into pieces which are a multiplied by a vector, and b multiplied by a vector. o what we can do is we can write this as a matrix equation. And we do this by combining these two vectors as rows of the matrix. And now we claim that this is the entire space, perp. o how do we find x in the null space of a matrix? Now notice how, if were to take either of these two vectors in and dot it with any vector in the null space, by construction, it automatically vanishes. Well, if have a vector v, what can do is can try and write it as some constant c_1 times the vector plus c_2 times the vector plus the vector c_3 .",0.3509933774834437
75,616,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: OK lets get started. once taught with a professor who was lamenting the fact that as the term progresses attendance in lecture tends to drop off. And gets pretty dramatic by the end of the term when youre lecturing, and nobodys there. And asked him what he did about it. And he thought about it and he said, theres only two things that can get students to come to lecture, candy and sex. Now weve already tried candy, so today were going to talk about sex. n fact were going to use graph theory to address a decades old debate concerning the relative promiscuity of men versus women. Now graphs are incredibly useful structures in computer science, and were going to be studying them for the next five or six lectures. They come up in all sorts of applications, scheduling, optimization, communications, the design and analysis of algorithms. n fact next week, youre going to see how to tanford graduate students became gazillionaires because they use graph theoretic in a clever way. But lets talk about sex. The issue that were going to address today is one of the most talked about, and most well studied, questions in all of human sociology. On average, who has more opposite gender partners, men or women? Now opposite gender is going to be important. And by this mean, one boy, and one girl. All right, m not making a political statement. ts just that the math is a lot easier that way, as youll see. Now d like to start by taking a pole here to see what you think about that. o raise your and if you think men, on average, have more opposite gender partners than women do. Only a few. ADENE: n life or PROFEOR: One on one. OK, so lets say over the course of their lives, lets say, or over the course of 2010, that men in America have more oppositegender partners than women in America, say in 2010. Raise your hand if you think men have more going on. All right a bunch of you. Raise your hand if you think women have more oppositegender partners? This is unusual. aybe even more voted for women, but its close. Raise your hand if you think its equal. All right, about the same. Raise your hand if you think theres no way to know, that its hopeless to really figure it out. All right, nobody goes for that. All right, good. All right well now in the popular literature, think the feelings are different than expressed here. Pretty much universally, in the literature, its believed that men have more oppositegender partners than women. And in fact, you could even think about that, if you think about literature, the leader of the harem is always a man. And hes got lots of women. n polygamist cultures, its always the man that has multiple wives, not the reverse. Now not surprisingly, this issue has been studied ""scientifically,"" ll put in quotes, extensively, in one of the largest studies ever done. Researchers from niversity of hicago interviewed 2,500 people, at random, over several years. They brought them in, on many occasions, to try to get the answer for the question once and for all. And they wrote this 700 page book, called The soul of ocial Organization of exuality: exual Practices in the . Actually walking around with this book has proved to be a little embarrassing. Last week my 11yearold daughter saw it, and she goes dad, why do you have this sex book. And grabbed it back and said, well thats for the course. m teaching. And thought d gotten away with it, and everything was fine. And then later that day she texted all of our friends about the new news that what do you know, her dad teaches sex ed at T. Anyway this study concludes that on average men have 74% more oppositegender partners than women. Theres one other central claims. And this is in the . OK now, when you think about it that sounds maybe reasonable, might be OK. But not according to AB News. They did a poll of 1,500 people in the country, in 2004, and concluded that the average disparity is much greater. n particular, in this study, they said that the average man has 20 partners m assuming over their lifetime and the average woman has six. And this gives a disparity 233%. o AB News did a smaller survey says that its 233% here, much more than 74%. Now AB News claimed this is one of the most scientific studies ever done. And there was a 2.5% margin of error. Now well actually talk about what that means mathematically later in the term when we do probability, and do study polling. Now of course should also mention that AB News is the one that said Al Gore won the presidential election in 2000. Now the study is called American ex urvey, a Peak Between the heets. That doesnt sound so scientific. And it was on TV, on Primetime Live in 2004. The promo for this is really good. t says, a groundbreaking AB News Primetime Live survey finds a range of eye popping sexual activities, fantasies, and attitudes in this country, confirming some conventional wisdom, exploding some myths, and venturing where few scientific surveys have gone before. By the end of today, were going to agree with that last statement. OK now who do you thinks right? niversity of hicago. Who votes for 74% as being pretty close? A few of you. ve already slammed these guys. Who votes for AB News as being more accurate? Yeah, nobody. Who votes for no way to tell? got some votes there, all right. o how do you tackle this problem? n theory we could do our own 6.042 survey. dont know how much wed really learn, and for sure d get fired. o dont think were going to do that. But fortunately, this is the kind of question that could be handled, and actually answered, by graph theory, even though it might be more interesting to interview thousands of people, and find out whats going on. Thats not as efficient as using graphs. o let me start by defining what a graph is. nformally graph is just a bunch of dots and lines connecting the dots, its actually very simple. o heres to graph. These are the nodes, and theyre connected with these lines, called edges. And often the nodes, and sometimes the edges, are labeled. For example, we might call this x1, x2, x3, x4, x5, x6, and x7. o thats an example of a graph. Now this being a math class, we got to give a formal definition of a graph. And well usually use the formal definition. A graph G is a pair of sets often called V and E. Where V is a set of elements called vertices or nodes. And it has to be nonempty here in this class. And well go back and forth between vertices and nodes. Even the text we use both words interchangeably. And E is a set of 2item subsets and V, and theyre called edges. o for example, over here in this picture, V is the set of nodes is x1, x2, x3, up to x7, thats the nodes. And E, the set of edges, is pairs, unordered pairs of vertices. o for example x1, x2 is an edge. And its the same as the set x2, x1, doesnt matter the order here. Later in a week or so, well talk about directed graphs where the order matters. x1, x3 is also an edge here, and so one. Think weve got, lets see, 1, 2, 3, 4, 5, 6, 7 edges in this graph. And the last one would be x5, x7. Edges are also sometimes written with this notation, x1 line x2, is another notation. And then later when you talk about directed edges, well put a little arrowhead on one end of this. Now the definition of a graph is really pretty simple. Just think of it as dots and lines, if you want. But theres often differences in how people define graphs. For example, in this class we dont allow the empty graph, i.e. the graph with no nodes. o were going to insist that every graph has to have at least one node in it. And thats just to make the theorems were going prove be true. Otherwise theres some theorems that are false for the special case of the empty graph. But we dont require the graph to have any edges. n fact, its possible you have a graph with nodes, but no edges. For example, this graph. Threenode graph. o here G equals VE, V equals x1, x2, x3. And E is just the empty set. Now for a general graph, when you do have edges, we say that two nodes, call them xi and xj, are adjacent if theyre connected by an edge, namely if xi xj is an edge. All right so for example, x5 is adjacent to x7, but its not adjacent to x4, theres no edge there. losely related is the definition of the incidence. An edge E, which is xixj, is said to be incident to its end points, xi and xj. OK so, for example, if labeled that edge as E, E is the edge x1, x2, and this incident to x1, and incident x2. Then we can talk about the degree of a node. The number of edges incident to a node is called the degree of the node. o for example, whats the degree of x5 over here? 3, so in this case, the degree of x5 equals 3. The degree of x7 is 1. These guys all have degree 0, theres no edges incident to them. Now in this class, were going to look at only simple graphs, at lease for a while. A graph is simple if it has no loops, or multiple edges. Now a loop is an edge that only connects up one node, thats a loop and we dont allow it. A multiple edge is weve got two edges that are really the same, they connect the same endpoints. Also called a multiedge. And those were not going to have in simple graphs. We dont allow this. We dont allow that. Any questions so far about what a graph is? o how are we going to use a graph to model the problem of oppositegender partners? Thats the question were after. o any thoughts about what the nodes of the graph are going to represent? What is it? ADENE: ales and females? PROFEOR: People. Yeah, so were going to have people. n fact, theres two kinds of people here. Theres men, and women. All right we got nodes here for the men. And in fact in America, theres a lot of nodes here. All right, and so this might be oh dont know, say thats Tom ruise and Nicole Kidman. Now whats the edge going to represent? ADENE: Partners. PROFEOR: Partners. They were oppositegender partners. And theres actually more edges probably here. We could have Penelope here, and Katie here. And well probably lots more, probably dont know them all. And Bens over here with Nicole. And Nicole got Jude and Keith. Theres actually a website you can go to get a lot of these things here. And Katie went with Josh. ts called whosedatedwho.com, and you get big graph, you could start filling in the edges. dont know how reliable it is. Now its really critical that were only looking edges from here to here. All right, so if theres an edge between Tom and Ben, dont want to know about it. Just oppositegender partners. OK now in the A, the number of nodes here is about 300 million. About three million people. And the number of men nodes, male nodes, call these V, and this is VW, by the way, m using cardinality notation. When put bars around a set, that is the denoting how many are in the set. n the theres about 147.6 men out of the 300. And the number of women oh we got a w here is about 152.4 million. o theres a little bit more nodes on this side of the graph, than that side in the . What about the edges? Any idea of how many edges there are here? We dont know. sure as heck dont know how many edges there are. o that we dont know. The cardinality of the edge set we dont know, and were not likely to figure out. dont even think these surveys, really, can estimate that. But what were trying to figure out is the ratio of the average degree of the men, to the average degree of the women. Because the number of oppositegender partners you have is your degree here, and youre looking for the average guy degree, compared to the average female degree here. Thats what were after. All right so lets find that quantity. Lets let A sub m equal the average number of oppositegender partners for men. And we can let A W be the same thing for women. All right. Now were trying to figure out the answer to this question. What is A m, the average guy degree, over the average woman degree. And in particular, the niversity of hicago says, they say its 1.74. That the average guy as 74% more oppositegender partners than the average woman. AB News says its 3.33, that is 233% more for the men, than the women. Now were going to figure this out what this ratio is. Just use a little bit of math here, and a little bit of graph theory. o lets write a formula for A m. Well were trying to figure out the average degree over here. Well, thats pretty simple. We just add up all the degrees, and divide by the number of nodes. And thatll give us the average degree. o the average degree is the sum of the degrees, over all men, x in the set of men, of the degree of x, divided by the number of men. an somebody give me a simpler expression for this? t doesnt have that nasty sum in it? ADENE: E. PROFEOR: E. The cardinality of E. m adding all the degrees here. Well thats just another way of counting all the edges, because every edge shows up once, and only once, in a degree count here. And this is where, we use the fact we have oppositegender partners. Because if had some edges over here they wouldnt get counted in sum of the degrees here. All right so this is just the cardinality of the number of edges, divided by the number of men. Any questions about that? Because this is an important statement about graphs in general. When have a graph like this which is called a bipartite graph, well talk about more in a little bit. But where the edges go from the left to the right if sum the degrees on the left, m just counting the number of edges. All right, lets figure out a formula for the average number of partners for the women. That simple thats just sum x over the women. The degree of x, divided by the number of women. Let me rewrite that so its clearer. Whats a simpler expression for this? ADENE: PROFEOR: Yeah, this sum, adding the degrees of the women, is just the number of edges, right. o that is cardinality of edges, divided by the number of women. All right, well now we can write, solve for our formula, average over men over average of the women. Thats E over V, divided by E over VW. Wow, this is nice. dont know the number of edges is, but it just canceled out. And this is just the number of women, divided by the number of men. And in fact we know that. Thats this number, divided by that number, which is about 1.0325. o we just proved, that on average, a man has 3%, or 3 and 1/4% more oppositegender partners than women. No need to do the interviews, or spend years doing. That is the answer. And it has nothing to do with the promiscuity of men, or women, nothing at all. o the hicago study is way off, and the AB New study is completely nuts. t just cant be right, this is a proof. Now what happened here? Well whats going on, whats the reason for why this is true? Yeah? ADENE: A male has a female partner then the female has a male partner. PROFEOR: Yeah. ADENE: Youre not looking at like how many males are going to one female. The promiscuity isnt even a part of the question. PROFEOR: Thats right. t takes two to tango. Every time you got a guy, you got a women. And you have the number of relationships going. The average for the men is that number, divided by the men. Average for the women is that same number, divided by the women. And so if theres more women, theyre going to have less partners on average. Has to be. o it really was a stupid question. ts very, very simple to answer. Now as it turns out there are endless studies like this, in the literature. n fact, a few years ago the Boston Globe ran an explosive story about the study habits of students on Bostonarea campuses. And their surveys show that, on average, minority students tended to study with nonminority students more than the other way around. And they want on great length consulting the experts as to why this might be true. Why is it the minority students study with nonminority students more than the other way around. Now can anyone tell me why it is certainly true, and not surprising, why thats the case? ADENE: Because theyre the minority. PROFEOR: Because theyre a minority. Theres fewer minorities than nonminorities. End of story, we dont need this sociology PhD from down the street to explain it to us. Were going to see a lot of other bogus studies later. This is not unusual, especially when we get the probability. Just every day theres a new one in probability. Any questions about this before we leave? nfortunately thats most all well say about sex today. OK. But now, in this example, we used an edge in the graph to denote some kind of affinity between two nodes. The two nodes liked each other in some sense if they were connected by an edge, or they had a relationship of some kind. Theres lots of examples in computer science where you use an edge to denote just the opposite. That the two nodes cant be near each other, or dont like each other. For example, consider the problem of scheduling final exams at T. And they do this after they find out all of your schedules, and they try to schedule the exams so that you dont have to take two at once, or theres as little of that as possible. For example, lets do an example here. ay we look at these five classes. Take 6041. And this may not be totally accurate, but roughly. o ve got five T classes, and m going to put an edge between pairs of classes that have overlapping student enrollment. o in this case, for example, weve assumed in the drawing of his graph, that you cant have our exam the same time is 6002, on the assumption theres students in both classes. But you could have our exam the same time as 6034. Because theres not an overlapping student in both classes, so the exams could be scheduled at the same time. o weve used a graph to represent which courses cant have their exam at the same time. Now lets also suppose we have a set of slots for the exam. And say theyre all on a Wednesday. And the first slot is Wednesday from 5:00 to 7:00. And the next one is 7:00 to 9:00. And then, the next one is 9:00 to 11:00. And then 11:00 to 1:00 in the morning, and then 1:00 to 3:00, getting pretty late. And your job is to figure out how not to have to use these later exam slots. Youd like to use as few as possible so youre not going too late night, or come before the holidays, so youre not having exams on hristmas and New Years, for example. o the goal is to assign slots to the nodes. Put every node in a slot so you dont have nodes hooked by an edge getting the same slot. Now this is an example of whats called a graph coloring problem. o lets define that. Given a graph G, and K colors, assign a color to each node, so that adjacent nodes get different colors. All right, and then the minimum number of colors you need is called the chromatic number of the graph. o the minimum value of K, for which such a coloring exist, is the chromatic number OF the graph. And its denoted by this symbol chi of G. Because usually you want to use a small number of colors. Now what does a color represent when were dealing with this problem? Whats the meaning of a color? ADENE: Time slot. PROFEOR: A time slot, OK. o lets call this time slot 1, 2, 3, 4, 5, so theres five possible colors. Now of course, we could color this graph with five colors, every node could just get its own color. But then somebodys taking their exam from 1:00 to 3:00 A, and thats a bit of a pain. Lets see if we can do less than five. Lets say give this color one, lets give this one color one, thats OK, because theyre not connected. cant give this one color one, so give it color two, say. Now this one cant give color one, because this guy got it, he cant get color two, because that guy got it. o it give it color three. And well, cant do one, two, or three here, so gotta go to color four. All right so 6042 will get the 11:00 P to 1:00 A slot, not so good. an we do any better? an we get away with three colors. ome say yes, some say no. How many people think you can do three colors on this graph? A bunch. How many think you cant do any better? All right, the vote is mostly for three. Lets see. Any ideas? Anybody see how to do three? Yeah? ADENE: Assign 4 to 6034 . PROFEOR: Assign 4 to 6043. ADENE: Or 1 to 6042. PROFEOR: cant do see 1 to 6042. t crashes, but can do yeah? Put ADENE: 1 in 6003. PROFEOR: 1 in 6003. ADENE: And get rid of 1 in 6034. PROFEOR: Get rid of ADENE: ake it 2. PROFEOR: ake this a 2. Oh, yeah. All right, these got 1, theyre not adjacent. These got 2, theyre not adjacent. This can now get 3. o we can have our exam from 9:00 to 11:00, which is better. All right, can anybody do it in two colors? an anybody offer a reason why two colors may not be possible? Yeah? ADENE: Because lets say you could do it with two colors. PROFEOR: Yep. ADENE: 6041 and 6002 have to be different colors. PROFEOR: Yes. ADENE: 6042 cant be 1, and it cant be 2. PROFEOR: Yeah, good. o you cant in two colors, because these three guys would violate that. Youve got a triangle here. Each one of these guys has to be different than the other two. o two colors cant work. Youve got to have at least three in this case. o three is optimal. We have just shown for this graph, the chromatic number is three. All right, now in general doing what we just did is very hard. No one knows a fast algorithm for determining the chromatic number. n fact, its a weird kind of problem, because its easy enough to check that a coloring is OK. f somebody put a coloring on the board, you can check, oh that works really simply. Just check every edge, and make sure the colors are different. But figuring it out, as best we know, youve got to try an exponential number of possibilities. o if had 100 nodes here, my running time of the algorithm to check all the possibilities would be exponential and a hundred. Yeah? ADENE: an that number just like the highest degree of each node, or nodes. PROFEOR: h no. But its no worse than something like that, as well see a few minutes. Thats a great observation. And were going to come back to that in a few minutes. But its not just that. OK now in fact even figuring out for an arbitrary graph if three colors can be done, called the threecoloring problem, thats really hard. No one knows how to solve that in less than exponential time. n fact, one of these NPcomplete problems is what its called. How many people here dont know about NPcompleteness? s everybody all right so all of you havent seen NPcompleteness. OK so there is a class of thousands of problems in fact theres books list these 1,000 problems that are all NPcomplete, somebodys proved they belong in the class. And what that means is that if somebody gave you a solution, like a coloring here, its easy to check really quickly if its valid. But figuring it out is really hard. And if you figured out how to solve one of those thousands of problems, like suddenly you figured out how to tell if any graph could work with three colors, you would solve automatically all other thousands in the book. o its this book of problems you will constantly run into in your career in computer science. And its bad when you run into one, because theres no good algorithm to solve it known. But if you just solved one of them, the other thousands would suddenly be solvable quickly. Even better, you win a million dollar prize. One of these illennium Prizes we talked about the first lecture. Even if you show you cant find a fast algorithm for one of them, that means that known of them have fast algorithms, and you also get a million dollars. o this is the central problem in computer science, and theory computing, is whether or not you could solve these NPcomplete problems. Now actually lots of people have claim to do it. And in fact, there was a lot of buzz in the community about a month ago when actually a reputable researcher at HP Labs said hed done it. He proved that you cant solve NPcomplete problems. And he got people going for probably at least a week, until they discovered a fatal flaw. And the proof was actually bogus. o no one still knows if you can solve these NPcomplete problems quickly. Now the problem is, in practice, you run into these things all the time, like T really does have to schedule the exams. o youve got to do something. You cant just go say, hey its NPcomplete, so no exams this year, or whatever. Thats not going to fly, so you got to do something. o now this is a problem many of you when you go into careers, youre going to be faced with this. You got to do something. Any thoughts about an algorithm for coloring graphs that might use a small number of colors? t doesnt have to always work, or youre going to win a lot of money if it does. But a simple algorithm, you cant take either the 100 steps. You got to be linear, probably, or quadratic time. That could get you a small number of colors. Any thoughts about what youd do? Yeah? ADENE: The number of degrees and nodes? PROFEOR: The number what about it? ADENE: The highest degree and that node, the 6042 is . PROFEOR: Yeah. ADENE: o you could use that. PROFEOR: Good, all right. o what do do with that so found a node with a high degree, theres three of them have degree three here. What do do with them? ADENE: Pick a different color to. PROFEOR: Pick a different color, that means ve colored some of the others. f pick a different color, do start with them, or do finish with a high degree nodes? Because youve got to assign the colors to them. And high degree is important to be thinking about. Were going to prove a theorem in just a minute about related to degree and coloring. ADENE: tart with them. PROFEOR: tart with them, and do what with it? olor? ADENE: Yeah, and then assign the ones that arent connected to the same slots. PROFEOR: OK, so could heres a degree of theory now can start with color one for that. And then what do do next? pick its neighbors have to get different colors, guess. Youd start coloring the neighbors. ADENE: y first instinct would be to color all the . PROFEOR: OK. And what color would use for them? ADENE: Different ones. PROFEOR: Different ones if theyre connected, or if theyre not connected youd still use different ones? ADENE: Only if theyre connected. PROFEOR: Only theyre connected use different ones. And so if theyre not connected, youd use the same colors? Yeah? Youre going close, and it actually works pretty well. The underlying principle youre sort of thinking about here is youve got some notion of the order in which youre going to process your graph. And youre going to start with a high degree nodes, in your case. And as you go along, youre going to start coloring the nodes. And youre going to make sure you color them legally. And it sounds like youre going to color them with a low color as you go along. And that is probably the most basic graph coloring approach. And almost you could almost say is a generic approach. o lets define that, and then see prove some facts about it. ost of the graph coloring algorithms in practice are based on this approach. And were going to call it the basic graph coloring algorithm. And for our graph G, with vertices V, and edges E. o the first step is going to be to order the nodes from 1 to n. Now in your case, you were suggesting an ordering where have the high degree nodes first. All right. But for now were not going to specify that. Were going to make it any ordering you want. And then were going to have a notion of an order on the colors, as well. And dont know how many colors, but theyre going to be numbered 1, 2, and so forth. And then were going to process the nodes one at a time, to N. We color the nodes, what is step , we color the th node V sub i with the lowest legal color. And by the legal mean you dont color at the same node as another node thats already been colored the same that its adjacent to. All right so lets try this. n fact, this is sort of the algorithm used initially to color exam graph over there. All right, so lets look at that. o lets say we let me erase the colors here, and put an ordering on the nodes. o lets say ordered them with 6034 first, so this would be V1. Then 6041 is V2. Then V3, V4, V5. f thats my ordering, what color would assign to 6034? ADENE: One. PROFEOR: One, 1, d color it first to get 1. What color does 6041 get? 1, as well, its the lowest possible color thats legal, and is not hooked to this guy, so 1 is legal. What color do give here? 2. Then color this one next cant do 2, cant do 1, so pick 3. And then get to 6042 last, and cant do one, two, or three, so do four. All right so algorithm, with that ordering, gave four colors. However we know theres a way to do a different ordering that gives us three colors. n particular, lets see if we do this what happens if we use this other ordering. Let me erase these. ay thats V1, V2, V3, V4, V5. Now get 1, this will be 2, 1. Whats this one get? 2. Ah, much better. 3. o different orderings result in different numbers of colors here. o the whole art now becomes finding a clever ordering. And so many people have already had good ideas, pick the largest degree nodes first. And in fact, if you simulate the algorithm on lots of graphs, you do better on average when you color the larger degree nodes first. And then if you start to use more exotic orderings, you can do even better. f you take a lot of graphs that are out there, and run your algorithm, and see how well you do, you do better with more sophisticated orderings. n fact, this was my senior thesis back when was undergraduate student. was trying to figure out better and better orderings that worked for graphs. And at the time it caused a bit of a problem. was a undergraduate at Princeton. And Princeton, to this day think, still has exams after the holidays, the hristmas holidays, New Years holidays. And the students wanted to have the exams before hristmas, because they hated going home for the holiday, and then youve got to worry about your exams when you come back. And the faculty said no, theres no way to get them all compressed into a small number of days. Now wasnt aware of all that of the time. But my thesis was go figure out good ordering. o tried lots of different orderings. And tried the largest degree first, and recursive versions of that actually worked very well. And then tried it on the Princeton exam graph. And lo and behold, you could actually squish it down, so you could give all the exams, think was, 4 and 1/2 days, plenty of time to give them before hristmas. Which caused a fair of scandal at the time, because then the faculty had to come clean that they just didnt want to bother having the exams before hristmas. Now this algorithm is an example of whats known as a greedy algorithm. Now in a greedy algorithm its always simple. You just go one step after the next, taking the best you can do at each stop. You never go back and try to make things better. You never do hill climbing, if youre familiar with that term. You just always keep it simple, one thing after the next, very fast. ometimes it works great in practice. ometimes it doesnt. But its always where you start, some simple approach like this. Now this algorithm actually, even if you dont try to monkey with the ordering, even for a worst case ordering of the nodes, that actually does pretty good for a lot of graphs. And in fact, it does really well as somebody already asked about if all the nodes have low degree. o lets state that as a theorem. And then were going to prove that. o if every node in a graph G has degree, at most, d so thats the biggest degree in the graph, D then this basic algorithm uses, at most, d plus 1 colors for G. No matter what the ordering is, youll never do worse than d plus 1 colors. o whats the value of d for our exam graph over here? d is 3. Every node has degree, at most, three. And so it says, that no matter what ordering you picked here, youd get at most four colors. Now you might do better. n fact, we found an ordering that got three. o its possible to do better. o lets prove this fact because this makes a difference. ay you have a graph with hundreds of nodes. But every node has degree, at most, three. Well that says you only need four colors even, if the graph has 1,000 nodes, and thats very useful. o in that kind of situation it does very well. o lets prove that. Any ideas as to what proof technique were going to use? ADENE: nvariant. PROFEOR: nvariant, close. Not quite an invariant, but close. ADENE: PROFEOR: What? ADENE: Well ordering principle. PROFEOR: You know well ordering principle, yeah, were going to use the equivalent version of that. Were going to use induction. f you like well its equivalent to well ordering. f you like well ordering you could do it that way. think its easier using induction here. o the proof is by induction. All right so the first thing we need is an induction hypothesis. Any thoughts about what the induction hypothesis should be? Yeah? ADENE: f you have a graph with n nodes then where the degree of any nodes is less than then you can do it. PROFEOR: Thats great. Youre going to do really well on the midterm, because you put an n into this thing, but theres not an n here to start. What are most people going to do we used to ask this actually. We asked this once on a test many years ago, and it was an utter disaster, because did everybody do? ay be one student, or two, put an n into there. But whats the naturally thing to do to induct on here when you look at this statement? Youre going to induct on d, because the first thing you do is you make this be your induction hypothesis. Theres only one thing to use, so youre going to have your predicate be p of d, and its going to be that. Now t didnt occur to us thats what everybody was going to do, but it should have. They all did that and it was a disaster. Because if you do this, well youve got to take a graph with maximum degree d, or d plus 1 in the inductive step, pull out all the nodes with degree d plus 1 to get a graph with now degree d. And thats a mess. You just pulled out a lot of nodes, potentially. olor that in d plus 1 colors, now put all that junk back in. And say only used one more color. Nightmare. And these were T students under pressure. t was a nightmare. o that does not work. And in fact, we will ask an induction question on graphs on every test you take in this course. t will happen. And so usually, with induction, you take this as your induction hypothesis. With graphs, you have to be careful. And worst part about this is we tell people when this doesnt work, use a stronger induction hypothesis. o students tried to make a stronger, but theyre still stuck on d, and it was still a disaster. With graphs, you do something different. And the first thing you do with a graph, usually, is put n in here. And if it doesnt work with n, the number of nodes, you put in e the number of edges. And induct on that. And so what you said is exactly the right thing to do. Dont do this, or least dont spend too much time on it. Pretty quickly try this. f every end node graph if every node in an n node graph G has degree at most degree, then the basic algorithm uses at most d, plus one colors. And now you induct on n. And almost always on graphs, thats the first thing to try. Even if its not in your theorem statement. Any questions about that? Well lets start with this, and see if we can make this one work. o whats the next step in our proof? What do we got to do? Base case. And the base case will be, not n equals 0, because we cant have a zero node graph, but n equals 1. And how many edges do we have? Zero. f theres one node, we dont allow loops, so its zero edges, which means that the degree of our graph has to be zero. Theres no edges. And of course theres only one node, so one color is going to work, and that happens to equal d plus 1. All right, so the base case is true. For one node graphs, you can always use d plus 1 colors, where d is the max degree. All right, next we have the inductive step. o here we assume P n is true for the induction. And now we look at an n plus 1 node graph to show P n plus 1 is true. o we let G be any N plus 1 node graph. We got to show you can color it in d plus 1 colors. And lets let d be the max degree, the largest degree in G. Weve got to show we can color it in d plus 1 colors. Well the basic algorithm, lets say. First thing we do is we order the nodes in an arbitrary order. And were going to show whatever order you pick is OK. All right so what are the nodes? Anyway at all. Now how am going to use the induction hypothesis? know, can assume, the for any N node graph can color it in the max degree plus 1 colors. How am going to use that to help me color G here, the n plus 1 node graph? Any thoughts? Yeah? ADENE: PROFEOR: Yeah, lets create an n node graph by looking at these nodes, and taking this one out of the time being. Remove the last V n plus 1 node in the order. That leaves an n node graph. o lets write that down. We remove the n plus 1 from G. And that creates a new graph, call it G prime with vertices, V prime and edges, E prime. o we create a new graph by removing that node. And we remove all the edges tied to that node. o for example over here, the last node was 6042, so we take out 6042, and all these edges. And this is a graph that were left with. That graph has n nodes. Whats the maximum degree in G prime? When pull out a node, can the degree of any node go up? No, m just taking stuff out. o know that G prime has maximum degree, at most, d. The degree didnt go up of any node. ight have gone down, but it didnt go up. o G prime has max degree, at most, d, and it has n nodes. o we can use the induction hypothesis P n. t says that the basic algorithm uses d plus 1, at most, d plus 1 colors for nodes V1 to V n. Any questions about that? o if this were the n plus first node, last node in the ordering take it out. The basic algorithm now, take the same order here, V1, V2, V3, V4, basic, well color that in d plus 1 colors. And all have left is to give this guy color, and ll have color G. Question? No. All right. o by induction ve colored these guys, V1 to V2, and d plus 1 colors, all that have left to do is color V n plus 1. And hopefully were not going to use color d plus 2, because then we sort of it wouldnt work. We got to use one of the first d plus 1. All right, so lets look at V n plus 1. And lets call its neighbors in G, 1, 2, d. t has, at most d neighbors, because every node in G has, at most, degree d. A neighbors a node youre adjacent to. All right so, V n plus 1 has at most d neighbors, is adjacent to, at most, d other nodes. Now what does that mean about the color can use on V n plus 1? What do know about what color can use for that? Yeah? ADENE: t cant be any of the colors of 1, 2, and so on. PROFEOR: t cant be any one of these colors that were assigned here. Thats true. o how many colors got ruled out? At most d, and how many am working with? d Plus 1. o got one left that can use safely. OK. o this means there exists at least one color in my set of d plus 1 colors. ts not used by any neighbor. And were going to give V n plus 1 that color. All right. o now ve colored every node in G, the n plus 1 node graph, safely using a total of d plus 1 colors. o that means the basic algorithm uses, at most, d plus 1 colors, on G. That means P n plus 1 is true whoops and the induction is complete. Any questions? Yeah. ADENE: ould you also start from the other way, and start 1, go to 2 nodes, 3 nodes at each step keeping all nodes at all other nodes. PROFEOR: What do you mean by keeping all nodes connected? ADENE: each node has an edge connecting to each other one. PROFEOR: OK so, then get a specific graph. start with this, add a node and make it adjacent. add a node and make it adjacent. ADENE: PROFEOR: Yeah. o youve constructed a particular graph. This is actually called, for the n nodes, its called Kn, is the n node complete graph, also called a clique, like a clique of friends, where everybody likes everybody, in a clique. And in fact for n here, for those n nodes, whats the max degree? ax degree is n minus 1. Whats the chromatic number of this graph? Whats the minimum number of colors? PROFEOR: And they all have to be different, which is d plus 1. o you have built a special graph for which the optimum of number colors is d plus 1. But that is not a proof that this is true for all graphs. Because youve looked at a particular graph here. ADENE: t means that you can still use your less than or equal to sign. PROFEOR: see, so youd add a node, and its only connected to a few of them. ADENE: No, its connected to all of them, but it still implies that you need less than or equal to the colors. t turns out it happens to be equal to. PROFEOR: Yes, in this case thats right. o youve made an argument for this case where it actually is equal, but that only worked for this graph. ADENE: worse case. PROFEOR: t is the worst case, so it meets the bound. t shows you cannot improve this bound. Yeah, is there a question up there? ADENE: All was going to say is that youve proved its the worst case. PROFEOR: Right, so what youve done here is youve shown that could not make that theorem any stronger. could not replace it with d here. All right. Because youve given an example where cant get d colors, where the maximum degree is d. But that doesnt To get a proof for a theorem, got to go through all this. That wouldnt give me a proof of the theorem. Theyre not equivalent. Ones an upper bound, ones an existence of a lower bound. This shows that for any graph, you need at most d plus 1. o any graph, at most. That shows there is a graph that you need at least. And they are not equivalent. All right. One is for all, and upper bound. The other is there exists a lower bound. o different in two ways that are important. This kind of proof is very typical for what youll see with induction in graphs. And youll get a lot of practice with it. Are there any other questions on this proof? OK. All right, see weve seen now, by that example, we cant improve the theorem. n some cases, though, the theorem is way off, for some graphs. an anybody think of a graph where the bound we get from the theorem, of d plus 1 colors, is way off from the actual chromatic number you need, the number of colors you need? Yeah? ADENE: two sets of PROFEOR: Good, OK. Yes, so what if we did this graph. Let me draw it out. o youve got a bunch of nodes here, bunch of nodes here. And every node here is connected to every node over the other side. And if this is an n no graph, and ve got n over 2 on each side, whats my degree here? Whats my max degree of this graph? ADENE: N over 2. PROFEOR: N over 2. o d is n over 2. Whats the chromatic number? How many colors do need for this? Two. All right, so d plus 1 is way off of two. There is a even worse example. Yeah? ADENE: That graph where you have one node center thats connected to a bunch of nodes regularly distributed about. PROFEOR: Yeah, the star graph. All right, so got one of the center, got n minus 1 outside. o here the maximum degree is n minus 1, just like a complete graph. But how many colors do need? Two. o its even worse here. All right now what about the basic algorithm? How well does the basic algorithm do on this graph? Or to the vertices some way? olor on one lowest color. How many colors is it going to use? ADENE: Two. PROFEOR: Two. t doesnt matter the vertices. V1, V2, V3, V4, because ll color this one 1. What am going to call that one? 1. Then get to the center, what am going to color it? 2. And now all the arms, what do they get colored? They all get 1. Whatever order you pick, you get two colors. All right so now theres a difference between the theorem just gives you an upper bound, it says, at most, d plus 1 colors. But in fact the algorithm can do a lot better than that, as on this example. o the algorithm might be a lot better. Everybody see that what were doing here? How the algorithm is better than the bound we proved by the theorem, even though the bound was pretty good for some graphs. Now it turns out mean were not going to win a million dollars for this algorithm. And in fact, this algorithm is sometimes very bad. And a really bad example its very close to this. n fact actually this one, lets look at how well does basic do one this one here. ake some ordering. V1, V2, V3. Whats the basic algorithm going to do on this complete its called a complete bipartite graph, is whats this called. ll define bipartite in a minute but whats the basic algorithm do here? Any idea does it take n over 2 colors, or does it take 2? Any ideas? 2. o take a vertex, and the first one, say V1s here, get 1. As long as keep picking vertices over on this side, theyre going to get 1. As soon as get to a vertex over here, what color does it have to get? ADENE: 2. PROFEOR: 2 because its touching the very first one we had here. o when get vertices over here, theyre all going to be 2. When go back over here, theyre going to be back to 1. o actually basic does good here too, gives you two colors. Yeah? ADENE: PROFEOR: Ah, those two arent connected. But this case, if ve got a vertex over here it is, by definition, connected to the vertex over here. Because every possible edge is here. But thats a great idea. What if they werent all connected, thats actually a great idea. n fact, the nasty example for the basic algorithm is very much like that. Lets draw it. Because so far, the basic algorithm is pretty much done perfectly on all the graphs we looked at even when the theorem wasnt tight. o here is a nasty graph. And it is very close to the graph we just look like, where all the edges are there. n this case, all the edges are there, except for the one straight across. o if this is the edge denotes likes, this is a world where you like everybody but your spouse. All right, so you have an edge to every one, except the one directly across from you. No edge there, and so forth. o it has almost every edge, but its missing these edges. Now the basic algorithm might do well here. What would be a good ordering for this graph to label these V1 through Vn? Yeah? ADENE: Go through everything on the left side, and then the right side. PROFEOR: Yeah, thats right. Because then color 1, color 1, color 1, all the way down. One color for the left, what does this one get? olor 2, because its hooked up against. And these all get color 2, so ve used two colors. Really good. Basic algorithms looking great. Now heres a harder question. an you figure out a bad ordering for this graph, where use a lot more than two colors. ADENE: PROFEOR: What is it? ADENE: t starts at the top of the cross, and then the next level then across. PROFEOR: Very good. V1, V2. Just as natural, really, if think about it, to order it this way. All right. What color does V1 get? 1. What color does V2 get? ADENE: 1. PROFEOR: 1 because its not hooked up here. What color does V3 get? ADENE: 2. PROFEOR: 2. What about V4? ADENE: 2. PROFEOR: 2. ts not hooked up. t cant get one, because thats up here. And its not the two, so it gets two What color does V5 get? ADENE: 3. PROFEOR: 3. Because its hooked up to one to two. V6 ? ADENE: 3. PROFEOR: 3, its hooked up to one and two, but not three. And you can see whats happening here. All the way down here hes hooked up to all the n over 2 minus 1 colors. o he also takes n over 2. o if you pick that ordering, not so good. You use n over two colors. o it really matters the ordering. Now should say graphs like actually any questions about what we did here? About this? All right, now should say that graphs like this have a special name, theyre called bipartite graphs. And thats important to remember. All right, so a graph G is said to be bipartite if the vertices can be split into two sets, or partitioned, and well call them a left set, and a right set, so that all the edges connect a node in the left set, to a node in the right set. o in fact, a lot of today weve been looking at bipartite graphs, because the nodes are here. Like the men, and the women, and the edges only go from the left to the right. And that is called bipartite. And its called bipartite because you can do it with two colors, or in two pieces. o you dont win a million dollars for deciding whether or not a graph can be colored in two colors. Thats easy. Youll even do it for homework one of these times. You do win the million dollars for deciding if a graph can be colored in three colors. Thats really hard to do. Now coloring problems come up in all sorts of applications. You know with this company, Akamai, that came out of T, weve talked about. We run a network of 75,000 servers. And theyre used to distribute content on the internet, and so forth. And we have to deploy a new version of our software on those servers, pretty much every week. Were pushing new software out. And you cant deploy on every server at the same time, because youve got to take down a server to deploy new software on it. Got to take it out of commission. And so we cant just take down all 75,000 servers, because then all the Facebook, and Netflix, and all those sites would stop. That would be bad. And we cant do them one at a time, because theres 75,000. And it takes a few hours for each one to get the traffic off, stop it, load new software, and turn it back on. And it would take us years to do one software install, which we got to do every week. o weve got to figure out a schedule for how many servers you take down at a given time, and which ones. And it turns out pairs of servers have certain critical functions. o theres certain pairs of servers you cant take down at the same time. o we have a gigantic 75,000 node coloring problem, where theres edges between servers. Nodes are servers, and theres an edge between if you cant install new software at the same time. And so when it turns out, when you run one of these graph coloring algorithms on it, you could do it with eight colors. t just turns out that way. o that means theres eight waves of install that go on to the network. And now eight times a few hours each means that we can do it in a day, and you can manage it. You know on a much smaller scale, the same problem exists for register allocation, for variables. Here youve got to assign every variable to register. But you cant have variables that are active at the same time associated with the same register. And you want to minimize the number of registers you need. o again, you have the graph coloring problem. The number of colors is the number of registers you need. And two variables cant get the same color if their active at the same time, so you put an edge between them. The most famous example of graph coloring is the map coloring problem, with the four coloring theorem. And so here, every country is a node. Adjacent countries have an edge between them, because you dont want to color adjacent countries the same color, or you cant tell theyre different countries. Now the last example we can talk about is an important problem in communication theory, communication networks, where again coloring comes up. Now here you need to assign frequencies to radio stations, or the cell towers. t comes up in mobile networks, or just in with radio stations. And if two towers have an overlapping area, they cant be given the same frequency, so you get collisions between the towers. And frequencies are very expensive. ompanies pay the government a lot of money to get certain spectrum. o suppose you had this problem. Heres tower A, this is As range, where it reaches. Heres tower B, so it overlaps some with A. Heres tower . Heres tower E. And heres tower D. All right now the question would be, how many radio frequencies do you need? Whats the minimum number of frequencies you need to enable all the towers here? We could make that be a graph. Theres a node for each tower. And an edge between towers, if they overlap. doesnt overlap with B, E does. E overlaps here. And then D overlaps here. o how many frequencies do you need for this graph? ADENE: Four. PROFEOR: Four would work, three is better. an you do two? No you cant do two, because you got here. But you could do three. You could do one, two, three, two, one. This problem comes up ADENE: PROFEOR: Did screw up? Ooh, no cant do that. One, two, yeah much better. All right, this problem comes up all over the place. m certain youll see it sometime in your career, youll have some problem, or youre scheduling something, and its really a graph problem in disguise. OK thats it for today.","And this is in the . And for our graph G, with vertices V, and edges E. o the first step is going to be to order the nodes from 1 to n. Now in your case, you were suggesting an ordering where have the high degree nodes first. ADENE: PROFEOR: Yeah, this sum, adding the degrees of the women, is just the number of edges, right. All right, and then the minimum number of colors you need is called the chromatic number of the graph. Because if you do this, well youve got to take a graph with maximum degree d, or d plus 1 in the inductive step, pull out all the nodes with degree d plus 1 to get a graph with now degree d. And thats a mess. ADENE: t cant be any of the colors of 1, 2, and so on. All right so this is just the cardinality of the number of edges, divided by the number of men. But what were trying to figure out is the ratio of the average degree of the men, to the average degree of the women. ADENE: f you have a graph with n nodes then where the degree of any nodes is less than then you can do it. o if every node in a graph G has degree, at most, d so thats the biggest degree in the graph, D then this basic algorithm uses, at most, d plus 1 colors for G. No matter what the ordering is, youll never do worse than d plus 1 colors. And then were going to process the nodes one at a time, to N. We color the nodes, what is step , we color the th node V sub i with the lowest legal color. ADENE: PROFEOR: Yeah, lets create an n node graph by looking at these nodes, and taking this one out of the time being. And if it doesnt work with n, the number of nodes, you put in e the number of edges. For example, consider the problem of scheduling final exams at T. And they do this after they find out all of your schedules, and they try to schedule the exams so that you dont have to take two at once, or theres as little of that as possible. And it is very close to the graph we just look like, where all the edges are there. All right, so a graph G is said to be bipartite if the vertices can be split into two sets, or partitioned, and well call them a left set, and a right set, so that all the edges connect a node in the left set, to a node in the right set. Like the men, and the women, and the edges only go from the left to the right. The number of edges incident to a node is called the degree of the node. o the average degree is the sum of the degrees, over all men, x in the set of men, of the degree of x, divided by the number of men. And of course theres only one node, so one color is going to work, and that happens to equal d plus 1. ADENE: No, its connected to all of them, but it still implies that you need less than or equal to the colors. And lets let d be the max degree, the largest degree in G. Weve got to show we can color it in d plus 1 colors. PROFEOR: And they all have to be different, which is d plus 1. o you have built a special graph for which the optimum of number colors is d plus 1. And this is just the number of women, divided by the number of men. Theres only one thing to use, so youre going to have your predicate be p of d, and its going to be that. And in fact, if you simulate the algorithm on lots of graphs, you do better on average when you color the larger degree nodes first. an anybody think of a graph where the bound we get from the theorem, of d plus 1 colors, is way off from the actual chromatic number you need, the number of colors you need? Because the number of oppositegender partners you have is your degree here, and youre looking for the average guy degree, compared to the average female degree here. o any thoughts about what the nodes of the graph are going to represent? o for example, over here in this picture, V is the set of nodes is x1, x2, x3, up to x7, thats the nodes. OK now in the A, the number of nodes here is about 300 million. Youre going to do really well on the midterm, because you put an n into this thing, but theres not an n here to start. Because youve given an example where cant get d colors, where the maximum degree is d. But that doesnt To get a proof for a theorem, got to go through all this. All right we got nodes here for the men. Now this algorithm actually, even if you dont try to monkey with the ordering, even for a worst case ordering of the nodes, that actually does pretty good for a lot of graphs. PROFEOR: You know well ordering principle, yeah, were going to use the equivalent version of that. o we can use the induction hypothesis P n. t says that the basic algorithm uses d plus 1, at most, d plus 1 colors for nodes V1 to V n. Any questions about that? ADENE: PROFEOR: What is it? And so what you said is exactly the right thing to do. And the first thing you do with a graph, usually, is put n in here. ADENE: The highest degree and that node, the 6042 is . And then were going to have a notion of an order on the colors, as well. o were going to insist that every graph has to have at least one node in it. We got to use one of the first d plus 1. But where the edges go from the left to the right if sum the degrees on the left, m just counting the number of edges. PROFEOR: The number what about it? f you take a lot of graphs that are out there, and run your algorithm, and see how well you do, you do better with more sophisticated orderings. And this is a graph that were left with. And so when it turns out, when you run one of these graph coloring algorithms on it, you could do it with eight colors. o in this case, for example, weve assumed in the drawing of his graph, that you cant have our exam the same time is 6002, on the assumption theres students in both classes. The issue that were going to address today is one of the most talked about, and most well studied, questions in all of human sociology. But fortunately, this is the kind of question that could be handled, and actually answered, by graph theory, even though it might be more interesting to interview thousands of people, and find out whats going on. know, can assume, the for any N node graph can color it in the max degree plus 1 colors. But now, in this example, we used an edge in the graph to denote some kind of affinity between two nodes. How am going to use that to help me color G here, the n plus 1 node graph? PROFEOR: OK, so could heres a degree of theory now can start with color one for that. Youre going to induct on d, because the first thing you do is you make this be your induction hypothesis. But that is not a proof that this is true for all graphs. And if this is an n no graph, and ve got n over 2 on each side, whats my degree here? ADENE: Because lets say you could do it with two colors. o by induction ve colored these guys, V1 to V2, and d plus 1 colors, all that have left to do is color V n plus 1. ADENE: All was going to say is that youve proved its the worst case. We got to show you can color it in d plus 1 colors. Now the problem is, in practice, you run into these things all the time, like T really does have to schedule the exams. But in fact the algorithm can do a lot better than that, as on this example. And were going to call it the basic graph coloring algorithm. And then get to 6042 last, and cant do one, two, or three, so do four. And by the legal mean you dont color at the same node as another node thats already been colored the same that its adjacent to. All right so what are the nodes? All right, lets figure out a formula for the average number of partners for the women. ADENE: The number of degrees and nodes? The number of colors is the number of registers you need. ADENE: Yeah, and then assign the ones that arent connected to the same slots. And this is where, we use the fact we have oppositegender partners. PROFEOR: 3, its hooked up to one and two, but not three. PROFEOR: t cant be any one of these colors that were assigned here. o if this were the n plus first node, last node in the ordering take it out. f theres one node, we dont allow loops, so its zero edges, which means that the degree of our graph has to be zero. And in fact, you could even think about that, if you think about literature, the leader of the harem is always a man. The cardinality of the edge set we dont know, and were not likely to figure out. ADENE: That graph where you have one node center thats connected to a bunch of nodes regularly distributed about. And if you figured out how to solve one of those thousands of problems, like suddenly you figured out how to tell if any graph could work with three colors, you would solve automatically all other thousands in the book. ADENE: an that number just like the highest degree of each node, or nodes. o how are we going to use a graph to model the problem of oppositegender partners? ADENE: E. PROFEOR: E. The cardinality of E. m adding all the degrees here. They brought them in, on many occasions, to try to get the answer for the question once and for all. PROFEOR: One, 1, d color it first to get 1. o lets say we let me erase the colors here, and put an ordering on the nodes. o now this is a problem many of you when you go into careers, youre going to be faced with this. Then get to the center, what am going to color it? And its called bipartite because you can do it with two colors, or in two pieces. For one node graphs, you can always use d plus 1 colors, where d is the max degree. And well, cant do one, two, or three here, so gotta go to color four. And were going to give V n plus 1 that color. One color for the left, what does this one get? And you have the number of relationships going. All right now what about the basic algorithm? And we remove all the edges tied to that node. All right, so you have an edge to every one, except the one directly across from you. We have just shown for this graph, the chromatic number is three. And its not the two, so it gets two What color does V5 get? o what do do with that so found a node with a high degree, theres three of them have degree three here. We just add up all the degrees, and divide by the number of nodes. Thats this number, divided by that number, which is about 1.0325. o we just proved, that on average, a man has 3%, or 3 and 1/4% more oppositegender partners than women. Given a graph G, and K colors, assign a color to each node, so that adjacent nodes get different colors. But whats the naturally thing to do to induct on here when you look at this statement? And the number of women oh we got a w here is about 152.4 million. The average for the men is that number, divided by the men. The degree of x, divided by the number of women.",0.1697730208525558
76,617,"just want to remind you of the main facts. The first thing that you have to do is, of course, we are going to have to be doing it several times today. That is the system we are trying to solve. And the first thing you have to do is find a characteristic equation which is general form, although this is not the form you should use for twobytwo, is A minus lambda equals zero. And its roots are the eigenvalues. And then with each eigenvalue you then have to calculate its eigenvector, which you do by solving the system (A minus lambda1, lets say, times ) alpha equals zero because the solution is the eigenvector alpha 1. And then the final solution that you make out of the two of them looks like alpha 1 times e to the lambda 1t. Of course you do that for each eigenvalue. You get the associated eigenvector. And then the general solution is made up out of a linear combination of these individual guys with constant coefficients. The lecture today is devoted to the two cases where things do not go as smoothly as they seem to in the homework problems you have been doing up until now. The first one will take probably most of the period. t deals with what happens when an eigenvalue gets repeated. But think since the situation is a little more complicated than it is where the case of a characteristic root gets repeated in the case of a secondorder equation as we saw it, you know what to do in that case, here there are different possibilities. And thought the best thing to do would be to illustrate them on an example. o here is a problem. t came out of a mild nightmare, but wont bore you with the details. Anyway, we have this circular fish tank. t is a very modern fish tank. t is divided into three compartments because one holds iamese fighting fish and one goldfish, and one They should not eat each other. And it is going to be a simple temperature problem. The three actual compartments have to be kept at different temperatures because one is for tropical fish and one is for arctic fish and one is for everyday garden variety fish. But the guy forgets to turn on the heater so the temperatures start out what they are supposed to be, tropical, icy, and normal. But as the day wears on, of course, the three compartments trade their heat and sort of tend to all end up at the same temperature. o we are going to let (x)i equal the temperature in tank i. Now, these are separated from each other by glass things. Everything is identical, each has the same volume, and the same glass partition separates them out and no heat can escape. This is very wellinsulated with very doublethick Thermopane glass or something like that. You can see in, but heat cannot get out very well. Heat essentially is conducted from one of these cells to the other. And lets assume that the water in each tank is kept stirred up because the fish are swimming around in it. That should be a pretty decent way of stirring a fish tank. The question is how do each of these, as a function of time, and want to know how they behave over time, so find these functions. Well, we are going to find them in solutions to differential equations. And the differential equations are not hard to set up. They are very much like the diffusion equation you had for homework or the equations we studied in the beginning of the term. Lets do one carefully because the others go exactly the same way. What determines the flow, the change in temperature? Well, it is the conductivity across the barriers. But there are two barriers because heat can flow into this first cell, both from this guy and it can flow across this glass pane from the other cell. We have to take account of both of those possibilities. t is like in your homework. The little diffusion cell that was in the middle could get contributions from both sides, whereas, the two guys on the end could only get contribution from one. But here, nobody is on the end. t is circular table. Everyone is dying equally. Everybody can get input from the other two cells. x1 prime is some constant of conductivity times the temperature difference between tank three and tank one. And then there is another term which comes from tank two. o a times tank two minus the temperature difference, tank two minus tank one. Lets write this out. Remember there will be other equations, too. But instead of doing this, lets do a more careful job with this first equation. When write it out, remember, the important thing is you are going to have x1, x2, x3 down the left, so they have to occur in the same order on the right in order to use these standard eigenvalue techniques. The coefficient of x1 is going to be minus a x1 and then another minus a x1. n other words, it is going to be minus 2 ax1. And then the x2 term will be plus a x2. And the x3 term will be plus a x3. Well, you can see now that is the equation for x1 prime in terms of the other variables. But there is symmetry. There is no difference between this tank, that tank, and that tank as far as the differential equations are concerned. And, therefore, can get the equations for the other two tanks by just changing 1 to 2, just switching the subscripts. When finally do it all, the equations are going to be, will write them first out as a system. Lets take a equal 1 because am going to want to solve them numerically, and want you to be able to concentrate on what is important, what is new now and not fuss because dont want to have an extra a floating around everywhere just contributing nothing but a mild confusion to the proceedings. o x1 prime, am going to take a equal 1 and simply write it minus 2 x1 plus x2 plus x3. And so now what would the equation for x2 prime be? Well, here x2 plays the role that x1 played before. And the only way to tell that x1 was the main guy here was it occurred with a coefficient negative 2, whereas, the other guys occurred with coefficient 1. That must be what happens here, too. ince x2 prime is our main man, this is minus x2 and this must be x1 here plus x3. And finally the last one is no different, x3 prime is x1 plus x2. And now it is the x3 that should get negative 2 for the coefficient. There is a perfectly reasonablelooking set of equations. Just how reasonable they are depends upon what their characteristic polynomial turns out to be. And all the work in these problems is trying to find nice models where you wont have to use atlab to calculate the roots, the eigenvalues, the roots of the characteristic polynomial. o we have to now find the characteristic polynomial. The matrix that we are talking about is the matrix, well, lets right away write A minus lambda cannot use the trace and determinant form for this equation because it is not a twobytwo matrix. t is a threebythree matrix. have to use the original form for the characteristic equation. But what is this going to be? Well, what is A? A is minus 2. am going to leave a little space here. 1, 1, 1 minus 2, 1. And finally 1, 1, negative 2. subtract lambda from the main diagonal, minus 2 minus lambda minus 2 minus lambda, minus 2 minus lambda. And now that equals zero is the characteristic equation. The term with the most lambdas in it is the main diagonal. That is always true, notice. Now, each of these would be happier writing lambda plus 2, so there would be a negative sign, negative sign, negative sign. The product of three negative signs is still a negative sign because three is an odd number. o it is minus the principle term. The product of these three is minus lambda plus 2 cubed. Now, the rest of the terms are going to be easy. There is another term 1 times 1 times 1, another term 1 times 1 times 1. o to that add 2, 1 and 1 for those two other terms. And now have the three going in this direction, but each one of them has to be prefaced with a minus sign. What does each one of them come to? Well, this is minus 2 minus lambda when multiply those three numbers together. And so are the other guys. This is 1 times 1 times minus 2 minus lambda, the same thing. There are three of them. inus because they are going this way, minus 3 because there are three of them, and what each one of them is is negative 2 negative lambda. That is equal to zero, and that is the characteristic equation. Now, it doesnt look very promising. On the other hand, have selected it for the lecture. imple psychology should tell you that it is going to come out okay. What am going to do is expand this. First imagine changing the sign. hate to have a minus sign in front of a lambda cubed, so lets make this plus and we will make this minus and we will make this plus. will just change all the signs, which is okay since it is an equation equals zero. That doesnt change its roots any. And now we are going to expand it out. What is this? Lambda plus 2 cubed. Lambda cubed plus, and dont get confused because it is this 2 that will kill you when you use the binomial theorem. f there is 1 here everybody knows what to do. f there is an A there everybody knows what to do. t is when that is a number not 1 that everybody makes mistakes, including me. The binomial coefficients are 1, 3, 3, 1 because it is a cubed, am expanding. o it is lambda cubed plus 3 times lambda squared times 2. wont explain to you what am doing. will just do it and hope that you all know what am doing. Plus 3 times lambda times 2 squared plus the last term, which is 2 cubed. And now we have the other term. All that is plus because changed its sign. The next thing is negative 2. And then the last thing is plus 3 times (minus 2 minus lambda). Lets keep it. o what is the actual characteristic equation? aybe can finish it. should stay over here instead of recopying all of it. Well, there is a lot more work to do. Lets see if we can at least write down the equation. What is it? t is lambda cubed. What is the lambda squared erm? t is six and that is all there is. How about the lambda term? Well, we have 12 lambda minus 3 lambda which makes plus 9 lambda. That looks good but constant terms have a way of screwing everything up. What is the constant term? t is A minus 2 minus 6. Zero. The constant term is zero. That converts this from a hard problem to an easy problem. Now it is a cinch to calculate the stuff. Lets go to this board and continue the work over here. The equation is lambda cubed plus 6 lambda squared plus 9 lambda is zero. t is very easy to calculate the roots of that. You factor it. Lambda is a common factor. And what is left? Lambda squared plus 6 lambda plus 9. That is the sort of thing you got all the time when you were studying critical damping. t is the square of lambda plus 3. Lambda squared plus 6 lambda plus 9 equals zero. o the eigenvalues, the roots are what? Well, they are lambda equals zero from this factor and then lambda equals minus 3. But what is new is that the minus 3 is a double root. That is a double root. Now, that, of course, is what is going to cause the trouble. Because, for each one of these, am supposed to calculate the eigenvector and make up the solution. But that assumed that had three things to get three different solutions. Here have only got two things. t is the same trouble we ran into when there was a repeated root. We were studying second or third order differential equations and the characteristic equation had a repeated root. And had to go into a song and dance and stand on my head and multiply things by t and so on. And then talk very hard arguing why that was a good thing to do to get the answer. Now, am not going to do the same thing here. nstead, am going to try to solve the problem instead. Lets get two points by at least doing the easy part of it. Lambda equals zero. What am supposed to do with lambda equals zero? am looking for the alpha that goes with that. And find that eigenvector by solving this system of equations. Lets write out what that system of equations is. Well, if lambda is zero, this isnt there. t is just the matrix A times alpha equals zero. And the matrix A is, never even wrote it anywhere. never wrote A. thought would get away without having to do it, but you never get away with anything. ts the principle of life. That is A. f subtract zero from the main diagonal, that doesnt do a great deal to A. And the resulting system of equations is those same things, except you have the a1s there, too. There is one. a1 minus 2 a2 plus a3 equals zero. am just subtracting zero from the main diagonal so there is nothing to do. a2 minus 2 a3 equals zero. Now am supposed to solve those. Of course we could do it. Well, how do you know how to solve a system of three linear equations? Well, elimination. You can always solve by elimination. Now we are much more sophisticated than that. You all have pocket calculators so you could use the inverse matrix, right? No. You cannot use the inverse matrix. What will happen if you punch in those coefficients and then punch in A inverse. What answer will it give you? 0, 0, 0. No, am sorry. t wont give you any answer. What will it say? t will say cannot calculate the inverse to that matrix because the whole purpose of this exercise was to find a value of lambda such that this system of equations is dependent. The coefficient determinant is zero and, therefore, the coefficient matrix does not have an inverse matrix. You cannot use that method. n other words, the inverse matrix will never work in these problems because the system of equations you will be trying to solve is always a nonindependent system. And, therefore, its determinant is always zero. And, therefore, there is no inverse matrix because the determinant of the coefficient is zero. All you can do is use elimination or physical insight and common sense. Now, because teach differential equations everybody assumes, mistakenly, as think, that really know something about them. get now and then graduate students, not in mathematics, but some obscure field of engineering or whatever drift into my office and say see you teach differential equations. Do you have a minute here? And before can say no they write their differential equation on the board. And almost invariably it is nothing have ever seen before. And they so look at me hopefully and expectantly. o what do ask them? dont ask them what they have tried. What ask them is where did this come from? What field did it come from? Because each field has its own little tricks. t gets the same differential equations all the time and has its own little tricks for solving them. You should do the same thing here. Well, of course we can solve this. And by now most of you have solved it just by inspection, just by sort of psyching out the answer. But a better way is to say look, suppose we had the solution, what would the solution look like? Well, it would look like (a1, a2, a3), whatever the values of those variables were which gave me the solution to the equation, times e to the 0t. But what is this? e to the 0t is one for all time. And, therefore, this is a constant solution. What am asking is to find a constant solution. Now, can , by inspection, find a constant solution to this? f so it must be the one. Well, there is an obvious constant solution. All the cells have the same temperature. f that is true then there is no reason why it should ever change as time goes on. The physical problem itself suggests what the answer must be. You dont have to solve equations. n other words, any constant like (1, 1, 1). Well, could it be (20, 20, 20)? Yeah, that is a constant multiple of (1, 1, 1). That is included. y basic constant solution, therefore, is simply (1, 1, 1) times e to the 0t. You dont have to include e to the 0t because it is one. Now, just to check, is (1, 1, 1) a solution to these equations? t certainly is. 1 plus 1 minus 2 is zero in every case. The equations are essentially the same, except they use different variables. By inspection or, if you like, by elimination, but not by finding the inverse matrix you solve those equations. And we have our first solution. Now lets go onto the second one. For the second one, we are going to have to use the eigenvalue lambda equals negative 3. And now what is the system of equations? Well, now have to take this and have to subtract negative 3 from the diagonal elements. inus 2 minus negative 3 is plus 1, right? Got that? Each of the diagonal elements, after subtract minus 3 turns into plus 1. And, therefore, the system becomes, the system have to solve is a1 plus a2 plus a3 equals zero. And what is the second equation? ymmetry is preserved. All the equations are essentially the same, except for the names of the variables so they all must give you the same thing after subtract minus 3 from the main diagonal. Well, that is what we call a dependent system of equations. All have is the same equation repeated twice, but still have to solve it. Now, what you see is that there are lots of solutions to this. Let me write down one of them. For example, suppose made a1 equal to 1 and made a2 a 0, then a3 would be negative 1. o here is a solution. That is the eigenvector. And with it, can make the solution by multiplying by e to the negative 3t. There is a solution. But that is not the only alpha could have chosen. uppose chose this one instead. uppose kept this 1, but this time made a3 zero. Well, in that case, there would be a2 that had to be minus 1. Now, is this essentially different from that one? t would still be multiplied by e to the minus 3t, but dont be fooled by the e to the minus 3t. That is our scalar. That is not what is essential. What is essential is the content of these two vectors. s either one a multiple of the other? The answer is no. Therefore, they are independent. They are pointing in two different directions in three space, these two vectors. And, therefore, have two independent solutions just by picking two different vectors that solve those three equations. This is also a solution. f call this the eigenvector alpha 1, then ought to call this one the alpha 2. Hey, we can keep on going through this. Why not make the first one zero? Well, what would happen if made the first one 0, and then 1, and minus 1? The answer is this one is no longer independent of those two. can get it by taking a combination of those two. Do you see what combination should take? This one minus that one. This guy minus that guy gives me that guy, isnt that right? 1 minus 1, 0 minus minus 1, minus 1 minus 0. This is not a new one. t looks new, but it is not. can get it by taking a linear combination of these two. t is not independent delta. And that would be true for any other possible solution you could get for these equations. Once you found two solutions, all the others will be linear combinations of them. Well, cannot use that one. t is not new. And the general solution, therefore, will be a combination, c1 times that one plus a constant times this one. Plus the first one that found c3 times (1, 1, 1) e to the 0t, which dont have to write in. That is the general solution to the system, (x1,x2, x3). What happens as time goes to infinity? Regardless of what the values of these two s this term goes to zero, that term goes to zero and what am left with is a constant solution. o all of these solutions tend to be the solution where all the cells are at the same temperature. Well, of course there must be some vocabulary word in this. There is. There are two vocabulary words. This is a good eigenvalue. There are also bad eigenvalues. This is a good repeated eigenvalue, but good is not the official word. An eigenvalue like this, which is repeated but where you can find enough eigenvectors, if lambda is a repeated eigenvalue, it occurs multiply in the characteristic polynomial as a root. But you can find enough independent eigenvectors Forget the ""but."" to make up the needed number of independent solutions. For example, if it is repeated once, that is it occurs doubly then somehow have got to get two solutions out of that as was able to here. f it occurred triply, have got to get three solutions out of it. would look for three independent eigenvectors and hope could find them. That is the good case because it tells you how to make up as many solutions as you need. And this kind of eigenvalue is called in the literature the complete eigenvalue. Now, how about the kind in which you cannot? Well, unfortunately, all my life have called it incomplete, which seems to be a perfectly reasonable thing to call it. However, terminology changes slowly over time. The notes, because wrote them, call it an incomplete eigenvalue. But the accepted term nowadays is defective. dont like that. t violates the ""eigenvalues with disabilities act"" or something. But have to give it to you because that is the word am going to try to use from now on, at least if remember to use it. t would be the word, for example, used in the linear algebra course 18.06 ""plug, plug,"" defective otherwise. A defective eigenvalue is one where you can get one eigenvector. f it is double, for example, if it a double eigenvalue. t is defective if you can get one eigenvector that goes with it, but you cannot find an independent one. The only other ones you can find are multiples of the first one. Then you are really in trouble because you just dont have enough solutions that you are supposed to get out of that, and you have to do something. What you do is turn to problem two on your problem set and solve it because that tells you what to do. And even give you an example to work. Problem two, that little matrix has a defective eigenvalue. t doesnt look defective, but you cannot tell. t is defective. But you, nonetheless, will be able to find two solutions because you will be following instructions. Now, the only other thing should tell you is one of the most important theorems in linear algebra, which is totally beyond the scope of this course and is beyond the scope of most elementary linear algebra courses as have taught around the country but, of course, not at T. But, nonetheless, it is the last theorem in the course. That means it is liable to use stuff. The theorem goes by different names. ometimes it is called the principle axis theorem. ometimes it is called the spectral theorem. But, anyway, what it says is, if A is a real endbyend matrix which is symmetric, you know what a symmetric matrix is? The formal definition is it is equal to its transpose. What that means is if you flip it around the main diagonal it looks just the same as before. omewhere on this board, right there, in fact, is a symmetric matrix. What happened to it? Right here was the symmetric matrix. erased the one thing which had to have. inus 2, 1, 1; 1, minus 2, 1; 1, 1 minus 2. That was our matrix A. The matrix is symmetric because if flip it around the diagonal it looks the same as it did before. Well, not exactly. The ones are sort of lying on their side, but you have to take account of that. s that right? The twos are backward. Well, you know what mean. Put that element there, this one here, that one there. Exchange these two. Notice the diagonal elements dont all have to be minus 2 for that. No matter what they were, they are the guys that arent moved when you do the flipping. Therefore, there is no condition on them. t is these other guys. Each guy here has to use the same guy there. This one has to be the same as that one, and so on. Then it will be real and symmetric. f you have a matrix that is real and symmetric, like the one we have been working with, the theorem is that all its eigenvalues are complete. That is a very unobvious theorem. All its eigenvalues are automatically complete. And it is a remarkable fact that you can prove that purely generally with a certain amount of pure reasoning no calculation at all. But it has to be true, and it is true. You will find there are whole branches of applied differential equations. You know, equilibrium theory, all the matrices that you deal with are always symmetric. And, therefore, this repeated eigenvalues is not something you have to worry about, finding extra solutions. Well, guess that is the end of the first part of the lecture. have a third of it left. Lets talk fast. would like to, with the remaining time, explain to you what to do if you were to get complex eigenvalues. Now, actually, the answer is follow the same program. n other words, if you solve the characteristic equation and you get a complex root, follow the program, calculate the corresponding complex eigenvectors. n other words, solve the equations. Everything will be the same except that the eigenvectors will turn out to be complex, that is will have complex entries. Dont worry about it. Then form the solutions. The solutions are now going to look once again like alpha times e to the a plus bi to the t. This will be complex and that will be complex, too. This will have complex entries. And then, finally, take the real and imaginary parts. Those will be real and they will give real and two solutions. n other words, the program is exactly like what we did for secondorder differential equations. We used the complex numbers, got complex solutions. And then, at the very last step, we took the real and imaginary parts to get two real solutions out of each complex number. would like to give you a simple example of working that out. And it is the system x prime equals x plus 2y. And y prime equals minus x minus y. Because it is springtime, it doesnt feel like spring but it will this weekend as it is getting warmer. And since, when am too tired to make up problem sets for you late at night, watch reruns of einfeld. am from New York. t is just in my bloodstream. Of course, the most interesting character on einfeld is George. We are going to consider usan who is the girlfriend who got killed by licking poison envelopes. And George carried on their love affair until usan was disposed of by the writers by this strange death. And we are going to consider x is modeling usans love for George. That is x. And Georges love for usan will be y. Now, dont mean the absolute love. f x and y are zero, dont mean that they dont love each other. just mean that that is the equilibrium value of the love. Everything else is measured as departures from that. o (0, 0) represents the normal amount of love, if love is measured. dont know what love units are. Hearts, guess. ix hearts, lets say. Now, in what sense does this model it? This is a normal equation and this is a neurotic equation. That is why this is George and this is usan who seemed very normal to me. usan is a normal person. When y is positive that means that George seems to be loving her more today than yesterday, and her natural response is to be more in love with him. That is what most people are. f y is negative, hey, whats the matter with George? He doesnt feel so good. aybe there is something wrong with him. he gets a little mad at him and this goes down. x prime is negative. And the same way why is this positive? Well, again, it is a psychological thing, but all the world loves a lover. When usan is in love, as she feels x is high, that makes her feel good. And she loves everything, in fact. Not just George. t is one of those things. You all know what am talking about. Now, George, of course, is what makes the writers happy. George is neurotic and, therefore, is exactly the opposite. He sees one day that he feels more in love with usan than he was yesterday. Does this make him happy? Not at all. Not at all. t makes y prime more negative. Why? Because all he can think of is, my God, suppose am really in love with this girl? uppose marry her. Oh, my God, 40 years of seeing the same person at breakfast all the time. must be crazy. And so it goes down. Here is our neurotic model. The question for differential equations is, what do the solutions to that look like? n other words, how does, in fact, their love affair go? Now, there is a reason why the writers picked that model, as you will see. t means they were able to get a years worth of episodes out of it. And why is that so? Well, lets solve it. The characteristic equation is lambda squared. The matrix that governs this system is A equals (1, 2; negative 1, negative 1) The trace of that matrix, the sum of the diagonal elements is zero. There is the zero lambda here. The determinant, which is the constant term, is negative 1, minus negative 2, which is plus 1. o the characteristic equation, by calculating the trace and determinant is lambda squared plus 1 equals 0. The eigenvalues are plus and minus i. Now, you dont have to pick both of them because the negative one lead to essentially the same solutions but with negative signs. Either one will do just as we solved second order equations. The system for finding the eigenvectors, well, we are going to have to accept the complex eigenvector. What is the system going to be? Well, take the matrix and subtract i. We will use i. ubtract i from the main diagonal. o the system is (1 minus i) times a1 plus 2a2 is zero. And lets, for good measure, write the other one down, too. t is negative a1 plus minus (1 minus i) times a2. Then what is the solution? Well, you get the solution the usual way. Lets take a1 equal to 1. Then what is a2? a2 is 1 minus i divided by 2 from the first equation. o the complex solution is 1 minus i over 2 times e to the it. Now you have to take the real and imaginary parts of that. This is the only part which technically would not trust you to do without having someone show you how to do it. What do you do? Well, of course, you know how to separate the real and imaginary parts of that. t is the first thing is to separate the vectors. dont know how to explain this. Just watch. The real part of it is 1, onehalf. t should be negative 1, so minus this plus that because didnt put that on the right side. t is minus onehalf plus i times (0, onehalf). Anybody want to fight? 1 plus i times 0 minus onehalf plus onehalf times i. You saw how did that? Okay. When you do these problem you do it the same way, but dont ask me to explain what just did. Here it is cosine t plus i sine t. And so the real part will give me one solution. The imaginary part will give me another. ince have a limited amount of time, lets just calculate the real part. What is it? Well, it is (1, minus ) times cosine t, i squared is negative 1, so minus (0, onehalf), the negative 1 from the i squared, times sine t. Now, what solution is that? This is (x, y). Take the final step. t doesnt have to look like that. x equals cosine t. Do you see that? x equals cosine t plus 0 times sine t. What is y? y is minus onehalf times cosine t, minus onehalf times sine t, plus sine t. Now, you may have the pleasure of showing eliminating t. You get a quadratic polynomial in x and y equals zero. This is an ellipse. As t varies, you can see this repeats its values at intervals of 2pi, this gives an ellipse. And if you want to use a little computer program, linear phase, this is not in the assignment, but the ellipses look like this and go around that way. And that is the model of George and usans love. x, usan. y, George. They go round and round in this little love circle, and it stretches on for 26 episodes.","And the first thing you have to do is find a characteristic equation which is general form, although this is not the form you should use for twobytwo, is A minus lambda equals zero. That is equal to zero, and that is the characteristic equation. The first thing that you have to do is, of course, we are going to have to be doing it several times today. And now what is the system of equations? The eigenvalues are plus and minus i. Now, you dont have to pick both of them because the negative one lead to essentially the same solutions but with negative signs. Well, it is (1, minus ) times cosine t, i squared is negative 1, so minus (0, onehalf), the negative 1 from the i squared, times sine t. Now, what solution is that? inus because they are going this way, minus 3 because there are three of them, and what each one of them is is negative 2 negative lambda. That is A. f subtract zero from the main diagonal, that doesnt do a great deal to A. And the resulting system of equations is those same things, except you have the a1s there, too. What is the system going to be? Now, what you see is that there are lots of solutions to this. f you have a matrix that is real and symmetric, like the one we have been working with, the theorem is that all its eigenvalues are complete. The matrix that we are talking about is the matrix, well, lets right away write A minus lambda cannot use the trace and determinant form for this equation because it is not a twobytwo matrix. But what is this going to be? o the complex solution is 1 minus i over 2 times e to the it. What is it? What is it? But what is new is that the minus 3 is a double root. The determinant, which is the constant term, is negative 1, minus negative 2, which is plus 1. o the characteristic equation, by calculating the trace and determinant is lambda squared plus 1 equals 0. The matrix that governs this system is A equals (1, 2; negative 1, negative 1) The trace of that matrix, the sum of the diagonal elements is zero. But have to give it to you because that is the word am going to try to use from now on, at least if remember to use it. And then the final solution that you make out of the two of them looks like alpha 1 times e to the lambda 1t. For the second one, we are going to have to use the eigenvalue lambda equals negative 3. Regardless of what the values of these two s this term goes to zero, that term goes to zero and what am left with is a constant solution. The solutions are now going to look once again like alpha times e to the a plus bi to the t. This will be complex and that will be complex, too. What is this? Then you are really in trouble because you just dont have enough solutions that you are supposed to get out of that, and you have to do something. And then with each eigenvalue you then have to calculate its eigenvector, which you do by solving the system (A minus lambda1, lets say, times ) alpha equals zero because the solution is the eigenvector alpha 1. Now, that, of course, is what is going to cause the trouble. And now it is the x3 that should get negative 2 for the coefficient. The question for differential equations is, what do the solutions to that look like? And, therefore, there is no inverse matrix because the determinant of the coefficient is zero. Well, you can see now that is the equation for x1 prime in terms of the other variables. And now that equals zero is the characteristic equation. Now, the only other thing should tell you is one of the most important theorems in linear algebra, which is totally beyond the scope of this course and is beyond the scope of most elementary linear algebra courses as have taught around the country but, of course, not at T. But, nonetheless, it is the last theorem in the course. This one has to be the same as that one, and so on. t will say cannot calculate the inverse to that matrix because the whole purpose of this exercise was to find a value of lambda such that this system of equations is dependent. But think since the situation is a little more complicated than it is where the case of a characteristic root gets repeated in the case of a secondorder equation as we saw it, you know what to do in that case, here there are different possibilities. You dont have to include e to the 0t because it is one. When write it out, remember, the important thing is you are going to have x1, x2, x3 down the left, so they have to occur in the same order on the right in order to use these standard eigenvalue techniques. And now have the three going in this direction, but each one of them has to be prefaced with a minus sign. t is six and that is all there is. But what is this? All the equations are essentially the same, except for the names of the variables so they all must give you the same thing after subtract minus 3 from the main diagonal. Lambda cubed plus, and dont get confused because it is this 2 that will kill you when you use the binomial theorem. And all the work in these problems is trying to find nice models where you wont have to use atlab to calculate the roots, the eigenvalues, the roots of the characteristic polynomial. Then what is the solution? There is the zero lambda here. For example, if it is repeated once, that is it occurs doubly then somehow have got to get two solutions out of that as was able to here. What you do is turn to problem two on your problem set and solve it because that tells you what to do. Lets take a equal 1 because am going to want to solve them numerically, and want you to be able to concentrate on what is important, what is new now and not fuss because dont want to have an extra a floating around everywhere just contributing nothing but a mild confusion to the proceedings. This is (x, y). Well, guess that is the end of the first part of the lecture. And then the last thing is plus 3 times (minus 2 minus lambda). Now you have to take the real and imaginary parts of that. That is the system we are trying to solve. That is the eigenvector. And what is the second equation? And if you want to use a little computer program, linear phase, this is not in the assignment, but the ellipses look like this and go around that way. The system for finding the eigenvectors, well, we are going to have to accept the complex eigenvector. Well, now have to take this and have to subtract negative 3 from the diagonal elements. Plus the first one that found c3 times (1, 1, 1) e to the 0t, which dont have to write in. When finally do it all, the equations are going to be, will write them first out as a system. That is why this is George and this is usan who seemed very normal to me. What that means is if you flip it around the main diagonal it looks just the same as before. Because, for each one of these, am supposed to calculate the eigenvector and make up the solution. This is 1 times 1 times minus 2 minus lambda, the same thing. hate to have a minus sign in front of a lambda cubed, so lets make this plus and we will make this minus and we will make this plus. Well, it would look like (a1, a2, a3), whatever the values of those variables were which gave me the solution to the equation, times e to the 0t. t is defective if you can get one eigenvector that goes with it, but you cannot find an independent one. The answer is this one is no longer independent of those two. Lets write out what that system of equations is. o all of these solutions tend to be the solution where all the cells are at the same temperature. And, therefore, the system becomes, the system have to solve is a1 plus a2 plus a3 equals zero. would like to, with the remaining time, explain to you what to do if you were to get complex eigenvalues. Well, what is A? What is the constant term? n other words, the inverse matrix will never work in these problems because the system of equations you will be trying to solve is always a nonindependent system. That is not what is essential. t is the first thing is to separate the vectors. That is the general solution to the system, (x1,x2, x3). When you do these problem you do it the same way, but dont ask me to explain what just did. o the system is (1 minus i) times a1 plus 2a2 is zero. And that is the model of George and usans love.",0.12001287001287
77,618,"We now come to our last major class of counting problems. We will count the number of ways that a given set can be partitioned into pieces of given sizes. We start with a set that consists of n different elements. And we have r persons. We want to give n1 items to the first person, give n2 items to the second person, and so on. And finally, we want to give nsubr items to the rth person. These numbers, n1, n2, up to nr are given to us, how many items each person should get. And these numbers must add to n so that every item in the original set is given to some person. We want to count to the number of ways that this can be done. This is the number of ways that we can partition a given set into subsets of prescribed sizes. Lets use c to denote the number of ways this can be done. We want to calculate this number c. nstead of calculating directly, were going to use the same trick that we employed when we counted combinations and derived the binomial coefficient. That is, were going to consider, in a much simpler counting problem, the problem of ordering n items, taking the n items in our original set and putting them in an ordered list. Of course, we know in how many ways this can be done. Ordering n items can be done in n factorial ways. This is the count of the number of permutations of n items. But now let us think of a different way of ordering the n items, an indirect way. t proceeds according to the following stages. We start with the n items. And we first distribute them to the different persons. Having done that, then we ask person one to take their items, order them, and put them in the first n1 slots of our list. Then person two takes their items and puts them into the next n2 slots in our list. We continue this way. And finally, the last person takes the items that they possess and puts them in the last nsubr slots in this list. n how many ways can this process be carried out? We have c choices on how to partition the given set into subsets. Then person one has n1 factorial choices on how to order the n1 items that that person processes. Person two has n2 factorial choices for how to order the n2 items that it possesses, and so on until the last person, who has nr factorial choices for ordering their elements. This multistage process results in an ordered list of the n terms. This is the number of ways these multistage process can be carried out. On the other hand, we know that the number of possible orderings of the items is n factorial. o we have this equality. We can solve this for c. And we find the answer, that the number of ways that the n items can be partitioned into subsets of the given sizes is n factorial divided by the product of the factorials of the different nis. This particular expression is called the multinomial coefficient, and it generalizes the binomial coefficient. The binomial coefficient was referring to the case where we essentially split our set into one subset with k elements, and then the second subset gets the remaining elements. o the special case where r is equal to 2, and n1 is equal to k, n2 equals to n minus k, this corresponds to a partition of a set into two subsets, or what is the same just selecting the first subset and putting everything else in the second subset. And you can check that in this particular case, the expression for the multinomial coefficient agrees with the expression that we had derived for the binomial coefficient.","We can solve this for c. And we find the answer, that the number of ways that the n items can be partitioned into subsets of the given sizes is n factorial divided by the product of the factorials of the different nis. We want to count to the number of ways that this can be done. This is the number of ways that we can partition a given set into subsets of prescribed sizes. We want to give n1 items to the first person, give n2 items to the second person, and so on. This is the count of the number of permutations of n items.",0.1588235294117647
78,619,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: Today were going to be talking about games. And know you guys as well as hope do. The main thing that you guys want to talk about with games is how to do that alphabeta thing. Because its pretty confusing. And its easy to get lost in a corner or something. Whereas doing the regular minimax, in my experience, most 6034 students can do that. And they do it right pretty much all the time. However, were going to focus on all the different components of games. And put up two provocative silver star ideas up on the board, which will come into play here. The now White principle is a new name. And it has never been revealed until today. Because made up name recently. o you will be the first people to hear it and decide if it works better than the term ""grandfather clause"" for the thing that m trying to describe. Because most grandfathers dont eat their children. o here weve got a beautiful game tree. t has nodes from A through R. This is our standard game tree from 6034. Weve got a maximizer up at the top whos trying to get the highest score possible. The minimizer is her opponent. And the minimizer is trying to get to the lowest score possible. And its really unclear who wins or loses at each point. Theyre just trying to get it to the highest or the lowest score. All right, so lets do a refresher. Hopefully the quiz didnt put people into such panic modes that they forgot ondays lecture. o lets make sure that we can do regular minimax algorithm on this tree and figure out the minimax value at A. o lets see how that works. All right, as you guys remember, the game search when using regular minimax is essentially a depth first search. And at each level, it chooses between all of the children whichever value that the parent wants. o here after it would choose the maximum of K and L, for instance. But thats getting ahead of ourselves. Because its a depth first search. o we best start at the top. ll help you guys up for a while. o were doing A. We need the maximum of B, , D, depth first search. We go to B. Were looking for the minimum of E and F. o having looked at E, our current minimum of E and F is just 2 for the moment. o this is going to be less than or equal to 2. All right, then we go down to F, which is a maximizer. And its children are K and L. o now m going to start making you guys do stuff. o what do you think? What is going to be the minimax value at F? The minimax value at F, what will that be? ADENE: PROFEOR: o that level is a maximizer max, min, max. F is a maximizer. K and L themselves are minimizers. But theyre pretty impotent minimizers. Because they dont get to choose. They just have to do K or L. o the minimax value is three. And yeah, the path it would like to go is K. o well say that the minimax value here is 3. ts in fact exactly equal to 3. o if this is 3 and this is 2, then everyone, we know that the value of B is? ADENE: PROFEOR: hear 3 and 2. Which one is it? ADENE: 2. PROFEOR: 2, thats right. o the value here is 2. Great, lets go down into this branch. o is going to be the minimum of G and 6. But we dont see that yet. Because were doing a depth first search. ts going to be the minimum of G. Now we need the maximum of and N. Were going to need the minimum. is the minimum of Q and R. o lets switch sides. The minimum of Q and R is? ADENE: 1. PROFEOR: Lets see. Thats right, its 1. o has a value of 1. But m going to stay over here. Because has a value of 1. Knowing that, then we know that G has a value of? ADENE: 7 PROFEOR: Thats right. 7 is higher than 1. And since G is a 7, we now know going up to that has a value of? ADENE: 6. PROFEOR: Yes, has a value of 6. Thats the minimum 6 and 7. o now m going to go back down. Because weve done one of the other subtrees. This is a 6. All right, great. Now were going to go down to D. Hopefully it wont be too bad. These things usually arent terrible. Because theyre made to be pruned a lot in alphabeta. o lets see, in D, we go down to . And thats just a 1. We go down to J. And lets see, whats the minimax value of J? ADENE: ts 20. PROFEOR: Thats right. 20 is the maximum of 20 and 2. Great, so whats the minimax value of D? Everyone said it 1. All right, so whats the minimax value at A? ADENE: 6. PROFEOR: 6 is right. 6 is higher than 2. ts higher than 1. Our value is 6. And our path is everyone A, , H. Thats it. Great, is everyone good with minimax? know that usually a lot of people are. Theres usually a few people who arent. o if youre one of the people who would like some clarifications on minimax, raise your hand. Theres probably a few other people who would like some too. OK. ADENE: When youre doing this minimax, whatever values are not showing, you keep going down the tree and then just look at whether youre trying to find the minimax. And just whatever values you get you go back up one? PROFEOR: Yes. The question was, when you go to do the minimax and lets say you got E was 2, and you know that B is going to be less than or equal to 2, but you dont know F yet. The question is, do you go down the tree, find the value at F, and then go back up? The answer is yes. By default, we use a depth first search. However, in non alphabeta version, just regular minimax, it turns out it probably doesnt matter what you do. suggested doing a depth first search to get yourself in the mindset of alphabeta. Because order is very, very important in alphabeta. But here, dont know, you could do some weird bottom up search. Whatever you want, its going to give you the right answer unless it asks what order they evaluated. But heres a hint. The order theyre evaluated in is depth first search order. o without even doing anything, E, K, L, Q, R, N, H, , O, P are the order of starting evaluation in this tree. ADENE: PROFEOR: o the question is, nodes like and G we dont have to put values next to. Technically, if we were doing this very formally, and we couldnt remember, and wasnt up there among the people, we would put 1 there. o at , we would put a 1. But people remembered that. o we didnt do it. But then at G, we would put a 7. o if we were writing it out very formally, we would have a 1 and a 7. And at this D, we would have a 1. And then at the A, we would put a 6. And then thats the answer. Also, weve even put things like less than or greater than part way along the way. However, believe that our alphabeta search is going to definitely fulfill everyones quota of pedantically putting lots of numbers next to nodes on a game tree. And so once youve done alphabeta, if you can do it correctly, youll think, oh, minimax, oh, those were the days. ts going to be easy. Because alphabeta is a little bit more complicated. Theres a lot of things that trip people up here. For alphabeta, however, will erase some of these numbers for the moment. Theyre still right. But we do it a little differently. o what do alphabeta and beta add to this formula? Well, this is all sort of a winning formula, except for its not. Because it takes too long. But its a very nice formula. is the maximizer, say. would try to think, if do this, whats he going to do? And then if he does that, what am going to do? And then what is he going to do if do that, et cetera, et cetera, all the way to the bottom. With alpha and beta, we add in what like to call nuclear options. d like in this game of maximizer minimizer you can think of it as like the old War or the Peloponnesian War, except the Peloponnesian War didnt have nukes, so probably the old War. And in the old War, or any situation where youre up against an adversary who actually, this doesnt really work as well for the old War. But in any situation where youre up against an adversary whose only goal in life is to destroy you, you always want to find out what the best thing you can possibly do is if they hit that button and send nukes in from uba, or if they send fighter pilots, or whatever is going on. o the idea of alpha and beta is that they are numbers that represent the failsafe, the worst case. Because obviously in the old War, sending nukes was not a good plan. But presumably, us sending nukes would be better than just being attacked and killed. o the alpha and beta represent the worst possible outcome youd be willing to accept for your side. Because right now, you know youre guaranteed to be able to force the conflict to that point or better. o the alpha is the nuclear option, the failsafe, of the maximizer. Nuclear options alpha is maximizers nuclear option. And beta is the minimizers nuclear option. o we ask ourselves and people who were paying attention at lecture or wrote stuff down know the answer already what could we possibly set to start off? Before we explore the tree and find anything, what will we set as our nuclear option, as our sort of failsafe? We could always fall back on this number. o you could set 0. You could try to set some low number for the maximizer. Because if you set a high number for the maximizer as its failsafe, its going to be really snooty and just say, oh, wont take this path. already have a failsafe thats better than all these paths. f you set, like, 100, you have no tree. Our default usually, in 6034, is to set negative infinity for alpha or negative some very large number if youre doing it in your lab. o if we set negative infinity as a default for alpha, that negative infinity is basically maximizer loses. o the maximizer goes in thinking, oh my god, if dont look at this game tree, automatically lose. Hes willing to take the first path possibly presented. And thats why that negative infinity is a good default for alpha. Anyone have a good idea what a good default for beta is, or just remember? Positive infinity, thats right. Because the minimizer comes in, and shes like, oh crap, the maximizer automatically wins if dont look at this node here. That makes sure the maximizer and the minimizer both are willing to look at the first path they see every time. Because look on this tree. f 10 was alpha, the maximizer would just reject out of hand everything except for P. And then we wouldnt have a tree. The maximizer would lose, because he would be like, hmm, this test game is very interesting. However, have another option pft, and then you throw over the table. Thats 10 for me, because you have to pick up the pieces. dont own this set. dont know. o that is why we set negative infinity and positive infinity as the default for alpha and beta. o how do alpha and beta propagate? And what do they do? The main purpose of alpha and beta is that, as we said, alpha lets say we have some chart of values. Alpha, which starts at negative infinity, is the worst that the maximizer is willing to accept. Because they know they can get that much or better. t starts out, thats the worst thing you can have. o its not a problem. nfinity is the highest that the minimizer is willing to accept. Thats beta. As you go along, though, the minimizer sees, oh, look at that. can guarantee that at best the maximizer gets 100. Haha, beta is now 100. The maximizer says, oh yeah? Well can guarantee that the lowest you can get me to go to is 0. o its going to be 0. And this keeps going on until maybe at 6 note, not drawn to scale. aybe at 6, the maximizer said, haha, you cant make me go lower than 6. And the minimizer says, aha, you cant make me go higher than 6. And then 6 is the answer. f you ever get to a point where beta gets lower than alpha, or alpha gets lower than beta, then you just say, screw this. m not even going to look at the remaining stuff. m going to just prune now and go somewhere else thats less pointless than this. Because if the alpha gets higher than the beta, what thats saying is the maximizer says, oh man, look at this, minimizer. The lowest you can make me go is, say, 50. And the minimizer says, thats strange. Because the highest that you can make me go is 40. o somethings generally amiss there. t usually means that one of the two of them doesnt even want to be exploring that branch at all. o you prune at that point. All right, so given that thats what were looking for, how do we move the alphas and betas throughout the tree? Theres a few different ways to draw them. And some of them consider to be very busy. Probably in recitation and tutorial you will see a way thats busier and has more numbers. Technically, every node has both an alpha and a beta. However, the one that that node is paying attention to is the alpha, if its a maximizer, and the beta if its a minimizer. o generally, for my purposes, only draw the alpha out for the maximizer and only draw the beta out for the minimizer. Very rarely, but it happens, theyll sometimes ask you, well, whats the beta of this node, which is a maximizer node? o its good to know how its derived. But think that it wastes your time to write it out. Thats my opinion. Well see how it goes. o the way that it works, the way that alpha and beta works, is the now White principal. o does everyone know the story of now White? o theres a beautiful princess. Theres an evil queen stepmother. irror mirror on the wall, whos the fairest of them all, finds out that its the stepdaughter. o much like in the real world, in now White, the stepdaughter, now White, had the beauty of her parents. he inherited those. However, much like in the real world, maybe or perhaps not, the stepmother had an even better plan. he hired a hunter to sort of hunt now White, pull out now Whites heart, and feed it to her so that she could gain now Whites beauty for herself. How many people knew that version of the story? A few people. Thats the original version of the story. Disney didnt put that in. The hunter then brought the heart of a deer, which think in Disney the hunter did kill a deer arbitrarily, but it was not explained that thats why he was doing it. o in alphabeta, its just like that. By which mean you start by inheriting the alpha and beta of your parents. But if you see something that you like amongst your children, you take it for yourself the now White principle. o lets see how that goes. Well, told you guys that the default alpha was ADENE: Negative infinity. PROFEOR: Negative infinity. o here alpha is negative infinity. And told you that the default beta was positive infinity. Were doing a depth first search here. All right, beta is infinity. All right, so we come here to E. Now, we could put an alpha. But never put an alpha or a beta for one of the terminal nodes. Because it cant really do anything. ts just 2. o as we go down, we take the alpha and beta from our parents. But as we go up to a parent, if the parent likes what it sees in the child, it takes it instead. o ask you all the question, would the minimizer prefer this 2 that it sees from its child or its own infinity for a beta? ADENE: 2 PROFEOR: t likes the 2. Thats absolutely right. o 2. All right, great, so now we go down to F. What is Fs alpha? Who says negative infinity? Who says 2? No one oh, you guys are good. ts negative infinity. Technically, it also will have a beta of 2. But were ignoring the beta. And the alphas that have been progressing downward from the parents negative infinity. Thats why called it the grandfather clause before. Because you would often look up to your grandparent to see what your default number is. o we get an alpha of negative infinity. We then go down to the K. ts a static evaluation. And now m going to start calling on people individually. o hopefully people paid attention to the mob, who were always correct. All right, so we go down to K. And we see a 3. F is a maximizer node. o what does F do now? ADENE: witches its alpha to 3. PROFEOR: Yes, switches its alpha to 3, great. All right, so thats already quite good. t switches alpha to 3. ts very happy. ts got a 3 here. Thats a nice value. o what does it do at L, the next node? ts gone to K, went back up to F. Depth first search, the next one would be L, right? ADENE: PROFEOR: Well, technically F could take Ls value of 0 if it liked it better than 3. But its a maximizer. o does it want to take that? ADENE: PROFEOR: OK, that technically would be correct. But m sorry. burdened you with a trick question. n fact, we dont look at L at all. Does everyone see that? ll explain. The alpha at F has reached 3. But the beta at B is 2. o B looks down and says, wait a minute. f go down to F, my enemys nuclear option, my enemy is the worst it can be for the best it can be for me is 3. F is trumpeting it around. was thinking of eating his heart, or whatever, but didnt want to. But its going to be 3. ts going to be 3 or higher down there at F. Theres no way want that. already have my own default escape plan. And thats 2. Thats going to be better than whatever comes out of that horrible F. o screw it. And we never look at L. Does everyone get that? That is the main principle of alphabeta pruning. f you see an alpha thats higher than the beta above it as said, if alpha goes up above the beta or if you see a beta, like if theres a beta down here, and its lower than the alpha above it, prune it. top doing that. And the question is, who prunes? Who decides that you dont look at L? The person who is thinking not to look at L is always up higher by at least two levels. o up here, B is saying, hmm, dont want to look at L. Because F is already so terrible for me that its just beyond belief. f this is 100, it might be 100. Even if its lower, m still going to get a three. Theres a sanity check that ve written that sort of came up with just in case youre not sure that you can skip it. Because on a lot of these tests, we ask you, which ones do you evaluate, which ones do you skip, right? Or we just say, which ones do you evaluate, and you dont write the ones that you skip. Heres my sanity test to see if you can skip it. Ask yourself, if that node that m about to skip contained a negative infinity or some arbitrarily small number, negative infinity being the minimizer wins, would it change anything? Now that ve answered that, if it contained a positive infinity, would it change anything? f the answer is no both times, then youre definitely correct in pruning it. o look at that 0. f it was a negative infinity, minimizer wins, what would happen? The maximizer would say, m not touching that was a 10 foot pole, choosing 3. The minimizer would say, screw that, ll take E. Lets say it was a positive infinity. The maximizer would say, eureka, holy grain, win. The minimizer would say, yeah, if m a moron, and go down to F, and then would go to E and take 2. o no matter what was there, the minimizer would go to E. And you could say, well, what if it was exactly 2? But still the maximizer would choose K. The minimizer would go to E. o theres no reason to go down there. We can just prune it off right now. Does everyone agree, everyone see what m talking about here? Great, so were now done with this branch. Because beta is 2. o now were up at old grandpappy A. And he has an alpha of negative infinity. Everyone, what will he do? Hell take the 2. ts better than negative infinity for him. ts not wonderful. But certainly anything is better than an automatic loss. All right, now our highest node is a 2. o lets keep that in mind for our alpha. OK, so lets go over here. Lets see, so what will be the value at ? What will be the beta value? ADENE: PROFEOR: You go back to which one? To G. m not at G yet. m actually just starting the middle branch. o m going to . And whats going to be its starting beta before go down? ADENE: nfinity. PROFEOR: nfinity, thats right default value. ts easier than it seemed. All right, so yes, beta is equal to infinity. This should be better erased. think its confusing people. Great, OK, so beta is equal to infinity at . Now we go down depth first search to G. Whats going to be our alpha at G? ADENE: inus infinity. PROFEOR: Ahh, it would seem so. However, take a look up at the greatgrandpappy A. t seems to have changed to 2. o this time its 2. Why is it 2 instead of negative infinity? Why can we let A be so noxious and not start with saying, oh, automatically lose? Well, A knows that no matter how awful things get in that middle branch, he can just say, screw the whole middle branch. m going to B. Thats something that the minimizer cant do. And we have to start at infinity for the minimizer. But the maximizer can. Because he has the choice at the top. Does everyone see that? He can just say, oh, m not even going to . Yeah, shows you. m going to A and taking the 2. o therefore alpha is actually 2 at G. All right, great, so weve got an alpha thats 2 at G. Were going to go down to . ts a minimizer. All right, whats going to be our beta value at ? ADENE: PROFEOR: Or which is the beta default, minus or positive infinity? What would be the minimizer? ADENE: Positive. PROFEOR: Positive infinity, thats right. is going to be a positive infinity for beta. Again, it picks it up from . Great, now we get to some actual values. o were at some actual values. We are at Q. o whats going to happen at when sees that Q is 1? ADENE: PROFEOR: What is beta? t says infinity. m sorry, its hard to read. Beta is infinity at . ADENE: OK, so its going to minimize, right? o its going to be like, OK, . PROFEOR: Thats right. o theyre going to put beta to 1. Because it sees Q. Great, so my next question is, whats going to happen at R? ADENE: PROFEOR: Very smart. Youve detected my trap. The question is, does it look at R? The answer is, no. t doesnt look at R. Why doesnt it look at R? Does everyone see? Yeah, alpha is now greater than the beta below it. Beta has gotten lower than alpha. This is the same thing was talking about before, when we figured out that the alpha here is 2. The maximizer says, wait a minute. The maximizer G says, if go to , the best m getting out of this is 1. Because if this is negative infinity, the minimizer will choose it. f this is positive infinity, hell choose 1. The best m going to get out of here is 1. f thats the case, might as well have just gone to B and not even gone to . o m not going to go to . ll go to N, maybe. aybe N is better. Does everyone see that? Great, so lets say that the maximizer does go to N. o whats going to happen with this alpha? ADENE: PROFEOR: Thats right, its going to be 7. 7 is better than 2. And the maximizer has control to get to that seven, at least if it gets to G. All right, now the minimizer at well do everyone this time. The minimizer at , seeing that 7, what does the minimizer do? Anyone? o it sees the 7. What does it do to its beta? t takes the 7 better than infinity, anyway. And yeah, then it checks H. And everybody, again, what happens at H? t takes the 6. ts lower than 7. All right, now well go back to having people do it on their own. Well, all the way back to the top, what does A do when it sees the 6 coming out of ? ADENE: hanges to 6. PROFEOR: hanges to 6, thats right. Alpha equals 6. Great homestretch, people, homestretch. o the minimizer, everyone, has a beta of infinity. And if wasnt a static node, it would have an alpha of 6. But it is a static node. o it just has a value of 1. o since it has a value of 1, everyone, the beta becomes 1. And what next, everyone? ADENE: Prune. PROFEOR: Prune, thats right. Why prune? Well, this time its A himself who can prune. A says, well darn, if go to D, m going to get 1 or something even worse than 1. might as well take my 6 while have it, prune all the rest all the way down. Everyone see that? Everyone cool with that? ts not too bad if you take it one step at a time. We did it. Our question is, which nodes are evaluated in order? Our answer is, everyone E, K, Q, N, H, . OK, not so obvious, guess. A few people followed me. But it is E, K, Q, N, H, . ts just depth first order. And we pruned some of them away. Great, so that is alphabeta. Any questions about that before give some questions about progressive deepening? All right, weve got a bunch. o first question. ADENE: nodes like F, B, , and D? PROFEOR: The question is, when asked for the order of evaluation, are we excluding F, B, , and D? The answer is were talking about here static evaluation. The static evaluator is a very important and interesting function. And ll get back to something a few students have asked me about the static evaluator later and try to explain what it is. ts basically the thing that pops out those numbers at the bottom of the leaves. o when we ask, what is the order of nodes that were statically evaluated, we mean leaves only. Thats a good question. Any other questions? Lets see, there was one up here before. But its gone. t might have been the same one. Question? ADENE: o a similar question. When you say, static nodes, that just means the leaf nodes? PROFEOR: eans the leaf nodes, thats right. The question is, does static nodes mean the leaf nodes. The answer is yes. ADENE: And so static evaluation is when you compare the value of a static node to something? PROFEOR: tatic evaluation is when you get that number, the static node. Let me explain. nless someone else has another question about alphabeta, let me explain static values. Because was about to do that. There is a question about alphabeta. ll come back to both of yours after answer this. ADENE: You were mentioning . And m a little bit confused. f youre looking at one node, and youre seeing either grab the value from the grandparent or grab it from the PROFEOR: o it always starts the question is, what is the now White principle? How does it work? Every node always starts off with taking the value of the same type, alpha or beta, from its grandparent. t always starts that way. Now, you say, why the grandparent? Wouldnt it take it from the parent? t actually does. But m not drawing out the alphas at all the minimizer levels. Because they dont do anything. Theyre only even there to pass them down. o all of the values pass down, down, down, down, down to begin. Every node, in fact, starts off with its grandparents with its parents values, OK? But then when the node sees a child, its completely done evaluating. ts finished. t cant be in the process. Lets say . When sees that G is completely done with all of its subbranches and is ready to return a value, or if its just a static evaluation, then its automatically completely done. Because it has no children. A static value like K of 3 is automatically completely done. ts got a 3. imilarly, when we came back to G after going to N, and we knew that the value was 7, that was completely done. The value was definitely 7. There was no other possibilities. ADENE: Thats after looking at the children, right? PROFEOR: Yes. o once youre done with all the children of G, then G comes up and says, guess what? Guess what, guys? o technically before that, you would have said that Gs alpha is greater than or equal to 1 when we looked at Q. And then we looked at . Wed say, its equal exactly to 7. Were done here. And then at that point, when its fresh and ripe and has all of its highest value or its best value, thats when the parent can eat its heart and gain that value itself. o thats when says, for instance, oh man, have an infinity. really like that 7 better. And it takes the 7. But then it saw H. And it said, oh man, thats a 6. Thats even better than 7. o it took the 6. ADENE: o shouldnt the alpha take 7 then? PROFEOR: o alpha takes 6. Because is a minimizer. took the 7 from G, but then right after that saw H and took the 6. Because 6 is even lower than 7. And then alpha took the 6. Because 6 was higher than 2. ADENE: o its not going to look below the branch? PROFEOR: Yeah, the problem is that the maximizer doesnt have control there. The minimizer has got control at . And the minimizer is going to make sure its as low as possible. The maximizer at A, his only control, or her only control, is the ability to send either way to B or or D. And then at that point, at , the minimizer gets to choose if we go to G or H. And its never going to choose G. Because G is higher than H. All right, awesome, was there another question? All right, lets go back to static evaluations. When first took this class, had some weird thoughts about static evolutions. heard some students ask me this. almost got a question about it onto one of the tests, but it was edited to some other weird question that was m to the b to the d minus 1 or something like that at the last minute. o m going to pose you guys the actual question that would have been on one of the older test, which is the following. had a student who came to me and said, you know, , when we do this alphabeta pruning, and all this other stuff, were trying to assume that were really saving that much time by getting rid of a few static evaluations. n fact, when we do progressive deepening, were always just counting, how many static evaluations do we have to do? And he said, look at these static evaluations. And theres just a 3 there. t takes no time to do the static evaluation. ts on the board. t takes much longer to do the alphabeta. ts faster by far to not do alphabeta. o then tried to explain to that student. said, OK, we need to be clear about what static evaluations are. You guys get it easy. We put these numbers on the board. A static evaluation lets say youre playing a game like chess. tatic evaluation takes a long time. When was in 6170, Java , the class that used to exist, we had a program called Antihess where used my 6034 skills to write the A. And the static evaluator took a long time. And we were timed. o getting the static evaluator faster, that was the most important thing. Why does it take a long time? Well, the static evaluator is an evaluation of the board position, the state of the game, at a snapshot of time. And thats not as easy as just saying, oh, heres the answer. Because in chess, first of all, not only did have to look at how many pieces had, what areas that controlled. Also well, it was antichess. But thats not withstanding. Lets pretend its regular chess. also had to look, if it was in regular chess and still had to do this in antichess if my king was in check. And what that meant is had to look at all of my opponents moves, possible moves, to see if anyone of them could take my king. Because in regular chess, its illegal to put your king into check. o you better not even allow that move. And regardless, getting into checkmate is negative infinity for you. o it takes a really long time to do static evaluations, at least good ones, usually. You want to avoid them. Because theyre not just some number on the page. They are some function you wrote that does a very careful analysis of the state of the game and says, m good to heuristically guess that my value is pi, or some other number, and then rates that compared to other states. Does that make sense to everyone? o the answer to the hypothetical question that might have been on the old test, when the person said, ve got this great idea where you do tons of static evaluation, and you dont have to do this long alphabeta, is, dont do that. The static evaluations actually take a long time. Does that clear it up for people who asked me before about what is a static evaluation, why are the leaf nodes called static? And you might ask, why are some of these static just arbitrarily? The answer is, when youre running out of time to expand deeper, and you just need to stop that stage of the game maybe its just getting too hairy, maybe its spreading out too much, you have some heuristic that says, this is where stop for now its a heuristic guess of the value. ts kind of like those heuristic values in the search tree. ts a guess of how much work you have left to get to the goal. Here, you say, well, wish could go deeper. But just dont have the time. o heres how think m doing at this level. ts not always right. And thats going to lead us into the answer to one of the questions about progressive deepening. o ll put up the progressive deepening question really quickly. o the question is this. Let me see, this is a maximizer yes. uppose that we do progressive deepening on the tree that is only two levels deep. What is progressive deepening in a nutshell if you dont remember from the lecture? The idea is this. n this tree, it doesnt work. But in trees that actually branch like 2 to the n, it doesnt take that much time to do some of the top levels first and then move on to the bottom levels. Just do them one at a time. o lets say we only did it up through J. We only did the top two levels of the tree. Wed like to reorder the tree so that alphabeta can prune as much as it possibly can, at least we hope. o lets pretend that we had a psychic awesome genius friend who told us that the static values when we went up to two levels remember, when we go to two levels, F, G, and J have to get a static value, right? Because were not going down. We do a static evaluation. They get the exact correct numbers 3, 7, and 20. Genius, brilliant. All right, so if that happens, what is the best way that we could reorder that tree? Oh yeah, so its A, B, , D with values of 2, 3, 7, 6, 1, 20. ll draw that. This is the nonreordered tree. Lets see, so its 2, 3, 7, 6, 1, 20. o whats the best way to reorder? Well, first of all, does anyone remember what Patrick said when he talked about progressive deepening? sually no one does, so dont worry about it. Because at that time you guys didnt think, oh, have to do this for the quiz. You were just thinking, oh man, weve already heard alphabeta and all this other stuff. And this is just a small fact. But its a very important fact. And now you know you have to do it for the quiz. o youre probably going to remember it. The way you do it is you try to guess, and you say, which one of these is going to be a winner? Whichever one think is going to be a winner at that level, put first. Why is that the case? Well, something interesting you may have noticed here whenever you have a winner, like the middle node, or whenever you have whatever is the current best for your alpha, you sort of have to explore out a lot of that area. Like for instance, the left node was our current best at 2. The middle branch was our current best, at that time was 6. t was the total best. We had to explore a good number of nodes. But on the right, we just saw, oh, theres 1. Were done. We cut everything off. n other words, the branch that turns out to be the one that you take, you have to do a pretty good amount of exploration to prove that its the right one. Whereas if its the wrong one, you can sometimes with just one node say, this is wrong, done. o therefore, if the one that turns out to be the eventual winner is first of all, then its really easy to reject all the other branches. Do people see that sort of conceptually a little bit, that if you get the best node right away, you can just reject all the wrong ones pretty quickly? Thats our goal. o how can we, quote, ""get the right one,"" the best one right away? Well, heres how we do it. Lets say were at B. Which one is the minimizer likely to pick assuming that our heuristic is good and that these guesses are pretty much close to the truth? t turns out theyre perfect, so this is going to work. o which one will the minimizer pick if it has to choose between E and F, do we think? ADENE: E. PROFEOR: E, perfect. Which one will it pick between G and H? ADENE: H. PROFEOR: H. Which one will it pick between and J? ADENE: . PROFEOR: OK, so what were saying is we think its going to pick E. We think its going to pick H. We think its going to pick . o first of all, we should put E before F, H before G, and before J. Because we think its going to pick those first. Those are probably our best ones to invalidate a poor branch. o now between 2, 6, and 1, which is what we think were going to get, which one do we think the maximizer is going to take? ADENE: 6. PROFEOR: 6. Then if it couldnt take 6, what would be its next best choice? 2, then 1. Thats just our order simple as that. t couldnt be anything easier that evolves really complex trees, a huge number of numbers, and reordering those trees. o you guys told me , B, D. You told me , B, D, think? Yeah, those are the ones the maximizer likes. And then the ones the minimizer likes you told me was H, and before G. Because H is smaller than G. You guys told me E before F. And you guys told me before J. And you guys would be correct in all regards. We have 6, 7, 2, 3, 1, 20. All the minimizers choose from smallest to highest. The maximizer chooses from highest to lowest of the ones that the minimizers will take. And if we did that, you can see we would probably save some time. Lets see how much time. Lets say we looked at H first. Well, if we looked at H first, we would still have actually had to look at Q and N. However, we would not have had to look at K. Do people see why? f we already knew this branch was 6, as soon as we saw 2 for the beta here 2 is less than 6 we could have pruned. We still would have had to look at over here. Because you have to look at at least one thing in the new subbranch. And it actually only would have saved us one node oops. o it winds up that in total, how many nodes would we have evaluated if we did that little scheme of reordering? Well, we normally had to do six E, K, Q, N, H, . How many do we evaluate if we do this progressive deepening scheme? How many times do we run the static evaluator, which of course you know the static evaluator takes a long time? Anyone have a guess? told you the only one we dont evaluate is K. Raise your hand. wont make anyone give this one. o said the only one we save on is K. o we still do E, Q, N, H, and over here. Theres two possible answers that will accept. o you have a higher chance of guessing it. Anyway? Does everyone agree that we did six before? f we didnt do any progressive deepening, we just did E, K, Q, N, H, . And now were not doing K. OK, people are saying five. All right, good. Thats not the right answer. But it at least shows that you can do taking away the one. We did at least five over here. Theres two possible answers, though. Because look over there. n order to do the progressive deepening, we had to do those static evaluations, right? o we either did all those static evaluations and these five E, K, Q, N, H, static evaluations. Because we didnt do the K. Or we might have saved ourselves. Because maybe we were smart and decided to cache the static values when we were going down the tree. ts an implementation detail that on this test when we asked that question we didnt say. What mean by cache is when we did it here and saw that E was a 2, and then here oh, we have to do the static value at E. f we were smart, we might have made a little hash table or something and put down 2 so we didnt have to do a static evaluation at E. And if that happened, well, we save E, H, and , and we do three fewer. Does everyone see that? However, thats still more than six. o it didnt save us time. o you might say, oh, progressive deepening is a waste of time. But its not. Because this is a very, very small, not very branchy tree that was made so that you guys could easily do alphabeta and take the quiz, and it wouldnt be bad. f this was actually branching even double at each level, it would have, what, 16 nodes down here at the bottom. Then you would want to be doing that progressive deepening. o now ask you a conceptual riddle question. ts not really that much of a riddle. But well see if anyone wants to answer. Again, wont call on you for this. According to this test, a student named teve says, OK, know have to pay to do the progressive deepening here. But lets ignore that. Because its small in a large tree, right? ts not going to take that much. Lets ignore the costs of the progressive deepening and only look at how much we do here. He says, when it comes to performing the alphabeta on the final level, m guaranteed to always prune at least as well or better if rearrange the nodes based on the best result from progressive deepening. Do you agree? ADENE: PROFEOR: an repeat it? OK, the question is, ignoring the cost that we pay progressively deepening here just forget about it at the final step, at the final iteration, the question is, am guaranteed to do at least as well or better in my alphabeta pruning when reorder based on the best order for progressive deepening? Here certainly we did. But the question is, is teve guaranteed? Answer? ADENE: PROFEOR: Thats the answer and the why, which we asked to explain. The answer we got is, doesnt that depend on the heuristic? Perfectly correct. The answer is, no, were not guaranteed, and it depends on the heuristic. o if we were guaranteed, that would be our heuristic was godlike, like this heuristic. f your heuristic already tells you the correct answer no matter what, dont do game search. Just go to the empty chess board, put all the pieces in the front rows, and run static evaluator on that. And itll say, oh, it looks like with this game not started that white is stupid, so black will win in 15 turns. And then youre done. And you dont do a search. We know that our heuristic is flawed in some way. t could be very flawed. f its flawed so badly that it tells us a very bad result of whats actually going to happen, even though we think the minimizer is going to go to H, maybe its wrong by a lot and it goes to G. t could take us up an even worse path and make us take longer. Question? ADENE: f its the heuristic, how could you cache the values so you didnt have to recalculate them later? PROFEOR: The question is, how can you cache the values if its a heuristic so you dont have to recalculate them later? The answer is, it wouldnt help if there werent these weird multilevel things where we stop at E for some reason, even though it goes down to five levels. The way you could cache it is it is a heuristic. But its consistent. And dont mean consistent from search. mean its a consistent heuristic in the game state E is, lets say thats the state where moved out my knight as the maximizer, and the minimizer said, youre doing the knight opening, really, and then did a counterattack. No matter how we get to E, or where we go to get to E, thats always going to be state E. ts always going to have the same heuristic value. ts not like some guy who goes around and just randomly pulls a number out of a hat. Were going to have some value that gives us points based on state E. And its going to be the same any time we go to state E. Does that make sense? t is a heuristic. But its always going to give the same value at E no matter how you got to E. But it could be really bad. n fact, you might consider a heuristic thats the opposite of correct and always tells us the worst move and claims its the best. Thats the heuristic that the minimizer program did to our computer, perhaps. n that case, when we do progressive deepening and we reorder, well probably get the worst pruning possible. We might not. But we may. o in that case, youre not guaranteed. hope thats given a few clues. n tutorial, you guys are going to see some more interesting problems that go into a few other details. at least plan on doing interesting game problem from last year, which asked a bunch of varied things that are a little bit different from these. o it should be a lot of fun, hopefully, or at least useful, to do the next quiz. o have a great weekend. Dont stress out too much about the quiz.","And yeah, the path it would like to go is K. o well say that the minimax value here is 3. ts in fact exactly equal to 3. o if this is 3 and this is 2, then everyone, we know that the value of B is? The question was, when you go to do the minimax and lets say you got E was 2, and you know that B is going to be less than or equal to 2, but you dont know F yet. What mean by cache is when we did it here and saw that E was a 2, and then here oh, we have to do the static value at E. f we were smart, we might have made a little hash table or something and put down 2 so we didnt have to do a static evaluation at E. And if that happened, well, we save E, H, and , and we do three fewer. And the maximizer has control to get to that seven, at least if it gets to G. All right, now the minimizer at well do everyone this time. However, the one that that node is paying attention to is the alpha, if its a maximizer, and the beta if its a minimizer. The way you do it is you try to guess, and you say, which one of these is going to be a winner? o the answer to the hypothetical question that might have been on the old test, when the person said, ve got this great idea where you do tons of static evaluation, and you dont have to do this long alphabeta, is, dont do that. ADENE: PROFEOR: o the question is, nodes like and G we dont have to put values next to. The maximizer at A, his only control, or her only control, is the ability to send either way to B or or D. And then at that point, at , the minimizer gets to choose if we go to G or H. And its never going to choose G. Because G is higher than H. All right, awesome, was there another question? m going to A and taking the 2. o therefore alpha is actually 2 at G. All right, great, so weve got an alpha thats 2 at G. Were going to go down to . o now between 2, 6, and 1, which is what we think were going to get, which one do we think the maximizer is going to take? The question is, do you go down the tree, find the value at F, and then go back up? n other words, the branch that turns out to be the one that you take, you have to do a pretty good amount of exploration to prove that its the right one. o is going to be the minimum of G and 6. But in trees that actually branch like 2 to the n, it doesnt take that much time to do some of the top levels first and then move on to the bottom levels. The minimizer would say, yeah, if m a moron, and go down to F, and then would go to E and take 2. o no matter what was there, the minimizer would go to E. And you could say, well, what if it was exactly 2? What is going to be the minimax value at F? ADENE: PROFEOR: Thats right, its going to be 7. Because if the alpha gets higher than the beta, what thats saying is the maximizer says, oh man, look at this, minimizer. ADENE: PROFEOR: Thats the answer and the why, which we asked to explain. And now you know you have to do it for the quiz. OK, the question is, ignoring the cost that we pay progressively deepening here just forget about it at the final step, at the final iteration, the question is, am guaranteed to do at least as well or better in my alphabeta pruning when reorder based on the best order for progressive deepening? And we have to start at infinity for the minimizer. The best m going to get out of here is 1. f thats the case, might as well have just gone to B and not even gone to . Well can guarantee that the lowest you can get me to go to is 0. o its going to be 0. almost got a question about it onto one of the tests, but it was edited to some other weird question that was m to the b to the d minus 1 or something like that at the last minute. The answer is, when youre running out of time to expand deeper, and you just need to stop that stage of the game maybe its just getting too hairy, maybe its spreading out too much, you have some heuristic that says, this is where stop for now its a heuristic guess of the value. No matter how we get to E, or where we go to get to E, thats always going to be state E. ts always going to have the same heuristic value. We go down to J. And lets see, whats the minimax value of J? Great, so lets say that the maximizer does go to N. o whats going to happen with this alpha? And since G is a 7, we now know going up to that has a value of? o that is why we set negative infinity and positive infinity as the default for alpha and beta. Because at that time you guys didnt think, oh, have to do this for the quiz. The maximizer G says, if go to , the best m getting out of this is 1. All right, so we go down to K. And we see a 3. Well, if we looked at H first, we would still have actually had to look at Q and N. However, we would not have had to look at K. Do people see why? The question is, does it look at R? Alpha, which starts at negative infinity, is the worst that the maximizer is willing to accept. PROFEOR: The question is, how can you cache the values if its a heuristic so you dont have to recalculate them later? o ask you all the question, would the minimizer prefer this 2 that it sees from its child or its own infinity for a beta? Lets say were at B. Which one is the minimizer likely to pick assuming that our heuristic is good and that these guesses are pretty much close to the truth? All right, then we go down to F, which is a maximizer. Well, all the way back to the top, what does A do when it sees the 6 coming out of ? o m going to pose you guys the actual question that would have been on one of the older test, which is the following. f its flawed so badly that it tells us a very bad result of whats actually going to happen, even though we think the minimizer is going to go to H, maybe its wrong by a lot and it goes to G. t could take us up an even worse path and make us take longer. What does it do to its beta? f you see an alpha thats higher than the beta above it as said, if alpha goes up above the beta or if you see a beta, like if theres a beta down here, and its lower than the alpha above it, prune it. ADENE: And so static evaluation is when you compare the value of a static node to something? But its going to be 3. ts going to be 3 or higher down there at F. Theres no way want that. But its always going to give the same value at E no matter how you got to E. But it could be really bad. The main purpose of alpha and beta is that, as we said, alpha lets say we have some chart of values. Were going to have some value that gives us points based on state E. And its going to be the same any time we go to state E. Does that make sense? o therefore, if the one that turns out to be the eventual winner is first of all, then its really easy to reject all the other branches. ts going to be the minimum of G. Now we need the maximum of and N. Were going to need the minimum. f youre looking at one node, and youre seeing either grab the value from the grandparent or grab it from the PROFEOR: o it always starts the question is, what is the now White principle? All right, so if that happens, what is the best way that we could reorder that tree? And then at the A, we would put a 6. Now we go down depth first search to G. Whats going to be our alpha at G? o you will be the first people to hear it and decide if it works better than the term ""grandfather clause"" for the thing that m trying to describe. And then 6 is the answer. PROFEOR: The question is, when asked for the order of evaluation, are we excluding F, B, , and D? o technically before that, you would have said that Gs alpha is greater than or equal to 1 when we looked at Q. And then we looked at . o lets pretend that we had a psychic awesome genius friend who told us that the static values when we went up to two levels remember, when we go to two levels, F, G, and J have to get a static value, right? had a student who came to me and said, you know, , when we do this alphabeta pruning, and all this other stuff, were trying to assume that were really saving that much time by getting rid of a few static evaluations. They are some function you wrote that does a very careful analysis of the state of the game and says, m good to heuristically guess that my value is pi, or some other number, and then rates that compared to other states. We go to B. Were looking for the minimum of E and F. o having looked at E, our current minimum of E and F is just 2 for the moment. o the alpha is the nuclear option, the failsafe, of the maximizer. o which one will the minimizer pick if it has to choose between E and F, do we think? ts just 2. o as we go down, we take the alpha and beta from our parents. o the question is this. Because if this is negative infinity, the minimizer will choose it. And thats going to lead us into the answer to one of the questions about progressive deepening. o the idea of alpha and beta is that they are numbers that represent the failsafe, the worst case. And at this D, we would have a 1. Because if you set a high number for the maximizer as its failsafe, its going to be really snooty and just say, oh, wont take this path. ts got a 3. imilarly, when we came back to G after going to N, and we knew that the value was 7, that was completely done. Because this is a very, very small, not very branchy tree that was made so that you guys could easily do alphabeta and take the quiz, and it wouldnt be bad. We then go down to the K. ts a static evaluation. All right, great, so now we go down to F. What is Fs alpha? m going to B. Thats something that the minimizer cant do. This is the same thing was talking about before, when we figured out that the alpha here is 2. And then at that point, when its fresh and ripe and has all of its highest value or its best value, thats when the parent can eat its heart and gain that value itself.",0.14877161055505
79,620,"GLBERT TRANG: Hi, m Gilbert trang, and professor of mathematics at T. And get a chance to say a few words about 18.06, Linear Algebra. ts one of the basic math courses. an say a little about linear algebra itself? lasses in linear algebra earlier years tended to be pretty much for pure math majors, and a lot of proofs, and usefulness of the subject kind of wasnt so clear. Whereas, its an incredibly useful subject. Data is coming in all the time. Were in the century of data, and data tends to come in a matrix, in a rectangular array of numbers. And how to understand that data is a giant, giant problem. And people use matrices in solving differential equations in economics, everywhere. o the subject had to change to bring out this important aspect, that its terrifically useful. Often networks are a great model, where you have like like the internet. Every website would be like a node in the network. And if one website is linked to another one, there would maybe be an edge in that network. o thats a network with a billion nodes. And a matrix describes all those links. Like when Google produces a PageRank, you enter well, you could enter linear algebra, and see what happens. dont know. hope something good. Well, anyway, thousands and millions of stuff would come up ranked in order, and that order comes from operating Googles very fast at it, very good at it operating on that giant matrix that describes the internet. OK, so a word about the course itself the T course. First of all, there will be students coming from all the departments. That includes management. Business data comes in matrix form just the way engineering data comes. o there is hardly a prerequisite for the course. Theres no big reason why calculus has to come first. Probably most T students will know before the course starts they will have multiplied a matrix by a vector, or multiplied two matrices. o theyve at least seen matrices before. But anybody could catch up on that quickly. And then, the course just takes off. Actually, we go back to ask, how do you understand multiplying a matrix by a vector? A key yeah, you guys will probably know how to do it, but let me say it another way A matrix times a vector produces a combination of the columns in that matrix, those column vectors in the matrix. o thats like the key step in linear algebra. What you can do with vectors is take linear combinations. Well, at T, the course is organized with three lectures a week. And use the chalkboard. hope you feel, in watching them, that thats OK. The nice thing about a chalkboard is you get to see whats written doesnt disappear. o your eye can continually check back and see how does it connect with whats happening at the moment. And then, there is one hour a week of recitation. Because thats a smaller class, it just means theres a teaching assistant there, who can help with problems, suggest new problems. t can be a problembased hour, where my lectures are more explanation hours. o about the textbook. The homeworks come from the book mostly. ometimes we add ATLAB problems, sort of specially constructed ones. But the central ideas of the subject are described in each section of the book, and then, naturally, exercises to practice with those ideas. And then, the neat thing about 18.06 cholar is you get short lectures, short videos, from six different TAs, did about six problemsolving videos each. And they are neat. The TAs are good. And thats something that can happen in the recitation with a smaller group. Theres chance for a discussion, whereas in the lecture well, still ask questions in the lecture, as youll probably see. But its a little harder for students to shout out an answer, so they can shout all they want in their recitations. With each lecture, we produce a written summary of what its about. o after you watch the lecture, you could look at that summary and it reinforces, remembering the key points of the lecture. And then we also added in some problems, four or five problems from the book that you can just look at and see, OK, do know what the question is here? Do know how to do it? think, as a result, youre learning linear algebra. A thought or two about linear algebra worldwide, because it really is worldwide. The feedback comes from all over the world. ts really nice to get. Also, enjoy going. o if somebody invites me to Egypt or Australia or hina, tend to go if can. Because thats a lovely part about mathematics. ts really universal. ts a language almost of its own that everybody can learn to speak. And hope these lectures help.","And then we also added in some problems, four or five problems from the book that you can just look at and see, OK, do know what the question is here? A key yeah, you guys will probably know how to do it, but let me say it another way A matrix times a vector produces a combination of the columns in that matrix, those column vectors in the matrix. But the central ideas of the subject are described in each section of the book, and then, naturally, exercises to practice with those ideas. o after you watch the lecture, you could look at that summary and it reinforces, remembering the key points of the lecture. And thats something that can happen in the recitation with a smaller group. Were in the century of data, and data tends to come in a matrix, in a rectangular array of numbers. Well, anyway, thousands and millions of stuff would come up ranked in order, and that order comes from operating Googles very fast at it, very good at it operating on that giant matrix that describes the internet. o thats like the key step in linear algebra. lasses in linear algebra earlier years tended to be pretty much for pure math majors, and a lot of proofs, and usefulness of the subject kind of wasnt so clear. And if one website is linked to another one, there would maybe be an edge in that network. Data is coming in all the time.",0.2208067940552017
80,621,"n our previous lesson, we saw how we can implement a stack, these are two popular implementations of stack one using arrays and another using linked list. A warrior should not just possess a weapon he must also know when and how to use it. As programmers we must know in what all scenarios we can use a particular data structure. n this lesson, m going to talk about one simple use case of stack. A stack can be used to reverse a list or collection, or simply to traverse a list or collection in reverse order. m going to talk about two problems. Reversal of string and reversal of linked list and m going to solve both these problems using stack. Lets first discuss reversal of string, have a string in the form of a character array here, have this string ""HELLO"". A string is a sequence of characters. This is a style string. n a string must be terminated with a null character, so this last character is a null character. Reversal means characters in the array should be rearranged like what im showing here in the right. null character is used only to mark the end of string, it is not part of string. Okay there are couple of efficient ways in which we can reverse a string. Lets first discuss how we can solve this problem using a stack and then we will see how efficient it is. What we can do is we can create a stack of characters, m showing logical representation of a stack here this is a stack of characters and right now its empty and now what we can do is we can traverse the characters in the string from left to right and start pushing them onto the stack. o first H goes into the stack then the next character this E then L then we have another L and then the last character is O. Once all all the characters in the string have gone into the stack we can once again start at the 0th index. Now we need to write the topmost character in the stack, at this index we can get the top most character by calling top operation and now we can perform a pop and now we can go to the next index fill in whatever is at top of stack and perform a pop again. We can go on doing this until stack is not empty, so all the positions in the character array will be overwritten. o finally we have reversed a string here. n a stack whatever goes in last comes out first. o if we will push a bunch of items onto a stack and once all items have pushed if we will start popping we will get the items in reverse order. First item pushed onto the stack will come out last. Lets quickly right code for this logic. m going to write ++ here. Things will be pretty similar in other languages, so it doesnt really matter. what m going to do in my code is am going to create a character array to store a string and then will ask user to input a string, once input the string would make a call to a function named Reverse passing it the array and length of string that will get by making a call to string length function(strlen in ) and finally m printing the reversed string. Now need to write the reverse function. n reverse function want to use a stack, a stack of characters, we have already seen how we can implement stack. n ++, we can create a class named stack that would have an array of characters and an integer variable named top to mark the top of stack in array and these variables can be private and we can work up on the stack using these public functions. n Reverse() function we can simply create an object of stack and use it. This class can be an array based implementation of stack or linked list based implementation of stack it doesnt really matter, in ++ and many other languages language libraries also give us implementation of stack in this program m not going to write my own stack m going to use stack from what we call standard template library in ++. will have to use this includes statement #include and now have a stack class available to me to create an object of this class need to write stack and within angular brackets datatype for which we want a stack, then after space name or identifier with this one statement here have created a stack of characters. Lets now write the core logic this n in the signature of reverse function is number of characters in string this array as we know array in or ++ is always passed by deference through a pointer, this followed by brackets is only an alternate syntax for *. ts interpreted like this by the compiler. Okay so now what m going to do is m going to run a loop starting 0 till n1. o will traverse the string from left to right and as traverse the string will push the character onto stack by calling push function will use a statement like this once push is done and do another loop for pop. will run a loop with this variable i starting at 0 and going till n1 and will first set as top of stack and then will perform a pop operation. f you want to know more about functions available with stack in TL like their signatures and how to use them, you can check the description of this video for some resources. This is all need to do in my reverse() function. Lets run discord and see what happens. need to enter a string. Lets enter ""HELLO"". this is what get as output which seems to be correct lets run this again and this time want to enter mycodeschool this looks alright to so we seem to be good so this function is solving my problem of reversal. Lets now see how efficient it is lets analyze its time complexity we know that all operations on stack take constant time so all these statements within loop, inside loop, will take constant time. The first loop is running n times and then the second look is also running n times, first look will execute in O(n) and the second loop will also execute in O(n). The loops are not nested they are one after another so in such scenario complexity of the whole function will also be O(n). Time complexity is O(n) but we are using some extra memory here for stack. we are pushing all the characters in the string on to stack, the extra space taken in stack will be proportional to number of characters in the string, will be proportional to n. o we can say that space complexity of this function is also O(n), in simple words extra space taken is directly proportional to n. There are efficient ways to reverse a string without using extra space. The most efficient way probably would be to use just two variables to mark the start and end index in the string initially, lets say am using variables i and j, initially i for this example is 0 and j is 4. While i is less than j we can swipe the characters at these positions. And once we have swapped, we can increment i and decrement j , if i is less than j , we can swap again and once again increment i and decrement j. Now is not less than j,i is equal to j. At this stage we can stop swapping and were done. This algorithm has space complexity O(n), we are using constant extra memory here. Time complexity of this approach once again is O(n). We will do n/2 swaps, so time taken will be proportional to n. Definitely because of space complexity this approach is better than our stack approach sometimes when we know that our input will be very small and time and space is not much of concern, we use a particular algorithm for ease of implementation. for its being intuitive, its clearly not the case when were using stack to reverse a string but for this other problem, reversal of linked list that we have said we will discuss using a stack gives us are neat and intuitive solution. have drawn a linked list of integers here. As we knew linked list are collections of entities as we call nodes. Each node contains two fields, one to store data and other to store address of next node. have assumed that these nodes in this example here are at address is 100, 150, 250 and 300 respectively. dentity of a linked list is address of the head node. we typically stored this address in a variable named head. n an array, it takes constant time to access any element so whether its the first element or last element it takes constant time to access it, it is so because array is stored as one contiguous block of memory so if we know the starting address of the array, lets say the starting address of this R 400 and size of each element in the array, character takes one bite so for this example each element is one byte then we can calculate address of any element. o we know that 84 is at 400+4 or 404 but in a linked list nodes are stored at disjoint locations in memory, to access any nor do we have to start at the head node, so we can do something as simple as having 2 pointers at start and end and accessing the elements. We have already seen in this series, two possible approaches that can be used to reverse a linked list. one was an iterative solution where we go on reversing links as we traverse the list using some temporary variables, another solution was using recursion. The time complexity of iterative solution is O(n). pace complexity is O(n). n recursive solution we do not create a stack explicitly but recursive uses the stack in computers memory that is used to execute function calls in such a case we say that we are using implicit stack. stack is not being created explicitly but still we are using an implicit stack. will come back to this and explain in detail. The time complexity of recursive solution once again is O(n). but the space complexity is O(n) this time. pace complexity is also O(n). Now lets see how the can use an explicit stack to solve this problem. Once again have drawn logical representation of stack here, right now the stack is empty. n a program this will be a stack of type pointer to Node. What m going to do now is m going to traverse this linked list using a temporary pointer to Node. The temporary variable will initially point to head. When we will go to a particular node we will push the address of that node onto the stack. so first 100 will go to stack and now we will move to the next Node, now 150 will go in stack and now we will go to 250 and then to the last node at 300. They are showing addresses here in the stack but basically the objects that we are pushing are pointers to Node or in other words references to nodes. if Node is defined like this in ++ we will have to use these statements to traverse linked list and push all the references. Lets say head is a pointer to Node which m assuming is a global variable that will store the address of head node. m using a temporary variable that is pointed to Node, initially am starting the address of head node in this temporary variable and then m running a loop and m traversing the linked list and as m traversing m pushing the reference on to stack once all the references are pushed onto stack, we can start popping them and as we will pop them, we will get references to Nodes in reverse order, it would be like going through the list in reverse order. While traversing the list in reverse order we can build the reverse links. The first thing thatll do is ill take a temporary variable that will be pointer to Node and store the address of address at the top of stack which right now is 300 now will set head as this address, so head now becomes 300 and then will pop. m running you through this example here as m writing code, head and temp right now are both 300 and now would run a loop like this, like what have written here. while stack is not empty this function empty() returns true if stack is empty. m using stack from standard template library in ++, so while stack is not empty m going to say that set tempnext as address at top of stack. Basically m using this pointer to Node temp to dereference and set this particular address field right now top is 250 so m building this reverse link next statement is a pop() and in the next statement am saying temp = tempnext which means temp will now point to this node at 250 stack is not empty so loop will execute again. we are writing address here now then we should pop and then move to 150 using this statement temp = tempnext. now we are building this link popping and then oops this should have been 150 and with the next temp =tempnext were going here, even though we have built this link by setting this field here this node is still pointing to this guy because the stack is empty now we will exit the loop,after the loop, after exit from the loop, have written one more line tempnext = NLL. so m setting the last, link part of last node in reversed list as NLL finally this is my reverse function have assumed that head is a global variable and its a pointer to Node if you want a complete source code you can check the description of this video for a link. sing the stack in this case is making a life easier reversing a linked list is still a complex problem, try to just print the elements of linked list in reverse order. if you will use our stack it will be really easy. will stop here for this lesson, if you know, if you want to know what meant by implicit stack you can once again checked the description of this video for some resources so this is it for this lesson thanks for watching.","m using a temporary variable that is pointed to Node, initially am starting the address of head node in this temporary variable and then m running a loop and m traversing the linked list and as m traversing m pushing the reference on to stack once all the references are pushed onto stack, we can start popping them and as we will pop them, we will get references to Nodes in reverse order, it would be like going through the list in reverse order. What we can do is we can create a stack of characters, m showing logical representation of a stack here this is a stack of characters and right now its empty and now what we can do is we can traverse the characters in the string from left to right and start pushing them onto the stack. we are pushing all the characters in the string on to stack, the extra space taken in stack will be proportional to number of characters in the string, will be proportional to n. o we can say that space complexity of this function is also O(n), in simple words extra space taken is directly proportional to n. There are efficient ways to reverse a string without using extra space. The first thing thatll do is ill take a temporary variable that will be pointer to Node and store the address of address at the top of stack which right now is 300 now will set head as this address, so head now becomes 300 and then will pop. so first 100 will go to stack and now we will move to the next Node, now 150 will go in stack and now we will go to 250 and then to the last node at 300. Now we need to write the topmost character in the stack, at this index we can get the top most character by calling top operation and now we can perform a pop and now we can go to the next index fill in whatever is at top of stack and perform a pop again. if Node is defined like this in ++ we will have to use these statements to traverse linked list and push all the references. what m going to do in my code is am going to create a character array to store a string and then will ask user to input a string, once input the string would make a call to a function named Reverse passing it the array and length of string that will get by making a call to string length function(strlen in ) and finally m printing the reversed string. Basically m using this pointer to Node temp to dereference and set this particular address field right now top is 250 so m building this reverse link next statement is a pop() and in the next statement am saying temp = tempnext which means temp will now point to this node at 250 stack is not empty so loop will execute again. When we will go to a particular node we will push the address of that node onto the stack. now we are building this link popping and then oops this should have been 150 and with the next temp =tempnext were going here, even though we have built this link by setting this field here this node is still pointing to this guy because the stack is empty now we will exit the loop,after the loop, after exit from the loop, have written one more line tempnext = NLL. o first H goes into the stack then the next character this E then L then we have another L and then the last character is O. Once all all the characters in the string have gone into the stack we can once again start at the 0th index. will have to use this includes statement #include and now have a stack class available to me to create an object of this class need to write stack and within angular brackets datatype for which we want a stack, then after space name or identifier with this one statement here have created a stack of characters. o will traverse the string from left to right and as traverse the string will push the character onto stack by calling push function will use a statement like this once push is done and do another loop for pop. so m setting the last, link part of last node in reversed list as NLL finally this is my reverse function have assumed that head is a global variable and its a pointer to Node if you want a complete source code you can check the description of this video for a link. Lets now write the core logic this n in the signature of reverse function is number of characters in string this array as we know array in or ++ is always passed by deference through a pointer, this followed by brackets is only an alternate syntax for *. n ++, we can create a class named stack that would have an array of characters and an integer variable named top to mark the top of stack in array and these variables can be private and we can work up on the stack using these public functions. this is what get as output which seems to be correct lets run this again and this time want to enter mycodeschool this looks alright to so we seem to be good so this function is solving my problem of reversal.",0.1659420289855072
81,622,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer highquality educational resources for free. To make a donation, or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: Particles in quantum mechanics. n particular, the ones that are identical and noninteracting. o basically, we were focusing on a type of Hamiltonian for a system of N particles, which could be written as the sum of contributions that correspond respectively to particle 1, particle 2, particle N. o essentially, a sum of terms that are all the same. And oneparticle terms is sufficient because we dont have interactions. o if we look at one of these Hs so one of these oneparticle Hamiltonians, we said that we could find some kind of basis for it. n particular, typically we were interested in particles in the box. We would label them with some wave number k. And there was an associated oneparticle energy, which for the case of oneparticle in box was h bar squared k squared over 2m. But in general, for the oneparticle system, we can think of a ladder of possible values of energies. o there will be some k1, k2, k3, et cetera. They may be distributed in any particular way corresponding to different energies. Basically, you would have a number of possible states for one particle. o for the case of the particle in the box, the wave functions and coordinate space that we had x, k were of the form e to the i k dot x divided by square root of V. We allowed the energies where h bar squared k squared over 2m. And this discretization was because the values of k were multiples of 2 pi over l with integers in the three different directions. Assuming periodic boundary conditions or appropriate discretization for closed boundary conditions, or whatever you have. o thats the oneparticle state. f the Hamiltonian is of this form, it is clear that we can multiply a bunch of these states and form another eigenstate for HN. Those we were calling product states. o you basically pick a bunch of these ks and you multiply them. o you have k1, k2, kN. And essentially, that would correspond, lets say, in coordinate representation to taking a bunch of xs and the corresponding ks and having a wave function of the form e to the i k alp sum over alpha k alpha x alpha, and then divided by V to the N over 2. o in this procedure, what did we do? We had a number of possibilities for the oneparticle state. And in order to, lets say, make a twoparticle state, we would pick two of these ks and multiply the corresponding wave functions. f you had three particles, we could pick another one. f you had four particles, we could potentially pick a second one twice, et cetera. o in general, basically we would put N of these crosses on these oneparticle states that weve selected. Problem was that this was not allowed by quantum mechanics for identical particles. Because if we took one of these wave functions and exchanged two of the labels, x1 and x2, we could potentially get a different wave function. And in quantum mechanics, we said that the wave function has to be either symmetric or antisymmetric with respect to exchange of a pair of particles. And also, whatever it implied for repeating this exchange many times to look at all possible permutations. o what we saw was that product states are good as long as you are thinking about distinguishable particles. But if you have identical particles, you had to appropriately symmetrize or antisymmetrize these states. o what we ended up was a wabe of, for example, symmetrizing things for the case of fermions. o we could take a bunch of these kvalues again. And a fermionic, or antisymmetrized version, was then constructed by summing over all permutations. And for N particle, there would be N factorial permutations. Basically, doing a permutation of all of these indices k1, k2, et cetera, that we had selected for this. And for the case of fermions, we had to multiply each permutation with a sign that was plus for even permutations, minus for odd permutations. And this would give us N factorial terms. And the appropriate normalization was 1 over square root of N factorial. o this was for the case of fermions. And this was actually minus. should have put in a minus here. And this would have been minus to the P. For the case of bosons, we basically dispensed with this factor of minus 1 to the P. o we had a Pk. Now, the corresponding normalization that we had here was slightly different. The point was that if we were doing this computation for the case of fermions, we could not allow a state where there is a double occupation of one of the oneparticle state. nder exchange of the particles that would correspond to these kvalues, would get the same state back, but the exchange would give me a minus 1 and it would give me 0. o the fermionic wave function that have constructed here, appropriately antisymmetrized, exists only as long as there are no repeats. Whereas, for the case of bosons, could put 2 over same place. could put 3 somewhere else, any number that liked, and there would be no problem with this. Except that the normalization would be more complicated. And we saw that appropriate normalization was a product over k nk factorial. o this was for fermions. This is for bosons. And the two can submerge into one formula by writing a symmetrized or antisymmetrized state, respectively indicated by eta, where we have eta is minus 1 for fermions and eta is plus 1 for bosons, which is 1 over square root of N factorial product over k nk factorials. And then the sum over all N factorial permutations. This phase factor for fermions and nothing for bosons of the appropriately permuted set of ks. And in this way of noting things, have to assign values nk which are either 0 or 1 for fermions. Because as we said, multiple occupations are not allowed. But there is no restriction for bosons. Except of course, that in this perspective, as go along this kaxis, have 0, 1, 0, 2, 1, 3, 0, 0 for the occupations. Of course, what need to construct whether am dealing with bosons or fermions is that the sum over k nk is the total number of particles that have. Now, the other thing to note is that once have given you a picture such as this in terms of which oneparticle states want to look at, or which set of occupation numbers have nk, then there is one and only one symmetrized or antisymmetrized state. o over here, could have permuted the ks in a number of possible ways. But as a result of symmetrization, antisymmetrization, various ways of permuting the labels here ultimately come to the same set of occupation numbers. o it is possible to actually label the state rather than by the set of ks. By the set of nks. t is kind of a more appropriate way of representing the system. o thats essentially the kinds of states that we are going to be using. Again, in talking about identical particles, which could be either bosons or fermions. Lets take a step back, remind you of something that we did before that had only one particle. Because will soon go to many particles. But before that, lets remind you what the one particle in a box looked like. o indeed, in this case, the singleparticle states were the ones that told you before, 2 pi over l some set of integers. Epsilon k, that was h bar squared k squared over 2m. f want to calculate the partition function for one particle in the box, have to do a trace of e to the minus beta h for one particle. The trace can very easily calculate in the basis in which this is diagonal. Thats the basis that is parameterized by these kvalues. o do a sum over k of e to the minus beta h bar squared k squared over 2m. And then in the limit of very large box, we saw that the sum over k can replace with V integral over k 2 pi cubed. This was the density of states in k. e to the minus beta h bar squared k squared over 2m. And this was three Gaussian integrals that gave us the usual formula of V over lambda cubed, where lambda was this thermal wavelength h root 2 pi mk. But we said that the essence of statistical mechanics is to tell you about probabilities of various microstates, various positions of the particle in the box, which in the quantum perspective is probability becomes a density matrix. And we evaluated this density matrix in the coordinate representation. And in the coordinate representation, essentially what we had to do was to go into the basis in which rho is diagonal. o we had x prime k. n the k basis, the density matrix is just this formula. ts the Boltzmann weight appropriately normalized by Z1. And then we go kx. And basically, again replacing this with V integral d cubed k 2 pi cubed e to the minus beta h bar squared k squared over 2m. These two factors of xk and x prime k gave us a factor of e to the kx, xk have as ik dot x prime minus x. ompleting the square. Actually, had to divide by Z1. There is a factor of 1 over V from the normalization of these things. The two Vs here cancel, but Z1 is proportional to V. The lambda cubes cancel and so what we have is 1 over V e to the minus x minus x prime squared pi over lambda squared. o basically, what you have here is that we have a box of volume V. There is a particle inside at some location x. And the probability to find it at location x is the diagonal element of this entity. ts just 1 over V. But this entity has offdiagonal elements reflecting the fact that the best that you can do to localize something in quantum mechanics is to make some kind of a wave packet. OK. o this we did last time. What we want to do now is to go from one particle to the case of N particles. o rather than having 1x prime, will have a whole bunch of x primes labeled 1 through N. And want to calculate the N particle density matrix that connects me from set of points x to another set of points x prime. o if you like in the previous picture, this would have been x1 and x1 prime, and then now have x2 and x2 prime, x3 and x3 prime, xN and xN prime. have a bunch of different coordinates and d like to calculate that. OK. Once more, we know that rho is diagonal in the basis that is represented by these occupations of oneparticle states. And so what can do is can sum over a whole bunch of plane waves. And have to pick N factors of k out of this list in order to make one of these symmetrized or antisymmetrized wave functions. But then have to remember, as said, that should not overcount distinct set of kvalues because permutations of these list of ks that have over here, because of symmetrization or antisymmetrization, will give me the same state. o have to be careful about that. Then, go from x prime to k. Now, the density matrix in the kbasis know. t is simply e to the minus beta, the energy which is sum over alpha h bar squared k alpha squared over 2m. o sum over the list of k alphas that appear in this series. There will be n of them. have to appropriately normalize that by the Nparticle partition function, which we have yet to calculate. And then go back from k to x. Now, lets do this. The first thing that mentioned last time is that would, in principle, like to sum over k1 going over the entire list, k2 going the entire list, k3 going over the entire list. That is, would like to make the sum over ks unrestricted. But then have to take into account the overcounting that have. f am looking at the case where all of the ks are distinct they dont show any double occupancy then have overcounted by the number of permutations. Because any permutation would have given me the same number. o have to divide by the number of permutations to avoid the overcounting due to symmetrization here. Now, when have something like this, which is a multiple occupancy, have overdone this division. have to multiply by this factor, and thats the correct number of overcountings that have. And as said, this was a good thing because the quantity that had the hardest time for, and comes in the normalizations that occurs here, is this factor of 1 over nk factorial. Naturally, again, all of these things do depend on the symmetry. o better make sure indicate the index. Whether m calculating this density matrix for fermions or bosons, it is important. n either case well, what need to do is to do a summation over P here for this one and P prime here or P prime here and P here. t doesnt matter, theres two sets of permutations that have to do. n each case, have to take care of this eta P, eta P prime. And then the normalization. o divide by twice, or the square of the square root. get the N factorial product over k nk factorial. And very nicely, the overcounting factor here cancels the normalization factor that would have had here. o we got that. Now, what do we have? We have P prime permutation of these objects going to x, and then we have here P permutation of these k numbers going to x. guess the first one got wrong. start with x prime. Go through P prime to k. And again, symmetries are already taken into account. dont need to write that. And have the factor of e to the minus beta h bar squared sum over alpha k alpha squared over 2m divided by ZN. OK, so lets bring all of the denominator factors out front. have a ZN. have an N factorial squared. Two factors of N factorial. have a sum over two sets of permutations P and P prime. The product of the associated phase factor of their parities, and then have this integration over ks. Now, unrestricted. ince it is unrestricted, can integrate independently over each one of the ks, or sum over each one of them. When sum, the sum becomes the integral over d cubed k alpha divided by 2 pi yeah, 2 pi cubed V. Basically, the density in replacing the sum over k alpha with the corresponding integration. o basically, this set of factors is what happened to that. OK, what do we have here? We have e to the i x well, lets be careful here. have e to the i x prime alpha acting on k of p prime alpha because permuted the klabel that went with, say, the alpha component here with p prime. From here, would have minus because its the complex conjugate. have x alpha k p alpha, because permuted this by k. have one of these factors for each V. With each one of them, there is a normalization of square root of V. o the two of them together will give me V. But thats only one of the Nparticle o there are N of them. o if want, can extend this product to also encompass this term. And then having done so, can also write here e to the minus beta h bar squared k alpha squared over 2m within the product. ADENE: after this is it quantity xk minus quantity. PROFEOR: o forgot the i. OK, good? o the Vs cancel out. All right, so thats fine. What do we have? We have 1 over ZN N factorial squared. Two sets of permutations summed over, p and p prime. orresponding parities eta p eta of p prime. And then, have a product of these integrations that have to do that are threedimensional Gaussians for each k alpha. What do get? Well, first of all, if didnt have this, if just was doing the integration of e to the minus beta h bar squared k squared over 2m, did that already. get a 1 over lambda cubed. o basically, from each one of them will get a 1 over lambda cubed. But the integration is shifted by this amount. Actually, already did the shifted integration here also for one particle. o get the corresponding factor of e to the minus ah. have to be a little bit careful over here because what am integrating is over k alpha squared. Whereas, in the way that have the list over here, have x prime alpha and x alpha, but a different k playing around with each. What should do? really want this integration over k alpha to look like what have over here. Well, as sum over all possibilities in each one of these terms, am bound to encounter k alpha. Essentially, have permuted all of the ks that originally had. o the k alpha has now been sent to some other location. But as sum over all possible alpha, will hit that. When hit that, will find that the thing that was multiplying k alpha is the inverse permutation of alpha. And the thing that was multiplying k alpha here is the inverse permutation of p. o then can do the integration over k alpha easily. And so what do have? have x prime of p prime inverse alpha the inverse permutation minus x of p inverse alpha squared pi over lambda squared. Now, this is still inconvenient because am summing over two N factorial sets of permutations. And expect that since the sum only involves comparison of things that are occurring N times, as go over the list of N factorial permutation squared, will get the same thing appearing twice. o it is very much like when we are doing an integration over x and x prime, but the function only depends on x minus x prime. We get a factor of volume. Here, it is easy to see that one of these sums can very easily do because it is just repetition of all of the results that have previously. And there will be N factorial such terms. o doing that, can get rid of one of the N factorials. And will have only one permutation left, Q. And what will appear here would be the parity of this Q that is the combination, or if you like, the relative of these two permutations. And have an exponential of minus sum over alpha x alpha minus x prime Q alpha squared pi over lambda squared. And think forgot a factor of lambda to the 3 . This factor of lambda 3. o this is actually the final result. And lets see what that precisely means for two particles. o lets look at two particles. o for two particle,s will have on one side coordinates of 1 prime and 2 prime. On the righthand side, have coordinates 1 and 2. And lets see what this density matrix tells us. t tells us that to go from x1 prime x2 prime, a two particle density matrix connecting to x1 x2 on the other side, have 1 over the twoparticle partition function that i havent yet calculated. Lambda to the sixth. N factorial in this case is 2. And then for two things, there are two permutations. o the identity maps 1 to 1, 2 to 2. And therefore, what will get here would be exponential of minus x1 minus x1 prime squared pi over lambda squared minus x2 minus x2 prime squared pi over lambda squared. o thats Q being identity and identity has essentially 0 parity. ts an even permutation. The next thing is when exchange 1 and 2. That would have odd parity. o would get minus 1 for fermions, plus for bosons. And what would get here is exponential of minus x1 minus x2 prime squared pi over lambda squared minus x2 minus x1 prime squared pi over lambda squared. o essentially, one of the terms the first term is just the square of what had before for one particle. take the oneparticle result, going from 1 to 1 prime, going from 2 to 2 prime and multiply them together. But then you say, cant tell apart 2 prime and 1 prime. aybe the thing that you are calling 1 prime is really 2 prime and vice versa. o have to allow for the possibility that rather than x1 prime here, should put x2 prime and the other way around. And this also corresponds to a permutation that is an exchange. ts an odd parity and will give you something like that. ay OK, have no idea what that means. ll tell you, OK, you were happy when put x prime and x here because that was the probability to find the particle somewhere. o let me look at the diagonal term here, which is a probability. This should give me the probability to find one particle at position x1, one particle at position x2. Because the particles were noninteracting, one particle it could be anywhere. had the 1 over V. s it 1 over V squared or something like that? Well, we find that there is factor out front that we havent yet evaluated. t turns out that this factor will give me a 1 over V squared. And if set x1 prime to x1, x2 prime to x2, which is what ve done here, this factor becomes 1. But then the other factor will give me eta e to the minus 2 pi over lambda squared x1 minus x2 squared. o the physical probability to find one particle or more correctly, a wave packet here and a wave package there is not 1 over V squared. ts some function of the separation between these two particles. o that separation is contained here. f really call that separation to be r, this is an additional weight that depends on r. This is r squared. o you can think of this as an interaction, which is because solely of quantum statistics. And what is this interaction? This interaction V of r would be minus kT log of 1 plus eta e to the minus 2 pi r squared over lambda squared. will plot out for you what this V or r looks like as a function of how far apart the centers of these two wave packets are. You can see that the result depends on eta. f eta is minus 1, which is for the case of fermions, this is 1 minus something. ts something that is less than 1. would be negative. o the whole potential would be positive, or repulsive. At large distances, indeed it would be exponentially going to 0 because can expand the log at large distances. o here have a term that is minus 2 pi r squared over lambda squared. As go towards r equals to 0, actually things become very bad because r goes to 0 will get 1 minus 1 and the log will diverge. o basically, there is, if you like, an effective potential that says you cant put these two fermions on top of each other. o there is a statistical potential. o this is for eta minus 1, or fermions. For the case of bosons, eta plus 1. t is log of 1 plus something, so its a positive number inside the log. The potential will be attractive. And it will actually saturate to a value of kT log 2 when r goes to 0. o this is again, eta of plus 1 for the case of bosons. o the one thing that this formula does not have yet is the value for this partition function ZN. t gives you the qualitative behavior in either case. And lets calculate what ZN is. Well, basically, that would come from noting that the trace of rho has to be 1. o ZN is trace of e to the minus beta H. And essentially, can take this ZN to the other side and evaluate this as x e to the minus beta H x. That is, can calculate the diagonal elements of this matrix that have calculated that have over there. o there is an overall factor of 1 over lambda cubed to the power of N. have N factorial. And then have a sum over permutations Q eta of Q. The diagonal element is obtained by putting x prime to be the same as x. o have exponential of minus x sum over alpha x alpha minus x of Q alpha. set x prime to be the same as x. quared. And then theres an overall pi over lambda squared. And if am taking the trace, it means that have to do integration over all xs. o m evaluating this trace in coordinate basis, which means that should put x and x prime to be the same for the trace, and then have to sum or integrate over all possible values of x. o lets do this. have 1 over N factorial lambda cubed raised to the power of N. OK. Now have to make a choice because have a whole bunch of terms because of these permutations. Lets do them one by one. Lets first do the case where Q is identity. That is, map everybody to themselves. Actually, let me write down the integrations first. will do the integrations over all pairs of coordinates of these Gaussians. These Gaussians will evaluate for different permutations. Lets look at the case where Q is identity. When Q is identity, essentially will put all of the x prime to be the same as x. t is like what did here for two particles and got 1. do the same thing for more than one particle. will still get 1. Then, will do the same thing that did over here. Here, the next term that did was to exchange 1 and 2. o this became x1 minus x2. ll do the same thing here. look at the case where Q corresponds to exchange of particles 1 and 2. And then that will give me a factor which is e to the minus pi over lambda squared x1 minus x2 squared. There are two of these making together 2 pi over lambda squared, which hope had there, too. But then there was a whole bunch of other terms that can do. can exchange, lets say, 7 and 9. And then will get here 2 pi over lambda squared x7 minus x9 squared. And theres a whole bunch of such exchanges that can make in which just switch between two particles in this whole story. And clearly, the number of exchanges that can make is the number of pairs, N N minus 1 over 2. Once am done with all of the exchanges, then have to go to the next thing that doesnt have an analog here for two particles. But if take three particles, can permute them like a triangle. o presumably there would be next set of terms, which is a permutation that is like 1, 2, 3, 2, 3, 1. Theres a bunch of things that involve two permutations, four permutations, and so forth. o there is a whole list of things that would go to here where these twoparticle exchanges are the simplest class. Now, as we shall see, there is a systematic way of looking at things where the twoparticle exchanges are the first correction due to quantum effects. Threeparticle exchanges would be higherorder corrections. And we can systematically do them in order. o lets see what happens if we compare the case where there is no exchange and the case where there is one exchange. When there is no exchange, am essentially integrating over each position over the volume. o what would get is V raised to the power of N. The next term? Well, have to do the integrations. The integrations over x3, x4, x5, all the way to x to the N, there is no factors. o they will give me factors of V. And there are N minus 2 of them. And then have to do the integration over x1 and x2 of this factor, but its only a function of the relative coordinate. o there is one other integration that can trivially do, which is the center of mass gives me a factor of V. And then am left with the integral over the relative coordinate of e to the minus 2 pi r squared over lambda squared. And forgot its very important. This will carry a factor of eta because any exchange is odd. And so there will be a factor of eta here. And said that would get the same expression for any of my N N minus 1 over 2 exchanges. o the result of all of these exchange calculations would be the same thing. And then there would be the contribution from threebody exchange and so forth. o lets reorganize this. can pull out the factor of V to the N outside. o would have V over lambda cubed to the power of N. o the first term is 1. The next term has the parity factor that distinguishes bosons and fermions, goes with a multiplicity of pairs which is N N minus 1 over 2. ince already pulled out a factor of V to the N and really had V to the N minus 1 here, better put a factor of 1 over V here. And then just am left with having to evaluate these Gaussian integrals. Each Gaussian integral will give me 2 pi times the variance, which is lambda squared divided by 2 pi. And then theres actually a factor of 2. And there are three of them, so will have 3/2. o what get here is lambda cubed divided by 2 to the 3/2. Now, you can see that any time go further in this series of exchanges, will have more of these Gaussian factors. And whenever have a Gaussian factor, have an additional integration to do that has an x minus something squared in it. will lose a factor of V. dont have that factor of V. And so subsequent terms will be even smaller in powers of V. And presumably, compensated by corresponding factors of lambda squared lambda cubed. Now, first thing to note is that in the very, very high temperature limit, lambda goes to 0. o can forget even this correction. What do get? get 1 over N factorial V over lambda cubed to the power of N. Remember that many, many lectures back we introduced by hand the factor of 1 over N factorial for measuring phase spaces of identical particles. And promise to you that we would get it when we did identical particles in quantum mechanics, so here it is. o automatically, we did the calculation, keeping track of identity of particles at the level of quantum states. Went through the calculation and in the high temperature limit, we get this 1 over N factorial emerging. econdly, we see that the corrections to ideal gas behavior emerge as a series in powers of lambda cubed over V. And for example, if were to take the log of the partition function, would get log of what would have had classically, which is this V over lambda cubed to the power of N divided by N factorial. And then the log of this expression. And this m going to replace with N squared. There is not that much difference. And since m regarding this as a correction, log of 1 plus something, will replace with the something eta N squared 2V lambda cubed 2 to the 3/2 plus higher order. What does this mean? Once we have the partition function, we can calculate pressure. Reminding you that beta p was the log Z by dV. The first part is the ideal gas that we had looked at classically. o once go to the appropriate largeend limit of this, what this gives me is the density n over V. And then when look at the derivative here, the derivative of 1/V will give me a minus 1 over V squared. o will get minus eta. N over V, the whole thing squared. o will have n squared lambda cubed 2 to the 5/2, and so forth. o see that the pressure of this ideal gas with no interactions is already different from the classical result that we had calculated by a factor that actually reflects the statistics. For fermions eta of minus 1, you get an additional pressure because of the kind of repulsion that we have over here. Whereas, for bosons you get an attraction. You can see that also the thing that determines this so basically, this corresponds to a second Virial coefficient, which is minus eta lambda cubed 2 to the 5/2, is the volume of these wave packets. o essentially, the corrections are of the order of n lambda cubed that is within one of these wave packets how many particles you will encounter. As you go to high temperature, the wave packets shrink. As you go to low temperature, the wave packets expand. f you like, the interactions become more important and you get corrections to ideal gas wave. ADENE: You assume that we can use perturbation, but the higher terms actually had a factor . And you cant really use perturbation in that. PROFEOR: OK. o what you are worried about is the story here, that took log of 1 plus something here and m interested in the limit of n going to infinity, that finite density n over V. o already in that limit, you would say that this factor really is overwhelmingly larger than that. And as you say, the next factor will be even larger. o what is the justification in all of this? We have already encountered this same problem when we were doing these perturbations due to interactions. And the answer is that what you really want to ensure is that not log Z, but Z has a form that is e to the N something. And that something will have corrections, potentially that are powers of N, the density, which is N over V. And if you try to force it into a perturbation series such as this, naturally things like this happen. What does that really mean? That really means that the correct thing that you should be expanding is, indeed, log Z. f you were to do the kind of handwaving that did here and do the expansion for Z, if you also try to do it over here you will generate terms that look kind of at the wrong order. But higher order terms that you would get would naturally conspire so that when you evaluate log Z, they come out right. You have to do this correctly. And once you have done it correctly, then you can rely on the calculation that you did before as an example. And we did it correctly when we were doing these cluster expansions and the corresponding calculation we did for Q. We saw how the different diagrams were appearing in both Q and the log Q, and how they could be summed over in log Q. But indeed, this mathematically looks awkward and kind of jumped a step in writing log of 1 plus something that is huge as if it was a small number. All right. o we have a problem. We want to calculate the simplest system, which is the ideal gas. o classically, we did all of our calculations first for the ideal gas. We had exact results. Then, lets say we had interactions. We did perturbations around that and all of that. And we saw that having to do things for interacting systems is very difficult. Now, when we start to do calculations for the quantum problem, at least in the way that set it up for you, it seems that quantum problems are inherently interacting problems. showed you that even at the level of two particles, it is like having an interaction between bosons and fermions. For three particles, it becomes even worse because its not only the twoparticle interaction. Because of the threeparticle exchanges, you would get an additional threeparticle interaction, fourparticle interaction, all of these things emerge. o really, if you want to look at this from the perspective of a partition function, we already see that the exchange term involved having to do a calculation that is equivalent to calculating the second Virial coefficient for an interacting system. The next one, for the third Virial coefficient, would need to look at the threebody exchanges, kind of like the point clusters, fourpoint clusters, all kinds of other things are there. o is there any hope? And the answer is that it is all a matter of perspective. And somehow it is true that these particles in quantum mechanics because of the statistics are subject to all kinds of complicated interactions. But also, the underlying Hamiltonian is simple and noninteracting. We can enumerate all of the wave functions. Everything is simple. o by looking at things in the right basis, we should be able to calculate everything that we need. o here, was kind of looking at calculating the partition function in the coordinate basis, which is the worst case scenario because the Hamiltonian is diagonal in the momentum basis. o lets calculate ZN trace of e to the minus beta H in the basis in which H is diagonal. o what are the eigenvalues and eigenfunctions? Well, the eigenfunctions are the symmetrized/antisymmetrized quantities. The eigenvalues are simply e to the minus beta H bar squared k alpha squared over 2m. o this is basically the thing that could write as the set of ks appropriately symmetrized or antisymmetrized e to the minus beta sum over alpha H bar squared k alpha squared over 2m k eta. Actually, m going to rather than go through this procedure that we have up there in which wrote these, what need to do here is a sum over all k in order to evaluate the trace. o this is inherently a sum over all sets of ks. But this sum is restricted, just like what had indicated for you before. Rather than trying to do it that way, note that these ks could also write in terms of these occupation numbers. o equivalently, my basis would be the set of occupation numbers times the energy. The energy is then e to the minus beta sum over k epsilon k nk, where epsilon k is this beta H bar squared k alpha squared over 2m. But could do this in principle for any epsilon k that have over here. o the result that am writing for you is more general. Then sandwich it again, since m calculating the trace, with the same state. Now, the states have this restriction that have over there. That is, for the case of fermions, my nk can be 0 or 1. But there is no restriction for nk on the bosons. Except, of course, that there is this overall restriction that the sum over k nk has to be N because am looking at Nparticle states. Actually, can remove this because in this basis, e to the minus beta H is diagonal. o can, basically, remove these entities. And m just summing a bunch of exponentials. o that is good because should be able to do for each nk a sum of e to something nk. Well, the problem is this that cant sum over each nk independently. Essentially in the picture that have over here, have some n1 here. have some n2 here. ome n3 here, which are the occupation numbers of these things. And for that partition function, have to do the sum of these exponentials e to the minus epsilon 1 n1, e to the minus epsilon 2 n2. But the sum of all of these ns is kind of maxed out by N. cannot independently sum over them going over the entire range. But weve seen previously how those constraints can be removed in statistical mechanics. o our usual trick. We go to the ensemble in which n can take any value. o we go to the grand canonical prescription. We remove this constraint on n by evaluating a grand partition function Q, which is a sum over all N of e to the beta mu N ZN. o we do, essentially, a Laplace transform. We exchange our n with the chemical potential mu. Then, this constraint no longer we need to worry about. o now can sum over all of the nks without worrying about any constraint, provided that multiply with e to the beta mu n, which is a sum over k nk. And then, the factor that have here, which is e to the minus beta sum over k epsilon of k nk. o essentially for each k, can independently sum over its nk of e to the beta mu minus epsilon of k nk. Now, the symmetry issues remain. This answer still depends on whether or not am calculating things for bosons or fermions because these sums are differently constrained whether m dealing with fermions. n which case, nk is only 0 or 1. Or bosons, in which case there is no constraint. o what do get? For the case of fermions, have a Q minus, which is product over all k. And for each k, the nk takes either 0 or 1. o if it takes 0, will write e to the 0, which is 1. Or it takes 1. t is e to the beta mu minus epsilon of k. For the case of bosons, have a Q plus. Q plus is, again, a product over Q. n this case, nk going from 0 to infinity, am summing a geometric series that starts as 1, and then the subsequent terms are smaller by a factor of beta mu minus epsilon of k. Actually, for future reference note that would be able to do this geometric sum provided that this combination beta mu minus epsilon of k is negative. o that the subsequent terms in this series are decaying. Typically, we would be interested in things like partition functions, grand partition functions. o we have something like log of Q, which would be a sum over k. And would have either the log of this quantity or the log of this quantity with a minus sign. can combine the two results together by putting a factor of minus eta because in taking the log, over here for the bosons would pick a factor of minus 1 because the thing is in the denominator. And then would write the log of 1. And then have in both cases, a factor which is e to the beta mu minus epsilon of k. But occurring with different signs for the bosons and fermions, which again can combine into a single expression by putting a minus eta here. o this is a general result for any Hamiltonian that has the characteristic that we wrote over here. o this does not have to be particles in a box. t could be particles in a harmonic oscillator. These could be energy levels of a harmonic oscillator. All you need to do is to make the appropriate sum over the oneparticle levels harmonic oscillator, or whatever else you have, of these factors that depend on the individual energy levels of the oneparticle system. Now, one of the things that we will encounter having made this transition from canonical, where we knew how many particles we had, to grand canonical, where we only know the chemical potential, is that we would ultimately want to express things in terms of the number of particles. o it makes sense to calculate how many particles you have given that you have fixed the chemical potential. o for that we note the following. That essentially, we were able to do this calculation for Q because it was a product of contributions that we had for the individual oneparticle states. o clearly, as far as this normalization is concerned, the individual oneparticle states are independent. And indeed, what we can say is that in this ensemble, there is a classical probability for a set of occupation numbers of one particle states, which is simply a product over the different oneparticle states of e to the beta mu minus epsilon k nk appropriately normalized. And again, the restriction on ns being 0 or 1 for fermions or anything for bosons would be implicit in either case. But in either case, essentially the occupation numbers are independently taken from distributions that ve discussed . o you can, in fact, independently calculate the average occupation number that you have for each one of these singleparticle states. And its clear that you could get that by, for example, bringing down a factor of nk here. And you can bring down a factor of nk by taking a derivative of Q with respect to beta epsilon k with a minus sign and normalizing it, so you would have log. o you would have an expression such as this. o you basically would need to calculate, since you are taking derivative with respect to epsilon k the corresponding log for which epsilon k appears. Actually, for the case of fermions, really there are two possibilities. n is either 0 or 1. o you would say that the expectation value would be when it is 1, you have e to the beta epsilon of k minus mu. Oops. e to the beta mu minus epsilon of k. The two possibilities are 1 plus e to the beta mu minus epsilon of k. o when look at some particular state, it is either empty. n which case, contributes 0. Or, it is occupied. n which case, it contributes this weight, which has to be appropriately normalized. f do the same thing for the case of bosons, it is a bit more complicated because have to look at this series rather than geometric 1 plus x plus x squared plus x cubed is 1 plus x plus 2x squared plus 3x cubed, which can be obtained by taking the derivative of the appropriate log. Or you can fall back on your calculations of geometric series and convince yourself that it is essentially the same thing with a factor of minus here. o this is fermions and this is bosons. And indeed, can put the two expressions together by dividing through this factor in both of them and write it as 1 over Z inverse e to the beta epsilon of k minus eta, where for convenience have introduced Z to be the contribution e to the beta. o for this system of noninteracting particles that are identical, we have expressions for log of the grand partition function, the grand potential. And for the average number of particles, which is an appropriate derivative of this, expressed in terms of the singleparticle energy levels and the chemical potential. o next time, what we will do is we will start this with this expression for the case of the particles in a box to get the pressure of the ideal quantum gas as a function of mu. But we want to write the pressure as a function of density, so we will invert this expression to get density as a function of chemical potential as a function of density here. And therefore, get the expression for pressure as a function of density.","And then, the factor that have here, which is e to the minus beta sum over k epsilon of k nk. Well, basically, that would come from noting that the trace of rho has to be 1. o ZN is trace of e to the minus beta H. And essentially, can take this ZN to the other side and evaluate this as x e to the minus beta H x. That is, can calculate the diagonal elements of this matrix that have calculated that have over there. econdly, we see that the corrections to ideal gas behavior emerge as a series in powers of lambda cubed over V. And for example, if were to take the log of the partition function, would get log of what would have had classically, which is this V over lambda cubed to the power of N divided by N factorial. And this would have been minus to the P. For the case of bosons, we basically dispensed with this factor of minus 1 to the P. o we had a Pk. o next time, what we will do is we will start this with this expression for the case of the particles in a box to get the pressure of the ideal quantum gas as a function of mu. And essentially, that would correspond, lets say, in coordinate representation to taking a bunch of xs and the corresponding ks and having a wave function of the form e to the i k alp sum over alpha k alpha x alpha, and then divided by V to the N over 2. o in this procedure, what did we do? And will have only one permutation left, Q. And what will appear here would be the parity of this Q that is the combination, or if you like, the relative of these two permutations. o there is one other integration that can trivially do, which is the center of mass gives me a factor of V. And then am left with the integral over the relative coordinate of e to the minus 2 pi r squared over lambda squared. When Q is identity, essentially will put all of the x prime to be the same as x. t is like what did here for two particles and got 1. do the same thing for more than one particle. And indeed, what we can say is that in this ensemble, there is a classical probability for a set of occupation numbers of one particle states, which is simply a product over the different oneparticle states of e to the beta mu minus epsilon k nk appropriately normalized. o for the case of the particle in the box, the wave functions and coordinate space that we had x, k were of the form e to the i k dot x divided by square root of V. We allowed the energies where h bar squared k squared over 2m. o m evaluating this trace in coordinate basis, which means that should put x and x prime to be the same for the trace, and then have to sum or integrate over all possible values of x. o lets do this. That really means that the correct thing that you should be expanding is, indeed, log Z. f you were to do the kind of handwaving that did here and do the expansion for Z, if you also try to do it over here you will generate terms that look kind of at the wrong order. And indeed, can put the two expressions together by dividing through this factor in both of them and write it as 1 over Z inverse e to the beta epsilon of k minus eta, where for convenience have introduced Z to be the contribution e to the beta. For the case of fermions, have a Q minus, which is product over all k. And for each k, the nk takes either 0 or 1. o if it takes 0, will write e to the 0, which is 1. o this is basically the thing that could write as the set of ks appropriately symmetrized or antisymmetrized e to the minus beta sum over alpha H bar squared k alpha squared over 2m k eta. Now, the other thing to note is that once have given you a picture such as this in terms of which oneparticle states want to look at, or which set of occupation numbers have nk, then there is one and only one symmetrized or antisymmetrized state. can combine the two results together by putting a factor of minus eta because in taking the log, over here for the bosons would pick a factor of minus 1 because the thing is in the denominator. And then have a sum over permutations Q eta of Q. The diagonal element is obtained by putting x prime to be the same as x. o have exponential of minus x sum over alpha x alpha minus x of Q alpha. The next term has the parity factor that distinguishes bosons and fermions, goes with a multiplicity of pairs which is N N minus 1 over 2. ince already pulled out a factor of V to the N and really had V to the N minus 1 here, better put a factor of 1 over V here. What we want to do now is to go from one particle to the case of N particles. For fermions eta of minus 1, you get an additional pressure because of the kind of repulsion that we have over here. And then have to do the integration over x1 and x2 of this factor, but its only a function of the relative coordinate. Or it takes 1. t is e to the beta mu minus epsilon of k. For the case of bosons, have a Q plus. o once go to the appropriate largeend limit of this, what this gives me is the density n over V. And then when look at the derivative here, the derivative of 1/V will give me a minus 1 over V squared. have x alpha k p alpha, because permuted this by k. have one of these factors for each V. With each one of them, there is a normalization of square root of V. o the two of them together will give me V. But thats only one of the Nparticle o there are N of them. And then that will give me a factor which is e to the minus pi over lambda squared x1 minus x2 squared. f eta is minus 1, which is for the case of fermions, this is 1 minus something. And as said, this was a good thing because the quantity that had the hardest time for, and comes in the normalizations that occurs here, is this factor of 1 over nk factorial. f want to calculate the partition function for one particle in the box, have to do a trace of e to the minus beta h for one particle. Q plus is, again, a product over Q. n this case, nk going from 0 to infinity, am summing a geometric series that starts as 1, and then the subsequent terms are smaller by a factor of beta mu minus epsilon of k. Actually, for future reference note that would be able to do this geometric sum provided that this combination beta mu minus epsilon of k is negative. You can see that also the thing that determines this so basically, this corresponds to a second Virial coefficient, which is minus eta lambda cubed 2 to the 5/2, is the volume of these wave packets. And have the factor of e to the minus beta h bar squared sum over alpha k alpha squared over 2m divided by ZN. The two Vs here cancel, but Z1 is proportional to V. The lambda cubes cancel and so what we have is 1 over V e to the minus x minus x prime squared pi over lambda squared. And for that partition function, have to do the sum of these exponentials e to the minus epsilon 1 n1, e to the minus epsilon 2 n2. This was the density of states in k. e to the minus beta h bar squared k squared over 2m. n is either 0 or 1. o you would say that the expectation value would be when it is 1, you have e to the beta epsilon of k minus mu. o what you are worried about is the story here, that took log of 1 plus something here and m interested in the limit of n going to infinity, that finite density n over V. o already in that limit, you would say that this factor really is overwhelmingly larger than that. Actually, m going to rather than go through this procedure that we have up there in which wrote these, what need to do here is a sum over all k in order to evaluate the trace. o we have something like log of Q, which would be a sum over k. And would have either the log of this quantity or the log of this quantity with a minus sign. o basically, what you have here is that we have a box of volume V. There is a particle inside at some location x. And the probability to find it at location x is the diagonal element of this entity. Of course, what need to construct whether am dealing with bosons or fermions is that the sum over k nk is the total number of particles that have. o there is an overall factor of 1 over lambda cubed to the power of N. have N factorial. This interaction V of r would be minus kT log of 1 plus eta e to the minus 2 pi r squared over lambda squared. o would have V over lambda cubed to the power of N. o the first term is 1. And the thing that was multiplying k alpha here is the inverse permutation of p. o then can do the integration over k alpha easily. And the answer is that what you really want to ensure is that not log Z, but Z has a form that is e to the N something. n either case well, what need to do is to do a summation over P here for this one and P prime here or P prime here and P here. But then have to remember, as said, that should not overcount distinct set of kvalues because permutations of these list of ks that have over here, because of symmetrization or antisymmetrization, will give me the same state. And then have in both cases, a factor which is e to the beta mu minus epsilon of k. But occurring with different signs for the bosons and fermions, which again can combine into a single expression by putting a minus eta here. The point was that if we were doing this computation for the case of fermions, we could not allow a state where there is a double occupation of one of the oneparticle state. And the two can submerge into one formula by writing a symmetrized or antisymmetrized state, respectively indicated by eta, where we have eta is minus 1 for fermions and eta is plus 1 for bosons, which is 1 over square root of N factorial product over k nk factorials. And it will actually saturate to a value of kT log 2 when r goes to 0. o this is again, eta of plus 1 for the case of bosons. The energy is then e to the minus beta sum over k epsilon k nk, where epsilon k is this beta H bar squared k alpha squared over 2m. Now, one of the things that we will encounter having made this transition from canonical, where we knew how many particles we had, to grand canonical, where we only know the chemical potential, is that we would ultimately want to express things in terms of the number of particles. o do a sum over k of e to the minus beta h bar squared k squared over 2m. Well, first of all, if didnt have this, if just was doing the integration of e to the minus beta h bar squared k squared over 2m, did that already. Once am done with all of the exchanges, then have to go to the next thing that doesnt have an analog here for two particles. Here, it is easy to see that one of these sums can very easily do because it is just repetition of all of the results that have previously. And we did it correctly when we were doing these cluster expansions and the corresponding calculation we did for Q. We saw how the different diagrams were appearing in both Q and the log Q, and how they could be summed over in log Q. But indeed, this mathematically looks awkward and kind of jumped a step in writing log of 1 plus something that is huge as if it was a small number. That essentially, we were able to do this calculation for Q because it was a product of contributions that we had for the individual oneparticle states. And in the coordinate representation, essentially what we had to do was to go into the basis in which rho is diagonal. And for the average number of particles, which is an appropriate derivative of this, expressed in terms of the singleparticle energy levels and the chemical potential. There is a factor of 1 over V from the normalization of these things. And that something will have corrections, potentially that are powers of N, the density, which is N over V. And if you try to force it into a perturbation series such as this, naturally things like this happen. That is, for the case of fermions, my nk can be 0 or 1. All you need to do is to make the appropriate sum over the oneparticle levels harmonic oscillator, or whatever else you have, of these factors that depend on the individual energy levels of the oneparticle system. t is simply e to the minus beta, the energy which is sum over alpha h bar squared k alpha squared over 2m. nder exchange of the particles that would correspond to these kvalues, would get the same state back, but the exchange would give me a minus 1 and it would give me 0. o the fermionic wave function that have constructed here, appropriately antisymmetrized, exists only as long as there are no repeats. And then, have a product of these integrations that have to do that are threedimensional Gaussians for each k alpha. But then the other factor will give me eta e to the minus 2 pi over lambda squared x1 minus x2 squared. o this was for the case of fermions. o really, if you want to look at this from the perspective of a partition function, we already see that the exchange term involved having to do a calculation that is equivalent to calculating the second Virial coefficient for an interacting system. o basically, we were focusing on a type of Hamiltonian for a system of N particles, which could be written as the sum of contributions that correspond respectively to particle 1, particle 2, particle N. o essentially, a sum of terms that are all the same. o lets calculate ZN trace of e to the minus beta H in the basis in which H is diagonal.",0.1539203860072376
82,623,"ANNONER: Open content is provided under a creative commons license. Your support will help T OpenourseWare continue to offer highquality educational resources for free. To make a donation, or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu . PROFEOR JOHN GTTAG: All right. That said, lets continue, and if you remember last time, we ended up looking at this thing called square roots bi. This was using something called a bisection method, which is related to something called binary search, which well see lots more of later, to find square roots. And the basic idea was that we had some sort of a line, and we knew the answer was somewhere between this point and this point. The line is totally ordered. And what that means, is that anything here is smaller than anything to its right. o the integers are totally ordered, the reals are totally ordered, lots of things are, the rationals are totally ordered. And that idea was, we make a guess in the middle, we test it so this is kind of a guess and check, and if the answer was too big, then we knew that we should be looking over here. f it was too small, we knew we should be looking over here, and then we would repeat. o this is very similar, this is a kind of recursive thinking we talked about earlier, where we take our problem and we make it smaller, we solve a smaller problem, et cetera. All right. o now, weve got it, ve got the code up for you. want you to notice the specifications to start. Were assuming that x is greater than or equal to 0, and epsilon is strictly greater than 0, and were going to return some value y such that y squared is within epsilon of x. d last time talked about the two assert statements. n some sense, strictly speaking they shouldnt be necessary, because the fact that my specification starts with an assumption, says, hey you, who might call square root, make sure that the things you call me with obey the assumption. On the other hand, as said, never trust a programmer to do the right thing, so were going to check it. And just in case the assumptions are not true, were just going to stop dead in our tracks. All right. Then were going to set low to low and high, and were going to perform exactly the process talked about. And along the way, m keeping track of how many iterations, at the end ll print how many iterations took, before return the final guess. All right, lets test it. o one of the things want you to observe here, is that instead of sitting there and typing away a bunch of test cases, took the trouble to write a function, called test bi in this case. All right, so what thats doing, is its taking the things would normally type, and putting them in a function, which can then call. Why is that better than typing them? Why was it worth creating a function to do this? Pardon? TDENT:: PROFEOR JOHN GTTAG: Then can can use it again and again and again. Exactly. By putting it in a function, if find a bug and change my program, can just run the function again. The beauty of this is, it keeps me from getting lazy, and not only testing my program and the thing that found the bug, but in all the things that used to work. Well talk more about this later, but it often happens that when you change your program to solve one problem, you break it, and things that used to work dont work. And so what you want to do, and again well come back to this later in the term, is something called regression testing. This has nothing to do with linear regression. And thats basically trying to make sure our program has not regressed, as to say, gone backwards in how well it works. And so we always test it on everything. All right? o ve created this function, lets give it a shot and see what happens. Well run test bi. Whoops! All right, well lets look at our answers. first tested it on the square root of 4, and in one iteration it found 2. like that answer. then tested it on the square root of 9, and as mentioned last time, didnt find 3. was not crushed. You know, was not really disappointed, it found something close enough to 3 that m happy. All right. tried it on 2, surely didnt expect a precise and exact answer to that, but got something, and if you square this, youll find the answer kept pretty darn close to 2. then tried it on 0.25 One quarter. And what happened was not what wanted. As youll see, it crashed. t didnt really crash, it found an assert statement. o if you look at the bottom of the function, youll see that, in fact, checked for that. assert the counter is less than or equal to 0. m checking that didnt leave my program because didnt find an answer. Well, this is a good thing, its better than my program running forever, but its a bad thing because dont have it the square root of 0.25. What went wrong here? Well, lets think about it for a second. You look like someone looks like theyre dying to give an answer. No, you just scratching your head? All right. Remember, said when we do a bisection method, were assuming the answer lies somewhere between the lower bound and the upper bound. Well, what is the square root of a quarter? t is a half. Well, what where did tell my program to look for an answer? Between 0 and x. o the problem was, the answer was over here somewhere, and so m never going to find it cleverly searching in this region, right? o the basic idea was fine, but failed to satisfy the initial condition that the answer had to be between the lower bound and the upper bound. Right? And why did do that? Because forgot what happens when you look at fractions. o what should do? Actually lied, by the way, when said the answer was over there. Where was the answer? omebody? t was over here. Because the square root of a quarter is not smaller than a quarter, its bigger than a quarter. Right? A half is strictly greater than a quarter. o it wasnt on the region. o how whats the fix? hould be a pretty simple fix, in fact we should be able to do it on the fly, here. What should change? Do need to change the lower bound? s the square root ever going to be less than 0? Doesnt need to be, so, what should do about the upper bound here? Oh, could cheat and make, OK, the upper bound a half, but that wouldnt be very honest. What would be a good thing to do here? Pardon? could square x, but maybe should just do something pretty simple here. uppose whoops. uppose make it the max of x and 1. Then if m looking for the square root of something less than 1, know it will be in my region, right? All right, lets save this, and run it and see what happens. ure enough, it worked and, did we get we got the right answer, 0.5 All right? And by the way, checked all of my previous ones, and they work too. All right. Any questions about bisection search? One of the things want you to notice here is the number iterations is certainly not constant. Yeah, when will looked at 4, it was a nice number like 1, 9 looked like it took me 18, 2 took me 14, if we try some big numbers it might take even longer. These numbers are small, but sometimes when we look at really harder problems, we got ourselves in a position where we do care about the number of iterations, and we care about something called the speed of convergence. Bisection methods were known to the ancient Greeks, and it is believed by many, even to the Babylonians. And as mentioned last time, this was the state of the art until the 17th century. At which point, things got better. o, lets think about it, and lets think about what were actually doing when we solve this. When we look for something like the square root of x, what were really doing, is solving an equation. Were looking at the equation f of guess equals the guess squared minus x. Right, thats what that is equal to, and were trying to solve the equation that f of guess equals 0. Looking for the root of this equation. o if we looked at it pictorially, what weve got here is, were looking at f of x, ve plotted it here, and were asking where it crosses the x axis. orry for the overloading of the word x. And m looking here at 16. quare root of 16, and my plot basically shows it crosses at 4 and well, think thats minus 4. The perspective is tricky and so were trying to find the roots. Now saac Newton and/or Joseph Raphson figured out how to do this kind of thing for all differentiable functions. Dont worry about what that means. The basic idea is, you take a guess, and you whoops and you find the tangent of that guess. o lets say guessed 3. look for the tangent of the curve at 3. All right, so ve got the tangent, and then my next guess is going to be where the tangent crosses the x axis. o instead of dividing it in half, m using a different method to find the next guess. The utility of this relies upon the observation that, most of the time and want to emphasize this, most of the time, that implies not all of the time the tangent line is a good approximation to the curve for values near the solution. And therefore, the x intercept of the tangent will be closer to the right answer than the current guess. s that always true, by the way? how me a place where thats not true, where the tangent line will be really bad. Yeah. uppose choose it right down there, guess 0. Well, the tangent there will not even have an x intercept. o m really going to be dead in the water. This is the sort of thing that people who do numerical programming worry about all the time. And there are a lot of a little tricks they use to deal with that, theyll perturb it a little bit, things like that. You should not, at this point, be worrying about those things. This method, interestingly enough, is actually the method used in most hand calculators. o if youve got a calculator that has a square root button, its actually in the calculator running Newtons method. Now know you thought it was going to do that thing you learned in high school for finding square roots, which never could quite understand, but no. t uses Newtons method to do it. o how do we find the intercept of the tangent, the x intercept? Well this is where derivatives come in. What we know is that the slope of the tangent is given by the first derivative of the function f at the point of the guess. o the slope of the guess is the first derivative. Right. Which dy over dx. hange in y divided by change in x. o we can use some algebra, which wont go through here, and what we would find is that for square root, the derivative, written f prime of the ith guess is equal to two times the ith guess. Well, should have left myself a little more room, sorry about that. All right? You could work this out. Right? The derivative of the square root is not a complicated thing. Therefore, and heres the key thing we need to keep in mind, well know that we can choose guess i plus 1 to be equal to the old guess, guess i, minus whatever the value is of the new guess of the old rather, the old guess divided by twice the old guess. All right, again this is straightforward kind of algebraic manipulations to get here. o lets look at an example. uppose we start looking for the square root of 16 with the guess 3. Whats the value of the function f of 3? Well, its going to be, we looked at our function there, guess squared, 3 times 3 is 9 think, minus 16, thats what x is in this case, which equals minus 7. That being the case, whats my next guess? Well start with my old guess, 3, minus f of my old guess, which is minus 7, divided by twice my old guess, which is 6, minus the minus, and get as my new guess 4.1666 or thereabouts. o you can see ve missed, but am closer. And then would reiterate this process using that as guess i, and do it again. One way to think about this intuitively, if the derivative is very large, the function is changing quickly, and therefore we want to take small steps. All right. f the derivative is small, its not changing, maybe want to take a larger step, but lets not worry about that, all right? Does this method work all the time? Well, we already saw no, if my initial guess is zero, dont get anywhere. n fact, my program crashes because end up trying to divide by zero, a really bad thing. Hint: if you implement Newtons method, do not make your first guess zero. All right, so lets look at the code for that. All right so yeah, how do get to the code for that? Thats interesting. All right. o we have that square root NR. NR for Newton Raphson. First thing want you to observe is its specification is identical to the specification of square root bi. What thats telling me is that if youre a user of this, you dont care how its implemented, you care what it does. And therefore, its fine that the specifications are identical, in fact its a good thing, so that means if someday Professor Grimson invents something thats better than Newton Raphson, we can all reimplement our square root functions and none of the programs that use it will have to change, as long as the specification is the same. All right, so, not much to see about this. As said, the specifications is the same, same assertions, and the its basically the same program as the one we were just looking at, but m starting with a different guess, in this case x over 2, well m going to, couple of different guesses we can start with, we can experiment with different guesses and see whether we get the same answer, and in fact, if we did, we would see we didnt get this, we got different answers, but correct answers. Actually now, well just comment that out. m going to compute the difference, just as did on the board, and off well go. All right. Now, lets try and compare these things. And what were going to look at is another procedure, you have the code for these things on your handout so we wont worry, dont need to show you the code, but lets look at how were going to test it. m doing a little trick by the way, m using raw input in my function here, as a just a way to stop the display. This way can torture you between tests by asking you questions. aking it stop. All right, so, well try some things. Well see what it does. tarting with that, well, lets look at some of the things it will do. Yeah, ll save it.. ts a little bit annoying, but it makes the font bigger. All right, so weve tested it, and we havent tested it yet, we have tested it but, we havent seen it, well, you know what m going to do? m going to tort m going to make the font smaller so we can see more. orry about this. Those of you in the back, feel free to move forward. All right. o weve got it, now lets test it. o were going to do here, were going to run compare methods. Well were seeing this famous computers are no damn good. All right. o were going to try it on 2, and at least well notice for 2, that the bisection method took eight iterations, the Newton Raphson only took three, so it was more efficient. They came up with slightly different answers, but both answers are within .01 which is what gave it here for epsilon, so were OK. o even though they have different answers, they both satisfy the same specification, so we have no problem. All right? Try it again, just for fun. gave it here a different epsilon, and youll note, we get different answers. Again, thats OK. Notice here, when asked for a more precise answer, bisection took a lot more iterations, but Newton Raphson took only one extra iteration to get that extra precision in the answer. o were sort of getting the notion that Newton Raphson maybe is considerably better on harder problems. Which, by the way, it is. Well make it an even harder problem, by making it looking an even smaller epsilon, and again, what youll see is, Newton Raphson just crept up by one, didnt take it long, and got the better answer, where bisection gets worse and worse. o as you can see, as we escalate the problem difficulty, the difference between the good method and the not quite as good method gets bigger and bigger and bigger. Thats an important observation, and as we get to the part of the course, we talk about computational complexity, youll see that what we really care about is not how efficient the program is on easy problems, but how efficient it is on hard problems. All right. Look at another example. All right, here gave it a big number, 123456789. And again, dont want to bore you, but you can see whats going on here with this trend. o heres an interesting question. You may notice that its always printing out the same number of digits. Why should this be? f you look at it here, whats going on? omething very peculiar is happening here. Were looking at it, and were getting some funny answers. This gets back to what talked about before, about some of the precision of floating point numbers. And the thing m trying to drive home to you here is perhaps the most important lesson well talk about all semester. Which is, answers can be wrong. People tend to think, because the computer says its so, it must be so. That the computer is speaks for God. And therefore its infallible. aybe it speaks for the Pope. t speaks for something thats infallible. But in fact, it is not. And so, something find myself repeating over and over again to myself, to my graduate students, is, when you get an answer from the computer, always ask yourself, why do believe it? Do think its the right answer? Because it isnt necessarily. o if we look at what weve got here, weve got something rather peculiar, right? Whats peculiar about what this computer is now printing for us? Why should be really suspicious about what see in the screen here? TDENT: PROFEOR JOHN GTTAG: Well, not only is it different, its really different, right? f it were just a little bit different, could say, all right, have a different approximation. But when its this different, something is wrong. Right? Well, later in the term when we get to more detailed numerical things, look at whats wrong. You can run into issues of things like overflow, underflow, with floating point numbers, and when you see a whole bunches of ones, its particularly a good time to be suspicious. Anyway the only point m making here is, paranoia is a healthy human trait. All right. We can look at some other things which will work better. And well now move on. OK. o weve looked at how to solve square root weve, looked at two problems, ve tried to instill in you this sense of paranoia which is so valuable, and now were going to pull back and return to something much simpler than numbers, and thats Python. All right? Numbers are hard. Thats why we teach whole semesters worth of courses in number theory. Python its easy, which is why we do it in about four weeks. All right. want to return to some nonscalar types. o weve been looking, the last couple of lectures, at floating point numbers and integers. Weve looked so far really at two nonscalar types. And those were tuples written with parentheses, and strings. The key thing about both of them is that they were immutable. And responded to at least one email about this issue, someone quite correctly said tuple are immutable, how can change one? y answer is, you cant change one, but you can create a new one that is almost like the old one but different in a little bit. Well now were going to talk about some mutable types. Things you can change. And were going to start with one that you, many of you, have already bumped into, perhaps by accident, which are lists. Lists differ from strings in two ways; one way is that its mutable, the other way is that the values need not be characters. They can be numbers, they can be characters, they can be strings, they can even be other lists. o lets look at some examples here. What well do, is well work on two boards at once. o could write a statement like, techs, a variable, is equal to the list, written with the square brace, not a parenthesis, T, al Tech, closed brace. What that basically does, is it takes the variable techs, and it now makes it point to a list with two items in it. One is the string T and one is the string al Tech. o lets look at it. And well now run another little test program, show lists, and printed it, and it prints the list T, al Tech. Now suppose introduce a new variable, well call it ivys, and we say that is equal to the list Harvard, Yale, Brown. Three of the ivy league colleges. What that does is, have a new variable, ivys, and its now pointing to another, what we call object, in Python and Java, and many other languages, think of these things that are sitting there in memory somewhere as objects. And wont write it all out, ll just write its got Harvard as one in it, and then its got Yale, and then its got Brown. And can now print ivys. And it sure enough prints what we expected it to print. Now, lets say have univs, for universities, equals the empty list. That would create something over here called univs, another variable, and it will point to the list, an object that contains nothing in it. This is not the same as none. ts it does have a value, it just happens to be the list that has nothing in it. And the next thing m going to write is univs dot append tex. What is this going to do? ts going to take this list and add to it something else. Lets look at the code. m going to print it, and lets see what it prints. ts kind of interesting. Whoops. Why did it do that? Thats not what expected. ts going to print that. The reason it printed that is accidentally had my finger on the control key, which said print the last thing you had. Why does it start with square braced square brace? take it yes, go ahead. TDENT: o youre adding a list to a list? PROFEOR JOHN GTTAG: o m adding a list to a list. What have what ve appended to the empty list is not the elements T and al Tech but the list that contains those elements. o ve appended this whole object. ince that object is itself a list, what get is a list of lists. Now should mention this notation here append is what is in Python called a method. Now well hear lots more about methods when we get to classes and inheritance, but really, a method is just a fancy word for a function with different syntax. Think of this as a function that takes two arguments, the first of which is univs and the second of which is techs. And this is just a different syntax for writing that function call. Later in the term, well see why we have this syntax and why it wasnt just a totally arbitrary braindead decision by the designers of Python, and many languages before Python, but in fact is a pretty sensible thing. But for now, think of this as just another way to write a function call. All right, people with me so far? Now lets say we wanted as the next thing well do, is were going to append the ivys to univ. tick another list on it. All right. o well do that, and now we get T, al Tech, followed by that list followed by the list Harvard, Yale, Brown. o now we have a list containing two lists. What are we going to try next? Well just to see what we know what were doing, lets look at this code here. ve written a little for loop, which is going to iterate over all of the elements in the list. o remember, before we wrote things like for i in range 10, which iterated over a list or tuple of numbers, here you can iterate over any list, and so were going to just going to take the list called univs and iterate over it. o the first thing well do is, well print the element, in this case it will be a list, right? Because its a list with two lists in it. Then the next thing in the loop, were going to enter a nested loop, and say for every college in the list e, were going to print the name of the college. o now if we look what we get do you not want to try and execute that? itll first print the list containing T and al Tech, and then separately the strings T and al Tech, and then the list containing Harvard, Yale, and Brown, and then the strings Harvard, Yale, and Brown. o were beginning to see this is a pretty powerful notion, these lists, and that we can do a lot of interesting things with them. uppose dont want all of this structure, and want to do whats called flattening the list. Well can do that by, instead of using the method append, use the concatenation operator. o can concatenate techs plus ivys and assign that result to univs, and then when print it youll notice just get a list of five strings. o plus and append do very different things. Append sticks the list on the end of the list, append flattens it, one level of course. f had lists of lists of lists, then it would only take out the first level of it. OK, very quiet here. Any questions about any of this? All right. Because were about to get to the hard part igh. All right. Lets look at the well, suppose want to, quite understandably, eliminate Harvard. All right, then get down here, where m going to remove it. o this is again another method, this is remove, takes two arguments, the first is ivys, the second is the string Harvard. ts going to search through the list until the first time it finds Harvard and then its going to yank it away. o what happened here? Did jump to the wrong place? TDENT: You hit two returns. PROFEOR JOHN GTTAG: hit two returns. Pardon? TDENT: You hit two returns. One was at TDENT: Pardo PROFEOR JOHN GTTAG: This one. TDENT: No, up one. PROFEOR JOHN GTTAG: p one. TDENT: Right. PROFEOR JOHN GTTAG: But why is Harvard there? TDENT: m sorry, didnt write it down. PROFEOR JOHN GTTAG: Lets look at it again. All right, its time to interrupt the world, and well just type into the shell. Lets see what we get here. All right, so lets just see what we got, well print univs. Nope, not defined. All right, well lets do a list equals, and well put some interesting things in it, well put a number in it, because we can put a number, well put T in it, because we can put strings, well put another number in it, 3.3, because we can put floating points, we can put all sorts of things in this list. We can put a list in the list again, as weve seen before. o lets put the list containing the string a, and ll print out, so now we see something pretty interesting about a list, that we can mix up all sorts of things in it, and thats OK. Youll notice have the string with the number 1, a string with T, and then it just a plain old number, not a string, again it didnt quite give me 3.3 for reasons weve talked before, and now it in the list a. o, suppose want to remove something. What should we try and remove from this list? Anybody want to vote? Pardon? All right, someone wants to remove T. ad but true. Now what do we get if we print l? T is gone. How do talk about the different pieces of l? Well can do this. l sub 0 whoops will give me the first element of the list, just as we could do with strings, and can look at l sub minus 1 to get the last element of the list, so can do all the strings, all the things that could do with strings. ts extremely powerful, but what we havent seen yet is mutation. Well, we have seen mutation, right? Because notice that what remove did, it was it actually changed the list. Didnt create a new list. The old l is still there, but its different than it used to be. o this is very different from what we did with slicing, where we got a new copy of something. Here we took the old one and we just changed it. On Thursday, well look at why that allows you to do lots of things more conveniently than you can do without mutation.","What we know is that the slope of the tangent is given by the first derivative of the function f at the point of the guess. o lets put the list containing the string a, and ll print out, so now we see something pretty interesting about a list, that we can mix up all sorts of things in it, and thats OK. And what were going to look at is another procedure, you have the code for these things on your handout so we wont worry, dont need to show you the code, but lets look at how were going to test it. o the first thing well do is, well print the element, in this case it will be a list, right? And that idea was, we make a guess in the middle, we test it so this is kind of a guess and check, and if the answer was too big, then we knew that we should be looking over here. As said, the specifications is the same, same assertions, and the its basically the same program as the one we were just looking at, but m starting with a different guess, in this case x over 2, well m going to, couple of different guesses we can start with, we can experiment with different guesses and see whether we get the same answer, and in fact, if we did, we would see we didnt get this, we got different answers, but correct answers. l sub 0 whoops will give me the first element of the list, just as we could do with strings, and can look at l sub minus 1 to get the last element of the list, so can do all the strings, all the things that could do with strings. All right, so lets look at the code for that. The beauty of this is, it keeps me from getting lazy, and not only testing my program and the thing that found the bug, but in all the things that used to work. All right so yeah, how do get to the code for that? And therefore, its fine that the specifications are identical, in fact its a good thing, so that means if someday Professor Grimson invents something thats better than Newton Raphson, we can all reimplement our square root functions and none of the programs that use it will have to change, as long as the specification is the same. What is this going to do? All right, so ve got the tangent, and then my next guess is going to be where the tangent crosses the x axis. The utility of this relies upon the observation that, most of the time and want to emphasize this, most of the time, that implies not all of the time the tangent line is a good approximation to the curve for values near the solution. All right, lets save this, and run it and see what happens. Thats an important observation, and as we get to the part of the course, we talk about computational complexity, youll see that what we really care about is not how efficient the program is on easy problems, but how efficient it is on hard problems. Were looking at the equation f of guess equals the guess squared minus x. Right, thats what that is equal to, and were trying to solve the equation that f of guess equals 0. o now if we look what we get do you not want to try and execute that? Now lets say we wanted as the next thing well do, is were going to append the ivys to univ. tarting with that, well, lets look at some of the things it will do. What that basically does, is it takes the variable techs, and it now makes it point to a list with two items in it. And so what you want to do, and again well come back to this later in the term, is something called regression testing. Well just to see what we know what were doing, lets look at this code here. ure enough, it worked and, did we get we got the right answer, 0.5 All right? All right, so lets just see what we got, well print univs. And therefore, the x intercept of the tangent will be closer to the right answer than the current guess. o if we looked at it pictorially, what weve got here is, were looking at f of x, ve plotted it here, and were asking where it crosses the x axis. hange in y divided by change in x. o we can use some algebra, which wont go through here, and what we would find is that for square root, the derivative, written f prime of the ith guess is equal to two times the ith guess. All right? All right? All right? All right? All right. All right. All right. All right. All right. All right. All right. All right. All right. All right. All right. All right. All right. All right. All right. All right. Between 0 and x. o the problem was, the answer was over here somewhere, and so m never going to find it cleverly searching in this region, right? The basic idea is, you take a guess, and you whoops and you find the tangent of that guess. All right, so weve tested it, and we havent tested it yet, we have tested it but, we havent seen it, well, you know what m going to do? m going to print it, and lets see what it prints. All right, so, not much to see about this. All right, lets test it. ve written a little for loop, which is going to iterate over all of the elements in the list. o weve looked at how to solve square root weve, looked at two problems, ve tried to instill in you this sense of paranoia which is so valuable, and now were going to pull back and return to something much simpler than numbers, and thats Python. o remember, before we wrote things like for i in range 10, which iterated over a list or tuple of numbers, here you can iterate over any list, and so were going to just going to take the list called univs and iterate over it. Think of this as a function that takes two arguments, the first of which is univs and the second of which is techs. Therefore, and heres the key thing we need to keep in mind, well know that we can choose guess i plus 1 to be equal to the old guess, guess i, minus whatever the value is of the new guess of the old rather, the old guess divided by twice the old guess. One of the things want you to notice here is the number iterations is certainly not constant. o one of the things want you to observe here, is that instead of sitting there and typing away a bunch of test cases, took the trouble to write a function, called test bi in this case. o if you look at the bottom of the function, youll see that, in fact, checked for that. All right, then get down here, where m going to remove it. o were going to try it on 2, and at least well notice for 2, that the bisection method took eight iterations, the Newton Raphson only took three, so it was more efficient. Youll notice have the string with the number 1, a string with T, and then it just a plain old number, not a string, again it didnt quite give me 3.3 for reasons weve talked before, and now it in the list a. o, suppose want to remove something. Well, what is the square root of a quarter? first tested it on the square root of 4, and in one iteration it found 2. like that answer. ts it does have a value, it just happens to be the list that has nothing in it. Then if m looking for the square root of something less than 1, know it will be in my region, right? That would create something over here called univs, another variable, and it will point to the list, an object that contains nothing in it. All right, so what thats doing, is its taking the things would normally type, and putting them in a function, which can then call. On the other hand, as said, never trust a programmer to do the right thing, so were going to check it. Well talk more about this later, but it often happens that when you change your program to solve one problem, you break it, and things that used to work dont work. What have what ve appended to the empty list is not the elements T and al Tech but the list that contains those elements. Then the next thing in the loop, were going to enter a nested loop, and say for every college in the list e, were going to print the name of the college. What that does is, have a new variable, ivys, and its now pointing to another, what we call object, in Python and Java, and many other languages, think of these things that are sitting there in memory somewhere as objects. And the basic idea was that we had some sort of a line, and we knew the answer was somewhere between this point and this point. One way to think about this intuitively, if the derivative is very large, the function is changing quickly, and therefore we want to take small steps. f the derivative is small, its not changing, maybe want to take a larger step, but lets not worry about that, all right?",0.1529041287613715
83,624,"Everything weve been doing in linear algebra so far, you might be thinking, its kind of a more painful way of doing things that you already knew how to do. Youve already dealt with vectors. m guessing that some of you all have already dealt with vectors in your calculus or your precalculus or your physics classes. But in this video hope to show you something that youre going to do in linear algebra that youve never done before, and that it would have been very hard to do had you not been exposed to these videos. Well m going to start with, once again, a different way of doing something you already know how to do. o let me just define some vector here, instead of making them bold, ll just draw it with the arrow on top. m going to define my vector to be can do with the arrow on top or can just make it super bold. m just going to define my vectors, its going to be a vector in R2. Lets just say that my vector is the vector 2, 1. f were to draw it in standard position, it looks like this. You go two to the right, up one, like that. Thats my, right there, that is my vector v. Now, if were to ask you, what are all of the possible vectors can create? o let me define a set. Let me define a set, s, and its equal to all of the vectors can create, if were to multiply v times some constant, so multiply some constant, some scalar, times my vector v, and just to maybe be a little bit formal, ll say such that c is a member of the real numbers. Now what would be a graphical representation of this set? Well, if we draw them all in standard position, c could be any real number. o if were to multiply, c could be 2. f c is 2, let me do it this way. f do 2 times our vector, m going to get the vector 4, 2. Let me draw that in standard position, 4, 2. ts right there. ts this vector right there. ts collinear with this first vector. ts along the same line, but just goes out further 2. Now couldve done another. could have done 1.5 times our vector v. Let me do that in a different color. And maybe that would be, that would be what? Thatd be 1.5 times 2, which is 3, 1.5. Where would that vector get me? d go one 1.5 and then d go 3, and then 1.5, d get right there. And can multiply by anything. can multiply 1.4999 times vector v, and get right over here. could do minus 0.0001 times vector v. Let me write that down. could do 0.001 times our vector v. And where were that put me? t would put me little super small vector right there. f did minus 0.01, it would make a super small vector right there pointing in that direction. f were to do minus 10, would get a vector going in this direction that goes way like that. But you can imagine that if were to plot all of the vectors in standard position, all of them that could be represented by any c in real numbers, ll essentially get ll end up drawing a bunch of vectors where their arrows are all lined up along this line right there, and all lined up in even in negative direction let me make sure draw it properly along that line, like that. think you get the idea. o its a set of collinear vectors. o let me write that down. And if we view these vectors as position vectors, that this vector represents a point in space in R2 this R2 is just our artesian coordinate plane right here in every direction if we view this vector as a position vector let me write that down if we view it as kind of a coordinate in R2, then this set, if we visually represent it as a bunch of position vectors, itll be represented by this whole line over here. And want to make that point clear because its essentially a line, of slope 2. Right? orry, slope 1/2. Your rise is 1. Your rise is 1 for going over 2. But dont want to go back to our Algebra 1 notation too much. But want to make this point that this line of slope 2 that goes through the origin, this is if we draw all of the vectors in the set as in their standard form, or if we draw them all as position vectors. f didnt make that clarification, or that qualification, could have drawn these vectors anywhere. Right? Because this 4, 2 vector, could have drawn over here. And then, to say that its collinear probably wouldnt have made as much visual sense to you. But think this collinearity of it makes more sense to you if you say, oh, lets draw them all in standard form. All of them start at the origin, and then their tails are at the origin, and their heads go essentially to the coordinate they represent. Thats what mean by their position vectors. They dont necessarily have to be position vectors, but for the visualization in this video, lets stick to that. Now was only able to represent something that goes through the origin with this slope. o you can almost view that this vector kind of represented its slope. You almost want to view it as a slope vector, if you wanted to tie it in to what you learned in Algebra 1. What if we wanted to represent other lines that had that slope? What if we wanted to represent the the same line, or guess a parallel line that goes through that point over there, the point 2 comma 4? Or if were thinking in position vectors, we could say that point is represented by the vector, and we will call that x. ts represented by the vector x. And the vector x is equal to 2, 4. That point right there. What if want to represent the line thats parallel to this that goes through that point 2, 4? o want to represent this line right here. ll draw it as parallel to this as can. think you get the idea, and it just keeps going like that in every direction. These two lines are parallel. How can represent the set of all of these vectors, drawn in standard form, or all of the vectors, that if were to draw them in standard form, would show this line? Well, you can think about it this way. f every one of the vectors that represented this line, if start with any vector that was on this line, and add my x vector to it, ll show up at a corresponding point on this line that want to be at. Right? Lets say do negative 2 times my original, so minus 2 times my vector v, that equaled what? inus 4, minus 2, so thats that vector there. But if were to add x to it, if were to add my x vector. o if were to do minus 2 times my vector v, but were to add x to it, so plus x. m adding this vector 2 comma 4 to it, so from here d go right 2 and up 4, so d go here. Or visually you could just say, heads to tails, so would go right there. o would end up at a corresponding point over there. o when define my set, s, as the set of all points where just multiply v times the scalar, got this thing that went through the origin. But now let me define another set. Let me define a set l, maybe l for line, thats equal to the set of all of vectors where the vector x, could do it bold or ll just draw an arrow on it, plus some scalar could use c, but let me use t, because m going to call this a parametrization of the line so plus some scalar, t times my vector v such that t could be any member of the real numbers. o what is this going to be? This is going to be this blue line. f were to draw all of these vectors in standard position, m going to get my blue line. For example, if do minus 2, this is minus 2, times my vector v, get here. Then if add x, go there. o this vector right here that has its endpoint right there its endpoint sits on that line. can do that with anything. f take this vector, this is some scalar times my vector v, and add x to it, end up with this vector, whose endpoint, if view it as a position vector, its endpoint dictates some coordinate in the xy plane. o it will that point. o can get to any of these vectors. This is a set of vectors right here, and all of these vectors are going to point theyre essentially going to point to something when draw them in standard form, if draw them in standard form theyre going to point to a point on that blue line. Now you might say, hey al, this was a really obtuse way of defining a line. mean we do it in Algebra 1, where we just say, hey you know, y is equal to mx plus b. And we figure out the slope by figuring out the difference of two points, and then we do a little substitution. And this is stuff you learned in seventh or eighth grade. This was really straightforward. Why am defining this obtuse set here and making you think in terms of sets and vectors and adding vectors? And the reason is, is because this is very general. This worked well in R2. o in R2, this was great. mean, we just have to worry about xs and ys. But what about the situation, mean notice, in your algebra class, your teacher never really told you much, at least in the ones took, about how do you present lines in three dimensions? aybe some classes go there, but they definitely didnt tell you how do you represent lines in four dimensions, or a hundred dimensions. And thats what this is going to do for us. Right here, defined x and v as vectors in R2. Theyre twodimensional vectors, but we can extend it to an arbitrary number of dimensions. o just to kind of hit the point home, lets do one more example in R2, where, its kind of the classic algebra problem where you need to find the equation for the line. But here, were going to call it the set definition for the line. Lets say we have two vectors. Lets say we have the vector a, which ll define as let me just says its 2, 1. o if were draw it in standard form, its 2, 1. Thats my vector a, right there. And lets say have vector b, let me define vector b. m going to define it as, dont know, let me define it as 0, 3. o my vector b, 0 dont move to the right at all and go up. o my vector b will look like that. Now m going to say that these are position vectors, that we draw them in standard form. When you draw them in standard form, their endpoints represent some position. o you can almost view these as coordinate points in R2. This is R2. All of these coordinate axes draw are going be R2. Now what if asked you, give me a parametrization of the line that goes through these two points. o essentially, want the equation if youre thinking in Algebra 1 terms want the equation for the line that goes through these two points. o the classic way, you would have figured out the slope and all of that, and then you would have substituted back in. But instead, what we can do is, we can say, hey look, this line that goes through both of those points you could almost say that both of those vectors lie on guess thats a better Both of these vectors lie on this line. Now, what vector can be represented by that line? Or even better, what vector, if take any arbitrary scalar can represent any other vector on that line? Now let me do it this way. What if were to take so this is vector b here what if were to take b minus a? We learned in, think it was the previous video, that b minus a, youll get this vector right here. Youll get the difference in the two vectors. This is the vector b minus the vector a. And you just think about it. What do have to add to a to get to b? have to add b minus a. o if can get the vector b minus a right, we know how to do that. We just subtract the vectors, and then multiply it by any scalar, then were going to get any point along that line. We have to be careful. o what happens if we take t, so some scalar, times our vector, times the vectors b minus a? What will we get then? o b minus a looks like that. But if we were to draw it in standard form remember, in standard form b minus a would look something like this. Right? t would start at 0, it would be parallel to this, and then from 0 we would draw its endpoint. o if we just multiplied some scalar times b minus a, we would actually just get points or vectors that lie on this line. Vectors that lie on that line right there. Now, thats not what we set out to do. We wanted to figure out an equation, or parametrization, if you will, of this line, or this set. Lets call this set l. o we want to know what that set is equal to. o in order to get there, we have to start with this, which is this line here, and we have to shift it. And we could shift it either by shifting it straight up, we could add vector b to it. o we could take this line right here, and add vector b to it. And so any point on here would have its corresponding point there. o when you add vector b, it essentially shifts it up. That would work. o we could, say, we could add vector b to it. And now all of these points for any arbitrary t is a member of the real numbers, will lie on this green line. Or the other option we could have done is we could have added vector a. Vector a would have taken any arbitrary point here and shifted it that way. Right? You would be adding vector a to it. But either way, youre going to get to the green line that we cared about, so you could have also defined it as the set of vector a plus this line, essentially, t times vector b minus a, where t is a member of the reals. o my definition of my line could be either of these things. The definition of my line could be this set, or it could be this set. And all of this seems all very abstract, but when you actually deal with the numbers, it actually becomes very simple. t becomes arguably simpler than what we did in Algebra 1. o this l, for these particular case of a and b, lets figure it out. y line is equal to let me just use the first example. ts vector b, so its the vector 0, 3 plus t, times the vector b minus a. Well whats b minus a? 0 minus 2 is minus 2, 3, minus 1 is 2, for t is a member of the reals. Now, if this still seems kind of like a convoluted set definition for you, could write it in terms that you might recognize better. f we want to plot points, if we call this the yaxis, and we call this the xaxis, and if we call this the xcoordinate, or maybe more properly that the xcoordinate and call this the ycoordinate, then we can set up an equation here. This actually is the xslope. This is the xcoordinate, thats the ycoordinate. Or actually, even better, whatever actually, let me be very careful there. This is always going to end up becoming some vector, l1, l2. Right? This is a set of vectors, and any member of this set is going to look something like this. o this could be li. o, this is the xcoordinate, and this is the ycoordinate. And just to get this in a form that you recognize, so were saying that l is the set of this vector x plus t times this vector b minus a here. f we wanted to write it in kind of a parametric form, we can say, since this is what determines our xcoordinate, we would say that x is equal to 0 plus t times minus 2, or minus 2 times t. And then we can say that y, since this is what determines our ycoordinate, y is equal to 3 plus t times 2 plus 2t. o we could have rewritten that first equation as just x is equal to minus 2t, and y is equal to 2t plus 3. o if you watch the videos on parametric equations, this is just a traditional parametric definition of this line right there. Now, you might have still viewed this as, al, this was a waste of time, this was convoluted. You have to define these sets and all that. But now m going to show you something that you probably well, unless you have done this before, but guess thats true of anything. But you probably havent seen in your traditional algebra class. Lets say have two points, and now m going to deal in three dimensions. o lets say have one vector. ll just call it point 1, because these are position vectors. Well just call it position 1. This is in three dimensions. Just make up some numbers, negative 1, 2, 7. Lets say have Point 2. Once again, this is in three dimensions, so you have to specify three coordinates. This could be the x, the y, and the z coordinate. Point 2, dont know. Lets make it 0, 3, and 4. Now, what if wanted to find the equation of the line that passes through these two points in R3? o this is in R3. Well, just said that the equation of this line so ll just call that, or the set of this line, let me just call this l. ts going to be equal to we could just pick one of these guys, it could be P1, the vector P1, these are all vectors, be careful here. The vector P1 plus some random parameter, t, this t could be time, like you learn when you first learn parametric equations, times the difference of the two vectors, times P1, and it doesnt matter what order you take it. o thats a nice thing too. P1 minus P2. t could be P2 minus P1 because this can take on any positive or negative value where t is a member of the real numbers. o lets apply it to these numbers. Lets apply it right here. What is P1 minus P2? P1 minus P2 is equal to let me get some space here. P1 minus P2 is equal, minus 1 minus 0 is minus 1. 2 minus 3 is minus 1. 7 minus 4 is 3. o that thing is that vector. And so, our line can be described as a set of vectors, that if you were to plot it in standard position, it would be this set of position vectors. t would be P1, it would be let me do that in green it would be minus 1, 2, 7. couldve put P2 there, just as easily plus t times minus 1, minus 1, 3, where, or such that, t is a member of the real numbers. Now, this also might not be satisfying for you. Youre like, gee, how do plot this in three dimensions? Wheres my x, ys, and zs? And if you want to care about x, ys, and zs, lets say that this is the zaxis. This is the xaxis, and lets say the yaxis. t kind of goes into our board like this, so the yaxis comes out like that. o what you can do, and actually probably wont graph, so the determinate for the xcoordinate, just our convention, is going to be this term right here. o we can write that x let me write that down. o that term is going to determine our xcoordinate. o we can write that x is equal to minus 1 be careful with the colors minus 1, plus minus 1 times t. Thats our xcoordinate. Now, our ycoordinate is going to be determined by this part of our vector addition because these are the ycoordinates. o we can say the ycoordinate is equal to ll just write it like this 2 plus minus 1 times t. And then finally, our zcoordinate is determined by that there, the t shows up because t times 3 or could just put this t into all of this. o that the zcoordinate is equal to 7 plus t times 3, or could say plus 3t. And just like that, we have three parametric equations. And when we did it in R2, did a parametric equation, but we learned in Algebra 1, you can just have a regular y in terms x. You dont have to have a parametric equation. But when youre dealing in R3, the only way to define a line is to have a parametric equation. f you have just an equation with xs, ys, and zs, if just have x plus y plus z is equal to some number, this is not a line. And well talk more about this in R3. This is a plane. The only way to define a line or a curve in three dimensions, if wanted to describe the path of a fly in three dimensions, it has to be a parametric equation. Or if shoot a bullet in three dimensions and it goes in a straight line, it has to be a parametric equation. o these guess you could call it these are the equations of a line in three dimensions. o hopefully you found that interesting. And think this will be the first video where you have an appreciation that linear algebra can solve problems or address issues that you never saw before. And theres no reason why we have to just stop at three, three coordinates, right here. We could have done this with fifty dimensions. We could have defined a line in fifty dimensions or the set of vectors that define a line, that two points sit on, in fifty dimensions which is very hard to visualize, but we can actually deal with it mathematically.","And just to get this in a form that you recognize, so were saying that l is the set of this vector x plus t times this vector b minus a here. But want to make this point that this line of slope 2 that goes through the origin, this is if we draw all of the vectors in the set as in their standard form, or if we draw them all as position vectors. Well, just said that the equation of this line so ll just call that, or the set of this line, let me just call this l. ts going to be equal to we could just pick one of these guys, it could be P1, the vector P1, these are all vectors, be careful here. But either way, youre going to get to the green line that we cared about, so you could have also defined it as the set of vector a plus this line, essentially, t times vector b minus a, where t is a member of the reals. Let me define a set l, maybe l for line, thats equal to the set of all of vectors where the vector x, could do it bold or ll just draw an arrow on it, plus some scalar could use c, but let me use t, because m going to call this a parametrization of the line so plus some scalar, t times my vector v such that t could be any member of the real numbers. Lets just say that my vector is the vector 2, 1. f were to draw it in standard position, it looks like this. Or if were thinking in position vectors, we could say that point is represented by the vector, and we will call that x. ts represented by the vector x. And the vector x is equal to 2, 4. And if we view these vectors as position vectors, that this vector represents a point in space in R2 this R2 is just our artesian coordinate plane right here in every direction if we view this vector as a position vector let me write that down if we view it as kind of a coordinate in R2, then this set, if we visually represent it as a bunch of position vectors, itll be represented by this whole line over here. have to add b minus a. o if can get the vector b minus a right, we know how to do that. o we can say the ycoordinate is equal to ll just write it like this 2 plus minus 1 times t. And then finally, our zcoordinate is determined by that there, the t shows up because t times 3 or could just put this t into all of this. This is the vector b minus the vector a. And you just think about it. f every one of the vectors that represented this line, if start with any vector that was on this line, and add my x vector to it, ll show up at a corresponding point on this line that want to be at. And so, our line can be described as a set of vectors, that if you were to plot it in standard position, it would be this set of position vectors. o we could take this line right here, and add vector b to it. o in order to get there, we have to start with this, which is this line here, and we have to shift it. f we wanted to write it in kind of a parametric form, we can say, since this is what determines our xcoordinate, we would say that x is equal to 0 plus t times minus 2, or minus 2 times t. And then we can say that y, since this is what determines our ycoordinate, y is equal to 3 plus t times 2 plus 2t. But you can imagine that if were to plot all of the vectors in standard position, all of them that could be represented by any c in real numbers, ll essentially get ll end up drawing a bunch of vectors where their arrows are all lined up along this line right there, and all lined up in even in negative direction let me make sure draw it properly along that line, like that. This is a set of vectors right here, and all of these vectors are going to point theyre essentially going to point to something when draw them in standard form, if draw them in standard form theyre going to point to a point on that blue line. o we could have rewritten that first equation as just x is equal to minus 2t, and y is equal to 2t plus 3. o if you watch the videos on parametric equations, this is just a traditional parametric definition of this line right there. Thats my, right there, that is my vector v. Now, if were to ask you, what are all of the possible vectors can create? How can represent the set of all of these vectors, drawn in standard form, or all of the vectors, that if were to draw them in standard form, would show this line? t would be P1, it would be let me do that in green it would be minus 1, 2, 7. couldve put P2 there, just as easily plus t times minus 1, minus 1, 3, where, or such that, t is a member of the real numbers. o if were to do minus 2 times my vector v, but were to add x to it, so plus x. m adding this vector 2 comma 4 to it, so from here d go right 2 and up 4, so d go here. And lets say have vector b, let me define vector b. m going to define it as, dont know, let me define it as 0, 3. o my vector b, 0 dont move to the right at all and go up. Let me define a set, s, and its equal to all of the vectors can create, if were to multiply v times some constant, so multiply some constant, some scalar, times my vector v, and just to maybe be a little bit formal, ll say such that c is a member of the real numbers. o if were to multiply, c could be 2. f c is 2, let me do it this way. This is a set of vectors, and any member of this set is going to look something like this. o what is this going to be? What if want to represent the line thats parallel to this that goes through that point 2, 4? f were to do minus 10, would get a vector going in this direction that goes way like that. f we want to plot points, if we call this the yaxis, and we call this the xaxis, and if we call this the xcoordinate, or maybe more properly that the xcoordinate and call this the ycoordinate, then we can set up an equation here. f take this vector, this is some scalar times my vector v, and add x to it, end up with this vector, whose endpoint, if view it as a position vector, its endpoint dictates some coordinate in the xy plane. The definition of my line could be this set, or it could be this set. We could have defined a line in fifty dimensions or the set of vectors that define a line, that two points sit on, in fifty dimensions which is very hard to visualize, but we can actually deal with it mathematically. Now m going to say that these are position vectors, that we draw them in standard form. Lets call this set l. o we want to know what that set is equal to. m just going to define my vectors, its going to be a vector in R2. o what you can do, and actually probably wont graph, so the determinate for the xcoordinate, just our convention, is going to be this term right here.",0.1944584382871536
84,625,"n our previous lesson, we wrote some code for binary search tree we wrote functions to insert and search data in BT .Now in this lesson we will go a little deeper and try to understand how things move in various sections of applications memory. When these functions get executed and this will give you a lot of clarity and this will give you some general insight into how memory is managed for execution of a program and how recursion which is so frequently used in case of trees works. The concepts that m going to talk about in this lesson have been discussed earlier in some of our previous lessons, but it will be good to go through these concepts again when we are implementing trees. o, heres the code that he had written. We have this function GetNewNode() to create a new node in dynamic memory and then we have this function nsert() to insert a new node in the tree and then we have this function to search some data in the tree and finally this is the main function. You can check the description of this video for link to the source code. Now in main function here we have this pointer to BTnode named root to store the address of the root note of my tree and am initially setting it as NLL to create an empty tree and then m making some calls to insert function to insert some data in the tree and finally m asking user to input a number and m making call to search function to find this number in the tree if the search function is returning me true m printing found, else am printing not found. Lets see what will happen in memory when this program will execute. The memory that is allocated to a program or application for its execution in a typical architecture can be divided into these four segments, there is one segment called text segment to store all the instructions in the program. The instructions would be compiled instructions in machine language. There is another segment to store all the global variables. A variable that is declared outside all the functions is called global variable. t is accessible to all the functions. The next segment stack is basically scratch space for function call execution, all the local variables the variables that are declared within functions live in stack. and finally the fourth section heap which we also called the free store, is the dynamic memory that can grow or shrink as per our need. the size of all other segments is fixed the size of all other segments is decided at compiletime but heap can grow during runtime and we cannot control allocation or deallocation of memory in any other segment during runtime but we can control allocation and deallocation in heap we have discussed all of this in detail in our lesson on dynamic memory allocation you can check the description for a link. Now what m going to do here is am going to draw stack and heap sections as these two rectangular containers. m kind of zooming into these two sections. Now will show you how things will move in these two sections of applications memory when this program will execute. When this program will start execution first the main function will be called. Now whenever a function is called some amount of memory from the stack is allocated for its execution. The allocated memory is called stack frame of the function call. All the local variables and the state of execution of the function call would be stored in the stack frame of the function call. n the main function we have this local variable root which is pointer to BTnode so m showing root here in this stack frame. We will execute instructions sequentially. n the first line in main function, we have declared root and we are initializing it and setting it as NLL. NLL is only a map macro for address Zero. o here in in this figure am setting address in root as 0. Now in the next line we are making a call to insert function so what will happen is execution of main will pause at this stage and a new stack frame will be allocated for execution of insert. ain will wait for this insert above to finish and return. Once this insert call finishes, main will resume at line 2. We have these two local variables root and data in insert function, in which we are collecting the arguments. Now for this call to insert function, we will go inside the first if condition here because root is NLL, at this line we will make call to GetNewNode function so once again execution of this insert call will pause and a new stack frame will be allocated for execution of GetNewNode function we have two local variables in GetNewNode data in which we are collecting argument and this pointer to BTnode named newnode. Now in this function we are using new operator to create a BTnode in heap. Now lets say we got a new node at address 200 new operator will return us this address 200 so this address will be set here in new node so we have this link here and now using this pointer newNode, we are setting value in these three fields of Node. Lets say the first field to store data so we are setting value 15 here and lets say this second cell is to store address of left child this is being set as NLL and address of right child is also being set as NLL and now GetNewNode() will return the address of new node and finish its execution. Whenever a function call finishes, the stack frame allocated to it is reclaimed. all to insert function will resume at this line and the return of GetNewNode() address 200, will be set in this the root which is local variable for insert call and now insert function, this particular call to insert function return the address of root.The address stored in this variable root which is 200 now and finish and now main will resume at this line and root of main will be set as 200. The return of this insert call, insert(root, 15) will be set here. Now in the execution of main, control will go to the next line and we have this call to insert function to insert number 10. Once again execution of main will be paused and a stack frame will be allocated for execution of insert. Now this time for insert call root is not NLL. o we will not go inside to first if, now we will access the data field of this node at address 200 by using this pointer named root in insert function and we will compare it with this value 10. 10 is lesser than 15 so we will go to this line and now were making a recursive call here because recursion is a function calling itself. and a function calling itself is not any different from a function A calling another function B so what will happen here is that execution of this particular insert call will be paused and a new stack frame will be allocated for the execution of this another insert call to which the arguments passed are address 0, in this local variable root, left child of Node at address 200 is NLL. so we are passing 0 and root and in data we are passing 10. Now for this particular insert call control will go inside first if and we will make a call to get new node function at this line so execution of this insert will pause and well go to GetNewNode() function here, we are creating a new node in heap. Lets say we got this new node at address 150. Now GetNewNode() will return 150 and finish execution of this call to insert will resume at this line, return of GetNewNode() will be set here and now this call to insert will return address 150 and finish. insert below will resume at this line and now in this insert call left child of this node at address 200 will be set as return of the previous insert call which is 150 so.. now these two node are linked and finally this insert call will finish. ontrol will return back to main at this line, root will be rewritten as 200 but earlier also it is was 200, its not changing. Next in the main function may have caused to insert number 20. m not going to show the simulation for this one once again allocated memory in stack will grow and shrink and finally when the control will return back to main function after this insert call is over, well have a node in heap with value 20 set as right child of this node at 200. Lets say we got this new node with value 20 at address 300 so as you can see the address of right child in node at address 200 is set as 300. Now next one is to insert number 25 this one is interesting lets see what will happen for this one. ain will be paused and we will go to this call to insert, in the root which is local to this call address passed is 200 and we have passed number 25 n data.Now here 25 is greater than the value in this node at address 200 so we will go inside this last else condition we need to insert in the right subtree so another card insert will be made we will pass address 300 as root and data passed will be 25 only. Now for this call once again the value in node at 300 for this call which is 300 is lesser than 25. 25 is greater than 20 so once again we will come to this last else and make a recursive call to insert in the right subtree the right subtree is empty this time so for this insert call at top the address in root here will be 0 so for this call we will go to the first if and make a call to GetNewNode(). Lets say this new node returns us node at address 100. m short of space so m not showing everything in GetNewNode() stack frame here. we will return back to this insert call at top and now this root is set as 100 address of the newly created node and now this call to insert will finish. We will come back to this insert below and this insert will resume at this line inside the last else and the right child of node address 300 will be set as 100, and now this insert will return back to address 300, whatever is set in its root and this insert below will resume at this line inside the last else right child of node at address 200 will be set as 300. t was 300 previously also so even after overwriting we will not change and this insert will finish now. Finally main will resume at this line, root of main will be set as return of this insert call. t will only be overwritten with same value. ts really important that this root in main and other links in nodes not properly updated quite often because of bugs in our code, will loose some links or some unwanted links are created. Now as you can see, we are creating all the notes in heap here. heap gives us this flexibility that we can decide the creation of node during runtime and we can control the lifetime of anything in heap any memory claimed in heap has to be explicitly deallocated using free in or delete operator in ++ else the memory in heap remains allocated till the program is running. The memory in stack as you can see gets Deallocated when function call finishes. The rest of the function calls here in ain function will execute in similar manner ll leave it for you to see and think about. Right now we have this tree in the heap, logically memory itself is a linear structure and this is how tree which is a nonlinear structure which is logically a nonlinear structure will fit in it. The way m showing the nodes at random locations linked to each other in this heap. hope this explanation gave you some clarity. n coming lessons we will solve some problems on trees. This is it for this lesson thanks for watching.","all to insert function will resume at this line and the return of GetNewNode() address 200, will be set in this the root which is local variable for insert call and now insert function, this particular call to insert function return the address of root.The address stored in this variable root which is 200 now and finish and now main will resume at this line and root of main will be set as 200. Now for this call to insert function, we will go inside the first if condition here because root is NLL, at this line we will make call to GetNewNode function so once again execution of this insert call will pause and a new stack frame will be allocated for execution of GetNewNode function we have two local variables in GetNewNode data in which we are collecting argument and this pointer to BTnode named newnode. Now in the next line we are making a call to insert function so what will happen is execution of main will pause at this stage and a new stack frame will be allocated for execution of insert. insert below will resume at this line and now in this insert call left child of this node at address 200 will be set as return of the previous insert call which is 150 so.. now these two node are linked and finally this insert call will finish. and a function calling itself is not any different from a function A calling another function B so what will happen here is that execution of this particular insert call will be paused and a new stack frame will be allocated for the execution of this another insert call to which the arguments passed are address 0, in this local variable root, left child of Node at address 200 is NLL. ain will be paused and we will go to this call to insert, in the root which is local to this call address passed is 200 and we have passed number 25 n data.Now here 25 is greater than the value in this node at address 200 so we will go inside this last else condition we need to insert in the right subtree so another card insert will be made we will pass address 300 as root and data passed will be 25 only. we will return back to this insert call at top and now this root is set as 100 address of the newly created node and now this call to insert will finish. Now for this particular insert call control will go inside first if and we will make a call to get new node function at this line so execution of this insert will pause and well go to GetNewNode() function here, we are creating a new node in heap. Now in the execution of main, control will go to the next line and we have this call to insert function to insert number 10. Next in the main function may have caused to insert number 20. m not going to show the simulation for this one once again allocated memory in stack will grow and shrink and finally when the control will return back to main function after this insert call is over, well have a node in heap with value 20 set as right child of this node at 200. 25 is greater than 20 so once again we will come to this last else and make a recursive call to insert in the right subtree the right subtree is empty this time so for this insert call at top the address in root here will be 0 so for this call we will go to the first if and make a call to GetNewNode(). We will come back to this insert below and this insert will resume at this line inside the last else and the right child of node address 300 will be set as 100, and now this insert will return back to address 300, whatever is set in its root and this insert below will resume at this line inside the last else right child of node at address 200 will be set as 300. t was 300 previously also so even after overwriting we will not change and this insert will finish now. Now GetNewNode() will return 150 and finish execution of this call to insert will resume at this line, return of GetNewNode() will be set here and now this call to insert will return address 150 and finish. We have this function GetNewNode() to create a new node in dynamic memory and then we have this function nsert() to insert a new node in the tree and then we have this function to search some data in the tree and finally this is the main function. o we will not go inside to first if, now we will access the data field of this node at address 200 by using this pointer named root in insert function and we will compare it with this value 10. Now lets say we got a new node at address 200 new operator will return us this address 200 so this address will be set here in new node so we have this link here and now using this pointer newNode, we are setting value in these three fields of Node. Finally main will resume at this line, root of main will be set as return of this insert call.",0.2462932454695222
85,626,"Lets say that have some set V that is a subspace in Rn. And just as a reminder, what does it mean? Thats just some set, or some subset of Rn where if take any two members of that subset so let say take the members a and b theyre both members my subspace. By the fact that this is a subspace, we then know that the addition of these two vectors, or a plus b, is also in my subspace. And this is our closure under addition. And by the fact that its a subspace, we also know that if we multiply any member of our subspace by a scalar so the fact that those guys are members of our subspace we also know that if pick one of them, lets say a, and multiply a by some scalar, that this is also going to be a member of our subspace. And we sometimes call this closure under scalar multiplication. And then a somewhat redundant statement is that V, well it must contain the zero vector. And thats true of all subspaces. V let me write it this way the zero vector is a member of V. And it would be the zero vector with n components here, because V is a subspace of Rn. And why say thats redundant, because if say that any multiple of these vectors is also in V, could just set the scalar to be equal to 0. o this statement kind of takes the statement into account. But in a lot of textbooks, they will always write, oh and the zero vector has to be a member of V. Although, thats kind of redundant with the closure under scalar multiplication. Fair enough. Now, lets say that also have some transformation T. t is a mapping, a function, from Rn to Rm. What want to understand, in this video is, have a subspace right here, V. want to understand whether the transformation of the subspace and what did we call that? We called that the image of our subspace, or our subset, either way. The image of V under T. n the last video, just to kind of help you visualize it. How did that work or we had some subset of Rn that looked like this. t was a triangle that looked something like that. And that was in Rn, this was actually in R2, it was a triangle that looked something like that. And we figured out its image under T. o we went from R2 to R2. and we had our transformation. And it ended up looking something like this. f remember it properly. t ended up looking like a gee, dont remember it fully, but it was like a triangle that was skewed like this, rotated. o it was a actually think it was more like think thats right. t was rotated a bit clockwise like that and it was skewed. But the exact particulars of that last video arent what matter. What matters is that you are able to visualize what an image under transformation means. t means you take some subset of R2, all of the vectors that define this triangle right here. Thats some subset of R2. You transform all of them, and then you get some subset in your codomain. You could call this the image, because the transformation of that triangle, or if we call this s, its equal to the transformation of s. Or you could say its the image of you can just call it the set s, but maybe it helps you to visualize call it the image of this triangle under T. Or maybe even a neater way of thinking about it is, this triangle that skewed, rotated triangle this one is the image of this right triangle under T. think that might make a little bit of visual sense to you. And just as a bit of reminder, in that last video these triangles, these werent subspaces. And just as you could take scalar multiples of some of the vectors that are members of this triangle, and youll find that theyre not going to be in that triangle. o this wasnt a subspace, this was just a subset of R2. All subsets are not subspaces, but all subspaces are definitely subsets. Although something can be a subset of itself. dont want to wander off too much. But this just helps you visualize what we mean by an image. t means all of the vectors that are mapped to, from the members of your subset. o want to know whether the image of V under T is a subspace. o in order for it to be a subspace, if take the transformation let me find two members of T. Well clearly if take the transformation of any members of V, m getting members of the image. Right? o can write this. learly the transformation of a and the transformations of b, these are both of members of our images of V under T. These are both members of that right there. o my question to you is what is the transformation of a plus the transformation of b? And the way have written this, these are two arbitrary members of our image of V under T. Or maybe should call it T of capital V. These are two arbitrary members. o what is this equal to? Well, we know from our properties, our definition of linear transformations, the sum of the transformations of two vectors is equal to the transformation of the sum of their of vectors. Now, is the transformation of a plus b, is this a member of TV? s it a member of our image? Well, a plus b is a member of V, and the image contains the transformation of all of the members of V. o the image contains the transformation of this guy. This guy, a plus b is a member of V. o youre taking a transformation of a member of V which, by definition, is in your image of V under T. o this is definitely true. Now, lets ask the next question. f take a scalar multiple of some member of my image of V under T, or my T of capital V, right there. f take the sum scalar, what is this equal to? By definition for linear transformation, this is the same thing as a transformation of the scalar times the vector. Now is this going to be a member of our image of V under T? Well we know that ca is definitely in V, right? Thats from the definition of a subspace. This is definitely in V. And so, if this is in V, the transformation of this has to be in Vs image under T. o this is in this is also a member of V. And obviously, you can set this equal to 0. The zero vector is a member of V, so any transformation of if you just put a 0 here, youll get the zero vector. o the zero vector is definitely dont care what this is, if you multiply it times 0, you are going to get the zero vector. o the zero vector is definitely also a member of TV. o we come on the result that T the image of V under T, is a subspace. Which is a useful result which we will be able to use later on. But this, guess, might naturally lead to the question, what if we go everything we have been dealing with so far have been subsets, with the case of this triangle, or subspaces, in the case of V. But what if were to take the image Rn under T, right? This is the image of Rn under T. Lets think about what this means. This means, what do we get when we take any member of Rn, what is the set of all of the vectors? Then when we take the transformation of all of the members of Rn, let me write this. This is equal to the set of the transformation of all of the xs, where each x is a member of Rn. o you take each of the members of Rn and transform them, and you create this new set. This is the image of Rn under T. Well, theres a couple of ways you can think of this. Remember when we defined lets see, T is a mapping from Rn to Rm. We defined this as the domain. All of the possible inputs for our transformation. And we define this as the codomain. And remember told you that the codomain is essentially part of the definition of the function or of the transformation, and its the space that we map to. ts not necessarily all of the things that were mapping to. For example, the image of Rn under transformation, maybe its all of Rm or maybe its some subset of Rn. The way you can think about it, and touched on this in that first video, is and theyll never, or at least the linear algebra books looked at, they didnt specify this but you can kind of view this as the range of T. These are the actual members of Rm that T maps to. That if you take the image of Rn under T, you are actually finding lets say that Rm looks like that. Obviously it will go in every direction. And lets say that when you take let me draw Rn right here. And we know that T is a mapping from Rn to Rm. But lets say when you take every element of Rn and you map them into Rm, lets say you get some subset of Rm, lets say you get something that looks like this. o let me see if can draw this nicely. o you literally map every point here, and it goes to one of these guys. Or one of these guys can be represented as a mapping from one of these members right here. o if you map all of them you get this subset right here. This subset is, this is T the image of Rn, the image of Rn under T. And in the terminology that you dont normally see in linear algebra a lot, you can also kind of consider it its range. The range of T. Now, this has a special name. This is called and dont want you to get confused this is called the image of T. mage of T. This might be a little confusing, image of T. o this is sometimes written as just im of T. Now you are a little confused here, you are like, before when we were talking about subsets, we would call this the image of R subset under T. And that is the correct terminology when youre dealing with a subset. But when you take, all of a sudden, the entire n dimensional space, and youre finding that image, we call that the image of the actual transformation. o we can also call this set right here the image of T. And now what is the image of T? Well, we know that we can write any and this is literally any so T is going from Rn to Rm. We can write T of x we can write any linear transformation like this as being equal to some matrix, some m by n matrix times a vector. And these vectors obviously are going to be members of Rn times sum Rn. And what is this? o what is the image let me write it in a bunch of different ways what is the image of Rn under T? o we could write that as T let me write it this way. We could write that as T of Rn, which is the same thing as the image of T. Notice were not saying under anything else, because now were saying the image of the actual transformation. Which we could also write as the image of T. Well what are these equal to? This is equal to the set of all the transformations of x. Well all the transformations of x are going to be Ax where x is a member of Rn. o x is going to be an ntuple, where each element has to be a real number. o what is this? o if we write A let me write my matrix A. ts just a bunch of column vectors, a1, a2. ts going to have n of these, right? Because it has n columns. And so a times any x is going to be so if multiply that times any x thats a member of Rn. multiply x1, x2, all the way to xn. Weve seen this multiple, multiple times. This is equal to x1 the scalar x1, times a1, plus x2 times a2, all the way to plus xn times an. And were saying we want the set of all of these sums of these column vectors, where x can take on any vector in Rn. Which means that the elements of x can take on any real scalar values. o the set of all of these is essentially all of the linear combinations of the columns of a, right? Because can set these guys to be equal to any value. o what is that equal to? That is equal to, and we touched on this, or we actually talked about this when we introduced the idea. This is equal to the column space of A. Or we just denoted it sometimes as of A. o thats a pretty neat result. f you take its almost obvious, mean its just m playing with words a little bit but any linear transformation can be represented as a matrix vector product. And so the image of any linear transformation, which means the subset of its codomain, when you map all of the elements of its domain into its codomain, this is the image of your transformation. This is equivalent to the column space of the matrix that youre transformation could be represented as. And the column space, of course, is the span of all the column vectors of your matrix. This is just all of the linear combinations, or the span, of all of your column vectors, which we do right here. Anyway hope you found that a little interesting, and you will be able to use these results in the future.","You could call this the image, because the transformation of that triangle, or if we call this s, its equal to the transformation of s. Or you could say its the image of you can just call it the set s, but maybe it helps you to visualize call it the image of this triangle under T. Or maybe even a neater way of thinking about it is, this triangle that skewed, rotated triangle this one is the image of this right triangle under T. think that might make a little bit of visual sense to you. This is equal to the set of the transformation of all of the xs, where each x is a member of Rn. o we can also call this set right here the image of T. And now what is the image of T? This is definitely in V. And so, if this is in V, the transformation of this has to be in Vs image under T. o this is in this is also a member of V. And obviously, you can set this equal to 0. This means, what do we get when we take any member of Rn, what is the set of all of the vectors? This subset is, this is T the image of Rn, the image of Rn under T. And in the terminology that you dont normally see in linear algebra a lot, you can also kind of consider it its range. Then when we take the transformation of all of the members of Rn, let me write this. This is called and dont want you to get confused this is called the image of T. mage of T. This might be a little confusing, image of T. o this is sometimes written as just im of T. Now you are a little confused here, you are like, before when we were talking about subsets, we would call this the image of R subset under T. And that is the correct terminology when youre dealing with a subset. This is equal to the set of all the transformations of x. Well all the transformations of x are going to be Ax where x is a member of Rn. Well, a plus b is a member of V, and the image contains the transformation of all of the members of V. o the image contains the transformation of this guy. And remember told you that the codomain is essentially part of the definition of the function or of the transformation, and its the space that we map to. o my question to you is what is the transformation of a plus the transformation of b? And so the image of any linear transformation, which means the subset of its codomain, when you map all of the elements of its domain into its codomain, this is the image of your transformation. Now is this going to be a member of our image of V under T? And just as you could take scalar multiples of some of the vectors that are members of this triangle, and youll find that theyre not going to be in that triangle. o the set of all of these is essentially all of the linear combinations of the columns of a, right? o what is the image let me write it in a bunch of different ways what is the image of Rn under T? And by the fact that its a subspace, we also know that if we multiply any member of our subspace by a scalar so the fact that those guys are members of our subspace we also know that if pick one of them, lets say a, and multiply a by some scalar, that this is also going to be a member of our subspace. The way you can think about it, and touched on this in that first video, is and theyll never, or at least the linear algebra books looked at, they didnt specify this but you can kind of view this as the range of T. These are the actual members of Rm that T maps to. t means you take some subset of R2, all of the vectors that define this triangle right here. This is the image of Rn under T. Lets think about what this means. V let me write it this way the zero vector is a member of V. And it would be the zero vector with n components here, because V is a subspace of Rn. This is just all of the linear combinations, or the span, of all of your column vectors, which we do right here. Well, we know from our properties, our definition of linear transformations, the sum of the transformations of two vectors is equal to the transformation of the sum of their of vectors. This is the image of Rn under T. Well, theres a couple of ways you can think of this. o in order for it to be a subspace, if take the transformation let me find two members of T. Well clearly if take the transformation of any members of V, m getting members of the image. Which we could also write as the image of T. Well what are these equal to?",0.2378426171529619
86,627,"Lets do some more nonhomogeneous equations. o lets take the same problem, but well change the righthand side. Because think you know how to solve the essentially, the homogeneous version. o the same problem as we did in the last video. The second derivative of y minus 3 times the first derivative y minus 4 times the function. And now in the last example, the nonhomogeneous part was 3e to the 2x. But were tired of dealing with exponent functions, so lets make it a trigonometric function. o lets say it equals 2 sin of x. o the first step you do is what weve been doing. You essentially solve the homogeneous equation. o this lefthand side is equal to 0. You do that by getting the characteristic equation r squared minus 3r minus 4 is equal to 0. You get the solutions, r is equal equal to 4, r is equal to minus 1, and then you get that general solution. We did this in the last video. You get the general solution of the homogeneous. aybe well call this the homogeneous solution. y homogeneous. Weve got the 1 e to the 4x plus 2e to the minus x. And thats all and good, but in order to get the general solution of this nonhomogeneous equation, have to take the solution of the nonhomogeneous equation, if this were equal to 0, and then add that to a particular solution that satisfies this equation. That satisfies when you take the second derivative minus 3 times the first minus 4 times the function, actually get 2 sin of x. And here once again well use undetermined coefficients. And undetermined coefficients, just think to yourself. What function, when take its second and first derivatives and add and subtract multiples of them to each other, will get sine of x? Well, two functions end up with sine of x when you take the first and second derivatives. And the sine and cosine of x. o its a good guess. And thats really what youre doing it the method of undetermined coefficients. You take a guess of a particular solution and then you solve for the undetermined coefficients. o lets say that our guess is y is equal to dont know, some coefficient times sine of x. And if this was sine of 2x, d put A times sine of 2x here. Just because still want no matter what happens here the sine of 2xs or maybe cosine of 2xs to still exist. f this was a sine of 2x, theres nothing could do to a sine of x, or nothing at least trivial that could do to the sine of x. t would end up with a sine of 2x. o whatevers here, want here. Plus B, some undetermined coefficient times cosine of x. And once again, this was sine of 2x. d want a cosine of 2x here. o lets figure out its first and second derivatives. o the first derivative of this y prime is equal to A cosine of x. osine derivative is minus sine, so minus B sine of x. And then the second derivitive ll write down here. The second derivative is equal to what? Derivative of cosine is minus sine, so minus A sine of x minus B cosine of x. think youre starting to see that the hardest thing in most differential equations problems is not making careless mistakes. ts a lot of algebra and a lot of fairly basic calculus. And the real trick is to not make careless mistakes. Every time say that, tend to make one. o m going to focus extra right now. o anyway, lets take these and substitute them back into this nonhomogeneous equation. Lets see if can solve for A and B. o the second derivative is that. Let me just rewrite it, just so that you see what m doing. o m going to take the second derivative, y prime prime, so thats minus A sine of x minus B cosine of x. m going to add minus 3 times the first derivative to that. And m going to write the sines under the sines and the cosines under the consines. o minus 3 times this. o the sine is, lets see. ts plus 3B sine of x minus 3 times this. o minus 3A cosine of x. And then minus 4 times our original function. o minus 4A sine of x. Right? inus 4 times that. inus 4 times this. inus 4B cosine of x. And when take the sum of all of those thats essentially the lefthand side to this equation when take the sum of all of that, that is equal to 2 sine of x. could have written them out in a line, but it would have just been more confusing. And now this makes it easy to add up the sine of xs and the cosine of xs. o if add up all the coefficients on the sine of x, get minus A plus 3B minus 4A. o that looks like minus 5A plus 3B sine of x plus and now what are the coefficients here? have minus B and then have another minus 4B, so minus 5B and then minus 3A. o minus 3A minus 5B cosine of x. The cosine of x should go right here. o anyway, how do solve for A and B? Well, have the minus 5A 3B is equal to whatever coefficients in front of sine of x here. o minus 5A plus 3B must be equal to 2. And then minus 3A minus 5B is the coefficient on cosine of x, although kind of squeezed in the cosine of x here, right? o this must be equal to whatever the coefficient on cosine of x is on the righthand side. Well the coefficient of cosine of x on the righthand side is 0. o that sets up a system of two unknowns with two equations. A linear system. o we get minus 5A plus 3B is equal to 2. And we get minus 3A minus 5B is equal to 0. And lets see if can simplify this a little bit. Lets see. This is a system of two unknowns, two equations. f multiply the top equation by 5 1/3s, right? Actually, let me multiply the top equation by 5 1/3s. get minus 25/3 A plus 5B is equal to 5 1/3s times this. 5 1/3s times 2 is 10 1/3s. And the bottom equation is minus 3A minus 5B is equal to 0. Lets add the two equations. get 10 1/3s is equal to these cancel out. Thats minus 25/3 minus 9/3 A is equal to 10 1/3s. This is getting a little bit messier than like, but well soldier on. o minus 25 minus 9. Whats minus 25 minus 9? o that is 34. o we get 34 over 3A is equal to 10/3. We can multiply both sides by 3. Divide both sides by 34. A is equal to 10/34, which is equal to 5/17. Nice ugly number. 5/17 and now we can solve for B. o lets see. inus 3 times A minus 3 times A. 5/17 minus 5B is equal to 0. o thats what? inus 15/17 is equal to plus 5B. just took this and put it on the righthand side. And then divide both sides by 5. Oh, you know what? realized made a careless mistake here. inus 25 minus 9. Thats the minus 34 over 3. so minus 34A is equal to 10. A is equal to minus 10/34 or minus 5/17. o minus 3 times minus 5/17. o 5/17 is equal to plus 5B, right? And then we get B is equal to 3/17. That was hairy. And notice, the hard part was not losing your negative sines. But anyway, we now have our particular solution to this. let me try to write in a nonnauseating color, although think picked a nauseating one. The particular solution is A minus 5/17 sine of x right? That was a coefficient on sine of x plus B plus 3/17 times cosine of x. And if we look at our original problem, the general solution out of this nonhomogeneous equation would be this which is the general solution to the homogeneous equation, which weve done many videos on plus now our particular solution that we solved using the method of undetermined coefficient. o if you just take that and add it to that, youre done. And am out of time. ee you in the next video.","Weve got the 1 e to the 4x plus 2e to the minus x. And thats all and good, but in order to get the general solution of this nonhomogeneous equation, have to take the solution of the nonhomogeneous equation, if this were equal to 0, and then add that to a particular solution that satisfies this equation. o the first derivative of this y prime is equal to A cosine of x. osine derivative is minus sine, so minus B sine of x. And then the second derivitive ll write down here. And we get minus 3A minus 5B is equal to 0. And the bottom equation is minus 3A minus 5B is equal to 0. And then minus 3A minus 5B is the coefficient on cosine of x, although kind of squeezed in the cosine of x here, right? You get the solutions, r is equal equal to 4, r is equal to minus 1, and then you get that general solution. get minus 25/3 A plus 5B is equal to 5 1/3s times this. o m going to take the second derivative, y prime prime, so thats minus A sine of x minus B cosine of x. m going to add minus 3 times the first derivative to that. o lets say that our guess is y is equal to dont know, some coefficient times sine of x. And if this was sine of 2x, d put A times sine of 2x here. That was a coefficient on sine of x plus B plus 3/17 times cosine of x. And if we look at our original problem, the general solution out of this nonhomogeneous equation would be this which is the general solution to the homogeneous equation, which weve done many videos on plus now our particular solution that we solved using the method of undetermined coefficient. That satisfies when you take the second derivative minus 3 times the first minus 4 times the function, actually get 2 sin of x. And here once again well use undetermined coefficients. The particular solution is A minus 5/17 sine of x right? The second derivative of y minus 3 times the first derivative y minus 4 times the function. Well, have the minus 5A 3B is equal to whatever coefficients in front of sine of x here. Derivative of cosine is minus sine, so minus A sine of x minus B cosine of x. think youre starting to see that the hardest thing in most differential equations problems is not making careless mistakes. inus 4B cosine of x. And when take the sum of all of those thats essentially the lefthand side to this equation when take the sum of all of that, that is equal to 2 sine of x. could have written them out in a line, but it would have just been more confusing. Thats minus 25/3 minus 9/3 A is equal to 10 1/3s. o if add up all the coefficients on the sine of x, get minus A plus 3B minus 4A. A is equal to minus 10/34 or minus 5/17. 5/17 minus 5B is equal to 0. o thats what? And then we get B is equal to 3/17. o we get minus 5A plus 3B is equal to 2. o minus 3A minus 5B cosine of x. The cosine of x should go right here. Thats the minus 34 over 3. so minus 34A is equal to 10. You do that by getting the characteristic equation r squared minus 3r minus 4 is equal to 0. The second derivative is equal to what?",0.3391812865497076
87,628,"The last time spent solving a system of equations dealing with the chilling of this hardboiled egg being put in an ice bath. We called T1 the temperature of the yoke and T2 the temperature of the white. What am going to do is revisit that same system of equations, but basically the topic for today is to learn to solve that system of equations by a completely different method. t is the method that is normally used in practice. Elimination is used mostly by people who have forgotten how to do it any other way. Now, in order to make it a little more general, am not going to use the dependent variables T1 and T2 because they suggest temperature a little too closely. Lets change them to neutral variables. will use x equals T1, and for T2 will just use y. am not going to rederive anything. am not going to resolve anything. am not going to repeat anything of what did last time, except to write down to remind you what the system was in terms of these variables, the system we derived using the particular conductivity constants, two and three, respectively. The system was this one, minus 2x plus 2y. And the y prime was 2x minus 5y. And so we solved this by elimination. We got a single secondorder equation with constant coefficients, which we solved in the usual way. From that derived what the x was, from that we derived what the y was, and then put them all together. will just remind you what the final solution was when written out in terms of arbitrary constants. t was c1 times e to the negative t plus c2 e to the negative 6t, and y was c1 over 2 e to the negative t minus 2c2 e to the negative 6t. That was the solution we got. And then went on to put in initial conditions, but we are not going to explore that aspect of it today. We will in a week or so. This was the general solution because it had two arbitrary constants in it. What want to do now is revisit this and do it by a different method, which makes heavy use of matrices. That is a prerequisite for this course, so am assuming that you reviewed a little bit about matrices. And it is in your book. Your book puts in a nice little review section. Twobytwo and threebythree will be good enough for 18.03 mostly because dont want you to calculate all night on bigger matrices, bigger systems. o nothing serious, matrix multiplication, solving systems of linear equations, endbyend systems. will remind you at the appropriate places today of what it is you need to remember. The very first thing we are going to do is, lets see. havent figured out the color coding for this lecture yet, but lets make this system in green and the solution can be in purple. nvisible purple, but have a lot of it. Lets abbreviate, first of all, the system using matrices. am going to make a column vector out of (x, y). Then you differentiate a column vector by differentiating each component. can write the lefthand side of the system as (x, y) prime. How about the righthand side? Well, say can just write the matrix of coefficients to negative 2, 2, 2, negative 5 times x,y. And say that this matrix equation says exactly the same thing as that green equation and, therefore, it is legitimate to put it up in green, too. The top here is x prime. What is the top here? After multiply these two get a column vector. And what is its top entry? t is negative 2x plus 2y. There it is. And the bottom entry the same way is 2x minus 5y, just as it is down there. Now, what want to do is, well, maybe should translate the solution. What does the solution look like? We got that, too. How am going to write this as a matrix equation? Actually, if told you to use matrices, use vectors, the point at which you might be most hesitant is this one right here, the very next step. Because how you should write it is extremely wellconcealed in this notation. But the point is, this is a column vector and am adding together two column vectors. And what is in each one of the column vectors? Think of these two things as a column vector. Pull out all the scalars from them that you can. Well, you see that c1 is a common factor of both entries and so is e to the negative t, that function. Now, if pull both of those out of the vector, what is left of the vector? Well, you cannot even see it. What is left is a 1 up here and a onehalf there. o am going to write that in the following form. will put out the c1, its the common factor in both, and put that out front. Then will put in the guts of the vector, even though you cannot see it, the column vector 1, onehalf. And then will put the other scalar function in back. The only reason for putting one of these in front and one in back is visual so to make it easy to read. There is no other reason. You could put the c1 here, you could put it here, you could put the e negative t in front if you want to, but people will fire you. Dont do that. Write it the standard way because that is the way that it is easiest to read. The constants out front, the functions behind, and the column vector of numbers in the middle. And so the other one will be written how? Well, here, that one is a little more transparent. c2, 1, 2 and the other thing is e to the negative 6t. There is our solution. That is going to need a lot of purple, but have it. And now want to talk about how the new method of solving the equation. t is based just on the same idea as the way we solve secondorder equations. Yes, question. Oh, here. orry. This should be negative two. Thanks very much. What am going to use is a trial solution. Remember when we had a secondorder equation with constant coefficients the very first thing did was said we are going to try a solution of the form e to the rt. Why that? Well, because Oiler thought of it and it has been known for 200 or 300 years that that is the thing you should do. Well, this has not been known nearly as long because matrices were only invented around 1880 or so, and people did not really use them to solve systems of differential equations until the middle of the last century, 19501960. f you look at books written in 1950, they wont even talk about systems of differential equations, or talk very little anyway and they wont solve them using matrices. This is only 50 years old. mean, my God, in mathematics that is very up to date, particularly elementary mathematics. Anyway, the method of solving is going to use as a trial solution. Now, if you were left to your own devices you might say, well, lets try x equals some constant times e to the lambda1 t and y equals some other constant times e to the lambda2 t. Now, if you try that, it is a sensible thing to try, but it will turn out not to work. And that is the reason have written out this particular solution, so we can see what solutions look like. The essential point is here is the basic solution am trying to find. Here is another one. Their form is a column vector of constants. But they both use the same exponential factor, which is the point. n other words, should not use here, in my trial solution, two different lambdas, should use the same lambda. And so the way to write the trial solution is (x, y) equals two unknown numbers, that or that or whatever, times e to a single unknown exponent factor. Lets call it lambda t. t is called lambda. t is called r. t is called m. have never seen it called anything but one of those three things. am using lambda. Your book uses lambda. t is a common choice. Lets stick with it. Now what is the next step? Well, we plug into the system. ubstitute into the system. What are we going to get? Well, lets do it. First of all, have to differentiate. The lefthand side asks me to differentiate this. How do differentiate this? olumn vector times a function. Well, the column vector acts as a constant. And differentiate that. That is lambda e to the lambda t. o the (x, y) prime is (a1, a2) times e to the lambda t times lambda. Now, it is ugly to put the lambda afterwards because it is a number so you should put it in front, again, to make things easier to read. But this lambda comes from differentiating e to the lambda t and using the chain rule. This much is the lefthand side. That is the derivative (x, y) prime. differentiate the x and differentiated the y. How about the righthand side. Well, the righthand side is negative 2, 2, 2, negative 5 times what? Well, times (x, y), which is (a1, a2) e to the lambda t. Now, the same thing that happened a month or a month and a half ago happens now. The whole point of making that substitution is that the e to the lambda t, the function part of it drops out completely. And one is left with what? An algebraic equation to be solved for lambda a1 and a2. n other words, by means of that substitution, and it basically uses the fact that the coefficients are constant, what you have done is reduced the problem of calculus, of solving differential equations, to solving algebraic equations. n some sense that is the only method there is, unless you do numerical stuff. You reduce the calculus to algebra. The Laplace transform is exactly the same thing. All the work is algebra. You turn the original differential equation into an algebraic equation for Y of s, you solve it, and then you use more algebra to find out what the original little y of t was. t is not different here. o lets solve this system of equations. Now, the whole problem with solving this system, first of all, what is the system? Lets write it out explicitly. Well, it is really two equations, isnt it? The first one says lambda a1 is equal to negative 2 a1 plus 2 a2. That is the first one. The other one says lambda a2 is equal to 2 a1 minus 5 a2. Now, purely, if you want to classify that, that is two equations and three variables, three unknowns. The a1, a2, and lambda are all unknown. And, unfortunately, if you want to classify them correctly, they are nonlinear equations because they are made nonlinear by the fact that you have multiplied two of the variables. Well, if you sit down and try to hack away at solving those without a plan, you are not going to get anywhere. t is going to be a mess. Also, two equations and three unknowns is indeterminate. You can solve three equations and three unknowns and get a definite answer, but two equations and three unknowns usually have an infinity of solutions. Well, at this point it is the only idea that is required. Well, this was a little idea, but assume one would think of that. And the idea that is required here is, think, not so unnatural, it is not to view these a1, a2, and lambda as equal. Not all variables are created equal. ome are more equal than others. a1 and a2 are definitely equal to each other, and lets relegate lambda to the background. n other words, am going to think of lambda as just a parameter. am going to demote it from the status of variable to parameter. f demoted it further it would just be an unknown constant. That is as bad as you can be. am going to focus my attention on the a1, a2 and sort of view the lambda as a nuisance. Now, as soon as do that, see that these equations are linear if just look at them as equations in a1 and a2. And moreover, they are not just linear, they are homogenous. Because if think of lambda just as a parameter, should rewrite the equations this way. am going to subtract this and move the lefthand side to the right side, and it is going to look like (minus 2 minus lambda) times a1 plus 2 a2 is equal to zero. And the same way for the other one. t is going to be 2a1 plus, what is the coefficient, (minus 5 minus lambda) a2 equals zero. That is a pair of simultaneous linear equations for determining a1 and a2, and the coefficients involved are parameter lambda. Now, what is the point of doing that? Well, now the point is whatever you learned about linear equations, you should have learned the most fundamental theorem of linear equations. The main theorem is that you have a square system of homogeneous equations, this is a twobytwo system so it is square, it always has the trivial solution, of course, a1, a2 equals zero. Now, we dont want that trivial solution because if a1 and a2 are zero, then so are x and y zero. Now that is a solution. nfortunately, it is of no interest. f the solution were x, y zero, it corresponds to the fact that this is an ice bath. The yoke is at zero, the white is at zero and it stays that way for all time until the ice melts. o that is the solution we dont want. We dont want the trivial solution. Well, when does it have a nontrivial solution? Nontrivial means nonzero, in other words. f and only if this determinant is zero. n other words, by using that theorem on linear equations, what we find is there is a condition that lambda must satisfy, an equation in lambda in order that we would be able to find nonzero values for a1 and a2. Lets write it out. will recopy it over here. What was it? Negative 2 minus lambda, two, here it was 2 and minus 5 minus lambda. All right. You have to expand the determinant. n other words, we are trying to find out for what values of lambda is this determinant zero. Those will be the good values which lead to nontrivial solutions for the as. This is the equation lambda plus 2. ee, this is minus that and minus that, the product of the two minus ones is plus one. o it is lambda plus 2 times lambda plus 5, which is the product of the two diagonal elements, minus the product of the two antidiagonal elements, which is 4, is equal to zero. And if write that out, what is that, that is the equation lambda squared plus 7 lambda, 5 lambda plus 2 lambda, and then the constant term is 10 minus 4 which is 6. How many of you have long enough memories, twoday memories that you remember that equation? When did the method of elimination, it led to exactly the same equation except it had rs in it instead of lambda. And this equation, therefore, is given the same name and another color. Lets make it salmon. And it is called the characteristic equation for this method. All right. Now am going to use now the word from last time. You factor this. From the factorization we get its root easily enough. The roots are lambda equals negative 1 and lambda equals negative 6 by factoring the equation. Now what am supposed to do? You have to keep the different parts of the method together. Now have found the only values of lambda for which will be able to find nonzero values for the a1 and a2. For each of those values of lambda, now have to find the corresponding a1 and a2. Lets do them one at a time. Lets take first lambda equals negative one. y problem is now to find a1 and a2. Where am going to find them from? Well, from that system of equations over there. will recopy it over here. What is the system? The hardest part of this is dealing with multiple minus signs, but you had experience with that in determinants so you know all about that. n other words, there is the system of equations over there. Lets recopy them here. inus 2, minus minus 1 makes minus 1. Whats the other coefficient? t is just plain old 2. Good. There is my first equation. And when substitute lambda equals negative one for the second equation, what do you get? 2 a1 plus negative 5 minus negative 1 makes negative 4. There is my system that will find me a1 and a2. What is the first thing you notice about it? You immediately notice that this system is fake because this second equation is twice the first one. omething is wrong. No, something is right. f that did not happen, if the second equation were not a constant multiple of the first one then the only solution of the system would be a1 equals zero, a2 equals zero because the determinant of the coefficients would not be zero. The whole function of this exercise was to find the value of lambda, negative 1, for which the system would be redundant and, therefore, would have a nontrivial solution. Do you get that? n other words, calculate the system out, just as have done here, you have an automatic check on the method. f one equation is not a constant multiple of the other you made a mistake. You dont have the right value of lambda or you substituted into the system wrong, which is frankly a more common error. Go back, recheck first the substitution, and if convinced that is right then recheck where you got lambda from. But here everything is going fine so we can now find out what the value of a1 and a2 are. You dont have to go through a big song and dance for this since most of the time you will have twobytwo equations and now and then threebythree. For twobytwo all you do is, since we really have the same equation twice, to get a solution can assign one of the variables any value and then simply solve for the other. The natural thing to do is to make a2 equal one, then wont need fractions and then a1 will be a2. o the solution is (2, 1). am only trying to find one solution. Any constant multiple of this would also be a solution, as long as it wasnt zero, zero which is the trivial one. And, therefore, this is a solution to this system of algebraic equations. And the solution to the whole system of differential equations is, this is only the (a1, a2) part. have to add to it, as a factor, lambda is negative, therefore, e to the minus t. There is our purple thing. ee how got it? tarting with the trial solution, first found out through this procedure what the lambdas have to be. Then took the lambda and found what the corresponding a1 and a2 that went with it and then made up my solution out of that. Now, quickly will do the same thing for lambda equals negative 6. Each one of these must be treated separately. They are separate problems and you are looking for separate solutions. Lambda equals negative 6. What do do? How do my equations look now? Well, the first one is minus 2 minus negative 6 makes plus 4. t is 4a1 plus 2a2 equals zero. Then hold my breath while calculate the second one to see if it comes out to be a constant multiple. get 2a1 plus negative 5 minus negative 6, which makes plus 1. And, indeed, one is a constant multiple of the other. really only have on equation there. will just write down immediately now what the solution is to the system. Well, the (a1, a2) will be what? Now, it is more natural to make a1 equal 1 and then solve to get an integer for a2. f a1 is 1, then a2 is negative 2. And should multiply that by e to the negative 6t because negative 6 is the corresponding value. There is my other one. And now there is a superposition principle, which if get a chance will prove for you at the end of the hour. f not, you will have to do it yourself for homework. ince this is a linear system of equations, once you have two separate solutions, neither a constant multiple of the other, you can multiply each one of these by a constant and it will still be a solution. You can add them together and that will still be a solution, and that gives the general solution. The general solution is the sum of these two, an arbitrary constant. am going to change the name since dont want to confuse it with the c1 used before, times the first solution which is (2, 1) e to the negative t plus c2, another arbitrary constant, times 1 negative 2 e to the minus 6t. Now you notice that is exactly the same solution got before. The only difference is that have renamed the arbitrary constants. The relationship between them, c1 over 2, am now calling c1 tilda, and c2 am calling c2 tilda. f you have an arbitrary constant, it doesnt matter whether you divide it by two. t is still just an arbitrary a constant. t covers all values, in other words. Well, think you will agree that is a different procedure, yet it has only one coincidence. t is like elimination goes this way and comes to the answer. And this method goes a completely different route and comes to the answer, except it is not quite like that. They walk like this and then they come within viewing distance of each other to check that both are using the same characteristic equation, and then they again go their separate ways and end up with the same answer. There is something special of these values. You cannot get away from those two values of lambda. omehow they are really intrinsically connected. Occurs the exponential coefficient, and they are intrinsically connected with the problem of the egg that we started with. Now what would like to do is very quickly sketch how this method looks when remove all the numbers from it. n some sense, it becomes a little clearer what is going on. And that will give me a chance to introduce the terminology that you need when you talk about it. Well, you have notes. Let me try to write it down in general. will first write it out twobytwo. am just going to sketch. The system looks like (x, y) equals, will still put it up in colors. Except now, instead of using twos and fives, will use (a, b; c, d). The trial solution will look how? The trial is going to be (a1, a2). That dont have to change the name of. am going to substitute in, and what the result of substitution is going to be lambda (a1, a2). am going to skip a step and pretend that the e to the lambda ts have already been canceled out. s equal to (a, b; c, d) times (a1, a2). What does that correspond to? That corresponds to the system as wrote it here. And then we wrote it out in terms of two equations. And what was the resulting thing that we ended up with? Well, you write it out, you move the lambda to the other side. And then the homogeneous system is we will look in general how? Well, we could write it out. t is going to look like a minus lambda, b, c, d minus lambda. That is just how it looks there and the general calculation is the same. Times (a1, a2) is equal to zero. This is solvable nontrivially. n other words, it has a nontrivial solution if an only if the determinant of coefficients is zero. Lets now write that out, calculate out once and for all what that determinant is. will write it out here. t is a minus lambda times d minus lambda, the product of the diagonal elements, minus the antidiagonal minus bc is equal to zero. And lets calculate that out. t is lambda squared minus a lambda minus d lambda plus ad, the constant term from here, negative bc from there, plus ad minus bc, where have seen that before? This equation is the general form using letters of what we calculated using the specific numbers before. Again, will code it the same way with that color salmon. Now, most of the calculations will be for twobytwo systems. advise you, in the strongest possible terms, to remember this equation. You could write down this equation immediately for the matrix. You dont have to go through all this stuff. For Gods sakes, dont say let the trial solution be blah, blah, blah. You dont want to do that. dont want you to repeat the derivation of this every time you go through a particular problem. t is just like in solving second order equations. You have a second order equation. You immediately write down its characteristic equation, then you factor it, you find its roots and you construct the solution. t takes a minute. The same thing, this takes a minute, too. What is the constant term? Ad minus bc, what is that? atrix is (a, b; c, d). Ad minus bc is its determinant. This is the determinant of that matrix. didnt give the matrix a name, did ? will now give the matrix a name A. What is this? Well, you are not supposed to know that until now. will tell you. This is called the trace of A. Put that down in your little books. The abbreviation is trace A, and the word is trace. The trace of a square matrix is the sum of the d elements down its main diagonal. f it were a threebythree there would be three terms in whatever you are up to. Here it is a plus b, the sum of the diagonal elements. You can immediately write down this characteristic equation. Lets give it a name. This is a characteristic equation of what? Of the matrix, now. Not of the system, of the matrix. You have a twobytwo matrix. You could immediately write down its characteristic equation. Watch out for this sign, minus. That is a very common error to leave out the minus sign because that is the way the formula comes out. ts roots. f it is a quadratic equation it will have roots; lambda1, lambda2 for the moment lets assume are real and distinct. For the enrichment of your vocabulary, those are called the eigenvalues. They are something which belonged to the matrix A. They are two secret numbers. You can calculate from the coefficients a, b, and c, and d, but they are not in the coefficients. You cannot look at a matrix and see what its eigenvalues are. You have to calculate something. But they are the most important numbers in the matrix. They are hidden, but they are the things that control how this system behaves. Those are called the eigenvalues. Now, there are various purists, there are a fair number of them in the world who do not like this word because it begins German and ends English. Eigenvalues were first introduced by a German mathematician, you know, around the time matrices came into being in 1880 or so. A little while after eigenvalues came into being, too. And since all this happened in Germany they were named eigenvalues in German, which begins eigen and ends value. But people who do not like that call them the characteristic values. nfortunately, it is two words and takes a lot more space to write out. An older generation even calls them something different, which you are not so likely to see nowadays, but you will in slightly older books. You can also call them the proper values. haracteristic is not a translation of eigen, but proper is, but it means it in a funny sense which has almost disappeared nowadays. t means proper in the sense of belong to. The only example can think of is the word property. Property is something that belongs to you. That is the use of the word proper. t is something that belongs to the matrix. The matrix has its proper values. t does not mean proper in the sense of fitting and proper or hope you will behave properly when we go to Aunt Agathas or something like that. But, as say, by far the most popular thing, slowly the word eigenvalue is pretty much taking over the literature. Just because its just one word, that is a tremendous advantage. Okay. What now is still to be done? Well, there are those vectors to be found. o the very last step would be to solve the system to find the vectors a1 and a2. For each (lambda)i, find the associated vector. The vector, we will call it (alpha)i. That is the a1 and a2. Of course its going to be indexed. You have to put another subscript on it because there are two of them. And a1 and a2 is stretched a little too far. By solving the system, and the system will be the system which will write this way, (a minus lambda, b, c, d minus lambda). t is just that system that was over there, but will recopy it, (a1, a2) equals zero, zero. And these are called the eigenvectors. Each of these is called the eigenvector associated with or belonging to, again, in that sense of property. Eigenvector, lets say belonging to, see that a little more frequently, belonging to lambda i. o we have the eigenvalues, the eigenvectors and, of course, the people who call them characteristic values also call these guys characteristic vectors. dont think have ever seen proper vectors, but that is because am not old enough. think that is what they used to be called a long time ago, but not anymore. And then, finally, the general solution will be, by the superposition principle, (x, y) equals the arbitrary constant times the first eigenvector times the eigenvalue times the e to the corresponding eigenvalue. And then the same thing for the second one, (a1, a2), but now the second index will be 2 to indicate that it goes with the eigenvalue e to the lambda 2t. have done that twice. And now in the remaining five minutes will do it a third time because it is possible to write this in still a more condensed form. And the advantage of the more condensed form is A, it takes only that much space to write, and B, it applies to systems, not just the twobytwo systems, but to endbyend systems. The method is exactly the same. Lets write it out as it would apply to endbyend systems. The vector started with is (x, y) and so on, but will simply abbreviate this, as is done in 18.02, by x with an arrow over it. The matrix A will abbreviate with A, as did before with capital A. And then the system looks like x prime is equal to x prime is what? Ax. That is all there is to it. There is our green system. Now notice in this form did not even tell you whether this a twobytwo matrix or an endbyend. And in this condensed form it will look the same no matter how many equations you have. Your book deals from the beginning with endbyend systems. That is, in my view, one of its weaknesses because dont think most students start with twobytwo. Fortunately, the book doubletalks. The theory is endbyend, but all the examples are twobytwo. o just read the examples. Read the notes instead, which just do twobytwo to start out with. The trial solution is x equals what? An unknown vector alpha times e to the lambda t. Alpha is what we called a1 and a2 before. Plug this into there and cancel the e to the lambda ts. What do you get? Well, this is lambda alpha e to the lambda t equals A alpha e to the lambda t. These two cancel. And the system to be solved, A alpha equals lambda alpha. And now the question is how do you solve that system? Well, you can tell if a book is written by a scoundrel or not by how they go A book, which is in my opinion completely scoundrel, simply says you subtract one from the other, and without further ado writes A minus lambda, and they tuck a little in there and write alpha equals zero. Why is the put in there? Well, this is what you would like to write. What is wrong with this equation? This is not a valid matrix equation because that is a square endbyend matrix, a square twobytwo matrix if you like. This is a scalar. You cannot subtract the scalar from a matrix. t is not an operation. To subtract matrices they have to be the same size, the same shape. What is done is you make this a twobytwo matrix. This is a twobytwo matrix with lambdas down the main diagonal and elsewhere. And the justification is that lambda alpha is the same thing as the lambda times alpha because is an identity matrix. Now, in fact, jumping from here to here is not something that would occur to anybody. The way it should occur to you to do this is you do this, you write that, you realize it doesnt work, and then you say to yourself dont understand what these matrices are all about. think d better write it all out. And then you would write it all out and you would write that equation on the lefthand board there. Oh, now see what it should look like. should subtract lambda from the main diagonal. That is the way it will come out. And then say, hey, the way to save lambda from the main diagonal is put it in an identity matrix. That will do it for me. n other words, there is a little detour that goes from here to here. And one of the ways judge books is by how well they explain the passage from this to that. f they dont explain it at all and just write it down, they have never talked to students. They have just written books. Where did we get finally here? The characteristic equation from that, had forgotten what color. That is in salmon. The characteristic equation, then, is going to be the thing which says that the determinant of that is zero. That is the circumstances under which it is solvable. n general, this is the way the characteristic equation looks. And its roots, once again, are the eigenvalues. And from then you calculate the corresponding eigenvectors. Okay. Go home and practice. n recitation you will practice on both twobytwo and threebythree cases, and we will talk more next time.","The characteristic equation, then, is going to be the thing which says that the determinant of that is zero. And the solution to the whole system of differential equations is, this is only the (a1, a2) part. And if write that out, what is that, that is the equation lambda squared plus 7 lambda, 5 lambda plus 2 lambda, and then the constant term is 10 minus 4 which is 6. That is all there is to it. am going to subtract this and move the lefthand side to the right side, and it is going to look like (minus 2 minus lambda) times a1 plus 2 a2 is equal to zero. This is the equation lambda plus 2. ee, this is minus that and minus that, the product of the two minus ones is plus one. The way it should occur to you to do this is you do this, you write that, you realize it doesnt work, and then you say to yourself dont understand what these matrices are all about. will just write down immediately now what the solution is to the system. The vector, we will call it (alpha)i. That is the a1 and a2. And then the same thing for the second one, (a1, a2), but now the second index will be 2 to indicate that it goes with the eigenvalue e to the lambda 2t. That is lambda e to the lambda t. o the (x, y) prime is (a1, a2) times e to the lambda t times lambda. The main theorem is that you have a square system of homogeneous equations, this is a twobytwo system so it is square, it always has the trivial solution, of course, a1, a2 equals zero. And the idea that is required here is, think, not so unnatural, it is not to view these a1, a2, and lambda as equal. Well, you write it out, you move the lambda to the other side. am going to change the name since dont want to confuse it with the c1 used before, times the first solution which is (2, 1) e to the negative t plus c2, another arbitrary constant, times 1 negative 2 e to the minus 6t. For twobytwo all you do is, since we really have the same equation twice, to get a solution can assign one of the variables any value and then simply solve for the other. f that did not happen, if the second equation were not a constant multiple of the first one then the only solution of the system would be a1 equals zero, a2 equals zero because the determinant of the coefficients would not be zero. This is the determinant of that matrix. And now the question is how do you solve that system? What is the system? What am going to do is revisit that same system of equations, but basically the topic for today is to learn to solve that system of equations by a completely different method. am going to substitute in, and what the result of substitution is going to be lambda (a1, a2). Then took the lambda and found what the corresponding a1 and a2 that went with it and then made up my solution out of that. That is just how it looks there and the general calculation is the same. o the solution is (2, 1). By solving the system, and the system will be the system which will write this way, (a minus lambda, b, c, d minus lambda). n other words, by using that theorem on linear equations, what we find is there is a condition that lambda must satisfy, an equation in lambda in order that we would be able to find nonzero values for a1 and a2. n other words, by means of that substitution, and it basically uses the fact that the coefficients are constant, what you have done is reduced the problem of calculus, of solving differential equations, to solving algebraic equations. o it is lambda plus 2 times lambda plus 5, which is the product of the two diagonal elements, minus the product of the two antidiagonal elements, which is 4, is equal to zero. am not going to repeat anything of what did last time, except to write down to remind you what the system was in terms of these variables, the system we derived using the particular conductivity constants, two and three, respectively. The whole point of making that substitution is that the e to the lambda t, the function part of it drops out completely. ince this is a linear system of equations, once you have two separate solutions, neither a constant multiple of the other, you can multiply each one of these by a constant and it will still be a solution. And the justification is that lambda alpha is the same thing as the lambda times alpha because is an identity matrix. have to add to it, as a factor, lambda is negative, therefore, e to the minus t. There is our purple thing. Not of the system, of the matrix. Well, you see that c1 is a common factor of both entries and so is e to the negative t, that function. That is the first one. And the advantage of the more condensed form is A, it takes only that much space to write, and B, it applies to systems, not just the twobytwo systems, but to endbyend systems. Well, this is what you would like to write. For each of those values of lambda, now have to find the corresponding a1 and a2. Write it the standard way because that is the way that it is easiest to read. The whole function of this exercise was to find the value of lambda, negative 1, for which the system would be redundant and, therefore, would have a nontrivial solution. And it is called the characteristic equation for this method. n other words, we are trying to find out for what values of lambda is this determinant zero. Now, the whole problem with solving this system, first of all, what is the system? But here everything is going fine so we can now find out what the value of a1 and a2 are. Now, if you were left to your own devices you might say, well, lets try x equals some constant times e to the lambda1 t and y equals some other constant times e to the lambda2 t. Now, if you try that, it is a sensible thing to try, but it will turn out not to work. t is going to be 2a1 plus, what is the coefficient, (minus 5 minus lambda) a2 equals zero. The trial is going to be (a1, a2). Now have found the only values of lambda for which will be able to find nonzero values for the a1 and a2. And what is in each one of the column vectors? And say that this matrix equation says exactly the same thing as that green equation and, therefore, it is legitimate to put it up in green, too. o the very last step would be to solve the system to find the vectors a1 and a2. And then you would write it all out and you would write that equation on the lefthand board there. You turn the original differential equation into an algebraic equation for Y of s, you solve it, and then you use more algebra to find out what the original little y of t was. And one of the ways judge books is by how well they explain the passage from this to that. Lets now write that out, calculate out once and for all what that determinant is. And, therefore, this is a solution to this system of algebraic equations. That is the use of the word proper. The very first thing we are going to do is, lets see. You dont have to go through a big song and dance for this since most of the time you will have twobytwo equations and now and then threebythree. The other one says lambda a2 is equal to 2 a1 minus 5 a2. c2, 1, 2 and the other thing is e to the negative 6t. Now, it is ugly to put the lambda afterwards because it is a number so you should put it in front, again, to make things easier to read. That is the way it will come out. You immediately notice that this system is fake because this second equation is twice the first one. o that is the solution we dont want. Now that is a solution. t is just that system that was over there, but will recopy it, (a1, a2) equals zero, zero. That is a very common error to leave out the minus sign because that is the way the formula comes out. And when substitute lambda equals negative one for the second equation, what do you get? And that is the reason have written out this particular solution, so we can see what solutions look like. And the same way for the other one. Well, this is lambda alpha e to the lambda t equals A alpha e to the lambda t. These two cancel. The first one says lambda a1 is equal to negative 2 a1 plus 2 a2. And the bottom entry the same way is 2x minus 5y, just as it is down there. Well, you can tell if a book is written by a scoundrel or not by how they go A book, which is in my opinion completely scoundrel, simply says you subtract one from the other, and without further ado writes A minus lambda, and they tuck a little in there and write alpha equals zero.",0.1342281879194631
88,629,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: o were going on to the third unit here. o were getting started with nit 3. And this is our intro to integration. ts basically the second half of calculus after differentiation. Today what ll talk about is what are known as definite integrals. Actually, it looks like, are we missing a bunch of overhead lights? s there a reason for that? Hmm. Lets see. Aah. All right. OK, thats a little brighter now. All right. o the idea of definite integrals can be presented in a number of ways. But will be consistent with the rest of the presentation in the course. Were going to start with the geometric point of view. And the geometric point of view is, the problem we want to solve is to find the area under a curve. The other point of view that one can take, and well mention that at the end of this lecture, is the idea of a cumulative sum. o keep that in mind that theres a lot going on here. And there are many different interpretations of what the integral is. Now, so lets draw a picture here. ll start at a place a and end at a place b. And have some curve here. And what have in mind is to find this area here. And, of course, in order to do that, need more information than just where we start and where we end. also need the bottom and the top. By convention, the bottom is the x axis and the top is the curve that weve specified, which is y = f(x). And we have a notation for this, which is the notation using calculus for this as opposed to some geometric notation. And thats the following expression. ts called an integral, but now its going to have what are known as limits on it. t will start at a and end at b. And we write in the function f(x) dx. o this is whats known as a definite integral. And its interpreted geometrically as the area under the curve. The only difference between this collection of symbols and what we had before with indefinite integrals is that before we didnt specify where it started and where it ended. Now, in order to understand what to do with this guy, m going to just describe very abstractly what we do. And then carry out one example in detail. o, to compute this area, were going to follow initially three steps. First of all, were going to divide into rectangles. And unfortunately, because its impossible to divide a curvy region into rectangles, were going to cheat. o theyre only quoteunquote rectangles. Theyre almost rectangles. And the second thing were going to do is to add up the areas. And the third thing were going to do is to rectify this problem that we didnt actually hit the answer on the nose. That we were missing some pieces or were choosing some extra bits. And the way well rectify that is by taking the limit as the rectangles get thin. nfinitesimally thin, very thin. Pictorially, again, that looks like this. We have a and our b, and we have our guy here, this is our curve. And m going to chop it up. First m going to chop up the x axis into little increments. And then m going to chop things up here. And ll decide on some rectangle, maybe some staircase pattern here. Like this. Now, dont care so much. n some cases the rectangles overshoot; in some cases theyre underneath. o the new area that m adding up is off. ts not quite the same as the area under the curve. ts this region here. But it includes these extra bits here. And then its missing this little guy here. This little bit there is missing. And, as say, these little pieces up here, this a little bit up here is extra. o thats why were not really dividing up the region into rectangles. Were just taking rectangles. And then the idea is that as these get thinner and thinner, the little ittybitty amounts that we miss by are going to 0. And theyre going to be negligible. Already, you can see its kind of a thin piece of area, so were not missing by much. And as these get thinner and thinner, the problem goes away and we get the answer on the nose in the limit. o heres our first example. ll take the first interesting curve, which is f(x) = x^2. dont want to do anything more complicated than one example, because this is a real labor here, what were going to go through. And to make things easier for myself, m going to start at a = 0. But in order to see what the pattern is, m going to allow b to be arbitrary. Lets draw the graph and start breaking things up. o heres the parabola, and theres this piece that we want, which is going to stop at this place, b, here. And the first step is to divide into n pieces. That means, well, graphically, ll just mark the first three. And maybe there are going to be many of them. And then ll draw some rectangles here, and m going to choose to make the rectangles all the way from the right. That is, ll make us this staircase pattern here, like this. Thats my choice. get to choose whatever level want, and m going to choose the right ends as the shape of the staircase. o m overshooting with each rectangle. And now have to write down formulas for what these areas are. Now, theres one big advantage that rectangles have. And this is the starting place. Which is that its easy to find their areas. All you need to know is the base and the height, and you multiply, and you get the area. Thats the reason why we can get started with rectangles. And in this case, these distances, m assuming that theyre all equal, equally spaced, intervals. And ll always be doing that. And so the spacing, the bases, the base length, is always b/n. All equal intervals. o thats the base length. And next, need the heights. And in order to keep track of the heights, m going to draw a little table here, with x and f(x), and plug in a few values just to see what the pattern is. The first place here, after 0, is b/n. o heres b/n, thats an xvalue. And the f(x) value is the height there. And thats just, evaluate f(x), f(x) is x^2. And thats (b/n)^2. And similarly, the next one is 2b/n. And the value here is (2b/n)^2. Thats this. This height here is 2b/n. Thats the second rectangle. And ll write down one more. 3b/n, thats the third one. And the height is (3b/n)^2. And so forth. Well, my next job is to add up these areas. And ve already prepared that by finding out what the base and the height is. o the total area, or the sum of the areas, lets say, of these rectangles, is Well, the first one is (b/n) (b/n)^2. The second one is 2b/n m sorry, is (b/n) (2b/n)^2. And it just keeps on going. And the last one is (b/n) (nb / n)^2. o its very important to figure out what the general formula is. And here we have a base. And here we have a height, and here we have the same kind of base, but we have a new height. And so forth. And the pattern is that the coefficient here is 1, then 2, then 3, all the way up to n. The rectangles are getting taller and taller, and this one, the last one is the biggest. OK, this is a very complicated gadget. and the first thing want to do is simplify it and then m actually going to evaluate it. But actually m not going to evaluate it exactly. m just going to evaluate the limit. Turns out, limits are always easier. The point about calculus here is that these rectangles are hard. But the limiting value is an easy value. o what were heading for is the simple formula, as opposed to the complicated one. Alright, so the first thing m going to do is factor out all these b/n factors. Theres a b/n, here, and theres a (b/n)^2, o all told, we have a (b/n)^3. As a common factor. And then the first term is 1, and the second term, whats left over, is 2^2. 2^2. And then the third term would be 3^2, although havent written it. n the last term, theres an extra factor of n^2. n the numerator. OK, is everybody with me here? Now, what d like to do is to eventually take the limit as n goes to infinity here. And the quantity thats hard to understand is this massive quantity here. And theres one change that d like to make, but its a very modest one. Extremely minuscule. Which is that m going to write 1, just to see that theres a general pattern here. Going to write 1 as 1^2. And lets put in the 3 here, why not. And now want to use a trick. This trick is not completely recommended, but will say a lot more about that when we get through to the end. want to understand how big this quantity is. o m going to use a geometric trick to draw a picture of this quantity. Namely, m going to build a pyramid. And the base of the pyramid is going to be n by n blocks. o imagine were in Egypt and were building a pyramid. And the next layer is going to be n1 by n1. o this next layer in is n1 by n1. o the total number of blocks on the bottom is n squared. Thats this rightmost term here. But the next term, which didnt write in but maybe should, the nexttothelast term was this one. And thats the second layer that ve put on. Now, this is, if you like, the top view. But perhaps we should also think in terms of a side view. o heres the same picture, were starting at n and we build up this layer here. And now were going to put a layer on top of it, which is a little shorter. o the first layer is of length n. And the second layers is of length n1, and then on top of that we have something of length n2, and so forth. And were going to pile them up. o we pile them up. All the way to the top, which is just one giant block of stone. And thats this last one, 1^2. o were going backwards in the sum. And so have to build this whole thing up. And get all the way up in this staircase pattern to this top block, up there. o heres the trick that you can use to estimate the size of this, and its sufficient in the limit as n goes to infinity. The trick is that can imagine the solid thing underneath the staircase, like this. Thats an ordinary pyramid, not a staircase pyramid. Which is inside. And this one is inside. And so, but its an ordinary pyramid as opposed to a staircase pyramid. And so, we know the formula for the volume of that. Because we know the formula for volumes of cones. And the formula for the volume of this guy, of the inside, is 1/3 base times height. And in that case, the base here so thats 1/3, and the base is n by n, right? o the base is n^2. Thats the base. And the height, it goes all the way to the top point. o the height is n. And what weve discovered here is that this whole sum is bigger than 1/3 n^3. Now, claimed that this line, by the way has slope 2. o you go 1/2 over each time you go up 1. And thats why you get to the top. On the other hand, can trap it on the outside, too, by drawing a parallel line out here. And this will go down 1/2 more on this side and 1/2 more on the other side. o the base will be n+1 by n+1 of this bigger pyramid. And itll go up 1 higher. o on the other end, we get that this is less than 1/3 (n+1)^3. Again, (n+1)^2 times n+1, again, this is a base times a height. Of this bigger pyramid. Yes, question. TDENT: and then equating it to volume. PROFEOR: The question is, it seems as if m adding up areas and equating it to volume. But m actually creating volumes by making these honest increments here. That is, the base is n but the height is 1. Thank you for pointing that out. Each one of these little staircases here has exactly height 1. o m honestly sticking blocks there. Theyre sort of square blocks, and m lining them up. And m thinking of n by n cubes, if you like. Honest cubes, there. And the height is 1. And the base is n^2. Alright, so claim that ve trapped this guy in between two quantities here. And now m ready to take the limit. f you look at what our goal is, we want to have an expression like this. And m going to This was the massive expression that we had. And actually, m going to write it differently. ll write it as b^3 times 1^2 plus 2^2 plus... plus n^2, divided by n^3. m going to combine all the ns together. Alright, so the right thing to do is to divide what had up there. Divide by n^3 in this set of inequalities there. And what get here is 1/3 is less than 1 plus 2^2 plus 3^2 plus... plus n^2 divided by n^3 is less than 1/3 (n+1)^3 / n^3. And thats 1/3 + (1 + 1/n)^3. And now, claim were done. Because this is 1/3, and the limit, as n goes to infinity, of this quantity here, is easily seen to be, this is, as n goes to infinity, this goes to 0. o this also goes to 1/3. And so our total here, so our total area, under x^2, which we sometimes might write the integral from 0 to b x^2 dx, is going to be equal to well, its this 1/3 that ve got. But then there was also a b^3 there. o theres this extra b cubed here. o its 1/3 b^3. Thats the result from this whole computation. Yes, question. TDENT: PROFEOR: o that was a very good question. The question is, why did we leave the b/n^3 out, for this step. And a part of the answer is malice aforethought. n other words, we know what were heading for. We know, we understand, this quantity. ts all one thing. This thing is a sum, which is growing larger and larger. ts not whats called a closed form. o, the thing thats not known, or not well understood, is how big is this quantity here. 1^2 + 2^2, the sum of the squares. Whereas, this is something thats quite easy to understand. o we factor it out. And we analyze carefully the piece which we dont know yet, how big it is. And we discovered that its very, very similar to n^3. But its more similar to 1/3 n^3. ts almost identical to 1/3 n^3. This extra piece here. o thats whats going on. And then we match that. ince this thing is very similar to 1/3 n^3 we cancel the n^3s and we have our result. Let me just mention that although this may seem odd, in fact this is what you always do if you analyze these kinds of sum. You always factor out whatever you can. And then you still are faced with a sum like this. o this will happen systematically, every time youre faced with such a sum. OK, now want to say one more word about notation. Which is that this notation is an extreme nuisance here. And its really sort of too large for us to deal with. And so, mathematicians have a shorthand for it. nfortunately, when you actually do a computation, youre going to end up with this collection of stuff anyway. But want to just show you this summation notation in order to compress it a little bit. The idea of summation notation is the following. o this thing tends The ideas are the following. ll illustrate it with an example first. o, the general notation is the sum of a_i, i = 1 to n, is a_1 plus a_2 plus dot dot dot plus a_n. o this is the abbreviation. And this is a capital sigma. And so, this quantity here, for instance, is 1/n^3 times the sum i^2, i = 1 to n. o thats what this thing is equal to. And what we just showed is that that tends to 1/3 as n goes to infinity. o this is the way the summation notation is used. Theres a formula for each of these coefficients, each of these entries here, or summands. And then this is just an abbreviation for what the sum is. And this is the reason why stuck in that 1^2 at the beginning, so that you could see that the pattern worked all the way down to i = 1. t isnt an exception to the rule. ts the same as all of the others. Now, over here, in this board, we also had one of these extremely long sums. And this one can be written in the following way. And hope you agree, this is rather hard to scan. But one way of writing it is, its the sum from i = 1 to n of now have to write down the formula for the general term. Which is b/n (ib/n)^2. o thats a way of abbreviating this massive formula into one which is just a lot shorter. And now, the manipulation that performed with it, which is to factor out this (b/n)^3, is something that m perfectly well allowed to do also over here. This is the distributive law. This, if factor out b^3 / n^3, m left with the sum, i = 1 to n, of i^2, right? o these notations make it a little bit more compact. What were dealing with. The conceptual phenomenon is still the same. And the mess is really still just hiding under the rug. But the notation is at least fits with fewer symbols, anyway. o lets continue here. ve given you one calculation. And now want to fit it into a pattern. And heres the thing that d like to calculate. o, first of all lets try the case m going to do two more examples. ll do two more examples, but theyre going to be much, much easier. And then things are going to get much easier from now on. o, the second example is going to be the function f(x) = x. f draw that, thats this function here, thats the line with slope 1. And heres b. And so this area here is the same as the area of the triangle with base b and height b. o the area is equal to 1/2 b * b, so this is the base. And this is the height. We also know how to find the area of triangles. And so, the formula is 1/2 b^2. And the third example Notice, by the way, didnt have to do this elaborate summing to do that, because we happen to know this area. The third example is going to be even easier. f(x) = 1. By far the most important example. Remarkably, when you get to 18.02, multivariable calculus, you will forget this calculation. omehow. And dont know why, but it happens to everybody. o, the function is just horizontal, like this. Right? ts the constant 1. And if we stop it at b, then the area were interested in is just this, from 0 to b. And we know that this is height 1, so this is area is base, which is b, times 1. o its b. Lets look now at the pattern. Were going to look at the pattern of the function, and its the area under the curve, which is this has this elaborate formula in terms of so this is just the area under the curve. Between 0 and b. And we have x^2, which turned out to be b^3 / 3. And we have x, which turned out to be well, let me write them over just a bit more to give myself some room. x, which turns out to be b^2 / 2. And then we have 1, which turned out to be b. o this, claim, is suggestive. f you can figure out the pattern, one way of making it a little clearer is to see that x is x^1. And 1 is x^0. o this is the case, 0, 1 and 2. And b is b^1 / 1. o, if you want to guess what happens when f(x) is x^3, well if its 0, you do b^1 / 1; if its 1, you do b^2 / 2; if its 2, you do b^3 / 3. o its reasonable to guess that this should be b^4 / 4. Thats a reasonable guess, would say. Now, the strange thing is that in history, Archimedes figured out the area under a parabola. o that was a long time ago. t was after the pyramids. And he used, actually, a much more complicated method than just described here. And his method, which is just fantastically amazing, was so brilliant that it may have set back mathematics by 2,000 years. Because people were so it was so difficult that people couldnt see this pattern. And couldnt see that, actually, these kinds of calculations are easy. o they couldnt get to the cubic. And even when they got to the cubic, they were struggling with everything else. And it wasnt until calculus fit everything together that people were able to make serious progress on calculating these areas. Even though he was the expert on calculating areas and volumes, for his time. o this is really a great thing that we now can have easy methods of doing it. And the main thing that want to tell you is thats we will not have to labor to build pyramids to calculate all of these quantities. We will have a way faster way of doing it. This is the slow, laborious way. And we will be able to do it so easily that it will happen as fast as you differentiate. o thats coming up tomorrow. But want you to know that its going to be However, were going to go through just a little pain before we do it. And ll just tell you one more piece of notation here. o you need to have a little practice just to recognize how much savings were going to make. But never again will you have to face elaborate geometric arguments like this. o let me just add a little bit of notation for definite integrals. And this goes under the name of Riemann sums. Named after a mathematician from the 1800s. o this is the general procedure for definite integrals. We divide it up into pieces. And how do we do that? Well, so heres our a and heres our b. And what were going to do is break it up into little pieces. And were going to give a name to the increment. And were going to call that delta x. o we divide up into these. o how many pieces are there? f there are n pieces, then the general formula is always the delta x is 1/n times the total length. o it has to be (ba) / n. We will always use these equal increments, although you dont absolutely have to do it. We will, for these Riemann sums. And now theres only one bit of flexibility that we will allow ourselves. Which is this. Were going to pick any height of f between in the interval, in each interval. o what that means is, let me just show it to you on the picture here. s, just pick any value in between, ll call it c_i, which is in there. And then go up here. And have the level, which is f(c_i). And thats the rectangle that choose. n the case that we did, we always chose the righthand, which turned out to be the largest one. But couldve chosen some level in between. Or even the lefthand end. Which would have meant that the staircase wouldve been quite a bit lower. o any of these staircases will work perfectly well. o that means were picking f(c_i), and thats a height. And now were just going to add them all up. And this is the sum of the areas of the rectangles, because this is the height. And this is the base. This notation is supposed to be, now, very suggestive of the notation that Leibniz used. Which is that in the limit, this becomes an integral from a to b of f(x) dx. And notice that the delta x gets replaced by a dx. o this is what happens in the limit. As the rectangles get thin. o thats as delta x goes to 0. And these gadgets are called Riemann sums. This is called a Riemann sum. And we already worked out an example. This very complicated guy was an example of a Riemann sum. o thats a notation. And well give you a chance to get used to it a little more when we do some numerical work at the end. Now, the last thing for today is, promised you an example which was not an area example. want to be able to show you that integrals can be interpreted as cumulative sums. ntegrals as cumulative sums. o this is just an example. And, so heres the way it goes. o were going to consider a function f, were going to consider a variable t, which is time. n years. And well consider a function f(t), which is in dollars per year. Right, this is a financial example here. Thats the unit here, dollars per year. And this is going to be a borrowing rate. Now, the reason why want to put units in here is to show you that theres a good reason for this strange dx, which we append on these integrals. This notation. t allows us to change variables, it allows this to be consistent with units. And allows us to develop meaningful formulas, which are consistent across the board. And so want to emphasize the units in this when set up this modeling problem here. Now, youre borrowing money, lets say, every day. o that means delta t = 1/365. Thats almost 1 / infinity, from the point of view of various purposes. o this is how much youre borrowing. n each time increment youre borrowing. And lets say that you borrow your rate varies over the year. mean, sometimes you need more money sometimes you need less. ertainly any business would be that way. And so here you are, youve got your money. And youre borrowing but the rate is varying. And so how much did you borrow? Well, in day 45, which corresponds to t is 45/365, you borrowed the following amount. Here was your borrowing rate times this quantity. o, dollars per year. And so this is, if you like want to emphasize the scaling that comes about here. You have dollars per year. And this is this number of years. o that comes out to be in dollars. This final amount. This is the amount that you actually borrow. o you borrow this amount. And now, if want to add up how much you get youve borrowed in the entire year. Thats this sum. i = 1 to 365 of f of, well, its (i / 365) delta t. Which ll just leave as delta t here. This is total amount borrowed. This is kind of a messy sum. n fact, your bank probably will keep track of it and they know how to do that. But when were modeling things with strategies, you know, trading strategies, of course, youre really some kind of financial engineer and you want to cleverly optimize how much you borrow. And how much you spend, and how much you invest. This is going to be very, very similar to the integral from 0 to 1 of f(t) dt. At the scale of 1/35, its probably 365, its probably enough for many purposes. Now, however, theres another thing that you would want to model. Which is equally important. This is how much you borrowed, but theres also how much you owe the back at the end of the year. And the amount that you owe the bank at the end of the year, m going to do it in a fancy way. ts, the interest, well say, is compounded continuously. o the interest rate, if you start out with P as your principal, then after time t you owe o borrow P, after time t, you owe P e^(rt), where r is your interest rate. ay .05 per year. That would be an example of an interest rate. And so, if you want to understand how much money you actually owe at the end of the year. At the end of the year what you owe is, well, you borrowed these amounts here. But now you owe more at the end of the year. You owe e^r times the amount of time left in the year. o the amount of time left in the year is 1 i / 365. Or 365 i days left. o this is 1 i / 365. And this is what you have to add up, to see how much you owe. And that is essentially the integral from 0 to 1. The delta t comes out. And you have here e^(r(1t)), so the t is replacing this i / 365, f(t) dt. And so when you start computing and thinking about whats the right strategy, youre faced with integrals of this type. o thats just an example. And see you next time. Remember to think about questions that youll ask next time.","And if we stop it at b, then the area were interested in is just this, from 0 to b. And we know that this is height 1, so this is area is base, which is b, times 1. o its b. Lets look now at the pattern. And this is the sum of the areas of the rectangles, because this is the height. And heres b. And so this area here is the same as the area of the triangle with base b and height b. o the area is equal to 1/2 b * b, so this is the base. And this is the base. And this is the height. Were going to look at the pattern of the function, and its the area under the curve, which is this has this elaborate formula in terms of so this is just the area under the curve. And the pattern is that the coefficient here is 1, then 2, then 3, all the way up to n. The rectangles are getting taller and taller, and this one, the last one is the biggest. And so, this quantity here, for instance, is 1/n^3 times the sum i^2, i = 1 to n. o thats what this thing is equal to. That is, the base is n but the height is 1. And in that case, the base here so thats 1/3, and the base is n by n, right? All you need to know is the base and the height, and you multiply, and you get the area. And the amount that you owe the bank at the end of the year, m going to do it in a fancy way. And 1 is x^0. And the base of the pyramid is going to be n by n blocks. And the second thing were going to do is to add up the areas. But one way of writing it is, its the sum from i = 1 to n of now have to write down the formula for the general term. And the base is n^2. And this is the reason why stuck in that 1^2 at the beginning, so that you could see that the pattern worked all the way down to i = 1. t isnt an exception to the rule. And the height is 1. And then ll draw some rectangles here, and m going to choose to make the rectangles all the way from the right. o the total area, or the sum of the areas, lets say, of these rectangles, is Well, the first one is (b/n) (b/n)^2. o heres the parabola, and theres this piece that we want, which is going to stop at this place, b, here. And the third thing were going to do is to rectify this problem that we didnt actually hit the answer on the nose. And the formula for the volume of this guy, of the inside, is 1/3 base times height. o this is the case, 0, 1 and 2. o, the second example is going to be the function f(x) = x. f draw that, thats this function here, thats the line with slope 1. And the main thing that want to tell you is thats we will not have to labor to build pyramids to calculate all of these quantities. And then this is just an abbreviation for what the sum is. And in order to keep track of the heights, m going to draw a little table here, with x and f(x), and plug in a few values just to see what the pattern is. And now, the manipulation that performed with it, which is to factor out this (b/n)^3, is something that m perfectly well allowed to do also over here. And this is what you have to add up, to see how much you owe. Because this is 1/3, and the limit, as n goes to infinity, of this quantity here, is easily seen to be, this is, as n goes to infinity, this goes to 0. o this also goes to 1/3. and the first thing want to do is simplify it and then m actually going to evaluate it. And the geometric point of view is, the problem we want to solve is to find the area under a curve. o heres the trick that you can use to estimate the size of this, and its sufficient in the limit as n goes to infinity. And the height, it goes all the way to the top point. And so, we know the formula for the volume of that. And the third example Notice, by the way, didnt have to do this elaborate summing to do that, because we happen to know this area. The other point of view that one can take, and well mention that at the end of this lecture, is the idea of a cumulative sum. And so, the formula is 1/2 b^2. And so this is, if you like want to emphasize the scaling that comes about here. And so our total here, so our total area, under x^2, which we sometimes might write the integral from 0 to b x^2 dx, is going to be equal to well, its this 1/3 that ve got. And this is going to be a borrowing rate. And so, if you want to understand how much money you actually owe at the end of the year. And the f(x) value is the height there. By convention, the bottom is the x axis and the top is the curve that weve specified, which is y = f(x). Which is that m going to write 1, just to see that theres a general pattern here. o the height is n. And what weve discovered here is that this whole sum is bigger than 1/3 n^3. f you can figure out the pattern, one way of making it a little clearer is to see that x is x^1. And the way well rectify that is by taking the limit as the rectangles get thin. This is going to be very, very similar to the integral from 0 to 1 of f(t) dt. And thats why you get to the top. And get all the way up in this staircase pattern to this top block, up there. And then the idea is that as these get thinner and thinner, the little ittybitty amounts that we miss by are going to 0. This is how much you borrowed, but theres also how much you owe the back at the end of the year. Which is that in the limit, this becomes an integral from a to b of f(x) dx. But want you to know that its going to be However, were going to go through just a little pain before we do it. And what have in mind is to find this area here. o the first layer is of length n. And the second layers is of length n1, and then on top of that we have something of length n2, and so forth. At the end of the year what you owe is, well, you borrowed these amounts here. Which is this. And m going to This was the massive expression that we had. o this is the way the summation notation is used.",0.1196039603960396
89,630,"The following content is provided under a reative ommons license. Your support will help T OpenourseWare continue to offer high quality educational resources for free. To make a donation, or view additional materials from hundreds of T courses, visit T OpenourseWare at ocw.mit.edu. PROFEOR: o you guys know the quiz is cumulative, right? Everything all the way back from lecture one, so would look at all the lectures and all the P sets, and look at all the stuff that we taught you, so data structures, algorithms, everything. And at least be able to know, for every one of them, whats the name, what it does, and wants the running time. Proofs and how it does it might be harder, but these be able to call it as a black box and argue about the running times. o have a dp problem, and have a nondp problem. Which problem would you like me to start with? OK. Do you guys know the saying, if a woodchucker would chuck wood, how much wood would a woodchucker chuck? Today were going to chuck wood. o you have a piece of wood that is l meters long, and they have n markings. o say the first mark is at 3 meters, the second mark is at 5 meters, so on, so forth. And 3, and 4, all the way up to mn. o we want to cut this piece of wood at all the markings. The thing is the woodchucker doesnt work for free. f you give it a piece of wood of length l, and you ask it to cut it at some marking, youre going to get two pieces of wood, length l1 and l2. The price for this is l1 times l2. o we like woodchucker, but woodchuckers would also like our wallets. o we want to cut this up by paying the minimum amount of money. Rings a bell? o ll let you guys think for a minute, then ll give you the running time, then well start talking. o we usually give you running times on quizzes. The running time is why you should know all the problems in their matching running times, because the moment we give you a running time you can automatically eliminate all the things that dont match, and just focus on a few things. o youre going to have to cut it at all the markings, eventually, but the order in which you cut is important. o if cut here first, then m going to pay three times l minus 3, whereas if cut in the middle first, m going to pay whatever this is, and 3 times l minus m3. o were trying to decide the order. Does this look like any familiar problem? ADENE: using dp, right? PROFEOR: dp, that is good. did say that were going to start with a dp problem, so this is dp. ts a good start. ADENE: PROFEOR: What? Not exactly. ADENE: Yeah. PROFEOR: o, it is not like any problems on the recitations. o far recitations did prefixes and suffixes. Were going to solve this using a running time of n cubed, which is like the parenthesis problem. t should be what you said, but dont know how to spell that, so were going to go for this instead. o running n cubed the moment said this you guys should know that this is the n cubed problem that we have in lecture notes. o make sure to have those on the cheat sheets, and try to understand them, right? OK, so given that ve said this, you should know the solution now. To make sure everyone is with me, were going to go through the solution, whole. o what is a subproblem? ADENE:maller piece of wood. PROFEOR: OK. ADENE: Like how to cut it up. PROFEOR: OK. o this is how you think of it informally. When you write it up, want to see this. want to see dp of something means something. o how you fill out your dp table. ts really useful to write this up on your exam before, because one, this will help you write the recursion correctly, and two, if the grader sees this they might skim over the recursion completely. And then you might have bugs there. We might not see them. Good for you. o this says how youre going to fill out the table. Right? dp of something equals something. Whats in a dp table? Numbers. ts never how to do something. ts always the numbers, so its always the maximum profit, or the minimum cost, or the shortest distance, or the longest something. o its always a number. o what we do here? ADENE: tart and dp, start location to the end location is PROFEOR: OK, so were going to get the mean distance, right? We usually do i j k and whatever else it takes. o start to end is? ADENE: The minimum cost of cutting that up. PROFEOR: inimum cost of cutting up the wood board from marking i, all the way to marking j. Theres a tiny problem here, that the initial theres no problem for this big piece of wood, right? f can only consider the board from i to j, so if can only consider the board from marking 1 to marking n, then get to this. o this part and this part get left out. ADENE: PROFEOR: Exactly. We add fake markings. Then 0 is 0, and mn plus 1 equals l. Very good. ADENE: equally spaced? PROFEOR: No. o these are numbers. f they were evenly spaced, think theres an algorithm. You might come up with a math and say, you always cut it up like this. o while we solve this, you guys have candy, right? You should eat the candy and be energetic and everything. o min cost of cutting up the board from marking i to marking j. like this. Have this on your exam if possible, because this will make our life easier, and its going to make your life easier when you get to the next step, which is how do we compute dp of i j? o suppose m looking at the subboard from m1 to m4 so m looking at only this. How do compute the best way to cut the board from m1 to m4? What are my options? ADENE: The locations you can cut it. PROFEOR: Exactly. o in order to cut this up, can either make a first cut at m2. o say make my first cut here, and then recursively cut this, and cut this. Or the other alternative is take the same guy m1, m2, m3, m4 cut it at m3, and then recursively cut this, and recursively cut this. o m iterating over all the markings inside the board. Now suppose m cutting it yes? ADENE: cutting both, or actually, never mind. PROFEOR: Yeah, when recursed, that takes care of it. o suppose m looking at m1 through m4, and m cutting it at m2. Whats the total cost? o whats the best way to cut, given that then know m going to cut there? ADENE: The sum of the dps. PROFEOR: OK, so its the best way to cut m1 through m2, plus best way to cut m2 through m4, plus the price m paying for this cut, right? Not just the sum of the dps. One more term. Whats this term? ADENE: 4 minus 1? Or the location of 4 minus the location of 1. PROFEOR: o, not quite, almost. o if m cutting a board into two pieces, the cost is the product of the length of the two pieces. m2 minus m1, times yes. OK, why did bother doing this? ome people think better with concrete numbers. f thats the case, then give yourself an example. Write some numbers on your sheet of paper, then see what letters match to what numbers, and copy it up using letters. And there you go, youve solved the problem. o where are i and j here? ADENE: i would be 1. PROFEOR: OK, so this is i. ADENE: Thats j. PROFEOR: ool, so lets try to write it up, now. o in order to cut the board from i to j, what am doing? o what am computing? sually the first word in your subproblem definition is the function that youre going to use. o its minimum, and m going iterate over something. ADENE: dp of i to it has to be all of j. dp of i, j, and youre looking to PROFEOR: o m computing dp of i j. ADENE: know, of j minus . ADENE: j minus i, then k j minus . PROFEOR: Theres a k, right? need a new variable for where m going to cut up, right? o fortunately, we have a lot of letters in the alphabet, i, j, k, so on and so forth, l, m. ADENE: i plus k. PROFEOR: o lets say that k is the place where we cut, to make our life easy. o m going to have dp of ADENE: Well i is the starting point. PROFEOR: OK ADENE: And then, the endpoint is i plus k, right? PROFEOR: o whats k here? ADENE: k is an actual number. ts not the offset, its the actual number, so it should be i to k. t depends how you define k. PROFEOR: o m going to make my life easy, and define k as exactly the marking at which cut. k is this 2 here. And this is easier, trust me. OK, plus? ADENE: k j? PROFEOR: OK, and? ADENE: ost of m ADENE: j minus i, m of k minus m of i times m of j minus m of k. PROFEOR: ool. Yeah, other way around doesnt matter. o now where does k go? We have to come up with numbers for the loop, right? ADENE: Between i and j. ADENE: j minus i. ADENE: Just for k in i to j. PROFEOR: o if have the board from 1 to 4, do cut at 1? can, but thats kind of weird. Because m recursing on the same subproblem. By the way, if you recurse to the same subproblem, what are you going to get as your running time? nfinite. o lets not do that. o were going to go from? ADENE: PROFEOR: o going from i would be bad. o i plus 1. 2? ADENE: j minus 1. PROFEOR: Very good. ADENE: Would it be m over i plus 1, because . PROFEOR: o k is which marking m cutting at. never want to cut inside a marking. However, dont even know these are integers. ADENE: They wouldnt be called . PROFEOR: o k is which marking, i, j, and k are which marking m cutting at. These are the only discrete things have. This board is all filled with real numbers. o if want to cut somewhere here, thats a real number dont like that. want to have integers. o my markings help me get integers. only want to cut at the marking, so always look at my problem in terms of which marking m cutting it. o this always iterates over markings. o this looks very much like the parentheses problem, right? ame subproblems, roughly the same recursion. Turns out that these problems, where youre not considering suffixes or prefixes, but rather youre considering substrings, are reasonably hard to come by, and reasonably hard to solve. o if we give these to you, chances are theyre going to be exactly like the parentheses problem, except for the cost function. This isnt what we had in the parentheses problem, right? o you should be prepared to solve problems that look exactly like the paren problem, but might have a different cost function. And this is how we solve it. OK. ADENE: When you say that the complexity determines which type of dp example we use, does that mean that a problem can be solved using any of dp examples? ts just that the only thing that changes is the complexity. PROFEOR: dont think you can map every approach onto every problem. For example, if you tried to map prefixes onto this, youd come up with a solution that doesnt look at all the possible choices, so your answer would be suboptimal. o youd come up with a fast, but incorrect algorithm. However, if you take the problem of find the longest increasing subsequence, you can definitely apply this technique to it. ts more general than suffixes or prefixes. o its going to work, but its going to be slower. o in theory, what you should do is, you have all these techniques. Given a problem, you try all the techniques. You see which ones apply, and out of those, you see which one gives you the best running time. n practice, if we give you the running time, you match it to the techniques that match the running time. You start backwards from the stuff that you know. OK. Does this problem make sense? weet. Now lets do a hard problem. Do people remember hashing? You have one minute to remember hashing while erase the board. o suppose we want to implement the set. The way were going to implement the set is, we have n elements. Were going to put them into the set, so for i goes from 1 through n, were going to insert element i, so first were going to insert all the elements into the set. And then after that, given a random number, we want to see is it in the set, or not. o for some other number used n before, so lets use for some other number f, we want to see is f in the set, or is f not in the set? What data structure would you use normally for this? A hash table, right? You stick everything into a hash table, then you try to find the elements. f you find them, then you say yes. f not, then you say no. Well, it turns out that this would take more memory than what we have. o instead, were going to do this. Were going to have a hash table of m bits. o these are m bits. And say we have a hash function that satisfies with uniform hashing, so given any element, the value is anywhere from 0 to m minus 1, and theyre all independent. o the way were going to insert an element is this table is T were going to say that T of h of ai equals 1. o this is a table of bits. For every element we hash the element, and we set the corresponding bit to 1. o were going to have some 1s, and some zeros in the table. ay if this is ai, it hashes somewhere here. OK so the question is, we inserted n elements into a table of size n. Given a new element, f, where f stands for false positive f is not one of the elements that we inserted. want to know whats the probability that the set will say that the element is in the set, so basically, the probability of a false positive. ADENE: o what are we doing about ? PROFEOR: Nothing. ADENE: s it chaining, is it open addressing? Does it even matter? PROFEOR: o were not inserting the elements into the table. This table only has bits. The elements are lost completely after we insert them. o the tradeoff is uses a lot less memory. nstead of having to store entire elements, you just store bits. On the downside youre going to have false positives. Because if have a different element, say f, if it hashes to the same location, then the set is going to say, yeah, its in the set. o you get false positives. Would you get false negatives? No, right? Because you start out with a table of 0s, and you only set the table to ones for the numbers that match to hashes of elements that are in the set. Did you have a question? OK. OK, do we understand the problem, before we attempt to solve it? ADENE: s it probably 1/m? PROFEOR: Youd wish, but no. ADENE: ts less than n/m. PROFEOR: OK, like that. o what are you thinking? ADENE: f there are no collisions previously, then it would equal to n/m, but there are collisions, probably collisions. PROFEOR: OK, m going to open up a window in your head and tell everyone else the small steps you took to get here. o we have this new number f. How are we going to check if its in the set or not? Were going to compute h of f, and were going to check if t of h of f is 0 or 1. f is different from all the other elements. o its hash value is independent from all the other hash values we had before. We dont really care about this anymore, after we have the independence assumption. o h of f is just some random position in the table. o the question is, given some random position in the table, will that be a 0 or a 1? How do you know? f knew how many 1s have in the table if have k 1s in the table, and automatically this means n minus k 0s then whats the probability that h of f will point to a 1? ADENE: k/m. PROFEOR: Yes. o the hash takes m possible values. k of them are 1s. o the probability that the hash is going to guess a 1 is k/m. o if we knew how many 1s we have, then this is the answer. We know that were going to have at most n 1s thats what youre thinking, right? o k is definitely smaller or equal to n, so the answer definitely has to be smaller or equal than n/m. Now if youre in a rush, you might say, well, we inserted n elements, so were definitely going to have n 1s here. That is not true. The hashes of all the elements are independent. o there is some probability that two elements will hash to the same value, and as the number of elements grows, that probability also grows. OK, so now by looking at this, we got rid of this part of the problem. We dont care that theres a new element. We dont care that its a false positive. All that we care about is how many 1s do we have in the table after inserting n values. Well, whats that? Thats m times the probability that a slot in the table is 1. Right, the probability that the slot in the table is 1 is k/m. o if we know this probability, and we multiply it by m, then we get k. People still with me? ADENE: And what does that variable represent, h? PROFEOR: This is k. Represents that my handwriting sucks, basically. ADENE: mean, why do we do m times the probability. Thats the the expected number of 1s in the table? PROFEOR: Yeah. Yeah, this is E of k, guess. o then our final answer is this thing divided by m. o the answer is the expected value of k, or you can just think of it as the average value of k, divided by m. o this is m times this probability, divided by m. o it is exactly this probability. o the thing that we want to focus on is, whats the probability that a random slot in the table is a 1? ADENE: ts equal to 1 minus the probability that it was never fixed. PROFEOR: Exactly, the first thing we do. 1 minus the probability that a slot is 0. This is easy, right, like it looks easy. But this makes a huge difference, because once were here, well, a slot is zero if none of the insertions made it a one. And the insertions are all independent. o this is like, youre flipping a coin. Whats the probability that after you flip it n times, you never get a head? o this is 1 minus ADENE: 1 over m to the something. PROFEOR: That o a slot is 0 means that no number was inserted in it. Were inserting n numbers, so its the probability that a single number was not necessarily in the slot, raised to the power of n. o we have n independent experiments, right? Every time you insert a number into the hash function, thats one experiment. The hash function gives you independent values for all the elements. o all the insertions are independent of each other. f, in a single insertion, youve hit that slot, then youve made it a 1 game over. o the slot is only a zero if none of the insertions make it the 1. o you take the probability that the insertion doesnt make it a one, and you raise it to the power n, because that has to happen n times in order for the whole thing to be successful. And the probability that the number was not inserted in a slot is 1 minus the probability that it was inserted. Right, were doing this again. 1 minus probability that a number hit. Well what this probability? niform hashing. ADENE: 1/m PROFEOR: 1/m. o this whole thing is 1 minus 1 minus 1, over m to the power n. 1 minus m minus 1, over m to the power n. ADENE: an we go through this again. From 1 minus probability of a slot is 0, to 1 minus probability of a number was not inserted in a slot? PROFEOR: OK. o first off, the point of the problem. ts our problem, right? Dont panic, dont be angry. Youre not going to have some this hard on the exam. The point of this is, want to go through probabilities a little bit, and want to go through hashing and the math behind hashing. Because remembering that will be useful. OK, so now you said youre having trouble with this step? OK, so lets see. Lets do this here. o we have this table here, right? And we have n elements e1, e2, e3, all the way through en. How do we put them in the table? We hash each of them, and each of them maps to a random slot in the table. f these are the slots, then e1 might map here, e2 might map here, e3 might map here, e4 might map here, so on and so forth. o have arrows, right? Every time do a hash, thats going to set something to a 1. The numbers dont necessarily map to different slots, because each number, on its own, maps to a random slot. o these are all going to be ones. And everything else becomes zero. f no number maps to a slot, it is 0. OK, lets look at one slot, any slot. o lets say m looking at this slot over here. an you guys see, by the way? OK, so lets look at this guy here. Whats the probability that its a 0? o the probability that the slot is a 0 is the probability that the first number didnt map to it otherwise it would be a 1 e1 didnt hash to that slot. e2 also couldnt match to that slot, right? o its the probability that e1 didnt hash to the slot, and e2 didnt hash into slot, and e3 didnt hash into the slot, so on so forth, right? All the way up until en didnt hash to the slot. This makes sense? Now these are all independent events, because all the hashes are independent, by the uniform hashing assumption. o then can turn ands into products. o can say that this equals to the probability that e1 didnt hash into the slot, times the probability that e2 didnt hash into the slot, times the probability that e3 didnt hash into the slot, so on and so forth, all the way to the probability that en didnt hash. o since m dealing with the same hash function, turns out that all the probabilities are the same. o there, the probability that some fixed number didnt hash, to the power n. o this is how got from here to here. Probabilities and the properties of hashes and hashing assumptions. o you guys should have those on your cheat sheet, and maybe if you have time, review probabilities a bit. ADENE: What is the probability that any given one doesnt hash, 1/m? o if e1 doesnt hash in that spot, isnt that probability 1/m? PROFEOR: Not quite. Youre close, but not quite. o youre saying that the probability that e1 doesnt hash to this slot is 1/m? ADENE: guess its 1 minus 1/m. PROFEOR: Exactly. The probability that it would hash here is 1/m, because it has to pick that one slot out of n possible slots. But if youre just saying, all want is that it doesnt hash here, well, it means it can hash anywhere else. o it has m minus 1 options. t can go to any of those m minus 1 places, just not to that one place. o m minus 1 over m. ADENE: ts interesting it went the other direction. nstead of saying, its 1, its 1 minus it. Wouldnt it have been just as easy to go the other direction, or no? PROFEOR: No. Not doing this makes the problem hard, so thats why were doing it. This kind of flipping is easy to do conceptually, but it might make a hard problem into a really easy problem, or at least into a doable problem. ADENE: snt it this the same thing? guess maybe not totally. PROFEOR: o it is exactly the same in terms of the math, but computing this without turning it into this is really hard. ADENE: Any given slot is 1, isnt it kind of like what we just said, except if the probability of any one mapping is 1/m, mapping to a 1, right? And then you take 1 over m raised to the n, thats the probability of it being a 1 at that one place, right? PROFEOR: No, not quite. Yeah. OK, so are we getting this? omewhat? Yes? ADENE: o the probability of a false positive, youre saying thats whats the probability that you get the 1, if you actually should the 0. ts because multiple things mapped to that one slot, right? PROFEOR: o the probability of a false positive is the probability that, given a new element, when we hash it we get the 1. The hash of that new element is independent of all the other hashes. ADENE: Then why is it simple in probability that you get the 1? PROFEOR: o if have a new element, m going to compute its hash, and m going to look in the table. f see a 1, m going to say, oh. ADENE: Oh, its a new element. OK. PROFEOR: Yeah, so its something that was not in the set. ADENE: OK. PROFEOR: Okay, cool. OK, so lets see if we can do a harder version of this. o this probability isnt great, but if we do one trick, we can make this really nice. And this puts together a trick called bloom filters that is used in all sorts of situations. o for Bloom filters, we still have n elements, and we still have a table of m bits. What changes this time is instead of having one function, we have k hash functions. o when we take an element and insert it, were taking element i. The way to insert it is were going to compute its hash value using all the hash functions, and set all the corresponding bits to 1. o insert ei becomes, for j in 1 through k, the table bit corresponding to the hash function, j, of the element is 1. o each element sets k bits to 1. Now how do we check if an element is in the table? ADENE: PROFEOR: ince, for every element, we set all the corresponding k bits to 1, now when we have a new element, were going to compute to the k positions, and if any of them is a 0, then we couldnt have possibly put that in the table. o all T of h j of f have to be 1. o for every element, we hashed it k times, and set the corresponding bits. f we have a new element, and by hashing we get here and here, but we also get here, and this guy was a zero, we know we definitely didnt put this in. o now whats the probability of a false positive? ADENE: y first intuition is just raising that to a power. ADENE: The probability that when you check PROFEOR: Oh, forgot to say something, by the way. The k hash functions think they satisfy simple uniform hashing. m not sure if thats the right thing, but they all have independent values from each other. o theyre all independent. o for any number you give, any hash function returns a value thats independent of all the other hash functions, and theyre all 0 through n minus 1. ADENE: Why is not that just raised to something? Because we know the probability OK, actually we need to recalculate that. ADENE: Because its the probability that all of them are 1, even though you havent hashed yet. PROFEOR: o the false positive, the probability of false positives is the probability that all the k slots that correspond to f are 1s, right? o, since the hash functions are all independent, this is the probability that one slot is the 1, raised to the power k. Right, because theyre all independent slots. o its the probability that one slot is a 1, raised to the power k. OK, so now whats the probability that one slot is a 1? t looks a lot like this problem, right? Except theres a tweak. How many times did we put the 1 in the table? o here, we put a 1 in the table for every element. o we have n sets, right? o n times were going to set t of something to 1. Right? For every element, we have one set. We set one bit to 1. t might have been said before thats something else. Yes? ADENE: o here its raised to the m k? PROFEOR: Yeah, pretty much. o here, for every element we hash it through all the k functions, and set the corresponding bits to 1. o one element generates k set operations, and we have n elements, so we set n k bits to 1. Does this make sense? ADENE: an two hash functions point to the same slot? PROFEOR: ure. But theyre all independent, and thats the only thing that matters. o every time we set the bit, which bit was set is independent of all the other bits we set, because all the hash functions are independent, and all the values are independent of each other. o this time, the table size is still m, so that didnt change. This time we set n bits to 1, this time we set n k bits to 1. o then the right thing to do is copy this answer, and replace n with n k. And if you have to write the proof, youd copypaste the proof and replace n with n k. o this is 1 minus m minus 1, over m, times n k. And of course you should go through the whole thing in your head and convince yourselves that this is true. ADENE: Does that say one of the elements is what? k, something? ADENE: ets. PROFEOR: Bit sets. o one element sets k bits in the table, not necessarily different bits, just independent bits. o if you have n elements altogether, they set n times k bits. This thing gets run n times k times, whereas here, the set operation gets run n times in total. Thats the difference in the two problems. Right here you have one function for each element, here you have k hash functions. This is hard, right? Well, its the hardest hashing problem that could think about and that makes us go through probabilities and through all the hash stuff. The problems on the exam will be easier, so one, dont panic. Two, review hashing, review probabilities. When said, from the theory, this is what you get, if you didnt understand that then please review the theory. ADENE: Why is it raised to the k? Because we did down there, if we replace n with n k, then wed just get everything except. PROFEOR: o this thing in here is the answer to the previous problem, except you take an n and you replace it with an n k. o this is the probability that one bit is set to 1. But here, when youre given an element, youre going to hash it through the k functions you take this guy youre going to hash it through the k functions, and youre going to check all the bits. o youre going to check k bits. o as long as any of the k bits is a zero, not a false positive. o we need all the k bits to be a 1. ADENE: Oh, see. ADENE: What if the hash functions are dependent? PROFEOR: Then become intractable. ADENE: And what if they are? think the in this problem, the way they are being hashed, that becomes dependent, because think there were some problems where, if something is being hashed somewhere, then the probability there could be hash functions that would put the other thing in the next slot. PROFEOR: Yes, so you want to reduce these problems to independent hashing. f you look at the proofs, all the proofs assume uniform hashing, simple uniform, whatever it takes to get the math down to independence. Because this is the only thing that we know how to solve with probabilities. f everything is independent, then things multiply and add up in the right places, and everything is easy. f things are dependent, then proofs become really, really hard. o whenever you have dependent things, you want to find a way to reduce that to independent things. s everyone tired, or do you guys really not like this problem? By the way, really cool trick so this turns out to be a lot better than that, and think the optimal value of k is around square roots of log n. And that gives you some filters with a really low false positive rate. ADENE: What do you mean by optimal? PROFEOR: inimize the false positives. o given n and m, pick a case so that this thing is minimized. ADENE: What was the answer again? Or actually, regardless of that, whats the percentage of false positives? PROFEOR: t depends on what your n and m are, right? The more bits you can afford ADENE: But if maximize your k, you said you came up with some k thats maximized PROFEOR: think k is ADENE: quare root of log n. ADENE: o then if you use that. PROFEOR: Lets not do the math. ts really, really good. o these are used for all sorts of practical problems, all the way from branch predictors in processors, to databases. ADENE: o is it better than 1%? Do you know that, at least? PROFEOR: Oh yeah, for practical uses, this gets you, think to 1% of 1% of 1%. o usually, put a Bloom filter before a really expensive check, and the Bloom filter gets rid of most of the false positives. And then you have a few more where you do the more expensive check. Okay, does this make sense? Any questions? ADENE: Do you more optimal if you repeated this Bloom filter independently of the other one, with more hash functions in that memory structure? PROFEOR: think doubling the memory size is better. o two filters is the same as having two n bits. think doubling gives you better results, always. OK, so general stuff. Were going to have a lot of conceptual questions, so please make sure, again, make sure that for everything that we did, go through the problem. nderstand the problem, know that there is a solution. Know the running time, maybe know how to implement the solution. Dont worry so much about the proof. Were going to have some problems where you have to come up with new things on your own, so get a good nights sleep before the exam. Really, if you have five hours left, then you have to choose between sleeping five hours or reading notes for five hours ADENE: Drink caffeine. PROFEOR: ts not going to help, so caffeine actually helps you stay up, but it decreases your performance. And so if youre on caffeine, youre not going to think. You can regurgitate stuff, but you cant think. o caffeinating yourself is a ADENE: thought it was like it gives you concentration. PROFEOR: o theres an optimum amount of sleep and caffeine combination. f you dont sleep and caffeinate yourself, guarantee that you will not solve any of the problems that require new algorithms. ADENE: affeine just squirts adrenaline in your brain. t doesnt do anything else. PROFEOR: o the thing is the memory is going to be better. f all youre doing is memorization stuff, then its going to be better. o youre going to do well on the pattern matching stuff. But when your brain is panicking, youre not going to come up with new solutions, right? sually, you have a problem, a hard problem. Youre thinking about it, and then at some point when youre relaxed, like when youre in the shower or when you wake up youre like, crap, found a solution. o the brain finds solutions when its relaxed, not when its like, holy shit, holy shit, holy shit. And adrenaline gets it in that mood. Thats what it does. And thats what caffeine does in the end. o a little bit of caffeine might help you get up and get you running, but dont caffeinate yourself to not sleep the entire night. Thats probably going to make you bomb the hard questions. Good luck on Friday. Eat candy.","PROFEOR: o this thing in here is the answer to the previous problem, except you take an n and you replace it with an n k. o this is the probability that one bit is set to 1. ADENE: PROFEOR: ince, for every element, we set all the corresponding k bits to 1, now when we have a new element, were going to compute to the k positions, and if any of them is a 0, then we couldnt have possibly put that in the table. This time we set n bits to 1, this time we set n k bits to 1. o then the right thing to do is copy this answer, and replace n with n k. And if you have to write the proof, youd copypaste the proof and replace n with n k. o this is 1 minus m minus 1, over m, times n k. And of course you should go through the whole thing in your head and convince yourselves that this is true. Right, the probability that the slot in the table is 1 is k/m. o the slot is only a zero if none of the insertions make it the 1. o you take the probability that the insertion doesnt make it a one, and you raise it to the power n, because that has to happen n times in order for the whole thing to be successful. PROFEOR: o the probability of a false positive is the probability that, given a new element, when we hash it we get the 1. ADENE: Between i and j. ADENE: j minus i. ADENE: Just for k in i to j. PROFEOR: o if have the board from 1 to 4, do cut at 1? ADENE: o the probability of a false positive, youre saying thats whats the probability that you get the 1, if you actually should the 0. ts because multiple things mapped to that one slot, right? o the thing that we want to focus on is, whats the probability that a random slot in the table is a 1? want to know whats the probability that the set will say that the element is in the set, so basically, the probability of a false positive. o the probability that the hash is going to guess a 1 is k/m. Because you start out with a table of 0s, and you only set the table to ones for the numbers that match to hashes of elements that are in the set. o when we take an element and insert it, were taking element i. The way to insert it is were going to compute its hash value using all the hash functions, and set all the corresponding bits to 1. o insert ei becomes, for j in 1 through k, the table bit corresponding to the hash function, j, of the element is 1. o each element sets k bits to 1. o, since the hash functions are all independent, this is the probability that one slot is the 1, raised to the power k. Right, because theyre all independent slots. o its the probability that one slot is a 1, raised to the power k. OK, so now whats the probability that one slot is a 1? PROFEOR: o the false positive, the probability of false positives is the probability that all the k slots that correspond to f are 1s, right? o the way were going to insert an element is this table is T were going to say that T of h of ai equals 1. o this is a table of bits. o the probability that the slot is a 0 is the probability that the first number didnt map to it otherwise it would be a 1 e1 didnt hash to that slot. o we have this new number f. How are we going to check if its in the set or not? o this is 1 minus ADENE: 1 over m to the something. PROFEOR: o if have a new element, m going to compute its hash, and m going to look in the table. Were inserting n numbers, so its the probability that a single number was not necessarily in the slot, raised to the power of n. o we have n independent experiments, right? And then you take 1 over m raised to the n, thats the probability of it being a 1 at that one place, right? For every element we hash the element, and we set the corresponding bit to 1. o were going to have some 1s, and some zeros in the table. Thats m times the probability that a slot in the table is 1. o can say that this equals to the probability that e1 didnt hash into the slot, times the probability that e2 didnt hash into the slot, times the probability that e3 didnt hash into the slot, so on and so forth, all the way to the probability that en didnt hash. o fortunately, we have a lot of letters in the alphabet, i, j, k, so on and so forth, l, m. ADENE: i plus k. PROFEOR: o lets say that k is the place where we cut, to make our life easy. Were going to compute h of f, and were going to check if t of h of f is 0 or 1. f is different from all the other elements. Because if have a different element, say f, if it hashes to the same location, then the set is going to say, yeah, its in the set. And then after that, given a random number, we want to see is it in the set, or not. But here, when youre given an element, youre going to hash it through the k functions you take this guy youre going to hash it through the k functions, and youre going to check all the bits. PROFEOR: OK ADENE: And then, the endpoint is i plus k, right? ADENE: Then why is it simple in probability that you get the 1? ADENE: Does that say one of the elements is what? PROFEOR: o the thing is the memory is going to be better. The way were going to implement the set is, we have n elements. ADENE: dp of i to it has to be all of j. dp of i, j, and youre looking to PROFEOR: o m computing dp of i j. ADENE: know, of j minus . o here, for every element we hash it through all the k functions, and set the corresponding bits to 1. o one element generates k set operations, and we have n elements, so we set n k bits to 1. o all T of h j of f have to be 1. o for every element, we hashed it k times, and set the corresponding bits. f knew how many 1s have in the table if have k 1s in the table, and automatically this means n minus k 0s then whats the probability that h of f will point to a 1? o youre going to have to cut it at all the markings, eventually, but the order in which you cut is important. PROFEOR: OK, so this is i. ADENE: Thats j. PROFEOR: ool, so lets try to write it up, now. The hash of that new element is independent of all the other hashes. ADENE: The probability that when you check PROFEOR: Oh, forgot to say something, by the way. o every time we set the bit, which bit was set is independent of all the other bits we set, because all the hash functions are independent, and all the values are independent of each other. And the probability that the number was not inserted in a slot is 1 minus the probability that it was inserted. ADENE: tart and dp, start location to the end location is PROFEOR: OK, so were going to get the mean distance, right? o youre saying that the probability that e1 doesnt hash to this slot is 1/m? 1 minus the probability that a slot is 0. And say we have a hash function that satisfies with uniform hashing, so given any element, the value is anywhere from 0 to m minus 1, and theyre all independent. The probability that it would hash here is 1/m, because it has to pick that one slot out of n possible slots. The more bits you can afford ADENE: But if maximize your k, you said you came up with some k thats maximized PROFEOR: think k is ADENE: quare root of log n. ADENE: o then if you use that. think the in this problem, the way they are being hashed, that becomes dependent, because think there were some problems where, if something is being hashed somewhere, then the probability there could be hash functions that would put the other thing in the next slot. OK so the question is, we inserted n elements into a table of size n. Given a new element, f, where f stands for false positive f is not one of the elements that we inserted. o there, the probability that some fixed number didnt hash, to the power n. o this is how got from here to here. ts not the offset, its the actual number, so it should be i to k. t depends how you define k. PROFEOR: o m going to make my life easy, and define k as exactly the marking at which cut. ADENE: What is the probability that any given one doesnt hash, 1/m? PROFEOR: o it is exactly the same in terms of the math, but computing this without turning it into this is really hard. o if we give these to you, chances are theyre going to be exactly like the parentheses problem, except for the cost function. By the way, if you recurse to the same subproblem, what are you going to get as your running time? PROFEOR: Yeah, so its something that was not in the set. ADENE: Any given slot is 1, isnt it kind of like what we just said, except if the probability of any one mapping is 1/m, mapping to a 1, right? ADENE: Why is it raised to the k? Because this is the only thing that we know how to solve with probabilities. PROFEOR: inimum cost of cutting up the wood board from marking i, all the way to marking j. Theres a tiny problem here, that the initial theres no problem for this big piece of wood, right? t should be what you said, but dont know how to spell that, so were going to go for this instead. Have this on your exam if possible, because this will make our life easier, and its going to make your life easier when you get to the next step, which is how do we compute dp of i j? o m going to have dp of ADENE: Well i is the starting point. o then our final answer is this thing divided by m. o the answer is the expected value of k, or you can just think of it as the average value of k, divided by m. o this is m times this probability, divided by m. o it is exactly this probability. And at least be able to know, for every one of them, whats the name, what it does, and wants the running time.",0.2043929225137278
